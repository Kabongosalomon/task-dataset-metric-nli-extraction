<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EXPLAINABLE DEEP ONE-CLASS CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Liznerski</surname></persName>
							<email>liznerski@cs.uni-kl.de</email>
							<affiliation key="aff0">
								<orgName type="department">ML group</orgName>
								<orgName type="institution">Technical University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
							<email>lukas.ruff@tu-berlin.de</email>
							<affiliation key="aff1">
								<orgName type="department">ML group</orgName>
								<orgName type="institution">Technical University of Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
							<email>vandermeulen@tu-berlin.de</email>
							<affiliation key="aff1">
								<orgName type="department">ML group</orgName>
								<orgName type="institution">Technical University of Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><forename type="middle">Joe</forename><surname>Franks</surname></persName>
							<email>franks@cs.uni-kl.de</email>
							<affiliation key="aff0">
								<orgName type="department">ML group</orgName>
								<orgName type="institution">Technical University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
							<email>kloft@cs.uni-kl.de</email>
							<affiliation key="aff0">
								<orgName type="department">ML group</orgName>
								<orgName type="institution">Technical University of Kaiserslautern</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
							<email>klaus-robert.mueller@tu-berlin.de</email>
							<affiliation key="aff1">
								<orgName type="department">ML group</orgName>
								<orgName type="institution">Technical University of Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
								<orgName type="institution">Brain Team</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EXPLAINABLE DEEP ONE-CLASS CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep one-class classification variants for anomaly detection learn a mapping that concentrates nominal samples in feature space causing anomalies to be mapped away. Because this transformation is highly non-linear, finding interpretations poses a significant challenge. In this paper we present an explainable deep one-class classification method, Fully Convolutional Data Description (FCDD), where the mapped samples are themselves also an explanation heatmap. FCDD yields competitive detection performance and provides reasonable explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet. On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps, FCDD sets a new state of the art in the unsupervised setting. Our method can incorporate ground-truth anomaly explanations during training and using even a few of these (∼ 5) improves performance significantly. Finally, using FCDD's explanations, we demonstrate the vulnerability of deep one-class classification models to spurious image features such as image watermarks. 1 * equal contribution 1 Our code is available at:</p><p>We show that FCDD's anomaly detection performance is close to the state of the art on the standard AD benchmarks with CIFAR-10 and ImageNet while providing transparent explanations. On MVTec-AD, an AD dataset containing ground-truth anomaly maps, we demonstrate the accuracy of FCDD's explanations (see <ref type="figure">Figure 1)</ref>, where FCDD sets a new state of the art. In further experiments we find that deep one-class classification models (e.g. DSVDD) are prone to the "Clever Hans" effect <ref type="bibr" target="#b26">(Lapuschkin et al., 2019)</ref> where a detector fixates on spurious features such as image watermarks. In general, we find that the generated anomaly heatmaps are less noisy and provide more structure than the baselines, including gradient-based methods <ref type="bibr" target="#b50">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b52">Sundararajan et al., 2017)</ref> and autoencoders <ref type="bibr" target="#b46">(Sakurada and Yairi, 2014;</ref><ref type="bibr" target="#b3">Bergmann et al., 2019)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Anomaly detection (AD) is the task of identifying anomalies in a corpus of data <ref type="bibr" target="#b11">(Edgeworth, 1887;</ref><ref type="bibr" target="#b1">Barnett and Lewis, 1994;</ref><ref type="bibr" target="#b7">Chandola et al., 2009;</ref><ref type="bibr">Ruff et al., 2021)</ref>. Powerful new anomaly detectors based on deep learning have made AD more effective and scalable to large, complex datasets such as high-resolution images <ref type="bibr" target="#b40">(Ruff et al., 2018;</ref><ref type="bibr" target="#b3">Bergmann et al., 2019)</ref>. While there exists much recent work on deep AD, there is limited work on making such techniques explainable. Explanations are needed in industrial applications to meet safety and security requirements <ref type="bibr" target="#b4">(Berkenkamp et al., 2017;</ref><ref type="bibr" target="#b21">Katz et al., 2017;</ref><ref type="bibr" target="#b47">Samek et al., 2020)</ref>, avoid unfair social biases <ref type="bibr" target="#b15">(Gupta et al., 2018)</ref>, and support human experts in decision making <ref type="bibr" target="#b20">(Jarrahi, 2018;</ref><ref type="bibr" target="#b33">Montavon et al., 2018;</ref><ref type="bibr" target="#b47">Samek et al., 2020)</ref>. One typically makes anomaly detection explainable by annotating pixels with an anomaly score and, in some applications, such as finding tumors in cancer detection <ref type="bibr" target="#b37">(Quellec et al., 2016)</ref>, these annotations are the primary goal of the detector.</p><p>One approach to deep AD, known as Deep Support Vector Data Description (DSVDD) <ref type="bibr" target="#b40">(Ruff et al., 2018)</ref>, is based on finding a neural network that transforms data such that nominal data is concentrated to a predetermined center and anomalous data lies elsewhere. In this paper we present Fully Convolutional Data Description (FCDD), a modification of DSVDD so that the transformed samples are themselves an image corresponding to a downsampled anomaly heatmap. The pixels in this heatmap that are far from the center correspond to anomalous regions in the input image. FCDD does this by only using convolutional and pooling layers, thereby limiting the receptive field of each output pixel. Our method is based on the one-class classification paradigm <ref type="bibr" target="#b34">(Moya et al., 1993;</ref><ref type="bibr" target="#b54">Tax, 2001;</ref><ref type="bibr" target="#b55">Tax and Duin, 2004;</ref><ref type="bibr" target="#b40">Ruff et al., 2018)</ref>, which is able to naturally incorporate known anomalies <ref type="bibr">Ruff et al. (2021)</ref>, but is also effective when simply using synthetic anomalies.  <ref type="bibr" target="#b3">(Bergmann et al., 2019)</ref>. Rows from top to bottom show: (1) nominal samples (2) anomalous samples (3) FCDD anomaly heatmaps (4) ground-truth anomaly maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Here we outline related works on deep AD focusing on explanation approaches. Classically deep AD used autoencoders <ref type="bibr" target="#b16">(Hawkins et al., 2002;</ref><ref type="bibr" target="#b46">Sakurada and Yairi, 2014;</ref><ref type="bibr" target="#b60">Zhou and Paffenroth, 2017;</ref><ref type="bibr" target="#b59">Zhao et al., 2017)</ref>. Trained on a nominal dataset autoencoders are assumed to reconstruct anomalous samples poorly. Thus, the reconstruction error can be used as an anomaly score and the pixel-wise difference as an explanation <ref type="bibr" target="#b3">(Bergmann et al., 2019)</ref>, thereby naturally providing an anomaly heatmap. Recent works have incorporated attention into reconstruction models that can be used as explanations <ref type="bibr" target="#b57">(Venkataramanan et al., 2019;</ref>. In the domain of videos, <ref type="bibr" target="#b45">Sabokrou et al. (2018)</ref> used a pre-trained fully convolutional architecture in combination with a sparse autoencoder to extract 2D features and provide bounding boxes for anomaly localization. One drawback of reconstruction methods is that they offer no natural way to incorporate known anomalies during training.</p><p>More recently, one-class classification methods for deep AD have been proposed. These methods attempt to separate nominal samples from anomalies in an unsupervised manner by concentrating nominal data in feature space while mapping anomalies to distant locations <ref type="bibr" target="#b40">(Ruff et al., 2018;</ref><ref type="bibr" target="#b6">Chalapathy et al., 2018;</ref><ref type="bibr" target="#b14">Goyal et al., 2020)</ref>. In the domain of NLP, DSVDD has been successfully applied to text, which yields a form of interpretation using attention mechanisms <ref type="bibr" target="#b41">(Ruff et al., 2019)</ref>. For images, <ref type="bibr" target="#b22">Kauffmann et al. (2020)</ref> have used a deep Taylor decomposition <ref type="bibr" target="#b32">(Montavon et al., 2017)</ref> to derive relevance scores.</p><p>Some of the best performing deep AD methods are based on self-supervision. These methods transform nominal samples, train a network to predict which transformation was used on the input, and provide an anomaly score via the confidence of the prediction <ref type="bibr" target="#b13">(Golan and El-Yaniv, 2018;</ref><ref type="bibr" target="#b18">Hendrycks et al., 2019b)</ref>. <ref type="bibr" target="#b17">Hendrycks et al. (2019a)</ref> have extended this to incorporate known anomalies as well. No explanation approaches have been considered for these methods so far.</p><p>Finally, there exists a great variety of explanation methods in general, for example model-agnostic methods (e.g. LIME <ref type="bibr" target="#b38">(Ribeiro et al., 2016)</ref>) or gradient-based techniques <ref type="bibr" target="#b50">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b52">Sundararajan et al., 2017)</ref>. Relating to our work, we note that fully convolutional architectures have been used for supervised segmentation tasks where target segmentation maps are required during training <ref type="bibr" target="#b30">(Long et al., 2015;</ref><ref type="bibr" target="#b36">Noh et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPLAINING DEEP ONE-CLASS CLASSIFICATION</head><p>We review one-class classification and fully convolutional architectures before presenting our method.</p><p>Deep One-Class Classification Deep one-class classification <ref type="bibr" target="#b40">(Ruff et al., 2018;</ref><ref type="bibr" target="#b43">2020b)</ref> performs anomaly detection by learning a neural network to map nominal samples near a center c in output space, causing anomalies to be mapped away. For our method we use a Hypersphere Classifier (HSC) <ref type="bibr" target="#b42">(Ruff et al., 2020a)</ref>, a recently proposed modification of Deep SAD <ref type="bibr" target="#b43">(Ruff et al., 2020b)</ref>, a semi-supervised version of DSVDD <ref type="bibr" target="#b40">(Ruff et al., 2018)</ref>. Let X 1 , . . . , X n denote a collection of samples and y 1 , . . . , y n be labels where y i = 1 denotes an anomaly and y i = 0 denotes a nominal sample. Then the HSC objective is</p><formula xml:id="formula_0">min W,c 1 n n i=1 (1 − y i )h(φ(X i ; W) − c) − y i log (1 − exp (−h(φ(X i ; W) − c))) ,<label>(1)</label></formula><p>where c ∈ R d is the center, and φ : R c×h×w → R d a neural network with weights W. Here h is the pseudo-Huber loss <ref type="bibr" target="#b19">(Huber et al., 1964)</ref>, h(a) = a 2 2 + 1 − 1, which is a robust loss that interpolates from quadratic to linear penalization. The HSC loss encourages φ to map nominal samples near c and anomalous samples away from the center c. In our implementation, the center c corresponds to the bias term in the last layer of our networks, i.e. is included in the network φ, which is why we omit c in the FCDD objective below.</p><p>Figure 2: Visualization of a 3×3 convolution followed by a 3×3 transposed convolution with a Gaussian kernel, both using a stride of 2.</p><p>Fully Convolutional Architecture Our method uses a fully convolutional network (FCN) <ref type="bibr" target="#b30">(Long et al., 2015;</ref><ref type="bibr" target="#b36">Noh et al., 2015)</ref> that maps an image to a matrix of features, i.e. φ : R c×h×w → R 1×u×v by using alternating convolutional and pooling layers only, and does not contain any fully connected layers. In this context, pooling can be seen as a special kind of convolution with fixed parameters.</p><p>A core property of a convolutional layer is that each pixel of its output only depends on a small region of its input, known as the output pixel's receptive field. Since the output of a convolution is produced by moving a filter over the input image, each output pixel has the same relative position as its associated receptive field in the input. For instance, the lower-left corner of the output representation has a corresponding receptive field in the lower-left corner of the input image, etc. (see <ref type="figure">Figure 2</ref> left side). The outcome of several stacked convolutions also has receptive fields of limited size and consistent relative position, though their size grows with the amount of layers. Because of this an FCN preserves spatial information.</p><p>Fully Convolutional Data Description Here we introduce our novel explainable AD method Fully Convolutional Data Description (FCDD). By taking advantage of FCNs along with the HSC above, we propose a deep one-class method where the output features preserve spatial information and also serve as a downsampled anomaly heatmap. For situations where one would like to have a full-resolution heatmap, we include a methodology for upsampling the low-resolution heatmap based on properties of receptive fields.  FCDD is trained using samples that are labeled as nominal or anomalous. As before, let X 1 , . . . , X n denote a collection of samples with labels y 1 , . . . , y n where y i = 1 denotes an anomaly and y i = 0 denotes a nominal sample. Anomalous samples can simply be a collection of random images which are not from the nominal collection, e.g. one of the many large collections of images which are freely available like 80 Million Tiny Images <ref type="bibr" target="#b56">(Torralba et al., 2008)</ref> or ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009</ref>). The use of such an auxiliary corpus has been recommended in recent works on deep AD, where it is termed Outlier Exposure (OE) <ref type="bibr" target="#b17">(Hendrycks et al., 2019a;</ref>. When one has access to "true" examples of the anomalous dataset, i.e. something that is likely to be representative of what will be seen at test time, we find that even using a few examples as the corpus of labeled anomalies performs exceptionally well. Furthermore, in the absence of any sort of known anomalies, one can generate synthetic anomalies, which we find is also very effective.</p><p>With an FCN φ : R c×h×w → R u×v the FCDD objective utilizes a pseudo-Huber loss on the FCN output matrix A(X) = φ(X; W) 2 + 1 − 1 , where all operations are applied element-wise. The FCDD objective is then defined as (cf., (1)):</p><formula xml:id="formula_1">min W 1 n n i=1 (1 − y i ) 1 u · v A(X i ) 1 − y i log 1 − exp − 1 u · v A(X i ) 1 .<label>(2)</label></formula><p>Here A(X) 1 is the sum of all entries in A(X), which are all positive. FCDD is the utilization of an FCN in conjunction with the novel adaptation of the HSC loss we propose in (2). The objective maximizes A(X) 1 for anomalies and minimizes it for nominal samples, thus we use A(X) 1 as the anomaly score. Entries of A(X) that contribute to A(X) 1 correspond to regions of the input image that add to the anomaly score. The shape of these regions depends on the receptive field of the FCN. We include a sensitivity analysis on the size of the receptive field in Appendix A, where we find that performance is not strongly affected by the receptive field size. Note that A(X) has spatial dimensions u × v and is smaller than the original image dimensions h × w. One could use A(X) directly as a low-resolution heatmap of the image, however it is often desirable to have full-resolution heatmaps. Because we generally lack ground-truth anomaly maps in an AD setting during training, it is not possible to train an FCN in a supervised way to upsample the low-resolution heatmap A(X) (e.g. as in <ref type="bibr" target="#b36">(Noh et al., 2015)</ref>). For this reason we introduce an upsampling scheme based on the properties of receptive fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Receptive Field Upsampling</head><p>Input: A ∈ R u×v (low-res anomaly heatmap)</p><formula xml:id="formula_2">Output: A ∈ R h×w (full-res anomaly heatmap) Define: [G2(µ, σ)]x,y 1 2πσ 2 exp − (x−µ 1 ) 2 +(y−µ 2 ) 2 2σ 2 A ← 0 for all output pixels a in A do f ← receptive field of a c ← center of field f A ← A + a · G2(c, σ) end for return A</formula><p>Heatmap Upsampling Since we generally do not have access to ground-truth pixel annotations in anomaly detection during training, we cannot learn how to upsample using a deconvolutional type of structure. We derive a principled way to upsample our lower resolution anomaly heatmap instead. For every output pixel in A(X) there is a unique input pixel which lies at the center of its receptive field. It has been observed before that the effect of the receptive field for an output pixel decays in a Gaussian manner as one moves away from the center of the receptive field <ref type="bibr" target="#b31">(Luo et al., 2016)</ref>. We use this fact to upsample A(X) by using a strided transposed convolution with a fixed Gaussian kernel (see <ref type="figure">Figure 2</ref> right side). We describe this operation and procedure in Algorithm 1 which simply corresponds to a strided transposed convolution. The kernel size is set to the receptive field range of FCDD and the stride to the cumulative stride of FCDD. The variance of the distribution can be picked empirically (see Appendix B for details). <ref type="figure" target="#fig_2">Figure 3</ref> shows a complete overview of our FCDD method and the process of generating full-resolution anomaly heatmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we experimentally evaluate the performance of FCDD both quantitatively and qualitatively. For a quantitative evaluation, we use the Area Under the ROC Curve (AUC) <ref type="bibr" target="#b51">(Spackman, 1989)</ref> which is the commonly used measure in AD. For a qualitative evaluation, we compare the heatmaps produced by FCDD to existing deep AD explanation methods. As baselines, we consider gradient-based methods <ref type="bibr" target="#b50">(Simonyan et al., 2013)</ref> applied to hypersphere classifier (HSC) models <ref type="bibr" target="#b42">(Ruff et al., 2020a)</ref> with unrestricted network architectures (i.e. networks that also have fully connected layers) and autoencoders <ref type="bibr" target="#b3">(Bergmann et al., 2019)</ref> where we directly use the pixel-wise reconstruction error as an explanation heatmap. We slightly blur the heatmaps of the baselines with the same Gaussian kernel we use for FCDD, which we found results in less noisy, more interpretable heatmaps. We include heatmaps without blurring in Appendix G. We adjust the contrast of the heatmaps per method to highlight interesting features; see Appendix C for details. For our experiments we don't consider model-agnostic explanations, such as LIME <ref type="bibr" target="#b38">(Ribeiro et al., 2016)</ref> or anchors <ref type="bibr" target="#b39">(Ribeiro et al., 2018)</ref>, because they are not tailored to the AD task and performed poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">STANDARD ANOMALY DETECTION BENCHMARKS</head><p>We first evaluate FCDD on the Fashion-MNIST, CIFAR-10, and ImageNet datasets. The common AD benchmark is to utilize these classification datasets in a one-vs-rest setup where the "one" class is used as the nominal class and the rest of the classes are used as anomalies at test time. For training, we only use nominal samples as well as random samples from some auxiliary Outlier Exposure (OE) <ref type="bibr" target="#b17">(Hendrycks et al., 2019a)</ref> dataset, which is separate from the ground-truth anomaly classes following <ref type="bibr" target="#b17">Hendrycks et al. (2019a;</ref>. We report the mean AUC over all classes for each dataset.</p><p>Fashion-MNIST We consider each of the ten Fashion-MNIST <ref type="bibr" target="#b58">(Xiao et al., 2017)</ref> classes in a one-vs-rest setup. We train Fashion-MNIST using EMNIST <ref type="bibr" target="#b8">(Cohen et al., 2017)</ref> or grayscaled CIFAR-100 <ref type="bibr" target="#b24">(Krizhevsky et al., 2009)</ref> as OE. We found that the latter slightly outperforms the former (∼3 AUC percent points). On Fashion-MNIST, we use a network that consists of three convolutional layers with batch normalization, separated by two downsampling pooling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>We consider each of the ten CIFAR-10 ( <ref type="bibr" target="#b24">Krizhevsky et al., 2009</ref>) classes in a one-vs-rest setup. As OE we use CIFAR-100, which does not share any classes with CIFAR-10. We use a model similar to <ref type="bibr">LeNet-5 (LeCun et al., 1998)</ref>, but decrease the kernel size to three, add batch normalization, and replace the fully connected layers and last max-pool layer with two further convolutions.</p><p>ImageNet We consider 30 classes from ImageNet1k <ref type="bibr" target="#b10">(Deng et al., 2009)</ref> for the one-vs-rest setup following <ref type="bibr" target="#b17">Hendrycks et al. (2019a)</ref>. For OE we use ImageNet22k with ImageNet1k classes removed <ref type="bibr" target="#b17">(Hendrycks et al., 2019a)</ref>. We use an adaptation of VGG11 <ref type="bibr" target="#b49">(Simonyan and Zisserman, 2015)</ref> with batch normalization, suitable for inputs resized to 224×224 (see Appendix D for model details).</p><p>State-of-the-art Methods We report results from state-of-the-art deep anomaly detection methods. Methods that do not incorporate known anomalies are the autoencoder (AE), DSVDD <ref type="bibr" target="#b40">(Ruff et al., 2018)</ref>, Geometric Transformation based AD (GEO) <ref type="bibr" target="#b13">(Golan and El-Yaniv, 2018)</ref>, and a variant of GEO by <ref type="bibr">Hendrycks et al. (2019b) (GEO+)</ref>. Methods that use OE are a Focal loss classifier <ref type="bibr" target="#b18">(Hendrycks et al., 2019b)</ref>, also GEO+, Deep SAD <ref type="bibr" target="#b43">(Ruff et al., 2020b)</ref>, and HSC <ref type="bibr" target="#b42">(Ruff et al., 2020a)</ref>. Quantitative Results The mean AUC detection performance on the three AD benchmarks are reported in <ref type="table" target="#tab_0">Table 1</ref>. We can see that FCDD, despite using a restricted FCN architecture to improve explainability, achieves a performance that is close to state-of-the-art methods and outperforms autoencoders, which yield a detection performance close to random on more complex datasets. We provide detailed results for all individual classes in Appendix F. Qualitative Results <ref type="figure" target="#fig_3">Figures 4 and 5</ref> show the heatmaps for Fashion-MNIST and ImageNet respectively. For a Fashion-MNIST model trained on the nominal class "trousers," the heatmaps show that FCDD correctly highlights horizontal elements as being anomalous, which makes sense since trousers are vertically aligned. For an ImageNet model trained on the nominal class "acorns," we observe that colors seem to be fairly relevant features with green and brown areas tending to be seen as more nominal, and other colors being deemed anomalous, for example the red barn or the white snow. Nonetheless, the method also seems capable of using more semantic features, for example it recognizes the green caterpillar as being anomalous and it distinguishes the acorn to be nominal despite being against a red background. <ref type="figure">Figure 6</ref> shows heatmaps for CIFAR-10 models with varying amount of OE, all trained on the nominal class "airplane." We can see that, as the number of OE samples increases, FCDD tends to concentrate the explanations more on the primary object in the image, i.e. the bird, ship, and truck. We provide further heatmaps for additional classes from all datasets in Appendix G.</p><p>Input Ours Grad AE <ref type="figure">Figure 6</ref>: Anomaly heatmaps for three anomalous test samples on a CIFAR-10 model trained on nominal class "airplane." The second, third, and fourth blocks show the heatmaps of FCDD, gradientbased heatmaps of HSC, and AE heatmaps respectively. For Ours and Grad, we grow the number of OE samples from 2, 8, 128, 2048 to full OE. AE is not able to incorporate OE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Explanations</head><p>We found the gradient-based heatmaps to mostly produce centered blobs which lack spatial context (see <ref type="figure">Figure 6</ref>) and thus are not useful for explaining. The AE heatmaps, being directly tied to the reconstruction error anomaly score, look reasonable. We again note, however, that it is not straightforward how to include auxiliary OE samples or labeled anomalies into an AE approach, which leaves them with a poorer detection performance (see <ref type="table" target="#tab_0">Table 1</ref>). Overall we find that the proposed FCDD anomaly heatmaps yield a good and consistent visual interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPLAINING DEFECTS IN MANUFACTURING</head><p>Here we compare the performance of FCDD on the MVTec-AD dataset of defects in manufacturing <ref type="bibr" target="#b3">(Bergmann et al., 2019)</ref>. This datasets offers annotated ground-truth anomaly segmentation maps for testing, thus allowing a quantitative evaluation of model explanations. MVTec-AD contains 15 object classes of high-resolution RGB images with up to 1024×1024 pixels, where anomalous test samples are further categorized in up to 8 defect types, depending on the class. We follow <ref type="bibr" target="#b3">Bergmann et al. (2019)</ref> and compute an AUC from the heatmap pixel scores, using the given (binary) anomaly segmentation maps as ground-truth pixel labels. We then report the mean over all samples of this "explanation" AUC for a quantitative evaluation. For FCDD, we use a network that is based on a VGG11 network pre-trained on ImageNet, where we freeze the first ten layers, followed by additional fully convolutional layers that we train. Synthetic Anomalies OE with a natural image dataset like ImageNet is not informative for MVTec-AD since anomalies here are subtle defects of the nominal class, rather than being out of class (see <ref type="figure" target="#fig_0">Figure 1</ref>). For this reason, we generate synthetic anomalies using a sort of "confetti noise," a simple noise model that inserts colored blobs into images and reflects the local nature of anomalies. See <ref type="figure" target="#fig_5">Figure 7</ref> for an example.</p><p>Semi-Supervised FCDD A major advantage of FCDD in comparison to reconstruction-based methods is that it can be readily used in a semi-supervised AD setting <ref type="bibr" target="#b43">(Ruff et al., 2020b)</ref>. To see the effect of having even only a few labeled anomalies and their corresponding ground-truth anomaly maps available for training, we pick for each MVTec-AD class just one true anomalous sample per defect type at random and add it to the training set. This results in only 3-8 anomalous training samples. To also take advantage of the ground-truth heatmaps, we train a model on a pixel level. Let X 1 , . . . , X n again denote a batch of inputs with corresponding ground-truth heatmaps Y 1 , . . . , Y n , each having m = h · w number of pixels. Let A(X) also again denote the corresponding output anomaly heatmap of X. Then, we can formulate a pixel-wise objective by the following:</p><formula xml:id="formula_3">min W 1 n n i=1   1 m m j=1 (1 − (Y i ) j )A (X i ) j   − log   1 − exp   − 1 m m j=1 (Y i ) j A (X i ) j     . (3)</formula><p>Results <ref type="figure" target="#fig_0">Figure 1</ref> in the introduction shows heatmaps of FCDD trained on MVTec-AD. The results of the quantitative explanation are shown in <ref type="table" target="#tab_1">Table 2</ref>. We can see that FCDD outperforms its competitors in the unsupervised setting and sets a new state of the art of 0.92 pixel-wise mean AUC.</p><p>In the semi-supervised setting -using only one anomalous sample with corresponding anomaly map per defect class-the explanation performance improves further to 0.96 pixel-wise mean AUC. FCDD also has the most consistent performance across classes.  <ref type="bibr" target="#b3">(Bergmann et al., 2019)</ref>. For competitors we include the baselines presented in the original MVTec-AD paper and previously published works from peer-reviewed venues that include the MVTec-AD benchmark. The competitors are Self-Similarity and L2 Autoencoder <ref type="bibr" target="#b3">(Bergmann et al., 2019)</ref>, AnoGAN <ref type="bibr" target="#b48">(Schlegl et al., 2017;</ref><ref type="bibr" target="#b3">Bergmann et al., 2019)</ref>, CNN Feature Dictionaries <ref type="bibr" target="#b35">(Napoletano et al., 2018;</ref><ref type="bibr" target="#b3">Bergmann et al., 2019)</ref>, Visually Explained Variational Autoencoder , Superpixel Masking and Inpainting , Gradient Descent Reconstruction with VAEs <ref type="bibr" target="#b9">(Dehaene et al., 2020)</ref>, and Encoding Structure-Texture Relation with P-Net for AD <ref type="bibr" target="#b61">(Zhou et al., 2020)</ref>.   <ref type="bibr" target="#b25">Lapuschkin et al. (2016;</ref> revealed that roughly one fifth of all horse images in PASCAL VOC <ref type="bibr" target="#b12">(Everingham et al., 2010)</ref> contain a watermark in the lower left corner. They showed that a classifier recognizes this as the relevant class pattern and fails if the watermark is removed. They call this the "Clever Hans" effect in memory of the horse Hans, who could correctly answer math problems by reading its master 2 . We adapt this experiment to one-class classification by swapping our standard setup and train FCDD so that the "horse" class is anomalous and use ImageNet as nominal samples. We choose this setup so that one would expect FCDD to highlight horses in its heatmaps and so that any other highlighting makes FCDD reveal a Clever Hans effect.  shows that a one-class model is indeed also vulnerable to learning a characterization based on spurious features: the watermarks in the lower left corner which have high scores whereas other regions have low scores. We also observe that the model yields high scores for bars, grids, and fences in <ref type="figure" target="#fig_7">Figure 8 (a)</ref>. This is due to many images in the dataset containing horses jumping over bars or being in fenced areas. In both cases, the horse features themselves do not attain the highest scores because the model has no way of knowing that the spurious features, while providing good discriminative power at training time, would not be desirable upon deployment/test time. In contrast to traditional black-box models, however, transparent detectors like FCDD enable a practitioner to recognize and remedy (e.g. by cleaning or extending the training data) such behavior or other undesirable phenomena (e.g. to avoid unfair social bias).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">THE CLEVER HANS EFFECT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In conclusion we find that FCDD, in comparison to previous methods, performs well and is adaptable to both semantic detection tasks (Section 4.1) and more subtle defect detection tasks (Section 4.2). Finally, directly tying an explanation to the anomaly score should make FCDD less vulnerable to attacks  in contrast to a posteriori explanation methods. We leave an analysis of this phenomenon for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A RECEPTIVE FIELD SENSITIVITY ANALYSIS</head><p>The receptive field has an impact on both detection performance and explanation quality. Here we provide some heatmaps and AUC scores for networks with different receptive field sizes. We observe that the detection performance is only minimally affected, but larger receptive fields cause the explanation heatmap to become less concentrated and more "blobby." For MVTec-AD we see that this can also negatively affect pixel-wise AUC scores, see <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>For CIFAR-10 we create eight different network architectures to study the impact of the receptive field size. Each architecture has four convolutional layers and two max-pool layers. To change the receptive field we vary the kernel size of the first convolutional layer between 3 and 17. When this kernel size is 3 then the receptive field contains approximately one quarter of the image; for a kernel size of 17 the receptive field is the entire image. <ref type="table" target="#tab_3">Table 3</ref> shows the detection performance of the networks. <ref type="figure" target="#fig_9">Figure 9</ref> contains example heatmaps.    MVTec-AD We create six different network architectures for MVTec-AD. They have six convolutional layers and three max-pool layers. We vary the kernel size for all of the convolutional layers between 3 and 13, which corresponds to a receptive field containing 1/16 of the image to the full image respectively. <ref type="table" target="#tab_4">Table 4</ref> shows the explanation performance of the networks in terms of pixel-wise mean AUC. <ref type="figure" target="#fig_0">Figure 10</ref> contains some example heatmaps. We observe that a smaller receptive field yields better explanation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPACT OF THE GAUSSIAN VARIANCE</head><p>Using the proposed heatmap upsampling in Section 3 FCDD provides full-resolution anomaly heatmaps. However, this upsampling involves the choice of σ for the Gaussian kernel. In this section, we demonstrate the effect of this hyperparameter on the explanation performance of FCDD on MVTec-AD. <ref type="table" target="#tab_5">Table 5</ref> shows the pixel-wise mean AUC, <ref type="figure" target="#fig_0">Figure 11</ref> corresponding heatmaps. <ref type="figure" target="#fig_0">Figure 11</ref>: Anomaly heatmaps for seven anomalous test samples of MVTec-AD. We grow σ from 4 (left) to 16 (right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ANOMALY HEATMAP VISUALIZATION</head><p>For anomaly heatmap visualization, the FCDD anomaly scores A (X) need to be rescaled to values in [0, 1]. Instead of applying standard min-max scaling that would divide all heatmap entries by max A (X), we use anomaly score quantiles to adjust the contrast in the heatmaps. For a collection of inputs X = {X 1 , . . . , X n } with corresponding full-resolution anomaly heatmaps A = {A (X 1 ), . . . , A (X n )}, the normalized heatmap I(X) for some A (X) is computed as</p><formula xml:id="formula_4">I(X) j = min A (X) j − min(A) q η ({A − min(A) | A ∈ A}) , 1 ,</formula><p>where j denotes the j-th pixel and q η the η-th percentile over all pixels and examples in A. The subtraction and min operation are applied on a pixel level, i.e. the minimum is extracted over all pixels and all samples of A and subtraction is then applied elementwise. Using the η-th percentile might leave some of the values above 1, which is why we finally clamp the pixels at 1.</p><p>The specific choice of η and set of samples X differs per figure. We select them to highlight different properties of the heatmaps. In general, the lower η the more red (anomalous) regions we have in the heatmaps because more values are left above one (before clamping to 1) and vice versa. The choice of X ranges from just one sample X, such that A (X) is normalized only w.r.t. to its own scores (highlighting the most anomalous regions within the image), to the complete dataset (highlighting which regions look anomalous compared to the whole dataset). For the latter visualization we rebalance the dataset so that X contains an equal amount of nominal and anomalous images to maintain consistent scaling. The choice of η and X is consistent per <ref type="figure">figure.</ref> In the following we list the choices made for the individual figures. <ref type="figure" target="#fig_0">Figures 1, 10</ref>, and 11 use η = 0.97 and set X to X for each heatmap I(X) to show relative anomalies. So each image is normalized with respect to itself only. <ref type="figure" target="#fig_3">Figure 4</ref> uses η = 0.85 and sets X to the complete balanced test set. <ref type="figure" target="#fig_9">6 and 9</ref> use η = 0.85 and set X to X for each heatmap I(X) to show relative anomalies. So each image is normalized with respect to itself only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MVTec-AD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fashion-MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 Figures</head><p>ImageNet <ref type="figure" target="#fig_4">Figure 5</ref> uses η = 0.97 and sets X to the complete balanced test set.</p><p>Pascal VOC <ref type="figure" target="#fig_7">Figure 8</ref> uses η = 0.99 and sets X to the complete balanced test set.</p><p>Heatmap Upsampling For the Gaussian kernel heatmap upsampling described in Algorithm 1, we set σ to 1.2 for CIFAR-10 and Fashion-MNIST, to 8 for ImageNet and Pascal VOC, and to 12 for MVTec-AD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DETAILS ON THE NETWORK ARCHITECTURES</head><p>Here we provide the complete FCDD network architectures we used on the different datasets.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fashion-MNIST</head><formula xml:id="formula_5">---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1,</formula><formula xml:id="formula_6">---------------------------------------------------------------- CIFAR-10 ----------------------------------------------------------------</formula><formula xml:id="formula_7">---------------------------------------------------------------- ImageNet, MVTec-AD, and Pascal VOC ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1,</formula><formula xml:id="formula_8">----------------------------------------------------------------</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E TRAINING AND OPTIMIZATION</head><p>Here we provide the training and optimization details for the individual experiments from Section 4.</p><p>We apply common pre-processing (e.g. data normalization) and data augmentation steps in our data loading pipeline. To sample auxiliary anomalies in an online manner during training, each nominal sample of a batch has a 50% chance of being replaced by a randomly picked auxiliary anomaly. This leads to balanced training batches for sufficiently large batch sizes. One epoch in our implementation still refers to the original nominal data training set size, so that approximately 50% of the nominal samples have been seen per training epoch. Below, we list further details for the specific datasets.</p><p>Fashion-MNIST We train for 400 epochs using a batch size of 128 samples. We optimize the network parameters using SGD <ref type="bibr" target="#b5">(Bottou, 2010)</ref> with Nesterov momentum (µ = 0.9) <ref type="bibr" target="#b53">(Sutskever et al., 2013)</ref>, weight decay of 10 −6 and an initial learning rate of 0.01, which decreases the previous learning rate per epoch by a factor of 0.98. The pre-processing pipeline is: (1) Random crop to size 28 with beforehand zero-padding of 2 pixels on all sides (2) random horizontal flipping with a chance of 50% (3) data normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>We train for 600 epochs using a batch size of 200 samples. We optimize the network using Adam <ref type="bibr" target="#b23">(Kingma and Ba, 2015)</ref> (β = (0.9, 0.999)) with weight decay 10 −6 and an initial learning rate of 0.001 which is decreased by a factor of 10 at epoch 400 and 500. The pre-processing pipeline is: (1) Random color jitter with all parameters 3 set to 0.01 (2) random crop to size 32 with beforehand zero-padding of 4 pixels on all sides (3) random horizontal flipping with a chance of 50% (4) additive Gaussian noise with σ = 0.001 (5) data normalization.</p><p>ImageNet We use the same setup as in CIFAR-10, but resize all images to size 256×256 before forwarding them through the pipeline and change the random crop to size 224 with no padding. Test samples are center cropped to a size of 224 before being normalized.</p><p>Pascal VOC We use the same setup as in CIFAR-10, but resize all images to size 224×224 before forwarding them through the pipeline and remove the Random Crop step.</p><p>MVTec-AD For MVTec-AD we redefine an epoch to be ten times an iteration of the full dataset because this improves the computational performance of the data pipeline. We train for 200 epochs using SGD with Nesterov momentum (µ = 0.9), weight decay 10 −4 , and an initial learning rate of 0.001, which decreases per epoch by a factor of 0.985. The pre-processing pipeline is: (1) Resize to 240×240 pixels (2) random crop to size 224 with no padding (3) random color jitter with either all parameters set to 0.04 or 0.0005, randomly chosen (4) 50% chance to apply additive Gaussian noise (5) data normalization. <ref type="table" target="#tab_9">Table 6</ref> shows the class-wise results on Fashion-MNIST for AE, Deep Support Vector Data Description (DSVDD) <ref type="bibr" target="#b40">(Ruff et al., 2018;</ref><ref type="bibr" target="#b2">Bergman and Hoshen, 2020)</ref> and Geometric Transformation based AD (GEO) <ref type="bibr" target="#b13">(Golan and El-Yaniv, 2018)</ref>. In <ref type="table" target="#tab_10">Table 7</ref> the class-wise results for CIFAR-10 are reported. Competitors without OE are AE <ref type="bibr" target="#b40">(Ruff et al., 2018)</ref>, DSVDD <ref type="bibr" target="#b40">(Ruff et al., 2018)</ref>, GEO <ref type="bibr" target="#b13">(Golan and El-Yaniv, 2018)</ref> and an adaptation of GEO (GEO+) <ref type="bibr" target="#b18">(Hendrycks et al., 2019b)</ref>. Competitors with OE are the focal loss classifier <ref type="bibr" target="#b18">(Hendrycks et al., 2019b)</ref>, again GEO+ <ref type="bibr" target="#b18">(Hendrycks et al., 2019b)</ref>, Deep Semi-supervised Anomaly Detection (Deep SAD) <ref type="bibr" target="#b43">(Ruff et al., 2020b;</ref><ref type="bibr">a)</ref> and the hypersphere Classifier <ref type="bibr" target="#b42">(Ruff et al., 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F QUANTITATIVE DETECTION RESULTS FOR INDIVIDUAL CLASSES</head><p>In <ref type="table" target="#tab_11">Table 8</ref> the class-wise results for Imagenet are shown, where competitors are the AE, the focal loss classifier <ref type="bibr" target="#b18">(Hendrycks et al., 2019b)</ref>, Geo+ <ref type="bibr" target="#b18">(Hendrycks et al., 2019b)</ref>, Deep SAD <ref type="bibr" target="#b43">(Ruff et al., 2020b)</ref> and HSC <ref type="bibr" target="#b42">(Ruff et al., 2020a)</ref>. Results from the literature are marked with an asterisk.   <ref type="figure" target="#fig_0">Figures 12, 13, and 14</ref> show the unblurred heatmaps for Fashion-MNIST, ImageNet, and CIFAR-10 respectively. Class-wise Anomaly Heatmaps Due to space restrictions we have only shown heatmaps for some of the classes in the main paper. Here we also report a collection of heatmaps for all classes.</p><p>We show heatmaps with adjusted contrast curves by setting X to the balanced set of all samples for all datasets in this section. Further, we set η = 0.85 for Fashion-MNIST and CIFAR-10, η = 0.99 for MVTec-AD, and η = 0.97 for ImageNet. Note that, to keep the heatmaps for different classes comparable, we use a unified normalization for all heatmaps in one figure. However, since for each class a separate anomaly detector is trained, this yields suboptimal visualizations for some of the classes (for example, the "toothbrush" images for MVTec-AD in <ref type="figure" target="#fig_0">Figure 18</ref> where the heatmaps just show a huge red blob). Tweaking the normalization for such classes reveals that the heatmaps actually tend to mark the correct anomalous regions, which in the case of "toothbrushes" can be seen in the explanation performance evaluation in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The rows in all heatmaps show the following: (1) Input samples (2) FCDD heatmaps (3) gradient heatmaps with HSC (4) autoencoder reconstruction heatmaps. Heatmaps for MVTec-AD add a fifth row containing the ground-truth anomaly map.</p><p>Heatmaps for Fashion-MNIST using auxiliary anomalies from CIFAR-100 are in <ref type="figure" target="#fig_0">Figure 15</ref>, using EMNIST for OE instead are in <ref type="figure" target="#fig_0">Figure 16</ref>. CIFAR-10 heatmaps are in <ref type="figure" target="#fig_0">Figure 17</ref>, and heatmaps for all classes of MVTec-AD are in <ref type="figure" target="#fig_0">Figure 18</ref>. Finally, we present ImageNet heatmaps in <ref type="figure" target="#fig_0">Figures 19 and 20</ref>.</p><p>"t-shirt/top" "trouser" "pullover" "dress" "coat" "sandal" "shirt" "sneaker" "bag" "ankle boot" <ref type="figure" target="#fig_0">Figure 15</ref>: Anomaly heatmaps for anomalous test samples in Fashion-MNIST using CIFAR-100 OE. Columns are ordered by increasing anomaly score from left to right. The subcaptions refer to the nominal class that each model is trained on, for which some examples are also displayed as a separate column on the left.</p><p>"t-shirt/top" "trouser" "pullover" "dress" "coat" "sandal" "shirt" "sneaker" "bag" "ankle boot" <ref type="figure" target="#fig_0">Figure 16</ref>: Anomaly heatmaps for anomalous test samples in Fashion-MNIST using EMNIST OE. Columns are ordered by increasing anomaly score from left to right. The subcaptions refer to the nominal class that each model is trained on, for which some examples are also displayed as a separate column on the left. "airplane" "automobile" "bird" "cat" "deer" "dog" "frog" "horse" "ship" "truck" <ref type="figure" target="#fig_0">Figure 17</ref>: Anomaly heatmaps for anomalous test samples in CIFAR-10. Columns are ordered by increasing anomaly score from left to right. The subcaptions refer to the nominal class that each model is trained on, for which some examples are also displayed as a separate column on the left. "bottle" "cable" "capsule" "carpet" "grid" "hazelnut" "leather" "metal nut" "pill" "screw" "tile" "toothbrush" "transistor" "wood" "zipper" <ref type="figure" target="#fig_0">Figure 18</ref>: Anomaly heatmaps for anomalous test samples in MVTec-AD. Columns are ordered by increasing anomaly score from left to right. The subcaptions refer to the nominal class that each model is trained on. "acorn" "airliner" "ambulance" "American alligator" "banjo" "barn" "bikini" "digital clock" "dragonfly" "dumbbell" "forklift" "goblet" "grand piano" "hotdog" "hourglass" "manhole cover" "mosque" "nail" "parking meter" "pillow" "revolver" <ref type="figure" target="#fig_0">Figure 19</ref>: Anomaly heatmaps for anomalous test samples in ImageNet, where classes 1-21 are shown. Columns are ordered by increasing anomaly score from left to right. The subcaptions refer to the nominal class that each model is trained on, for which some examples are also displayed as a separate column on the left. "dial telephone" "schooner" "snowmobile" "soccer ball" "stingray" "strawberry" "tank" "toaster" "volcano" </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>FCDD explanation heatmaps for MVTec-AD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the overall procedure to produce full-resolution anomaly heatmaps with FCDD. X denotes the input, φ the network, A the produced anomaly heatmap and A the upsampled version of A using a transposed Gaussian convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Anomaly heatmaps for anomalous test samples of a Fashion-MNIST model trained on nominal class "trousers" (nominal samples are shown in (a)). In (b) CIFAR-100 was used for OE and in (c) EMNIST. Columns are ordered by increasing anomaly score from left to right, i.e. what FCDD finds the most nominal looking anomaly on the left to the most anomalous looking anomaly on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Anomaly heatmaps of an ImageNet model trained on nominal class "acorns." Here (a) are nominal samples and (b) are anomalous samples. Columns are ordered by increasing anomaly score from left to right, i.e. what FCDD finds the most nominal looking on the left to the most anomalous looking on the right for (a) nominal samples and (b) anomalies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Confetti noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>unsupervised semi-supervised AE-SS* AE-L2* AnoGAN* CNNFD* VEVAE* SMAI* GDR* P-NET* FCDD FCDD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Heatmaps for horses on PASCAL VOC. Here (a) shows anomalous samples ordered from most nominal to most anomalous from left toright, and (b)  shows examples that indicate that the model is a "Clever Hans," i.e. has learned a characterization based on spurious features (watermarks).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure</head><label></label><figDesc>Figure 8 (b) shows that a one-class model is indeed also vulnerable to learning a characterization based on spurious features: the watermarks in the lower left corner which have high scores whereas other regions have low scores. We also observe that the model yields high scores for bars, grids, and fences in Figure 8 (a). This is due to many images in the dataset containing horses jumping over bars or being in fenced areas. In both cases, the horse features themselves do not attain the highest scores because the model has no way of knowing that the spurious features, while providing good discriminative power at training time, would not be desirable upon deployment/test time. In contrast to traditional black-box models, however, transparent detectors like FCDD enable a practitioner to recognize and remedy (e.g. by cleaning or extending the training data) such behavior or other undesirable phenomena (e.g. to avoid unfair social bias).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Anomaly heatmaps for three anomalous test samples on CIFAR-10 models trained on nominal class "airplane." We grow the receptive field size from 18 (left) to 32 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Anomaly heatmaps for seven anomalous test samples of MVTec-AD. We grow the receptive field size from 53 (left) to 243 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :Figure 13 :Figure 14 :</head><label>121314</label><figDesc>Anomaly heatmaps for anomalous test samples of a Fashion-MNIST model trained on nominal class "trousers." In (a) CIFAR-100 was used for OE and in (b) EMNIST. Anomaly heatmaps of an ImageNet model trained on nominal class "acorns." (a) are nominal and (b) anomalous samples. Anomaly heatmaps for three anomalous test samples (Input left) on a CIFAR-10 model trained on nominal class "airplane." The second, third, and fourth blocks show the heatmaps of FCDD (Ours), gradient-based heatmaps of HSC, and AE heatmaps respectively. For Ours and Grad, we grow the number of OE samples from 2, 8, 128, 2048 to full OE. AE is not able to incorporate OE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 20 :</head><label>20</label><figDesc>Anomaly heatmaps for anomalous test samples in ImageNet, where classes 22-30 are shown. Columns are ordered by increasing anomaly score from left to right. The subcaptions refer to the nominal class that each model is trained on, for which some examples are also displayed as a separate column on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean AUC (over all classes and 5 seeds per class) for Fashion-MNIST, CIFAR-10, and ImageNet. Results from existing literature are marked with an asterisk<ref type="bibr" target="#b2">(Bergman and Hoshen, 2020;</ref><ref type="bibr" target="#b13">Golan and El-Yaniv, 2018;</ref><ref type="bibr" target="#b18">Hendrycks et al., 2019b;</ref><ref type="bibr" target="#b42">Ruff et al., 2020a)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">without OE</cell><cell></cell><cell></cell><cell></cell><cell>with OE</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>AE</cell><cell cols="8">DSVDD* GEO* Geo+* Focal* Geo+* Deep SAD* HSC* FCDD</cell></row><row><cell cols="2">Fashion-MNIST 0.82</cell><cell>0.93</cell><cell>0.94</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>0.89</cell></row><row><cell>CIFAR-10</cell><cell>0.59*</cell><cell>0.65</cell><cell>0.86</cell><cell>0.90</cell><cell>0.87</cell><cell>0.96</cell><cell>0.95</cell><cell>0.96</cell><cell>0.95</cell></row><row><cell>ImageNet</cell><cell>0.56</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>0.56</cell><cell>0.86</cell><cell>0.97</cell><cell>0.97</cell><cell>0.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Pixel-wise mean AUC scores for all classes of the MVTec-AD dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Mean AUC (over all classes and 5 seeds per class) for CIFAR-10 and neural networks with varying receptive field size.</figDesc><table><row><cell>Receptive field size</cell><cell>18</cell><cell>20</cell><cell>22</cell><cell>24</cell><cell>26</cell><cell>28</cell><cell>30</cell><cell>32</cell></row><row><cell>AUC</cell><cell cols="8">0.9328 0.9349 0.9344 0.9320 0.9303 0.9283 0.9257 0.9235</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Pixel-wise mean AUC (over all classes and 5 seeds per class) for MVTec-AD and neural networks with varying receptive field size.</figDesc><table><row><cell cols="2">Receptive field size 53</cell><cell>91</cell><cell>129 167 205 243</cell></row><row><cell>AUC</cell><cell cols="3">0.88 0.85 0.79 0.76 0.75 0.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Pixel-wise mean AUC (over all classes and 5 seeds per class) for MVTec-AD and different σ.</figDesc><table><row><cell>σ</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell></row><row><cell cols="8">AUC 0.8567 0.8836 0.9030 0.9124 0.9164 0.9217 0.9208</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>AUC scores for all classes of Fashion-MNIST<ref type="bibr" target="#b58">(Xiao et al., 2017)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell>without OE</cell><cell></cell><cell>with OE</cell></row><row><cell></cell><cell>AE</cell><cell cols="2">DSVDD* Geo*</cell><cell>FCDD</cell></row><row><cell cols="2">T-Shirt/Top 0.85</cell><cell>0.98</cell><cell>0.99</cell><cell>0.82</cell></row><row><cell>Trouser</cell><cell>0.91</cell><cell>0.90</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Pullover</cell><cell>0.78</cell><cell>0.91</cell><cell>0.91</cell><cell>0.84</cell></row><row><cell>Dress</cell><cell>0.88</cell><cell>0.94</cell><cell>0.90</cell><cell>0.92</cell></row><row><cell>Coat</cell><cell>0.88</cell><cell>0.89</cell><cell>0.92</cell><cell>0.87</cell></row><row><cell>Sandal</cell><cell>0.45</cell><cell>0.92</cell><cell>0.93</cell><cell>0.90</cell></row><row><cell>Shirt</cell><cell>0.70</cell><cell>0.83</cell><cell>0.83</cell><cell>0.75</cell></row><row><cell>Sneaker</cell><cell>0.96</cell><cell>0.99</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell>Bag</cell><cell>0.87</cell><cell>0.92</cell><cell>0.91</cell><cell>0.86</cell></row><row><cell cols="2">Ankle Boot 0.96</cell><cell>0.99</cell><cell>0.99</cell><cell>0.94</cell></row><row><cell>Mean</cell><cell>0.82</cell><cell>0.93</cell><cell>0.94</cell><cell>0.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>AUC scores for all classes of CIFAR-10<ref type="bibr" target="#b24">(Krizhevsky et al., 2009</ref>).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">without OE</cell><cell></cell><cell></cell><cell></cell><cell>with OE</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">AE* DSVDD* GEO* Geo+* Focal* Geo+* Deep SAD* HSC* FCDD</cell></row><row><cell>Airplane</cell><cell>0.59</cell><cell>0.62</cell><cell>0.75</cell><cell>0.78</cell><cell>0.88</cell><cell>0.90</cell><cell>0.94</cell><cell>0.97</cell><cell>0.95</cell></row><row><cell cols="2">Automobile 0.57</cell><cell>0.66</cell><cell>0.96</cell><cell>0.97</cell><cell>0.94</cell><cell>0.99</cell><cell>0.98</cell><cell>0.99</cell><cell>0.96</cell></row><row><cell>Bird</cell><cell>0.49</cell><cell>0.51</cell><cell>0.78</cell><cell>0.87</cell><cell>0.79</cell><cell>0.94</cell><cell>0.90</cell><cell>0.93</cell><cell>0.91</cell></row><row><cell>Cat</cell><cell>0.58</cell><cell>0.59</cell><cell>0.72</cell><cell>0.81</cell><cell>0.80</cell><cell>0.88</cell><cell>0.87</cell><cell>0.90</cell><cell>0.90</cell></row><row><cell>Deer</cell><cell>0.54</cell><cell>0.61</cell><cell>0.88</cell><cell>0.93</cell><cell>0.82</cell><cell>0.97</cell><cell>0.95</cell><cell>0.97</cell><cell>0.94</cell></row><row><cell>Dog</cell><cell>0.62</cell><cell>0.66</cell><cell>0.88</cell><cell>0.90</cell><cell>0.86</cell><cell>0.94</cell><cell>0.93</cell><cell>0.94</cell><cell>0.93</cell></row><row><cell>Frog</cell><cell>0.51</cell><cell>0.68</cell><cell>0.83</cell><cell>0.91</cell><cell>0.93</cell><cell>0.97</cell><cell>0.97</cell><cell>0.98</cell><cell>0.97</cell></row><row><cell>Horse</cell><cell>0.59</cell><cell>0.67</cell><cell>0.96</cell><cell>0.97</cell><cell>0.88</cell><cell>0.99</cell><cell>0.97</cell><cell>0.98</cell><cell>0.96</cell></row><row><cell>Ship</cell><cell>0.77</cell><cell>0.76</cell><cell>0.93</cell><cell>0.95</cell><cell>0.93</cell><cell>0.99</cell><cell>0.97</cell><cell>0.98</cell><cell>0.97</cell></row><row><cell>Truck</cell><cell>0.67</cell><cell>0.73</cell><cell>0.91</cell><cell>0.93</cell><cell>0.92</cell><cell>0.99</cell><cell>0.96</cell><cell>0.97</cell><cell>0.96</cell></row><row><cell>Mean</cell><cell>0.59</cell><cell>0.65</cell><cell>0.86</cell><cell>0.90</cell><cell>0.87</cell><cell>0.96</cell><cell>0.95</cell><cell>0.96</cell><cell>0.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>AUC scores for 30 classes of ImageNet<ref type="bibr" target="#b10">(Deng et al., 2009)</ref>.In this section we report some further anomaly heatmaps, unblurred baseline heatmaps, as well as class-wise heatmaps for all datasets.Unblurred Anomaly Heatmap Baselines Here we show unblurred baseline heatmaps for the figures in Section 4.1.</figDesc><table><row><cell></cell><cell>without OE</cell><cell></cell><cell></cell><cell>with OE</cell><cell></cell><cell></cell></row><row><cell></cell><cell>AE</cell><cell cols="5">Focal* Geo+* Deep SAD* HSC* FCDD</cell></row><row><cell>Acorn</cell><cell>0.45</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>0.99</cell><cell>0.97</cell></row><row><cell>Airliner</cell><cell>0.80</cell><cell>×</cell><cell>×</cell><cell>0.97</cell><cell>1.00</cell><cell>0.98</cell></row><row><cell>Ambulance</cell><cell>0.25</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>1.00</cell><cell>0.99</cell></row><row><cell>American alligator</cell><cell>0.61</cell><cell>×</cell><cell>×</cell><cell>0.93</cell><cell>0.98</cell><cell>0.97</cell></row><row><cell>Banjo</cell><cell>0.45</cell><cell>×</cell><cell>×</cell><cell>0.97</cell><cell>0.98</cell><cell>0.91</cell></row><row><cell>Barn</cell><cell>0.59</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>1.00</cell><cell>0.97</cell></row><row><cell>Bikini</cell><cell>0.46</cell><cell>×</cell><cell>×</cell><cell>0.97</cell><cell>0.99</cell><cell>0.94</cell></row><row><cell>Digital clock</cell><cell>0.63</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>0.97</cell><cell>0.92</cell></row><row><cell>Dragonfly</cell><cell>0.62</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Dumbbell</cell><cell>0.42</cell><cell>×</cell><cell>×</cell><cell>0.93</cell><cell>0.92</cell><cell>0.88</cell></row><row><cell>Forklift</cell><cell>0.28</cell><cell>×</cell><cell>×</cell><cell>0.91</cell><cell>0.99</cell><cell>0.94</cell></row><row><cell>Goblet</cell><cell>0.63</cell><cell>×</cell><cell>×</cell><cell>0.92</cell><cell>0.94</cell><cell>0.90</cell></row><row><cell>Grand piano</cell><cell>0.45</cell><cell>×</cell><cell>×</cell><cell>1.00</cell><cell>0.97</cell><cell>0.95</cell></row><row><cell>Hotdog</cell><cell>0.48</cell><cell>×</cell><cell>×</cell><cell>0.96</cell><cell>0.99</cell><cell>0.97</cell></row><row><cell>Hourglass</cell><cell>0.58</cell><cell>×</cell><cell>×</cell><cell>0.96</cell><cell>0.97</cell><cell>0.92</cell></row><row><cell>Manhole cover</cell><cell>0.70</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>Mosque</cell><cell>0.72</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>0.99</cell><cell>0.97</cell></row><row><cell>Nail</cell><cell>0.57</cell><cell>×</cell><cell>×</cell><cell>0.93</cell><cell>0.94</cell><cell>0.92</cell></row><row><cell>Parking meter</cell><cell>0.45</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>0.93</cell><cell>0.87</cell></row><row><cell>Pillow</cell><cell>0.40</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>0.94</cell><cell>0.94</cell></row><row><cell>Revolver</cell><cell>0.60</cell><cell>×</cell><cell>×</cell><cell>0.98</cell><cell>0.98</cell><cell>0.93</cell></row><row><cell>Rotary dial telephone</cell><cell>0.58</cell><cell>×</cell><cell>×</cell><cell>0.90</cell><cell>0.98</cell><cell>0.91</cell></row><row><cell>Schooner</cell><cell>0.65</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>0.99</cell><cell>0.96</cell></row><row><cell>Snowmobile</cell><cell>0.54</cell><cell>×</cell><cell>×</cell><cell>0.98</cell><cell>0.99</cell><cell>0.97</cell></row><row><cell>Soccer ball</cell><cell>0.46</cell><cell>×</cell><cell>×</cell><cell>0.97</cell><cell>0.93</cell><cell>0.86</cell></row><row><cell>Stingray</cell><cell>0.84</cell><cell>×</cell><cell>×</cell><cell>0.99</cell><cell>0.99</cell><cell>0.97</cell></row><row><cell>Strawberry</cell><cell>0.44</cell><cell>×</cell><cell>×</cell><cell>0.98</cell><cell>0.99</cell><cell>0.97</cell></row><row><cell>Tank</cell><cell>0.57</cell><cell>×</cell><cell>×</cell><cell>0.97</cell><cell>0.99</cell><cell>0.96</cell></row><row><cell>Toaster</cell><cell>0.59</cell><cell>×</cell><cell>×</cell><cell>0.98</cell><cell>0.92</cell><cell>0.79</cell></row><row><cell>Volcano</cell><cell>0.90</cell><cell>×</cell><cell>×</cell><cell>0.90</cell><cell>1.00</cell><cell>0.97</cell></row><row><cell>Mean</cell><cell>0.56</cell><cell>0.56</cell><cell>0.86</cell><cell>0.97</cell><cell>0.97</cell><cell>0.94</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://en.wikipedia.org/wiki/Clever_Hans</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pytorch.org/docs/1.4.0/torchvision/transforms.html#torchvision.transforms.ColorJitter</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fairwashing explanations with off-manifold detergent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-K</forename><surname>Dombrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kessel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6757" to="6766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Outliers in Statistical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MVTec AD-A comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Safe model-based reinforcement learning with stability guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schoellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="908" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Anomaly detection using one-class neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06360</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EMNIST: Extending MNIST to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative energy-based projection on a normal data manifold for anomaly localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Combrexelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On discordant observations. The London, Edinburgh, and Dublin Philosophical Magazine and</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Y</forename><surname>Edgeworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="364" to="375" />
			<date type="published" when="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9758" to="9769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DROCC: Deep robust one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Simhadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11335" to="11345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Social GAN: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2255" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Outlier Detection Using Replicator Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DaWaK</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2454</biblScope>
			<biblScope unit="page" from="170" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15637" to="15648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Jarrahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Business Horizons</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="577" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reluplex: An efficient SMT solver for verifying deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="97" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">107198</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analyzing classifiers: Fisher vectors and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2912" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unmasking clever hans predictors and assessing what machines really learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1096</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Superpixel masking and inpainting for selfsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards visually explaining variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8642" to="8651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep Taylor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Methods for interpreting and understanding deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">One-class classifier networks for target recognition applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Moya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Congress on Neural Networks</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="797" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by CNN-based selfsimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiple-instance learning for anomaly detection in digital mammography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Quellec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lamard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cozic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cazuguel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1604" to="1614" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why should i trust you?</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Anchors: High-precision model-agnostic explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4390" to="4399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-attentive, multi-context one-class classification for unsupervised anomaly detection on text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schnake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4061" to="4071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00339</idno>
		<title level="m">Rethinking assumptions in deep anomaly detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A unifying review of deep and shallow anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2021.3052449</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Anomaly detection using autoencoders with nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yairi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</title>
		<meeting>the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07631</idno>
		<title level="m">Toward interpretable machine learning: Transparent deep neural networks and beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Signal detection theory: Valuable tools for evaluating inductive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Spackman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Machine Learning</title>
		<meeting>the Sixth International Workshop on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="160" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">One-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Delft University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Support Vector Data Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="45" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Attention guided anomaly detection and localization in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahalanobis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08616</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<title level="m">Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spatio-temporal autoencoder for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Multimedia</title>
		<meeting>the 25th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Anomaly detection with robust deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Paffenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Encoding structure-texture relation with p-net for anomaly detection in retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
