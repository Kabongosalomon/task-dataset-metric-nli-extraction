<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshed</forename><surname>Ohn-Bar</surname></persName>
							<email>eohnbar@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Robotics Research Laboratory</orgName>
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
							<email>mtrivedi@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Robotics Research Laboratory</orgName>
								<orgName type="institution">University of California San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim to study the modeling limitations of the commonly employed boosted decision trees classifier. Inspired by the success of large, data-hungry visual recognition models (e.g. deep convolutional neural networks), this paper focuses on the relationship between modeling capacity of the weak learners, dataset size, and dataset properties. A set of novel experiments on the Caltech Pedestrian Detection benchmark results in the best known performance among non-CNN techniques while operating at fast run-time speed. Furthermore, the performance is on par with deep architectures (9.71% log-average miss rate), while using only HOG+LUV channels as features. The conclusions from this study are shown to generalize over different object detection domains as demonstrated on the FDDB face detection benchmark (93.37% accuracy). Despite the impressive performance, this study reveals the limited modeling capacity of the common boosted trees model, motivating a need for architectural changes in order to compete with multi-level and very deep architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The increase in data availability and computational power have led to a revolution in object recognition. Modern object recognition algorithms combine deep multi-layer architectures with large datasets in order to achieve state-of-the-art recognition performance <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. A main advantage of such learning architectures stems from the architectural ability to increase modeling capacity (e.g. network parameters and depth) in a straightforward manner. At the same time, boosted detectors <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref> remain highly successful for fast detection of objects <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b20">[21]</ref>. The Viola and Jones <ref type="bibr" target="#b3">[4]</ref> learning architecture has remained largely unchanged, with boosting <ref type="bibr" target="#b21">[22]</ref> used for training a cascade of weak learners (e.g. decision trees). State-of-theart pedestrian detectors often employ such models <ref type="bibr" target="#b14">[15]</ref>, while pursuing improved feature representations in order to achieve detection performance gains <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Despite being a different learning architecture compared to the modern Convolutional Network (CNN) <ref type="bibr" target="#b0">[1]</ref>, the boosted decision tree model also allows for a straightforward increase in modeling capacity for handling large datasets ( <ref type="figure">Fig. 1</ref>). Motivated by this observation, we perform an extensive set of experiments designed to better understand the limitations of the commonly employed boosted decision trees model. The large Caltech pedestrian benchmark <ref type="bibr" target="#b5">[6]</ref> is suitable for such a task, with some preliminary work <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref> showing significant performance gains by increasing dataset size and modeling capacity of the weak learners.</p><p>In particular, the contributions of this work are as follows.  <ref type="figure">Fig. 1</ref>: By varying dataset properties with different augmentation techniques (a scaling example is shown) and model capacity (tree depth), we study limitations of the boosted decision tree classifier. Consequently, the insights revealed are employed in order to train state-of-the-art pedestrian and face detectors.  Limitations of a classifier: The relationship between increasing the modeling capacity of the decision trees jointly with dataset size has been established in previous literature arXiv:1701.01692v1 [cs.CV] 6 Jan 2017 <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, yet its limits have not been fully explored. Surveying current literature, we found inconsistent or limited usage of this relationship in boosting for object detection. In particular, it is not known whether the modeling capacity has saturated or whether it can be further increased with additional data for the detection task. Therefore, we sought to perform a rigorous study with the relationship between dataset size and model capacity as the main research question. In the process of studying this problem, insights regarding the limitations of the model arise. The conclusion of this study is mostly negative, as increasing the modeling capacity of the weak learners in the current boosted decision tree detector provides limited generalization power. Saturation of performance occurs quickly, with no further gains shown with additional data augmentation. Therefore, architectural changes are needed in order to resolve the issue of efficiently increasing modeling capacity in the boosted tree classifier.</p><p>Performance gains: The experiments in this paper result in consistent gains in object detection performance, insights into optimal dataset properties, and best training practices on Caltech. For instance, we demonstrate that the commonly employed video-based augmentation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref> on the Caltech training set is limited when compared to other augmentation options. With no significant modifications to the learning or feature extraction pipelines, we improve over the Aggregate Channel Feature (ACF) detector <ref type="bibr" target="#b14">[15]</ref> baseline by ∼10% points while still employing the 10 HOG+LUV feature channels. A simple filtering of the channels with 4 filter banks (LDCF <ref type="bibr" target="#b15">[16]</ref>) results in the best performing non-CNN detector on Caltech to date. The results of the study are shown to generalize to face detection as well. Note that we intentionally avoid using deep CNN features here, as these often involve training on a large external data. On the other hand, simple channel features <ref type="bibr" target="#b14">[15]</ref> allow for isolation of the strengths, limitations, and the role of the dataset on the boosting classifier. Furthermore, the careful examination of data augmentation and model capacity allows for a more appropriate comparison between CNN (where extensive data augmentation is commonly employed) and non-CNN detectors on datasets such as Caltech. We hope the more meaningful comparison will motivate future developments in object detection. We also note that several recent CNN-based models on Caltech <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref> make use of the boosted trees classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. EXPERIMENTAL SETTINGS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Boosting Framework</head><p>This section presents the tools that will be used for the remainder of the study. Boosting <ref type="bibr" target="#b21">[22]</ref> involves greedily minimizing a loss function for the following classification rule</p><formula xml:id="formula_0">H(x) = t α t h t (x)<label>(1)</label></formula><p>where h t (x) are weak learners, α t are scalars, and x ∈ R K is a feature vector. In this work, we employ the commonly used soft cascade with decision trees as the weak learners <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. These are composed of decision stumps such that each non-leaf node j produces a binary decision using a feature index k, a threshold τ ∈ R, and a polarity p ∈ {±1}, h j (x) ≡ p j sign(x[k j ] − τ j ). An important parameter is the maximum tree depth, which was generally taken to be 2 for object detection, yet recent studies <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b29">[30]</ref> propose depths of 3-4 as suitable to accommodate an increase in dataset size. Hence, our first issue is efficient training of boosted models on large datasets. The need for quick training is key to algorithmic development and competitive performance, especially when considering current state-of-theart CNN object detections are trained on millions of samples.</p><p>Training with randomness: In this work, we adopt the quick boosting approach from <ref type="bibr" target="#b30">[31]</ref>. Because stump training is costly, O(K × N ) for K features and N samples, a common practice is to quantize feature values into bins (256 in this work) and sample a random subset of the features when searching for the optimal feature index. Most approaches employ the ACF detector <ref type="bibr" target="#b14">[15]</ref>, which samples 1/16 of the total feature set in each iteration. We observed significant variation in performance over multiple runs with this random sampling strategy, by up to 3-4%. This is an issue, yet an appropriate discussion was not found in related literature. Therefore, we modify the last stage out of four of generating hard negatives with increasing number of weak classifiers, [64, 256, 1024, 4096]. In the last stage, the feature set is exhaustively searched over for the initial 512 weak learners, after which random sampling of 1/16 is followed. This procedure was found to be necessary for careful analysis, and it balances reproducibility with speed (unlike the very slow exhaustive search in all iterations <ref type="bibr" target="#b22">[23]</ref>) reducing variability to within ∼1% in performance. In experiments where significance is uncertain, multiple random seeds are used for training initialization, with the best one shown. Batch training is another possible direction which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Augmentation Techniques</head><p>As previously mentioned in Section I, large gains in boosting-based pedestrian detection came from dataset size increase and increased depth of the decision tree (introduced in Nam et al. <ref type="bibr" target="#b15">[16]</ref>), although the design choices for these have not been well justified or studied. Consequently, existing state-ofthe-art detectors employ dense frame sampling (e.g. sampling every 3 rd frame resulting in a ten-fold increase in dataset size, Caltech×10), yet this type of video-based augmentation will be shown to present several issues leading to sub-optimal detector performance.</p><p>Identifying the most successful augmentation techniques is essential, as the increased dataset size regularizes training of models with increased capacity. We note that others have studied data augmentation, specifically for pedestrian detection, but mostly with a goal of lowering annotation effort (i.e. render pedestrian images into real backgrounds <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>) and not improved performance.</p><p>In addition to sampling more frames from the Caltech videos, the following augmentations are studied:</p><p>Color: Color alteration has been proposed in <ref type="bibr" target="#b0">[1]</ref> using Principal Component Analysis (PCA) on the RGB pixel values in the training set. Specifically, a random Gaussian variable, α ∈ R 3 is drawn and the quantity [</p><formula xml:id="formula_1">p 1 , p 2 , p 3 ][α 1 λ 1 , α 2 λ 2 , α 3 λ 3 ],</formula><p>where p i and λ i are eigenvector-eigenvalue pairs, is added to the RGB image pixels.</p><p>Flipping: Most current trained detectors employ horizontal mirroring of images in order to double the number of positive samples.</p><p>Translation (T): Two main parameters, namely the maximum amount of pixel shift allowed (m) and the sampling intervals (n), determine how many additional positive samples are generated. We note these in the experiments as Tnm. For instance, T31 generates 4 additional samples at 1 pixel shift in each direction (west/east/north/south).</p><p>Scale+Crop (S): As experiments began, we discovered high sensitivity of the classifier to even small noise injected in the ground truth location. As an alternative to translation, we propose to scale each positive sample by a factor (either in the horizontal, vertical, or both directions), re-center, and crop it (shown in <ref type="figure">Fig. 1</ref>). Hence, the procedure is slightly different from the common cropping employed in training CNN detectors (corner/center cropping). This augmentation is similar to slight ground truth width/height jitter without changes to the center of the box.</p><p>Occlusion: Occlusion handling is an important challenge for pedestrian detection <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Incorporation of samples with higher levels of occlusion is another technique for increasing dataset size and studying generalization capability.</p><p>FG/BG Transfer: The visibility masks of pedestrians in Caltech can be employed in order to transfer foreground/background between images in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Settings</head><p>Our training settings specify a model size of 41 × 100 pixels. With padding, the sliding window becomes 64 × 128 pixels. In order to deal with smaller pedestrians ('reasonable' test settings involve pedestrians of height 50 pixels and up), we upsample the images by one octave. Most of the augmentation experiments will be performed on Caltech×3 (sampling every 10 th frame from video), but higher samplings of Caltech×7.5 (every 4 th frame), Caltech×15, and Caltech×30 (every frame) sampling will also be studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL ANALYSIS</head><p>This section demonstrates the importance of data augmentation, impact of augmentation type, and significance of increased weak classifier model capacity in training a generalizable pedestrian detector. All results are measured in logaverage miss rate (a lower value corresponds to better detection performance) as described in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Baseline and notation: Our experiments begin with <ref type="figure">Fig.  3</ref>(a), where we first verify the impact of randomness with multiple random seeds in training (see Section II-A) on performance variation using Caltech×3. The L3N 50 model is the commonly trained maximum depth 3 model using the available implementation of <ref type="bibr" target="#b5">[6]</ref> with 50k total negative samples (in each hard negative mining round we collect N/2 samples and replace these hard samples so that no more than N samples are trained over in a given round). We note that the only form of augmentation from Section II-B commonly employed in training boosting-based pedestrian detectors besides higher frame sampling from video is flip augmentation. Hence, flip augmentaiton is included in the L3N 50 baseline. It achieves comparable results to the best known ACF results on Caltech of 29.76% log-average miss rate <ref type="figure">(Fig. 2)</ref>.</p><p>Flip augmentation: Surprisingly, <ref type="figure">Fig. 3(a)</ref> depicts how the removal of the flip augmentation in the L3N 50 baseline results in a significant reduction in log-average miss rate from 29.28% to 26.67%! The reason for the performance reduction becomes apparent immediately, when aspect ratio (ar) standardization (used in training/testing all current object detectors on Caltech <ref type="bibr" target="#b5">[6]</ref>) is removed during training in L3N 50/ar. Specifically, the standardization is useful in training since it is enforced in evaluation, yet it leads to non-aligned bounding boxes. Flipping these boxes introduces additional poorly-aligned samples which the boosting model does not handle well, even with increased modeling capacity in later stages of the experiments (depth 5 or more models). The solution is simple, either train L3N 50/ar models (without aspect ratio standardization and with flip augmentation) or remove flipped sampling (L3N 50/f lip). Since this is a one-time initial modification, we slightly rename our 'new baselines', so that L3N 50 refers to L3N 50/f lip (without flip, with aspect ratio standardization of training samples) and L3N 50 * refers to L3N 50/ar (with flip, no aspect standardization). The two are shown to perform similarly in Figs. 3(a) and 3(b) (yet L3N 50/f lip is faster to train as it contains half the samples). We note that this observation was not made in any of the related research studies, and consequently not employed by top performing boosted detectors.</p><p>Additional augmentation: Depicted in <ref type="figure">Fig. 3(b)</ref>, the L3N 50 and the L3N 50 * models do not benefit from color augmentation, but do benefit from the scaling+crop scheme (best with a scaling factor of S1.1 in horizontal, vertical, and both directions) and translation augmentation (by 1 pixel shifts, T 31). These experiments were run multiple times with different random seeds to ensure statistical significance. The improvements will be shown to be more dramatic with increased modeling capacity (deeper trees). We make two final notes, increasing the number of negatives to L3N 100 does not further improve performance, and overall performance has reached a plateau. Therefore, the modeling capacity is increased in L5N 50.</p><p>Increased model capacity: Still on Caltech× 3, the analysis of <ref type="figure">Fig. 3(c)</ref> demonstrates a consistent trend with the previous experiments. Note how flip augmentation in L5N 50 + f lip is still not beneficial, but it is much better handled. The best results are reported in <ref type="figure">Fig. 3(c)</ref>. We observe a significant jump in performance reaching 20.85% with the fast ACF model. These are the best reported results to date (nearly optimal from our experiments). Scaling is shown to be more useful than false positives per image <ref type="bibr" target="#b9">10</ref>   translation, as even slight off-center shifts hinder performance and are not handled well by the classifier. This is encouraging, yet we begin to understand how limited the boosting model is in sensitivity to dataset properties. Samples with even minimal occlusion and FG/BG transfer were also not shown to be beneficial for increasing the dataset size. Furthermore, we experimented with addition of external pedestrian datasets (e.g. ETH <ref type="bibr" target="#b35">[36]</ref>) and increasing tree depth, but no benefit was shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video augmentation:</head><p>A main conclusion is that very little video augmentation (Caltech×3 vs. Caltech×10 in current state-of-the-art detectors <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b22">[23]</ref>) was needed to reach best performance. Denser sampling of every 4 th frame is shown in <ref type="figure" target="#fig_1">Fig. 4(a)</ref>, demonstrating an interesting trend. First, we must increase the number of negatives to 200k (N 200) to reach comparable performance. This fact exposes another known limitation of currently used boosting models which is not reported in related research, one of dataset imbalance. This is necessary for implicitly regularizing deeper models (shown up to depth 8 in <ref type="figure" target="#fig_1">Fig. 4(a)</ref>). Second, we observe that the S1.1 scheme provides consistent improvements across different video sampling strategies and model choices ( <ref type="figure" target="#fig_1">Fig.  4(b)</ref>). This demonstrates the sub-optimal yet commonly employed procedure of video augmentation. Third, additional augmentation or increased model capacity does not improve over the 20.69% of L5N 200. To ensure no further gains can be made, we further increase tree depth to 6 to handle additional data and further increase dataset in <ref type="figure" target="#fig_1">Fig. 4(b)</ref>, with no visible improvement. Increasing tree depth beyond 6 leads to overfitting behavior, and additional data must be sampled (although our experiments show no final gains).</p><p>Better features: The achieved 20.69% miss rate is the best known ACF results to date, nearly matching the more computationally intensive feature-rich Checkerboards (CB) detector <ref type="bibr" target="#b36">[37]</ref> (18.47% miss rate). CB employs 61 filters for each of the 10 core HOG+LUV channels, resulting in significant increase to computational requirements. To further study the proposed modifications, we extract features using 4 8 × 8 LDCF filters <ref type="bibr" target="#b15">[16]</ref> computed using PCA eigenvectors of feature patches (referred to as the LDCF84 model). The method still runs 5 times faster than the CB comparison, while reaching a 17.15% miss rate. This demonstrates the generalization of the proposed best practices to other, feature-rich approaches as well. As a final experiment, we report results without the approximation of features in the multi-scale feature pyramid <ref type="bibr" target="#b14">[15]</ref>. This allows for a more clear performance comparison against other methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b36">[37]</ref> which do not employ approximation. The gains are even more impressive, reaching 15.40% with a lightweight LDCF84 model, significantly outperforming CB.</p><p>Summary: Despite impressive gains in performance stemming from simple, intuitive, and theory-driven considerations in training, the boosted detector can only handle mild deformations as augmentation. For instance, color jitter is not helpful, flipping is problematic (and removed in most of the experiments without reduction in performance), and slight translation shifts produce a more difficult learning task that is not modeled well even with deeper trees. In order to address these limitations, we proposed to use an augmentation technique which maintains centered ground truth boxes. Dataset size is key in training deeper trees, yet performance saturates at depth 5/6 even with extensive augmentation. Hence, unlike additional layers in CNN architectures, further increase of the tree depth provides a limited benefit in terms of classification power. The experiments motivate future work for the effective increase of the modeling capacity of tree models.</p><p>Run-time analysis: Although not the main focus of the study, we report run-time for the interested reader. The ACF-L5 and LDCF84 models run on a CPU at 6.7 and 2.5 frames per second (fps), respectively. These models are significantly faster than all of the top performing feature-rich models, which range in 0.1-1 fps (e.g. CB <ref type="bibr" target="#b22">[23]</ref> runs at 0.5 fps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Context Analysis</head><p>Context in the form of scene representation is essential for robust pedestrian detection. The deeper tree models are able to better capture relationships between features in the image. In particular, we have observed how deeper models are more likely to select features in the padding area of the model around the pedestrian. As a final limitation experiment on Caltech, we propose to use a location prior model in order to study to limitations of the model in capturing spatial context.</p><p>First, we studied increasing model padding further with tree depth as means of better representing contextual information, yet this did not impact performance. On the other hand, due to the application domain, it is reasonable to study context in terms of perspective and position of objects. A second approach was shown to improve performance further, hinting that there is still room for future improvements in context modeling.</p><p>Given a detection, p = x y w h s , we construct φ p = pp for each training sample. Consequently, entries in φ p are employed as features for capturing relationship between position, size, confidence features, and their products. The object score, s, is recomputed using an SVM <ref type="bibr" target="#b37">[38]</ref> on φ p . We note that this generalizes (and outperforms) the approach in <ref type="bibr" target="#b38">[39]</ref>, which only employs a subset of the proposed φ p feature set (setting φ p = h 2 y 2 hy h y ).</p><p>As shown in <ref type="figure">Fig. 2</ref>, context integration consistently improve performance of both the ACF and LDCF84 models by a further 0.5-1% reduction in miss rate. Hence, there is still room for improvement in order to better capture contextual cues within the models. We term our models, ACF++ and LDCF84++, where the first '+' stands for the final models trained with data augmentation <ref type="figure" target="#fig_1">(Fig. 4)</ref>, and the second '+' for the contextual reasoning. We observe how the proposed modifications produce consistent improvements over both the ACF and LDCF baselines in <ref type="figure">Fig. 2</ref> by about 10%. The proposed high-performing models are new baselines in a way,  and further gains may be obtained with combined with insights from other studies listed in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on Caltech-New</head><p>The proposed techniques are tested on the improved annotation Caltech pedestrians dataset presented in <ref type="bibr" target="#b25">[26]</ref>, as shown in <ref type="table" target="#tab_3">Table I</ref>. The cleaner annotations have a great impact on training the ACF++ and LDCF++ models, which is consistent with our observations in previous experiments. Consequently, the best detection results to date are achieved among both CNN and non-CNN methods on the challenging MR N -4 metric, with a miss rate of 18.29%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalization to Face Detection</head><p>The proposed training procedure is applied to face detection on the recently introduced large WIDER face dataset <ref type="bibr" target="#b39">[40]</ref> and the commonly employed FDDB dataset <ref type="bibr" target="#b40">[41]</ref>. Specifically, we do not standardize aspect ratio and further augment the dataset as discussed in Section III. As shown in <ref type="table" target="#tab_3">Table II</ref> on a validation set, the detection performance improve when up to depth 9 decision trees are employed. This is due to the new challenges in face detection benchmarks (e.g. rotation) not commonly found in pedestrian detection settings. <ref type="figure">Fig. 6</ref> demonstrates consistent gains in performance, resulting in a state-of-theart face detector (on FDDB, at 2000 false positives, 93.37% true positive rate is achieved). On the WIDER test set, <ref type="figure">Fig.  5</ref> depicts a large improvement over the ACF baseline results reported in <ref type="bibr" target="#b39">[40]</ref>. The performance gap is larger on the more challenging 'moderate' and 'hard' test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUDING REMARKS</head><p>This study presented a set of novel experiments aimed at better understanding the boosted decision tree model currently </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Easy set</head><p>Moderate set Hard set <ref type="figure">Fig. 5</ref>: Results on the test set of the WIDER face dataset <ref type="bibr" target="#b39">[40]</ref> (our curves are shown in red, LDCF+) compared with the current state-of-the-art (updated plots on January 5, 2017), Multiscale Cascade CNN <ref type="bibr" target="#b39">[40]</ref>, Two-stage CNN <ref type="bibr" target="#b39">[40]</ref>, ACF <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b46">[47]</ref>, Faceness <ref type="bibr" target="#b42">[43]</ref>, Multitask Cascade CNN <ref type="bibr" target="#b51">[52]</ref>, and CMS-RCNN <ref type="bibr" target="#b52">[53]</ref>. Results are shown for the three difficulty settings described in <ref type="bibr" target="#b39">[40]</ref>. Observe the large improvement in performance over the ACF baseline.   <ref type="figure">Fig. 6</ref>: Results on the FDDB <ref type="bibr" target="#b40">[41]</ref> dataset (discrete score evaluation) compared with the state-of-the-art (DP2MFD <ref type="bibr" target="#b41">[42]</ref>, Faceness <ref type="bibr" target="#b42">[43]</ref>, HeadHunter <ref type="bibr" target="#b43">[44]</ref>, Kumar et al. <ref type="bibr" target="#b44">[45]</ref>, Mul-tiresHPM <ref type="bibr" target="#b45">[46]</ref>, ACF-multiscale <ref type="bibr" target="#b46">[47]</ref>, CCF <ref type="bibr" target="#b18">[19]</ref>, CascadeCNN <ref type="bibr" target="#b47">[48]</ref>, DDFD <ref type="bibr" target="#b48">[49]</ref>, NPDFace <ref type="bibr" target="#b49">[50]</ref>, and Pico <ref type="bibr" target="#b50">[51]</ref>). True positive rate at 2000 false positives is shown for each method. Our results significantly improve over previous state-of-theart with HOG+LUV features <ref type="bibr" target="#b43">[44]</ref>, while competing with CNN approaches. employed for many vision tasks. As CNN-based models employ extensive augmentation, we sought to investigate modifying the boosted detector to handle such augmentation as well. Careful analysis regarding the generalization power and modeling capacity, dataset size and balance, and overfitting handling produced insights as to the performance limits of such detectors, as well as state-of-the-art pedestrian and face detectors. A main takeaway of this work is in the limited representation ability of the boosted model. Unlike CNN models, which consistently benefit from additional modeling capacity (e.g. depth) and data, increasing weak learner complexity saturates early, not allowing for further gains with deeper decision trees even with extensive augmentation. Although careful data augmentation was beneficial, architectural changes for handling large datasets are required on the decision-tree training level for effective increase of modeling capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Experimental analysis of different training settings on Caltech pedestrians using additional video augmentation (Caltech×7.5 unless stated).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Comparison of our key results (in thick lines, ACF++ and LDCF84++) with published methods on the Caltech test set. Our version of the popular ACF and LDCF models demonstrate large gains in detection performance over the baselines with no significant modifications to the model or feature extraction pipeline.</figDesc><table><row><cell></cell><cell>1</cell><cell>29.76% ACF−Caltech+</cell></row><row><cell></cell><cell></cell><cell>26.21% DeepCascade+</cell></row><row><cell></cell><cell>.80</cell><cell>24.80% LDCF</cell></row><row><cell></cell><cell>.64</cell><cell>23.32% SCF+AlexNet</cell></row><row><cell></cell><cell></cell><cell>22.49% Katamari</cell></row><row><cell></cell><cell>.50</cell><cell>21.89% SpatialPooling+</cell></row><row><cell></cell><cell>.40</cell><cell>21.86% SCCPriors</cell></row><row><cell></cell><cell></cell><cell>20.86% TA−CNN</cell></row><row><cell></cell><cell>.30</cell><cell>18.47% Checkerboards</cell></row><row><cell></cell><cell></cell><cell>17.32% CCF+CF</cell></row><row><cell>miss rate</cell><cell>.20</cell><cell>17.10% Checkerboards+ 11.89% DeepParts 11.75% CompACT−Deep 19.71% ACF++</cell></row><row><cell></cell><cell></cell><cell>14.98% LDCF84++</cell></row><row><cell></cell><cell>.10</cell><cell></cell></row><row><cell></cell><cell>05</cell><cell></cell></row><row><cell></cell><cell>false positives per image</cell><cell></cell></row><row><cell cols="2">Fig. 2:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Experimental analysis of different training settings on Caltech pedestrians using Caltech×3.</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell>.80,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.80,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.80,</cell></row><row><cell></cell><cell>.64,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.64,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.64,</cell></row><row><cell></cell><cell>.50,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.50,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.50,</cell></row><row><cell></cell><cell>.40,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.40,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.40,</cell></row><row><cell></cell><cell>.30,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.30,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.30,</cell></row><row><cell>miss rate</cell><cell>.20,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>miss rate</cell><cell>.20,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>miss rate</cell><cell>.20,</cell><cell>25.21% L5N50 25.70% L5N50+flip 25.12% L5N50*</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23.90% L5N50+T31</cell></row><row><cell></cell><cell>.10,</cell><cell cols="2">30.45% L3N50-seed0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.10,</cell><cell cols="2">30.43% L3N50*+Color 27.74% L3N50*+S1.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.10,</cell><cell>25.11% L5N50+T32 25.44% L5N50+T62</cell></row><row><cell></cell><cell></cell><cell cols="2">29.30% L3N50-seed1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">26.14% L3N50*+S1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.00% L5N50+T93</cell></row><row><cell></cell><cell></cell><cell cols="2">29.28% L3N50-seed2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">26.67% L3N50*+S1.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20.85% L5N50+S1.1</cell></row><row><cell></cell><cell></cell><cell cols="2">26.46% L3N50/ar</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">26.57% L3N50*+S1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>24.71% L5N50+S1.05</cell></row><row><cell></cell><cell>.05,</cell><cell cols="2">26.67% L3N50/flip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.05,</cell><cell cols="2">25.85% L3N50+S1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.05,</cell><cell>26.63% L5N50+S1.15</cell></row><row><cell></cell><cell></cell><cell cols="2">26.81% L3N50/ar+flip</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">26.28% L3N50+T31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>21.36% L5N100+S1.1</cell></row><row><cell></cell><cell></cell><cell>-3</cell><cell>10 -2</cell><cell>10 -1</cell><cell>10 0</cell><cell>10 1</cell><cell></cell><cell></cell><cell>10 -3</cell><cell>10 -2</cell><cell>10 -1</cell><cell>10 0</cell><cell>10 1</cell><cell></cell><cell>10 -3</cell><cell>10 -2</cell><cell>10 -1</cell><cell>10 0</cell><cell>10 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">false positives per image</cell><cell></cell><cell></cell><cell>false positives per image</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b)</cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell></row><row><cell>miss rate</cell><cell>.05, .10, .20, .30, .40, .50, .64, .80, 1</cell><cell cols="4">10 -3 25.02% L5N50 10 -2 24.82% L5N50+S1.1 21.97% L5N100+S1.1 20.69% L5N200+S1.1 20.77% L6N200+S1.1 21.74% L6N200+S1.1+T31 10 -1 21.63% L6N400+S1.1 22.20% L6N800 23.75% L7N200+S1.1 27.70% L8N50+S1.1 Fig. 3: false positives per image 10 0 22.88% L8N200+S1.1</cell><cell>10 1</cell><cell>miss rate</cell><cell>.05, .10, .20, .30, .40, .50, .64, .80, 1</cell><cell cols="4">false positives per image 10 -2 10 -1 10 0 24.00% L6N200 (x15) 10 -3 22.57% L6N200+S1.1 (x15) 23.68% L6N400 (x15) 21.60% L6N400+S1.1 (x15) 24.42% L6N400 (x30) 22.15% L6N400+S1.1 (x30)</cell><cell>10 1</cell><cell>miss rate</cell><cell>.05, .10, .20, .30, .40, .50, .64, .80, 1</cell><cell>false positives per image 10 -2 10 -1 10 0 21.66% L5N200*+S1.1 10 -3 20.69% L5N200+S1.1 20.47% L5N200+S1.1/approx 17.15% LDCF84+S1.1 15.40% LDCF84+S1.1/approx</cell><cell>10 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b)</cell><cell></cell><cell></cell><cell></cell><cell>(c)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Results on the new annotations on Caltech<ref type="bibr" target="#b25">[26]</ref> using log average miss rate over [10 −2 , 10 0 ] and [10 −4 , 10 0 ] (MR N -2 and MR N -4 , respectively). Our ACF+ (with data augmentation) and ACF++ (with contextual reasoning) is on par with state-of-the-art detectors while enjoying low computational complexity. Our LDCF84++ employs just 4 filters on top of the ACF features and outperforms the VGG baseline by a large margin on the more challenging metric MR N -4 .</figDesc><table><row><cell>Method</cell><cell>MR N -2 ( MR N -4 )</cell></row><row><cell>RotatedFilters [26]</cell><cell>12.87 (24.10)</cell></row><row><cell>RotatedFilters+VGG [26]</cell><cell>9.32 (21.72)</cell></row><row><cell>ACF+</cell><cell>15.17 (27.28)</cell></row><row><cell>ACF++</cell><cell>13.27 (25.26)</cell></row><row><cell>LDCF84+</cell><cell>11.76 (21.08)</cell></row><row><cell>LDCF84++</cell><cell>9.71 (18.29)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Improvement due to incorporation of the insights in this paper when training face detection models.</figDesc><table><row><cell>Results on WIDER-validation</cell><cell></cell></row><row><cell>Method</cell><cell>AP (%)</cell></row><row><cell>ACF-L3N 50 *</cell><cell>27.97</cell></row><row><cell>ACF-L9N 200 *</cell><cell>34.64</cell></row><row><cell>ACF-L9N 200 * +S1.1 (ACF+)</cell><cell>46.80</cell></row><row><cell>LDCF84-L9N 200 * +S1.1 (LDCF+)</cell><cell>47.66</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We acknowledge support of associated industry partners and thank our UCSD CVRR colleagues, especially Rakesh Rajaram, for helpful discussions. We also thank the reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multistage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised scene adaptation for faster multi-scale pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient pedestrian detection via rectangular features based on a statistical shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The combinator: Optimal combination of multiple pedestrian detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Beeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goedemé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV-CVRSUAD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pedestrian detection at 100 frames per second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centersurround contrast features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pedestrian detection using boosted features over many frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Word channel based multiscale pedestrian detection without image resizing and using only one classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">PAMI</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to detect vehicles by clustering appearance patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection with spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast pedestrian detection for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Vesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boosting the margin: A new explanation for the effectiveness of voting methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On performance evaluation of driver hand detection algorithms: Challenges, dataset, and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITSC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bagging, boosting, and c4.5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Artifical Intelligence</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pedestrian detection with spatially pooled features and structured ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quickly boosting decision trees -pruning underachieving features early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pedestrian detection using augmented training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fredriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation of virtual and real worlds for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An exploration of why and when pedestrian detection fails</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITSC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Can appearance patterns improve pedestrian detection?&quot; in IV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Intell. Sys. and Tech</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiresolution models for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">FDDB: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A deep pyramid deformable part model for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1508.04389</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual phrases for exemplar face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Occlusion coherence: Detecting and localizing occluded faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno>abs/1506.08347</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aggregate channel features for multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCB</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multi-view face detection using deep convolutional neural networks,&quot; in ICMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A fast and accurate unconstrained face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A method for object detection based on pixel intensity comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frljak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Pandzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
		<idno>abs/1305.4537</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multi-task cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>abs/1604.02878</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">CMS-RCNN: contextual multi-scale region-based CNN for unconstrained face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno>abs/1606.05413</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
