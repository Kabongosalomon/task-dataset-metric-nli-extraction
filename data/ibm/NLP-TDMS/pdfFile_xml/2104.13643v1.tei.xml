<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Unreasonable Effectiveness of Centroids in Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolaj</forename><surname>Wieczorek</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Poland</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dabrowski</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Synerise Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Synerise Warsaw</orgName>
								<orgName type="institution">University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Synerise</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Unreasonable Effectiveness of Centroids in Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image retrieval task consists of finding similar images to a query image from a set of gallery (database) images. Such systems are used in various applications e.g. person re-identification (ReID) or visual product search. Despite active development of retrieval models it still remains a challenging task mainly due to large intra-class variance caused by changes in view angle, lighting, background clutter or occlusion, while inter-class variance may be relatively low. A large portion of current research focuses on creating more robust features and modifying objective functions, usually based on Triplet Loss. Some works experiment with using centroid/proxy representation of a class to alleviate problems with computing speed and hard samples mining used with Triplet Loss. However, these approaches are used for training alone and discarded during the retrieval stage. In this paper we propose to use the mean centroid representation both during training and retrieval. Such an aggregated representation is more robust to outliers and assures more stable features. As each class is represented by a single embeddingthe class centroid -both retrieval time and storage requirements are reduced significantly. Aggregating multiple embeddings results in a significant reduction of the search space due to lowering the number of candidate target vectors, which makes the method especially suitable for production deployments. Comprehensive experiments conducted on two ReID and Fashion Retrieval datasets demonstrate effectiveness of our method, which outperforms the current stateof-the-art. We propose centroid training and retrieval as a viable method for both Fashion Retrieval and ReID applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Instance retrieval is a problem of matching an object from a query image to objects represented by images from a gallery set. Applications of retrieval systems span person/vehicle re-identification, face recognition, video surveillance, explicit content filtering, medical diagnosis and fashion retrieval.</p><p>Most existing instance retrieval solutions use Deep Metric Learning methodology <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">16]</ref>, in which a deep learning model is trained to transform images to a vector representation, so that samples from the same class are close to each other. At the retrieval stage, the query embedding is scored against all gallery embeddings and the most similar ones are returned. Until recently, a lot of works used classification loss for the training of retrieval models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20]</ref>. Currently most works use comparative/ranking losses and the Triplet Loss is one of the most widely used approaches. However, state-of-the-art solutions often combine a comparative loss with auxiliary losses such as classification or center loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">16]</ref>. * Both authors contributed equally to this research.</p><p>(a) Centroid-based retrieval (b) Instance-based retrieval <ref type="figure">Figure 1</ref>: Comparison of centroid-based and instance-based retrieval. Dashed lines indicate distance between the query image (coloured frame) and the nearest neighbour from each class. a) The centroid is calculated as the mean of all samples (shaded images) belonging to each class. The query is assigned the class of the nearest centroid, which is the correct "gold" class. b) The distance is calculated between all samples and the query. It is erroneously assigned the "blue" class, as the blue-class sample is its nearest neighbour.</p><p>Even though Triplet Loss is superior to most other approaches, it has problems that were indicated by numerous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b21">21]</ref>: 1) Hard negative sampling is the dominant approach in creating training batches containing only informative triplets in a batch, but it may lead to bad local minima and prevent the model from achieving top performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18]</ref>; 2) Hard negative sampling is computationally expensive, as the distance needs to be calculated between all samples in the batch <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">16]</ref>; 3) Triplet Loss is prone to outliers and noisy labels due to hard negative sampling and the nature of point-to-point losses <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18]</ref> To alleviate problems stemming from the point-to-point nature of Triplet Loss, changes to point-to-set/point-to-centroid formulations were proposed, where the distances are measured between a sample and a prototype/centroid representing a class. Centroids are aggregations of each item's multiple representations. A centroid approach results in one embedding per item, decreasing both memory and storage requirements. There are a number of approaches investigating the prototype/centroid formulation and their main advantages are as follows: 1) Lower computational cost <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">16]</ref>, of even linear complexity instead of cubic <ref type="bibr" target="#b1">[2]</ref>; 2) Higher robustness to outliers and noisy labels <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18]</ref>; 3) Faster training <ref type="bibr" target="#b10">[11]</ref>; 4) Comparable or better performance than the standard point-to-point triplet loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>We propose to go a step further and use the centroid-based approach for both training and inference, with applications to fashion retrieval and person re-identification. We implement our centroidbased model by augmenting the current state-of-the-art model in fashion retrieval <ref type="bibr" target="#b12">[13]</ref> with a new loss function we call Centroid arXiv:2104.13643v1 [cs.CV] 28 Apr 2021  <ref type="bibr" target="#b12">[13]</ref> are marked in red.</p><p>Triplet Loss. The baseline model has a number of losses optimized simultaneously, which account for various aspects of the retrieval problem. An additional centroid-based loss can thus be easily added in order to amend one of the recurring problems: lack of robustness against variability in object galleries. Centroids are computed with simple averaging of image representations. We show that this straightforward model amendment allows to lower the latency of requests and decrease infrastructure costs, at the same time producing new state-of-the-art results in various evaluation protocols, datasets and domains. We also discuss why such formulation of the retrieval problem is viable and advantageous compared to standard image-based approaches.</p><p>The contributions of this work are fourfold:</p><p>• We introduce the Centroid Triplet Loss -a new loss function for instance retrieval tasks • We propose to use class centroids as representations during retrieval. • We show through thorough experiments that the centroidbased approach establishes new state-of-the-art results across different datasets and domains (fashion retrieval and person re-identification). • We show that the centroid-based approach for retrieval tasks brings significant inference speed-ups and storage savings compared to the standard instance-level approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROPOSED METHOD</head><p>The image retrieval task aims to find the most similar object to the query image. In both fashion retrieval and person re-identification it is usually done on an instance-level basis: each query image is scored against all images from the gallery. If an object has several images assigned (e.g. photos from multiple viewpoints, under variable lighting conditions), then each image is treated separately. As a result, the same object may occur multiple times in the ranking result. Such a protocol can be beneficial as it allows to match images that were taken in similar circumstances, with similar angle, depicting the same part of the object or a close-up detail. On the other hand, the advantage can easily turn disadvantageous as a photo of a detail of a completely different object may be similar to the details in the query image, causing a false match.</p><p>We propose to use an aggregated item representation using all available samples. This approach results in a robust representation which is less susceptible to a single-image false matches. Using aggregated representations, each item is represented by a single embedding, leading to a significantly reduced search spacesaving memory and reducing retrieval times significantly. Apart from being more computationally efficient during retrieval, the centroid-based approach also improves retrieval results compared to non-centroid-based approaches.Note that training the model in a centroid-based setting does not restrict the evaluation protocol to centroid-only evaluation, but also improves results in the typical setting of instance-level evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Centroid Triplet Loss</head><p>Triplet Loss originally works on an anchor image , a positive (same class) example and a negative example belonging to another class . The objective is to minimize the distance between − , while push away the sample. The loss function is formulated as follows:</p><formula xml:id="formula_0">L = ∥ ( ) − ( )∥ 2 2 − ∥ ( ) − ( )∥ 2 2 + +<label>(1)</label></formula><p>where [ ] + = ( , 0), denotes embedding function learned during training stage and is a margin parameter.</p><p>We propose the Centroid Triplet Loss (CTL). Instead of comparing the distance of an anchor image to positive and negative instances, CTL measures the distance between and class centroids and representing either the same class as the anchor or a different class respectively. CTL is therefore formulated as:</p><formula xml:id="formula_1">L = ∥ ( ) − ∥ 2 2 − ∥ ( ) − ) ∥ 2 2 + + (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Aggregating item representations</head><p>During training stage each mini-batch contains distinct item classes with samples per class, resulting in batch size of × . Let denote a set of samples for class in the mini-batch such that S = { 1 , ..., } where represents an embedding of i-th sample, such that ∈ , with being the sample representation size. For effective training, each sample from S is used as a query and the rest − 1 samples are used to build a prototype centroid , which can be expressed as:</p><formula xml:id="formula_2">= 1 |S \ { }| ∑︁ ∈S \{ } ( )<label>(3)</label></formula><p>where represents the neural network encoding images to dimensional embedding space.</p><p>During evaluation query images are supplied from the query set Q, and centroids for each class are precalculated before the retrieval takes place. To construct these centroids we use all embeddings from the gallery set G for class . The centroid of each class ∈ is calculated as the mean of all embeddings belonging to the given class:</p><formula xml:id="formula_3">= 1 |G | ∑︁ ∈ G ( )<label>(4)</label></formula><p>We apply centroid computation and CTL to the fashion retrieval state-of-the-art model described in <ref type="bibr" target="#b12">[13]</ref>. This model embeds images with a baseline CNN model (using variations of the ResNet architecture) and passes them through a simple feed-forward architecture with average pooling and batch normalization. Three separate loss functions are computed at various stages of forward propagation. We add centroid computation for training just after embedding with the CNN. Centroids for inference are computed in the next step (after batch normalization) for consistency with the original model. The resulting architecture is displayed in <ref type="figure" target="#fig_0">Figure 2</ref>. Note that our centroid-based training and evaluation method can be also transplanted to other models, as CTL can be computed next to other existing loss functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Datasets</head><p>DeepFashion (Fashion Retrieval). The dataset was introduced by [6] an contains over 800,000 images, which are spread across several fashion related tasks. The data we used is a Consumer-to-shop Clothes Retrieval subset that contains 33,881 unique clothing products and 239,557 images.</p><p>Street2Shop (Fashion Retrieval). The dataset contains over 400,000 shop photos and 20,357 street photos. In total there are 204,795 distinct clothing items in the dataset. It is one of the first modern large-scale fashion dataset and was introduced by <ref type="bibr" target="#b3">[4]</ref>.</p><p>Market1501 (Person Re-identification). Introduced in <ref type="bibr" target="#b19">[19]</ref> in 2015, it contains 1501 classes/ identities scattered across 32,668 bounding boxes and captured by 6 cameras at Tsinghua University. 751 classes are used for training, and 750 with distractors are used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DukeMTMC-reID (Person Re-identification).</head><p>It is a subset of DukeMTMC dataset <ref type="bibr" target="#b8">[9]</ref>. It contains 1,404 classes/identities, 702 are used for training and 702 along with 408 distractor identities are used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implement our centroid-based solution on top of the current fashion retrieval state-of-the-art model <ref type="bibr" target="#b12">[13]</ref>, which itself is based on a top-scoring ReID model <ref type="bibr" target="#b6">[7]</ref>. We train our model on various Resnet-based backbones pretrained on ImageNet, and report results for Fashion Retrieval and Person Re-Identification tasks. We evaluate the model both in centroid-based and instance-based setting. Instance-based setting means that pairs of images are evaluated, identically as in the evaluation setting of <ref type="bibr" target="#b12">[13]</ref>. We use the same training protocol presented in the aforementioned papers (e.g. random erasing augmentation, label smoothing), without introducing any additional steps.</p><p>Feature extractor. We test two CNNs: Resnet-50 and Resnet50-IBN-A to compare our results on those two networks. Like <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, we use = 1 for the last convolutional layer and Resnet-50 native 2048 dimensional embedding size.</p><p>Loss functions. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> use a loss function consisting of three parts: (1) Triplet loss calculated on the raw embeddings, (2) Center Loss <ref type="bibr" target="#b11">[12]</ref> as an auxiliary loss, (3) classification loss computed on batchnormalized embeddings. To train our model based on centroids we use the same three losses and add CTL, which is computed between query vectors and class centroids. Center Loss was weighted by a factor of 5 −4 , all other losses were assigned a weight of 1.</p><p>Our Fashion Retrieval parameter configuration is identical as in <ref type="bibr" target="#b12">[13]</ref>. We use Adam optimizer with base learning rate of 1 −4 and multistep learning rate scheduler, decreasing the learning rate by a factor of 10 after 40 th and 70 th epoch. Like in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> the Center Loss was optimized separately by SGD optimizer with = 0.5. Each model was trained 3 times, for 120 epochs each. For Person Re-Identification, the configuration is identical as in <ref type="bibr" target="#b6">[7]</ref>. The base learning rate is 3.5 −4 , decayed at 40 th and 70 th epoch. The models were trained for 120 epochs each.</p><p>Resampling. For Triplet Loss it is important to have enough positive samples per class, but some classes may have few samples. Therefore it is a common practice to define a target sample size and resample class instances if |S | &lt; , resulting in repeated images in the mini-batch. We empirically verify that in our scenario it is beneficial to omit the resampling procedure. As resampling introduces noise to class centroids, we use only the unique class instances which are available.</p><p>Retrieval procedure. We follow <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b12">[13]</ref> in utilizing batchnormalized vectors during inference stage. Likewise, we use cosine similarity as the distance measure. For the ReID datasets we use a cross-view matching setting, which is used in other ReID papers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. This protocol ensures that for each query its gallery samples that were captured by the same camera are excluded during retrieval.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fashion Retrieval Results</head><p>We present the evaluation results for fashion retrieval in <ref type="table" target="#tab_0">Table 1</ref>. We evaluate two models: SOTA denotes the model presented in <ref type="bibr" target="#b12">[13]</ref>, and CTL -our centroid-based model. Each model was evaluated in two modes: 1) standard instance-level evaluation on per-image basis (for both SOTA and CTL models), and 2) centroid-based evaluation, (denoted by CE in <ref type="table" target="#tab_0">Table 1)</ref>: evaluation of CTL model on per-object basis, where all images from each class were used to build the class centorid and retrieval was done in centroid domain. Our CTL model performs better than the current state-of-theart in most metrics across all tested datasets. Especially noticeable is the surge in mAP metric, which can be explained by the fact that usage of centroids reduces the search space. The reduction of the search space with centroid-based evaluation is coupled with reduction of the number of positive instances (from several to just one). Accuracy@K metrics on the other hand are not influenced by the change of search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Person ReID Results</head><p>We present the evaluation results for person re-identification in <ref type="table" target="#tab_2">Table 3</ref>. Similarly as in fashion retrieval, we evaluate the following models: SOTA denotes the current state-of-the-art in ReID <ref type="bibr" target="#b9">[10]</ref>, and CTL -our centroid-based model. We only report centroid-based evaluation results for CTL-model, as previous methods often restrict the search space arbitrarily. For example, <ref type="bibr" target="#b9">[10]</ref> (the current SOTA on both ReID test datasets) reduce the search space during retrieval with spatial and temporal constraints to decrease the number of candidates by eliminating cases where the person could not have possibly moved by a certain distance in the given time. Their approach requires extra information in the dataset and world knowledge necessary to construct the filtering rules, apart from just image understanding. Despite reliance on image matching alone, our centroid-based search space reduction achieves nearly the same or even better results across all metric on both datasets, outperforming <ref type="bibr" target="#b9">[10]</ref> across most metrics and establishing the new state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Memory Usage and Inference Times</head><p>To test memory and computation efficiency of our centroid-based method compared to standard image-based retrieval, we compare the wall-clock time taken for evaluating all test datasets and the storage required for saving all embeddings. <ref type="table" target="#tab_1">Table 2</ref> shows the statistics for all datasets for instance-level and centroid-based scenarios. It can be seen that the centroid-based approach significantly reduces both retrieval time and the disk space required to store the embeddings. The reduction is caused by the fact that there are often several images per class, thus representing a whole group of object images with a centroid reduces the number of vectors necessary for a successful retrieval to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS</head><p>We introduce Centroid Triplet Loss -a new loss function for instance retrieval tasks. We empirically confirm that it significantly improves the accuracy of retrieval models. In addition to the new loss function, we propose the usage of class centroids during retrieval inference, further improving the accuracy metrics on retrieval tasks. Our methods are evaluated on four datasets from two different domains: Person Re-identification and Fashion Retrieval, and establish new state-of-the-art results on all datasets. In addition to accuracy improvements, we show that centroid-based inference leads to very significant computation speedups and lowering of memory requirements. The combination of increased accuracy with faster inference and lower resource requirements make our method especially useful in applied industrial settings for instance retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of our CTL-Model. Parts added over</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Fashion Retrieval Results. S or L in the model name indicates input image size, either Small (256x128) or Large (320x320). R50 or R50IBN suffix indicates which backbone CNN was used, Resnet50 or Resnet50-IBN-A respectively. 'CE' at the end of model name denotes Centroid-based Evaluation.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="5">mAP Acc@1 Acc@10 Acc@20 Acc@50</cell></row><row><cell>DeepFashion</cell><cell cols="3">SOTA (S-R50) [13] CTL-S-R50 CTL-S-R50 CE SOTA (L-R50IBN) [13] 0.430 0.324 0.344 0.404 0.294 0.281 0.298 0.378 CTL-L-R50IBN 0.431 0.376 CTL-L-R50IBN CE 0.492 0.373</cell><cell>0.583 0.612 0.613 0.711 0.711 0.712</cell><cell>0.655 0.685 0.689 0.772 0.776 0.777</cell><cell>0.742 0.770 0.774 0.841 0.847 0.850</cell></row><row><cell></cell><cell>SOTA (S-R50) [13]</cell><cell>0.320</cell><cell>0.366</cell><cell>0.611</cell><cell>0.606</cell><cell>-</cell></row><row><cell>Street2Shop</cell><cell cols="3">CTL-S-R50 CTL-S-R50 CE SOTA (L-R50IBN) [13] 0.468 0.353 0.498 0.432 0.418 0.537 CTL-L-R50IBN 0.459 0.533</cell><cell>0.594 0.619 0.698 0.689</cell><cell>0.643 0.660 0.736 0.728</cell><cell>0.702 0.721 -0.782</cell></row><row><cell></cell><cell>CTL-L-R50IBN CE</cell><cell cols="2">0.598 0.537</cell><cell>0.709</cell><cell>0.750</cell><cell>0.792</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of storage and time requirements between instance and centroid-based models across tested datasets</figDesc><table><row><cell>Dataset</cell><cell>Mode</cell><cell># in gallery</cell><cell>Embeddings filesize (MB)</cell><cell>Total eval time (s)</cell></row><row><cell>Deep Fashion</cell><cell>Instances Centroids</cell><cell>22k 16k</cell><cell>175 130</cell><cell>81.35 59.83</cell></row><row><cell>Street2Shop</cell><cell>Instances Centroids</cell><cell>350k 190k</cell><cell>2700 1500</cell><cell>512.30 146.28</cell></row><row><cell>Market1501</cell><cell>Instances Centroids</cell><cell>16k 0.75k</cell><cell>120 6</cell><cell>4.75 0.26</cell></row><row><cell>Duke-MTMC</cell><cell>Instances Centroids</cell><cell>17k 1.1k</cell><cell>140 9</cell><cell>3.61 0.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Person Re-Identification Results</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">mAP Acc@1 Acc@5 Acc@10</cell></row><row><cell>Market1501</cell><cell cols="2">SOTA [10] 0.955 CTL-S-R50 0.983 0.980 0.980</cell><cell>0.989 0.986</cell><cell>0.991 0.995</cell></row><row><cell>Duke-MTMC-ReID</cell><cell cols="2">SOTA [10] 0.927 CTL-S-R50 0.961 0.956 0.945</cell><cell>0.968 0.962</cell><cell>0.971 0.979</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Similarity Reasoning and Filtration for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01368</idno>
		<ptr target="http://arxiv.org/abs/2101.01368" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A theoretically sound upper bound on the triplet loss for improving the efficiency of deep distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toan</forename><surname>Thanh Toan Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carneiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Combination of multiple global descriptors for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Jae</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung</forename><forename type="middle">Soo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Insik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongtack</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10663</idno>
		<ptr target="https://github.com/naver/cgd" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.382</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.382" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3343" to="3351" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Lagunes-Fortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<title level="m">Centroids Triplet Network and Temporally-Consistent Embeddings for In-Situ Object Recognition</title>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep-Fashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.124</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.124" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Strong Baseline and Batch Normalization Neck for Deep Person Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Gu</surname></persName>
		</author>
		<ptr target="https://github.com/michuanhaohao/reid-strong-baseline" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-Scale Image Retrieval with Attentive Deep Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.374</idno>
		<idno type="arXiv">arXiv:1612.06321</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.374" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-48881-3_2</idno>
		<idno type="arXiv">arXiv:1609.01775</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-48881-3_2" />
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9914</biblScope>
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatialtemporal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peigen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33018933</idno>
		<idno type="arXiv">arXiv:1812.03282</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33018933" />
	</analytic>
	<monogr>
		<title level="m">33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8933" to="8940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Centroid-based Deep Metric Learning for Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rudzicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brudno</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8683393</idno>
		<idno type="arXiv">arXiv:1902.02375</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2019.8683393" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3652" to="3656" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46478-7_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46478-7_31" />
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), Vol. 9911 LNCS</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Strong Baseline for Fashion Retrieval with Person Reidentification Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolaj</forename><surname>Wieczorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Michalowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Wroblewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Dabrowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications in Computer and Information Science</title>
		<imprint>
			<publisher>Springer Science and Business Media Deutschland GmbH</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1332</biblScope>
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-030-63820-7_33</idno>
		<idno type="arXiv">arXiv:2003.04094</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-63820-7_33" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.140</idno>
		<idno type="arXiv">arXiv:1604.07528</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.140</idno>
		<idno type="arXiv">arXiv:1604.07528</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss again: Learning robust person re-identification with fast approximated triplet loss and label distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/TAMU-VITA/FAT" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Classification is a Strong Baseline for Deep Metric Learning g</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12649v2</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rethinking Classification Loss Designs for Person Re-identification with a Unified View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04991</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.133</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.133" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00380</idno>
		<idno type="arXiv">arXiv:1905.00953</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00380" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="3701" to="3711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Point to set similarity based deep feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.534</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.534" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Yihong Gong, and Nanning Zheng</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
