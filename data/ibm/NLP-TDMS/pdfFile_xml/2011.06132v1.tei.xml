<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating a Local Translation Mechanism into Non-autoregressive Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
							<email>xiangk@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
							<email>zhisongz@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating a Local Translation Mechanism into Non-autoregressive Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way. We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence. We integrate LAT into the conditional masked language model (CMLM; Ghazvininejad et al., 2019) and similarly adopt iterative decoding. Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup. Further analysis indicates that our method reduces repeated translations and performs better at longer sentences. The code for our model is available at https://github. com/shawnkx/NAT-with-Local-AT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditional neural machine translation (NMT) models <ref type="bibr" target="#b19">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b3">Gehring et al., 2017;</ref><ref type="bibr" target="#b21">Vaswani et al., 2017)</ref> commonly make predictions in an incremental token-by-token way, which is called autoregressive translation (AT). Although this strategy can capture the full translation history, it has relatively high decoding latency. To make the decoding more efficient, non-autoregressive translation (NAT) <ref type="bibr" target="#b5">(Gu et al., 2018)</ref> is introduced to generate multiple tokens at once instead of one-by-one. However, with the conditional independence property <ref type="bibr" target="#b5">(Gu et al., 2018)</ref>, NAT models do not directly consider the dependencies among output tokens, which may cause errors of repeated translation and * Zhisong and Xiang contributed equally for this paper <ref type="figure">Figure 1</ref>: An example of the LAT mechanism. For each decoding position, a short sequence of tokens is generated in an autoregressive way. sop is the special startof-piece symbol. 'pos*' denotes the hidden state from the decoder at that position. incomplete translation . There have been various methods in previous work <ref type="bibr" target="#b17">(Stern et al., 2019;</ref><ref type="bibr" target="#b6">Gu et al., 2019;</ref><ref type="bibr" target="#b13">Ma et al., 2018;</ref><ref type="bibr" target="#b14">Ma et al., 2019;</ref><ref type="bibr" target="#b20">Tu et al., 2020)</ref> to mitigate this problem, including iterative decoding <ref type="bibr" target="#b11">(Lee et al., 2018;</ref><ref type="bibr" target="#b4">Ghazvininejad et al., 2019)</ref>.</p><p>In this work, we introduce a novel mechanism, i.e., local autoregressive translation (LAT), to take local target dependencies into consideration. For a decoding position, instead of generating one token, we predict a short sequence of tokens (which we call a translation piece) for the current and next few positions in an autoregressive way. A simple example is shown in <ref type="figure">Figure 1</ref>.</p><p>With this mechanism, there can be overlapping tokens between nearby translation pieces. We take advantage of these redundancies, and apply a simple algorithm to align and merge all these pieces to obtain the full translation output. Specifically, our algorithm builds the output by incrementally aligning and merging adjacent pieces, based on the hypothesis that each local piece is fluent and there are overlapping tokens between adjacent pieces as aligning points. Moreover, the final output se-quence is dynamically decided through the merging algorithm, which makes the decoding process more flexible.</p><p>We integrate our mechanism into the conditional masked language model (CMLM) <ref type="bibr" target="#b4">(Ghazvininejad et al., 2019)</ref> and similarly adopt iterative decoding, where tokens with low confidence scores are masked for prediction in more iterations. With evaluations on five translation tasks, i.e., <ref type="bibr">WMT'14 EN↔DE,</ref><ref type="bibr">WMT'16 EN↔RO and IWSLT'14 DE→EN,</ref><ref type="bibr">we</ref> show that our method could achieve similar or better performance compared with CMLM and AT models while gaining nearly 2.5 and 7 times speedups, respectively. Furthermore, our method is shown to effectively reduce repeated translations and perform better at longer sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CMLM with LAT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>We integrate our LAT mechanism into CMLM, which predicts the full target sequence based on the source and partial target sequence. We adopt a lightweight LSTM-based sequential decoder as the local translator upon the CMLM decoder outputs. For a target position i, the CMLM decoder produces a hidden vector pos i , based on which the local translator predicts a short sequence of tokens in an autoregressive way, i.e., t 1 i , t 2 i , ..., t K i . Here K is the number of location translation steps, which is set to 3 in our experiments to avoid affecting the speed much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoding</head><p>During inference, a special token, sop (start of piece) is fed into the local translator to generate a short sequence based on the pos i . After generating the local pieces for all target positions in parallel, we adopt a simple algorithm to merge them into a full output sequence. This merging algorithm is described in detail in Section 3. We also perform iterative decoding following the same Mask-Predict strategy <ref type="bibr" target="#b4">(Ghazvininejad et al., 2019;</ref><ref type="bibr" target="#b2">Devlin et al., 2019)</ref>. In each iteration, we take the output sequence from the last iteration and mask a subset of tokens with low confidence scores by a special mask symbol. Then the masked sequence is fed together with the source sequence to the decoder for the next decoding iteration.</p><p>Following <ref type="bibr" target="#b4">Ghazvininejad et al. (2019)</ref>, a special token LENGTH is added to the encoder, which is utilized to predict the initial target sequence length. Nevertheless, our algorithm can dynamically adjust the final output sequence and we find that our method is not sensitive to the choice of target length as long as it falls in a reasonable range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>The training procedure is similar to that of <ref type="bibr" target="#b4">Ghazvininejad et al. (2019)</ref>. Given a pair of source and target sequences S and T , we first sample a masking size from a uniform distribution from <ref type="bibr">[1, N ]</ref>, where N is the target length. Then this size of tokens are randomly picked from the target sequence and replaced with the mask symbol. We refer to the set of masked tokens as T mask . Then for each target position, we adopt a teacher-forcing styled training scheme to collect the cross-entropy losses for predicting the corresponding groundtruth local sequences, the size of which is K = 3.</p><p>Assume that we are at position i, we simply setup the ground-truth local sequence t 1</p><formula xml:id="formula_0">i , t 2 i , ..., t K i as T i , T i+1 , ..., T i+K−1 ,</formula><p>where T i denotes the i-th token in the full target ground-truth sequence. We include all tokens in our final loss, whether they are in T mask or not, but adopt different weights for the masked tokens that do not appear in the inputs. Therefore, our token prediction loss function is:</p><formula xml:id="formula_1">L = − N i=1 K j=1 1 t j i ∈ T mask log(p(t j i )) − N i=1 K j=1 1 t j i / ∈ T mask α log(p(t j i ))</formula><p>Here, we adopt a weight α for the tokens that are not masked in the target input, which is set as 0.1 so that the model could be trained more on the unseen tokens. Furthermore, we randomly delete certain positions (the number of deletion is randomly sampled from [1, 0.15*N ]) from the target inputs to encourage the model to learn insertion-styled operations. The final loss is the addition of the token prediction and the target length prediction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Merging Algorithm</head><p>In decoding, the model generates local translation pieces for all decoding positions. We adopt a simple algorithm that incrementally builds the output through a piece-by-piece merging process. Our hypothesis is that if the local autoregressive translator is well-trained, then 1) the token sequence inside each piece is fluent and well-translated, 2) there are going to study here will study in the s1 s2 going to study here will study in the overlaps between nearby pieces, acting as aligning points for merging. We first illustrate the core operation of merging two consecutive pieces of tokens. Algorithm 1 describes the procedure and <ref type="figure" target="#fig_0">Figure 2</ref> provides an example. Given two token pieces s1 and s2, we first use the Longest Common Subsequence (LCS) algorithm to find matched tokens (Line 1). If there is nothing that can be matched, then we simply do concatenation (Line 3), otherwise we solve the conflicts of the alternative spans by comparing their confidence scores (Line 9-14). Finally we can arrive at the merged output after resolving all conflicted spans.</p><p>In the above procedure, we need to specify the score of a span. Through preliminary experiments, we find a simple but effective scheme. From the translation model, each token gets a model score of its log probability. For the score of a span, we average the scores of all the tokens inside. If the span is empty, we utilize a pre-defined value, which is empirically set to log 0.25. For aligned tokens, we choose the highest scores among them for later merging process (Line 16).</p><p>With this core merging operation, we apply a left-to-right scan to merge all the pieces in a pieceby-piece fashion. For each merging operation, we only take the last K tokens of s1 and the first K tokens of s2, while other tokens are directly copied. This ensures that the merging will only be local, to mitigate the risk of wrongly aligned tokens. Here, K is again the local translation step size.</p><p>Our merging algorithm can be directly applied at the end of each iteration in the iterative decoding. However, since the output length of the merging algorithm is not always the same as the number of input pieces, we further adopt a length adjustment procedure for intermediate iterations. Briefly speaking, we adjust the output length to the predicted length by adding or deleting certain amounts of special mask symbols. Please refer to the Ap- pendix for more details.</p><p>Although our merging algorithm is actually autoregressive, it does not include any neural network computations and thus can run efficiently. In addition to efficiency, our method also makes the decoding more flexible, since the final output is dynamically created through the merging algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We evaluate our proposed method on five translation tasks, i.e., WMT'14 EN↔DE, WMT'16 EN↔RO and IWSLT'14 DE→EN. Following previous works <ref type="bibr" target="#b8">(Hinton et al., 2015;</ref><ref type="bibr" target="#b9">Kim and Rush, 2016;</ref><ref type="bibr" target="#b5">Gu et al., 2018;</ref><ref type="bibr" target="#b25">Zhou et al., 2020)</ref>, we train a vanilla base transformer <ref type="bibr" target="#b21">(Vaswani et al., 2017)</ref> on each dataset and use its translations as the training data. The BLEU score <ref type="bibr" target="#b15">(Papineni et al., 2002)</ref> is used to evaluate the translation quality. Latency, the average decoding time (ms) per sentence with batch size 1, is employed to measure the inference speed. All models' decoding speed is measured on a single NVIDIA TITAN RTX GPU.</p><p>We follow most of the hyperparameters for the CMLM <ref type="bibr" target="#b4">(Ghazvininejad et al., 2019)</ref>    LSTM-based neural network of size 512. Finally, we average 5 best checkpoints according to the validation loss as our final model. Please refer to the Appendix for more details of the settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head><p>The main results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Compared with CMLM at the same number of decoding iterations (row 2 vs. 3 and row 4 vs. 5), LAT performs much better while keeping similar speed, especially when the iteration number is 1. Note that since our method is not sensitive to predicted length, we only take one length candidate from our length predictor instead of 5 as in CMLM. Furthermore, LAT with 4 iterations could achieve similar or better results than CMLM with 10 iterations (row 5 vs. 6) but have a nearly 2.5x decoding speedup. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>On local translation step. We also explore the effects of the number of local translation steps (K) on the IWSLT'14 DE-EN dataset. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Generally, with more local translation steps, there can be certain improvements on BLEU but with an extra cost at inference time.</p><p>On repeated translation. We compute the ngram repeat rate (nrr, what percentage of n-grams are repeated by certain nearby n-grams) of different systems on WMT'14 EN-DE test set and the result is shown in <ref type="table" target="#tab_2">Table 2</ref>. The nrr of CMLM with one iteration is much higher than other systems, showing that it suffers from a severe repeated translation problem. On the other hand, LAT can mitigate this problem thanks to the merging algorithm.</p><p>On sentence length. We explore how various systems perform on sentences with various lengths. The WMT'14 EN-DE test set is split into 5 length buckets by target length. <ref type="figure" target="#fig_1">Figure 3</ref> show that LAT performs better than CMLM on longer sentences, which indicates the effectiveness of our methods at capturing certain target dependencies. <ref type="bibr" target="#b5">Gu et al. (2018)</ref> begin to explore nonautoregressive translation, the aim of which is to generate sequences in parallel. In order to mitigate multimodality issue, recent work mainly tries to narrow the gap between NAT and AT. <ref type="bibr" target="#b12">Libovickỳ and Helcl (2018)</ref> design a NAT model using CTC loss. <ref type="bibr" target="#b11">Lee et al. (2018)</ref> uses iteration decoding to refine translation. The conditional masked language model (CMLM) <ref type="bibr" target="#b4">(Ghazvininejad et al., 2019)</ref> predicts partial target tokens based on the source text and partially masked target sentence. <ref type="bibr" target="#b14">Ma et al. (2019)</ref> employs normalizing flows as the the latent variable to produce sequences.  designs an efficient approximation for CRF for NAT. Besides that, there are some works trying to improving the decoding speed of the autoregressive models. For example,  propose a semi-autoregressive translation model, which adopts locally non-autoregressive, but autoregressive decoding. And works mentioned in <ref type="bibr" target="#b7">Hayashi et al. (2019)</ref> use techniques such as knowledge distillation, block-sparse regularization to improve the decoding speed of autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we incorporate a novel local autoregressive translation mechanism (LAT) into nonautoregressive translation, predicting multiple short sequences of tokens in parallel. With a simple and efficient merging algorithm, we integrate LAT into the conditional masked language model (CMLM <ref type="bibr" target="#b4">Ghazvininejad et al., 2019)</ref> and similarly adopt iterative decoding. We show that our method could achieve similar results to CMLM with less decoding iterations, which brings a 2.5x speedup. Moreover, analysis shows that LAT can reduce repeated translations and perform better at longer sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Preprocessing</head><p>We follow the standard pre-processing procedure in prior works <ref type="bibr" target="#b21">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b11">Lee et al., 2018)</ref>. All datasets are segmented into subwords through byte pair encoding (BPE) <ref type="bibr" target="#b16">(Sennrich et al., 2016)</ref>. The BPE code is learnt from the combination of source and target data for WMT datasets. For IWSLT, the bpe code is learned from the source and target data separately.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Optimization</head><p>We sample weights from N (0, 0.02), initialize biases to zero, and set layer normalization parameters to β = 0, γ = 1. For regularization, we use 0.3 dropout, 0.01 L2 weight decay, and smoothed cross-entropy loss with = 0.1. We train batches of 128k tokens using Adam <ref type="bibr" target="#b10">(Kingma and Ba, 2015)</ref> with β = (0.9, 0.999) and = 10 −6 . The learning rate warms up to a peak of 5 × 10 −4 within 10,000 steps, and then decays with the inverse square-root schedule. We train our models for 300k steps with batch size 128k <ref type="bibr" target="#b4">(Ghazvininejad et al., 2019)</ref> for WMT datasets. For the IWSLT dataset, we train our models for 50k steps with batch size 32k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Model Parameter Size</head><p>The averaged size of parameters for all models are shown in <ref type="table" target="#tab_8">Table 6</ref>. These three kinds of models have similar number of parameters. LAT models have the most number of parameters due to the LSTM-based local translator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Validation Performance</head><p>The performance of different models on translation tasks' validation sets is reported in the <ref type="table" target="#tab_7">Table 5</ref>. We could find the similar trend to the performance on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Length Adjustment for Intermediate Iterations</head><p>Since our merging algorithm produces the output dynamically, the output length is usually not the same as the number of input pieces. In iterative decoding, we find it helpful to adjust the output sequence's length to the input length in intermediate iterations. This is achieved by adding or deleting the special mask symbols. Notice that for the final iteration, we do not apply any adjustments and keep the merged output sequence as it is. For the length adjustment in the intermediate iterations, our goal is to adjust the output length of the merger (L out ) to be close to the input target length (L in ). If these two lengths are already equal or their relative difference is within a certain range (which is empirically set to 5%), we will do nothing. Otherwise, there can be two cases: 1) when L in is larger than L out , we further insert L in − L out mask tokens into the sequence; 2) otherwise, we try to delete L out −L in mask tokens. Notice that the addition or deletion operations happen after the masking procedure for the next iteration.</p><p>Here, we describe the addition case in detail. Suppose we need to further insert M masks into the output sequence, we decide the insertion places according to the position gaps. We adopt a simple position scheme for all the tokens. For each original token t j i (the j-th token in the i-th piece) in the input translation pieces, we set i + j as its position. For each token in the output sequence after merging, since it can originate from multiple input tokens through aligning, we take the averaged value of all its source input tokens' positions. We calculate the position gap between each pair of nearby unmasked tokens in the output sequence and maintain a priority queue for all these gaps. Then we insert M masks once at a time. For each time, we select the current maximal gap, insert a mask to that position, and subtract that gap by 1. The case for deletion would be similar but in the opposite direction: select the minimal gap, delete one mask if there are any, and increase that gap by 1. We will delete nothing if there are no masked tokens in the selected gap.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An example of merging two pieces of tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The BLEU scores of various systems with respect to the reference sentence lengths on WMT'14 EN-DE testset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Merging two pieces. Two pieces of tokens: s1, s2. Output: A merged sequence s . // Call Longest Common Subsequence 1 MatchedPairs = LCS(s1, s2); 2 if MatchedPairs.size() == 0 then</figDesc><table><row><cell>3</cell><cell>return s1+s2 ;</cell><cell>// Simple concat</cell></row><row><cell>4 else</cell><cell></cell><cell></cell></row><row><cell>5</cell><cell>s = [] ;</cell><cell>// Initialize</cell></row><row><cell>6</cell><cell>p1, p2 = -1, -1 ;</cell><cell>// Previous idxes</cell></row><row><cell></cell><cell cols="2">// Add sentinel indexes.</cell></row><row><cell></cell><cell cols="2">MatchedPairs += [(∞, ∞)];</cell></row><row><cell>8</cell><cell cols="2">foreach i1, i2 in MatchedPairs do</cell></row><row><cell>9</cell><cell cols="2">span1 = s1[p1+1:i1];</cell></row><row><cell>10</cell><cell cols="2">span2 = s2[p2+1:i2];</cell></row><row><cell></cell><cell cols="2">// Solve conflicts by scores.</cell></row><row><cell>11</cell><cell cols="2">if score(span1) ≥ score(span2) then</cell></row><row><cell>12</cell><cell>s += span1;</cell><cell></cell></row><row><cell>13</cell><cell>else</cell><cell></cell></row><row><cell>14</cell><cell>s += span2;</cell><cell></cell></row><row><cell></cell><cell cols="2">// Align the matched ones.</cell></row><row><cell>15</cell><cell>if i1 = ∞ then</cell><cell></cell></row><row><cell>16</cell><cell cols="2">s += [align(s1[i1], s2[i2])];</cell></row><row><cell>17</cell><cell>p1, p2 = i1, i2;</cell><cell></cell></row></table><note>Input:718 return s ;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The comparisons (on BLEU score and decoding latency) of CMLM, LAT and AT models.</figDesc><table><row><cell>in the base</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="5"># local translation steps (K)</cell></row><row><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>BLEU</cell><cell cols="5">32.9 33.8 34.4 34.5 34.2</cell></row><row><cell>latency (ms)</cell><cell>69</cell><cell>72</cell><cell>74</cell><cell>77</cell><cell>79</cell></row></table><note>N-gram repeat rates of various models on WMT'14 EN-DE test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The performance of LAT models with respect to the number of local translation steps (K) on IWSLT'14 DE-EN test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>lists some details.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Vocab. Size Data size</cell></row><row><cell>IWSLT</cell><cell>10k</cell><cell>150k</cell></row><row><cell>WMT14 EN↔DE</cell><cell>32k</cell><cell>4.5M</cell></row><row><cell>WMT16 EN↔RO</cell><cell>40k</cell><cell>600k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Pre-processing details of various translation benchmarks. Vocab. size denotes vocabulary size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The comparisons (on BLEU score and decoding latency) of CMLM, LAT and AT models on development sets.</figDesc><table><row><cell cols="2">Model Parameter size</cell></row><row><cell>AT</cell><cell>60M</cell></row><row><cell>CMLM</cell><cell>62M</cell></row><row><cell>LAT</cell><cell>64M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Number of Parameters of different models.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Levenshtein transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11179" to="11189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<title level="m">Findings of the third workshop on neural generation and translation. EMNLP-IJCNLP 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-toend non-autoregressive neural machine translation with connectionist temporal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3016" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bag-of-words as target for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="332" to="338" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flowseq: Nonautoregressive conditional sequence generation with generative flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4273" to="4283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Insertion transformer: Flexible sequence generation via insertion operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast structured decoding for sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3011" to="3020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Engine: Energy-based inference networks for non-autoregressive machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00850</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-autoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-autoregressive machine translation with auxiliary regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5377" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imitation learning for nonautoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1304" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding knowledge distillation in non-autoregressive machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
