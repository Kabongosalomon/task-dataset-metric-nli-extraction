<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anycost GANs for Interactive Image Synthesis and Editing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
							<email>jilin@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research 3 CMU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
							<email>rizhang@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research 3 CMU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frieder</forename><surname>Ganz</surname></persName>
							<email>ganz@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research 3 CMU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
							<email>songhan@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
							<email>junyanz@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Adobe Research 3 CMU</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Anycost GANs for Interactive Image Synthesis and Editing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>input projected 2x faster 4x faster 8x faster edit: no smile 8x faster edit: hair color 8x faster (a) input images (b) Anycost GANs provide consistent outputs for projected latent code (c) Consistency remains after editing less hair color <ref type="figure">Figure 1</ref>: Anycost GAN can be executed at flexible computation costs (fast preview with low cost and high-quality output with full cost), enabling interactive image editing with quick preview. The low-cost sub-generator produces consistent outputs compared to the full-cost generator during both image projection and latent code traversal, making the sub-generator an accurate proxy for various editing tasks (e.g., no smile, changing hair color). Interactive demos are available here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Generative adversarial networks (GANs) have enabled photorealistic image synthesis and editing. However, due to the high computational cost of large-scale generators (e.g., StyleGAN2), it usually takes seconds to see the results of a single edit on edge devices, prohibiting interactive user experience. In this paper, inspired by quick preview features in modern rendering software, we propose Anycost GAN for interactive natural image editing. We train the Anycost GAN to support elastic resolutions and channels for faster image generation at versatile speeds. Running subsets of the full generator produce outputs that are perceptually similar to the full generator, making them a good proxy for quick preview. By using sampling-based multi-resolution training, adaptive-channel training, and a generator-conditioned discriminator, the anycost generator can be evaluated at various configurations while achieving better image quality compared to separately trained models. Furthermore, we * Part of the work done during an internship at Adobe Research.</p><p>develop new encoder training and latent code optimization techniques to encourage consistency between the different sub-generators during image projection. Anycost GAN can be executed at various cost budgets (up to 10× computation reduction) and adapt to a wide range of hardware and latency requirements. When deployed on desktop CPUs and edge devices, our model can provide perceptually similar previews at 6-12× speedup, enabling interactive image editing. The code and demo are publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b16">[17]</ref> have excelled at synthesizing diverse and realistic images from randomly sampled latent codes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b5">6]</ref>. Furthermore, a user can transform the generated outputs (e.g., add smiling to a portrait) by tweaking the latent code <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b67">68]</ref>. In real-world use cases, a user would often like to edit a natural image rather than generating random samples. To achieve this, one can project the image into the image manifold of GANs by finding a latent code that reconstructs the image, and then modify the code to produce final outputs <ref type="bibr" target="#b81">[82]</ref>.</p><p>Despite its photorealistic results and versatile editing ability, modern deep generative models incur huge computational costs, prohibiting edge deployment. For example, the StyleGAN2 generator <ref type="bibr" target="#b44">[45]</ref> consumes 144G MACs, 36× larger compared to ResNet-50 <ref type="bibr" target="#b26">[27]</ref>. The expensive model often introduces a several-second delay for a single edit, leading to a sub-optimal user experience and shorter battery life when used on an edge device.</p><p>Modern 2D/3D content creation workflows, such as the preview rendering feature in Maya and Blender, as well as the playback feature in Adobe Premiere Pro, allow users to easily control the tradeoff between image quality and rendering speed. A user can turn off certain visual effects, reduce the resolution and fidelity, or use a fast method during user interaction. Once the edit is finalized, a user can use an expensive method with additional visual effects at a higher resolution. In rendering literature, this tradeoff can be easily achieved by reducing the number of sampled rays in ray/path tracing <ref type="bibr" target="#b39">[40]</ref>, or early stopping of iterative solvers in progressive radiosity <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref>. In this work, we aim to bring a smooth tradeoff between visual quality and interactivity to deep generative models.</p><p>We propose "Anycost" GANs for interactive image synthesis and editing. Our goal is to train a generator that can be executed at a wide range of computational costs while producing visually consistent outputs: we can first use a lowcost generator for fast, responsive previews during image editing, and then use the full-cost generator to render highquality final outputs. We train the anycost generators to support multi-resolution outputs and adaptive-channel inference. The smaller generators are nested inside the full generator via weight-sharing. Supporting different configurations in one single generator introduces new challenges for minimax optimization, as the adversarial optimization involves too many players (different sub-generators). Vanilla GAN training methods fail catastrophically in this setting. To stabilize the training process, we propose to perform stage-wise training: first train sub-generators at multiple resolutions but with full channels, and then train sub-generators with reduced channels. To account for different sub-generators' capacities and architectures, we train a weight-sharing discriminator that is conditioned on the architectural information of the specific sub-generator.</p><p>We train Anycost GAN to support two types of channel configurations: uniform channel reduction ratio and flexible ratios per layer. The combined architecture space leads to high flexibility in terms of computation cost, containing sub-generators at &gt; 10× computation difference. We can directly obtain low-cost generators by taking a subset of weights without any fine-tuning, which allows us to easily switch between quality and efficiency. To handle diverse hardware capacities, we use evolutionary search to automat-ically find the best sub-generator under different computational budgets, while achieving the best output consistency w.r.t. the full-cost generator <ref type="figure">(Figure 1b</ref>).</p><p>To better maintain consistency during the image projection process, we further propose consistency-aware encoder training and iterative optimization for image projection. We optimize the reconstruction loss, not only for the full-cost generator, but also for the sub-generators, which significantly improves the consistency during both image projection and editing steps ( <ref type="figure">Figure 1c</ref>).</p><p>Our single, anycost generator can provide visually consistent outputs at various computation budgets. Compared to small generators of the same architecture or existing compression methods based on distillation, our method can provide better generation quality and higher consistency w.r.t. to the full generator. The generated outputs from generators at different costs also share high-level visual cues, for example, producing generated faces with consistent facial attributes ( <ref type="table" target="#tab_5">Table 4</ref>). Our method provides 12.2× speed-up on Xeon CPU and 8.5× speed up on Jetson Nano GPU for faster preview. Combined with consistency-aware image projection, our anycost generator maintains consistency after various editing operations ( <ref type="figure" target="#fig_3">Figure 8</ref>) and offers an efficient and interactive image editing experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Adversarial Networks. GANs <ref type="bibr" target="#b16">[17]</ref> have enabled photorealistic synthesis for many vision and graphics applications <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15]</ref>. Some recent examples include high-resolution face synthesis <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, ImageNet-scale class-conditional generation <ref type="bibr" target="#b5">[6]</ref>, and semantic photo synthesis <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b60">61]</ref>. As image quality and model capacity have increased <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b40">41]</ref>, so have computational costs and inference time. For example, it takes several seconds for a single forward pass of StyleGAN2 <ref type="bibr" target="#b44">[45]</ref> on a desktop CPU or a tablet. While most research demos are running on the latest GPUs, deploying them on edge devices and laptops efficiently is critical for interactive editing applications. In this work, we tackle this problem by learning hardware-adaptive generators that work across a wide range of devices and latency constraints.</p><p>Model acceleration and dynamic networks. Efficient deep networks have enabled fast inference and reduced model sizes with a focus on image classifiers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b21">22]</ref>. Commonly used approaches include pruning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b72">73]</ref>, quantization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b79">80]</ref>, knowledge distillation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b9">10]</ref>, efficient architecture design <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b66">67]</ref>, and autoML-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">51]</ref>. Recently, several works adopt the above ideas to compress generative networks with a focus on image-conditional GANs <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b13">14]</ref>, such as pix2pix <ref type="bibr" target="#b37">[38]</ref> and Cycle-GAN <ref type="bibr" target="#b82">[83]</ref>. The work most related to ours is Aguinaldo et al. <ref type="bibr" target="#b2">[3]</ref>, which adopts knowledge distillation to compress DC-  <ref type="figure">Figure 2</ref>: Anycost GAN for image synthesis and editing. Given an input image, we project it into the latent space with encoder E and backward optimization. We can modify the latent code with user input to edit the image. During editing, a sub-generator of small cost is used for fast and interactive preview; during idle time, the full cost generator renders the final, high-quality output. The outputs from the full and sub-generators are visually consistent during projection and editing.</p><formula xml:id="formula_0">generative model latent space E w0 wopt optimization G preview G'(wedit) low-cost G' wedit =</formula><p>GAN <ref type="bibr" target="#b63">[64]</ref> on low-res images of MNIST, CIFAR, and CelebA datasets. A concurrent work <ref type="bibr" target="#b32">[33]</ref> also explores channel reduction of unconditional GANs on low-resolution datasets. Our experiments show that knowledge distillation alone is insufficient to transfer the knowledge of a GAN on large-scale datasets ( <ref type="figure" target="#fig_2">Figure 5</ref>). Compared to prior work, our method works well for large-scale, high-resolution unconditional GANs. More importantly, our method can produce output images with dynamic resolution and fidelity, not possible by the prior work <ref type="bibr" target="#b2">[3]</ref>. We also include techniques to leverage the anycost generator in real image editing scenarios.</p><p>Image editing via GANs. There exist two major ways of using GANs for image editing: (1) conditional GANs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b47">48]</ref>, which learn to directly translate an input image into a target domain, and (2) image projection <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b6">7]</ref>, where the algorithm first projects a real image into the latent space of an unconditional GAN, modifies the latent code to achieve an edit, and synthesizes a new image accordingly.</p><p>Recently, interest in projection-based editing has been revived due to the increasing quality of unconditional GANs. Several methods have been proposed, including choosing better or multiple layers to project and edit <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19]</ref>, fine-tuning network weights for each image <ref type="bibr" target="#b4">[5]</ref>, modeling image corruption and transformations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, and discovering meaningful latent directions <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25]</ref>. In this work, we aim to learn efficient generators for downstream image projection and editing, which offers unique challenges. Namely, we wish the projected latent code from a sub-generator provide a faithful "preview" of the full model. In addition, user edits with the sub-generator should also make the corresponding changes to the full model. We tackle these challenges using our new consistency-aware encoder training and optimization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Anycost GANs</head><p>We introduce the problem setting and potential challenges in Section 3.1. We then describe our training method for learning multi-resolution adaptive-channel generators in Section 3.2. In Section 3.3, we describe our consistency-aware encoder training and latent code optimization, designed for image projection and editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setup</head><p>An unconditional generator G learns to synthesize an image x ∈ R H×W ×3 given a random noise z ∈ Z, where Z is a low-dimensional latent space. In this work, we study StyleGAN2 <ref type="bibr" target="#b44">[45]</ref>, which first learns a non-linear mapping f : Z → W+ that produces w = f (z), w ∈ R 18×512 , and then generates the image from W+ space: x = G(w) <ref type="bibr" target="#b0">[1]</ref>.</p><p>Anycost GAN enables a subsection of the generator G to execute independently and produce a similar output x = G (w) to the full generator G(w). We can then use G (w) for fast preview during interactive editing, and full G(w) to render final high-quality outputs. The model can be deployed on diverse hardware (e.g., GPUs, CPUs, smartphones), and users can choose between different preview qualities.</p><p>A natural choice for enabling diverse inference cost is to use different image resolutions, as done in StackGAN <ref type="bibr" target="#b77">[78]</ref> and ProGAN <ref type="bibr" target="#b41">[42]</ref>. However, reducing resolution from 1024×1024 to 256×256 only reduces the computation by 1.7×, despite 16× fewer pixels to synthesize * . Therefore, we need additional dimensions to further reduce the cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Anycost Generators</head><p>We propose to learn an anycost generator to produce proxy outputs at diverse resolutions and channel capacities. The detailed architecture is shown in <ref type="figure" target="#fig_0">Figure 3</ref>.</p><p>Multi-resolution training. For elastic resolutions, the architectures of the ProGAN <ref type="bibr" target="#b41">[42]</ref> and StyleGAN family <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>   training objectives ( <ref type="figure" target="#fig_1">Figure 4b</ref>). Our generator gradually produces higher resolution outputs after each block g k :</p><formula xml:id="formula_1">+ + + + 256×256 128×128 64×64 g_arch ③ G-conditioned Discriminator</formula><formula xml:id="formula_2">x = G(w) = g K • g K−1 • ... • g k • ... • g 2 • g 1 (w),<label>(1)</label></formula><p>where K is the total number of network blocks. We use the intermediate low-res outputs for preview, and denote:</p><formula xml:id="formula_3">x k = G k (w) = g k • g k−1 • ... • g 2 • g 1 (w), k ≤ K,<label>(2)</label></formula><p>resulting a series of outputs in increasing resolutions</p><formula xml:id="formula_4">[x 1 , ...,x K ].</formula><p>Existing work MSG-GAN <ref type="bibr" target="#b40">[41]</ref> trains the generator to support different resolutions by using a discriminator taking images of all resolutions at the same time ( <ref type="figure" target="#fig_0">Figure 3b</ref>). However, such an all-resolution training mechanism leads to lower quality results on large-scale datasets like FFHQ <ref type="bibr" target="#b44">[45]</ref> (as measured by Fréchet Inception Distance, FID <ref type="bibr" target="#b29">[30]</ref>), compared to training single-resolution models separately (Table 1). To overcome the image fidelity degradation when supporting multi-resolutions, we propose a sampling-based training objective, where a single resolution is sampled and trained at each iteration, both for the generator G and the discriminator D. As shown in <ref type="figure" target="#fig_0">Figure 3c</ref>, when sampling a lower resolution (e.g., 128×128), the translucent parts are not executed. We use the intermediate output of G for a lower resolution. It is passed through a fromRGB convolution "readoff" layer to increase channels, and then fed to an intermediate layer of D. Our multi-resolution training objective is formulated as:</p><formula xml:id="formula_5">L multi-res = E x,k [log D(x k )] + E w,k [log(1 − D(G k (w)))].</formula><p>(3) We observe that the sampling-based training leads to better convergence: in fact, our multi-resolution model gives better FID at all resolutions, compared to single-resolution models trained at corresponding resolutions, as shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Adaptive-channel training. To further enable our generator to run at different costs, we train the generator to support variable channels. For adaptive-channel training, we allow different channel number multipliers for each layer (either a uniform ratio, used across all layers or flexible ratios for each layer). For each training iteration, we randomly sample a channel multiplier configuration and update the corresponding subset of weights (yellow part in <ref type="figure" target="#fig_0">Figure 3c</ref>). We hope that the most "important" channels are preserved during sampling to minimize any degradation. To this end, we initialize our model using the multi-resolution generator from the previous stage and sort the channels of convolutional layers according to the magnitude of kernels, from highest to lowest. We always sample the most important αc ones according to the initial sorting, where α ∈ [0.25, 0.5, 0.75, 1] and c is the number of channels in the layer. The adaptive-channel training objective is written as follows:</p><formula xml:id="formula_6">L ada-ch = E x,k [log D(x k )] + E w,k,C [log(1 − D(G k C (w)))],<label>(4)</label></formula><p>where C means the channel configurations for each layer.</p><p>With such a learning objective, all the sub-networks with fewer channels can generate reasonable images. However, these images may be visually different compared to the full network, failing to provide an accurate "preview" <ref type="figure" target="#fig_1">(Figure 4c</ref>). To keep the output consistent across different sub-networks, we add the following consistency loss:</p><formula xml:id="formula_7">L total = L ada-ch + E w,k,C [ (G k C (w), G(w))],<label>(5)</label></formula><p>where is a pre-defined distance metric. We use a combination of MSE loss and LPIPS loss <ref type="bibr" target="#b78">[79]</ref>, which empirically gives the most visually consistent results ( <ref type="figure" target="#fig_1">Figure 4d</ref>). Note that all the sub-networks and the full network share weights across different channels and are jointly trained.</p><p>Generator-conditioned discriminator. Unlike conventional GAN training, our system trains many sub-generators of different channels and resolutions at the same time. We find that one single discriminator cannot provide good supervision for all sub-generators of different channel configurations, resulting in an inferior image fidelity/FID ( <ref type="table">Table 2)</ref>.</p><p>To handle the challenge, we design the discriminator to be conditioned on the generator architecture.</p><p>A straightforward method for generator-conditioning is to correspondingly shrink the channels of the discriminator with the generator. However, we find that such practice often sacrifices the quality of smaller channel settings, due to its reduced capacity of the discriminator ("reduced ch" in <ref type="table">Table 2)</ref>. Also, such a technique only supports uniform channel ratios. Instead, we take a learning-based approach to implement the conditioning. As shown in <ref type="figure" target="#fig_0">Figure 3c</ref>, we first encode the channel configuration in g arch using a one-hot encoding (for each layer, we can choose one of the four ratios, forming a one-hot vector of length 4; we concatenate the vectors from all layers to form g arch), which is passed through a fully connected layer to form the per-channel modulation. The feature map is modulated using the conditioned weight and bias before passing to the next layer. For real images, we randomly draw a g arch vector. To stabilize training, we only apply the G-conditioned modulation units to the last two blocks of the discriminator.</p><p>Searching under different budgets. By training the generator to support flexible ratios for each layer, we support an exponential number of sub-generator architectures. At a given computation budget, selecting the proper subgenerator configuration is important for keeping generation quality and consistency. We use an evolutionary search algorithm <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref> to find an effective sub-generator architecture under diverse resource budgets (e.g., computation, latency). Given a certain budget, our evolutionary search minimizes the difference between the desired sub-generator and the full generator's outputs, measured by a perceptual loss. The details can be found in Section B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Image Projection with Anycost Generators</head><p>To edit an existing image x, we need to first project the image into the latent space of a generator <ref type="bibr" target="#b81">[82]</ref> by solving w * = arg min w (G(w), x), where we use a combination of LPIPS <ref type="bibr" target="#b78">[79]</ref> and MSE loss for . We follow <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> to project the image into the extended W+ space, rather than the Z space due to its better expressiveness. We follow the two-step approach, as introduced in iGAN [82]: encoderbased initialization followed by gradient-based latent code optimization.</p><p>Consistency-aware image projection. Encoder-based projection directly trains an encoder E for projection by optimizing E * = arg min E E x (G(E(x)), x) over many training images. For a specific image sample, we can further improve the results with optimization-based projection by solving w * = arg min w (G(w), x) with iterative gradient descent. While our generator can produce consistent results across sub-generators for a randomly sampled latent code, the predicted/optimized latent codes E(x) may not follow the prior distribution. As a result, the sub-generators may not produce consistent results on some optimized latent codes. Therefore, we modify the objects to produce a latent code that works for both the full generators as well as randomly sampled sub-generators as follows:</p><formula xml:id="formula_8">E * = arg min E E x [ (G(E(x)), x) + αE k,C (G k C (E(x)), x)] (6) w * = arg min w [ (G(w), x) + αE k,C (G k C (w, x)]<label>(7)</label></formula><p>We set hyper-parameter α = 1 in our experiments.</p><p>Image editing with anycost generators. After projection, we can perform image editing by simply changing the latent code and synthesize a new one using G(w + ∆w), where ∆w is a vector that encodes a certain change. Several methods <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b24">25]</ref> have been proposed to discover such latent directions that control certain aspects of the input (e.g., smiling/non-smiling for faces, color, and shape for cars). To produce a preview with low latency, we can run G k C (w + ∆w). In experiments, we observe that as long as the initial projection is consistent across the full and subgenerators, the edited results are visually similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>We conduct experiments on both FFHQ <ref type="bibr" target="#b43">[44]</ref> (resolution 1024) and LSUN car dataset <ref type="bibr" target="#b74">[75]</ref> (resolution 512) due to the large scale and high resolution. Our generators are based on StyleGAN2 <ref type="bibr" target="#b44">[45]</ref> (config-F). We train our models to support four resolutions from the highest (e.g., 1024, 512, 256, 128 for FFHQ), since lower resolutions are too blurry for a high-quality preview. For dynamic channels, we support multipliers ranging [0.25, 0.5, 0.75, 1] w.r.t. the original channel numbers. For fast ablation studies, we train models For adaptive channel support, we consider two settings: (1) training with uniform channel ratios (i.e., using the same channel reduction ratios for all layers); (2) training with flexible channel ratios for each layer. For the latter setting, after GAN training, we leverage evolutionary search to find the best channel &amp; resolution configurations under certain computation budgets. The details of the evolutionary search can be found in Section B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head><p>We first show how we improve the image quality while supporting different resolutions and channels with ablation studies. The results are evaluated on FFHQ dataset.</p><p>Multi-resolution training. We compare the results of sampling-based multi-resolution training against singleresolution training and MSG-GAN <ref type="bibr" target="#b40">[41]</ref> in <ref type="table" target="#tab_2">Table 1</ref>. With our technique, we can train one generator that generates multi-resolution outputs at a better quality (lower FID) for both high-resolution (1024) and lower-resolution (256) settings, even compared to specialized single-resolution models. Compared to MSG-GAN <ref type="bibr" target="#b40">[41]</ref>, which trains all resolutions at each iteration, our method consistently gains better FIDs. We hypothesize that feeding images of all resolutions to the discriminator poses a stricter requirement for the generator (all the outputs have to be realistic to fool the discriminator), breaking the balance between the generator and the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G-conditioned discriminators for dynamic channels.</head><p>Using one fixed discriminator is not enough to handle a set of generators at different channel capacities and resolutions. We use a simpler setting for ablation by only supporting four uniform channel reduction ratios, i.e., using the same channel multiplier for all layers. As shown in <ref type="table">Table 2</ref>, using the same discriminator for all sub-generators (denoted as "same D") cannot consistently match the FIDs compared to single generators trained for a specific resolution and channel width <ref type="table">Table 2</ref>: FIDs on FFHQ at different resolutions and channels. All the settings except "vanilla" only train one generator and evaluate it at multiple configurations. We mark a better FID green and a worse FID red compared to multi-G baseline. Conditioned discriminator ("conditioned") provides the best FID over different channel widths and resolutions. The model is based on Config-E for faster ablation. (denoted as "vanilla"). It also leads to an unstable FID distribution (e.g., for resolution 128, 1.0× channel gives worse FID compared to 0.5×, despite increased computational resources), since a single discriminator can only provide suitable supervisions for a small subset of sub-generators. To improve the performance of each generator, we implement the discriminator to be conditioned on the generator architecture. A straight-forward method is to also reduce the channels of the discriminator using the same ratio as the generator (denoted as "reduced ch" in <ref type="table">Table 2</ref>). This improves the FIDs under some conditions and makes the FIDs monotonic as a function of computation (wider subgenerators give better FID). However, the narrow generators (e.g., 0.25×) have degraded FIDs due to limited discriminator capacity. This also does not work for non-uniform channel ratio settings, where each layer can use a different channel ratio, leading to exponential combinations. Instead, our conditioned discriminator (denoted as "conditioned") uses a learned modulation according to the generator architecture (represented as an architecture vector), without reducing the discriminator capacity. For different channels and resolutions, it provides consistently better FIDs compared to the multi-generator baseline, and also improves over the reduced-channel method.</p><p>Outperforming compression baselines. One baseline for fast generation preview is to train a small model that mimics the full generator and use the small model for interactive image editing. However, this method does not allow flexible model capacity to fit diverse hardware platforms and latency requirements. The small models also have inferior generation quality and consistency w.r.t. to the full generator, compared to our anycost generator.</p><p>We compare the FID-computation trade-off with compression-based baselines in <ref type="figure" target="#fig_2">Figure 5a</ref>. "Vanilla" means single models trained from scratch. We compare with <ref type="bibr" target="#b2">[3]</ref>, which uses a distillation-based method for unconditional GAN compression ("Distill"), and also adapt a general CNN compression method <ref type="bibr" target="#b28">[29]</ref> to GAN ("ChannelPrune"). Since the FID cannot be computed across different resolutions, we provide the results for resolution 256 and 128 separately (we can search over all resolutions and channels when optimizing for consistency, like in <ref type="figure">Figure 7</ref>). For anycost GAN, we provide the results of two settings: uniform channel ratio (denoted as "Anycost (uniform)") and flexible channel ratios (denoted as "Anycost (flexible)"). Anycost GAN outperforms existing compression methods at a wide range of computation budgets, despite using only a single generator, achieving lower FID/difference at the same computation. The "uniform" and "evolve" settings achieve a similar trade-off curve. However, with evolutionary search, we can support more fine-grained computation budgets to fit more deployment scenarios.</p><p>Apart from generation quality, our sub-generators also provide more consistent outputs w.r.t. to the full generator ( <ref type="figure" target="#fig_2">Figure 5b)</ref>. We measure the LPIPS difference <ref type="bibr" target="#b78">[79]</ref> between the generated images from sub-networks and the full network. Compared to distillation-based training, our model reduces the LPIPS difference by half. For models trained from scratch, we cannot report LPIPS consistency, as the models are independently trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Anycost Image Synthesis</head><p>In this section, we provide the results of Anycost GAN trained on high-resolution images (1024 × 1024 for FFHQ <ref type="bibr" target="#b44">[45]</ref> and 512 × 384 for LSUN Car <ref type="bibr" target="#b74">[75]</ref>). The models are trained with Config-F <ref type="bibr" target="#b44">[45]</ref> for high-quality synthesis.</p><p>Qualitative results. In <ref type="figure">Figure 6</ref>, we show several samples from our anycost generators (uniform channel setting). Despite only training a single generator, Anycost GAN maintains output consistency across different resolutions and different channel widths compared to the full generator's output, providing a fast preview when a user is exploring the latent space or applying editing operations. The samples are generated with truncation rate ψ = 0.5.</p><p>We also provide the results of anycost generator (flexible channels +evolutionary search setting) in <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 7</ref>. With evolutionary search, we can provide sub-  generators at fine-grained computation budgets at 2-10× reduction. The sub-generators share high consistency for the projected and edited images, paving a way for fast editing preview.</p><p>Quantitative results. We also provide the quantitative results of the high-resolution images. The high-resolution FIDs and path lengths <ref type="bibr" target="#b43">[44]</ref> are shown in <ref type="table" target="#tab_4">Table 3</ref>. We only provide the results of uniform channel setting, since the flexible setting shares a similar FID vs. computation tradeoff ( <ref type="figure" target="#fig_2">Figure 5</ref>). Compared to StyleGAN2, Anycost GANs achieves similar or better FIDs and path lengths at various channel widths, despite only one generator is trained. Anycost GANs enjoy better flexibility for inference and also better synthesis quality compared to the baselines.</p><p>Attribute consistency. Our low-cost results not only preserve the visual consistency (LPIPS difference), but also de-  <ref type="figure">Figure 6</ref>: Anycost GAN (uniform) maintains output consistency across different resolutions and channels widths. Zoom in for better view. For faces (a), our method preserves the prominent facial structure such as age, hair style, and pose. Some small details such as wrinkles are omitted. For cars (b), our method succeeds to keep the color, shape, and pose. Some small details are omitted such as license plates.</p><p>pict similar visual attributes as the full results do. We check the binary attributes (e.g., is smiling, is wearing eyeglasses) of images generated by sub-generators (0.5× and 0.25× channels) and the full generator. Following <ref type="bibr" target="#b43">[44]</ref>, we trained the attribute classifier to predict 40 attributes on CelebA-HQ for evaluation. We compare the prediction results on 10,000 randomly generated images and report the match rates in <ref type="table" target="#tab_5">Table 4</ref>. For all the categories, we can achieve &gt; 95% consistency with 0.5× channel (except for "Earrings", which full generator 2x reduction 6x 4x 8x 10x <ref type="figure">Figure 7</ref>: Anycost GAN (flexible) maintains output consistency across fine-grained computation reductions. Zoom in for better view. To save space, we display some images in a smaller size. Check <ref type="figure" target="#fig_2">Figure 15</ref> for a larger view. is challenging due to its small size), which is higher than the accuracy of attribute classifiers on CelebA <ref type="bibr" target="#b20">[21]</ref>, indicating that our model preserves high-level visual attributes across full generators and sub-generators. The attributes in <ref type="table" target="#tab_5">Table 4</ref> are chosen due to their high inconsistency between two separately trained generators ("Separate").</p><p>Latency reduction. We measure the latency speed-up on both desktop CPU (Intel Xeon Silver) and mobile GPU (NVIDIA Jetson Nano) in <ref type="table" target="#tab_7">Table 5</ref>. Anycost generators can generate consistent previews at 11.9× walltime speed-up on Xeon CPU and 8.5× speed-up on Jetson Nano. Interestingly, the model achieves a super-linear speed-up on Intel CPU. We hypothesize that the activation and weight sizes of the full-cost generator exceed the cache on CPU (16.5MB), leading to increased cache miss rate and worse inference efficiency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Anycost Image Projection and Editing</head><p>Encoder training. We compare the encoder only optimized for the full model and the one optimized for both the full network and the sub-networks. The results on CelebA test set are in <ref type="table" target="#tab_8">Table 6</ref>. For generality, we used a simple encoder architecture: we used the ResNet-50 <ref type="bibr" target="#b26">[27]</ref> backbone architecture and a linear layer to regress the latent code in W+ space. We train the encoder both on real images (FFHQ + CelebA train set following <ref type="bibr" target="#b65">[66]</ref>) and generated images for 200 epochs. We apply random horizontal flip, random color jittering, and random grayscale as augmentation <ref type="bibr" target="#b25">[26]</ref>. To compare with existing literature, we measure the LPIPS loss using AlexNet backbone instead of VGG. Apart from the reconstruction loss for the full generator, we also measure the average reconstruction performance for all the sub-generator architectures found by our evolutionary algorithm. Our generator has better reconstruction performance compared to an advanced encoder design <ref type="bibr" target="#b65">[66]</ref> that uses a Feature Pyramid Network <ref type="bibr" target="#b52">[53]</ref> structure.</p><p>Optimization-based projection. For optimization-based projection, we used L-BFGS solver <ref type="bibr" target="#b53">[54]</ref> with 100 iterations. We find that L-BFGS converges faster than Adam <ref type="bibr" target="#b45">[46]</ref>. We use the encoder's prediction as the starting point to represent a more realistic use case, which also results in better con-vergence compared to starting from average latent w <ref type="bibr" target="#b44">[45]</ref>. Interestingly, we find that a lower optimization loss results in a better reconstruction, but the latent code may not be suitable for latent space-based editing (see Section D.2). Therefore, we do not benchmark the quantitative results here.</p><p>The qualitative results are shown in <ref type="figure">Figure 1</ref>.</p><p>Image editing with anycost generators. We show that anycost generators remain consistent after image editing. We compute several editing directions on FFHQ dataset in the W space following <ref type="bibr" target="#b67">[68]</ref>. We compare the outputs of the full generators and 8×-smaller computation sub-generators after editing smiling, age, eye-glasses, and hair color in the latent space. The results are shown in <ref type="figure" target="#fig_3">Figure 8</ref> and <ref type="figure">Figure 1</ref> . Anycost generator gives consistent outputs under various attribute editing. We show more projection and editing examples in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper, we have proposed Anycost GAN, a scalable training method for learning unconditional generators that can adapt to diverse hardware and user latency requirements. We have demonstrated its application in image projection and editing. Several limitations still exist for our method. First, the control over the channel numbers might be difficult for users who are new to neural networks. In the future, we plan to provide more intuitive controls such as synthesizing similar color, texture, illumination, or shape as the original model does. Second, our current model aims to approximate every single output pixel equally well. In practice, certain objects (e.g., face) might be more important than others (e.g., background). We would like to learn models that can support spatially-varying trade-off between fidelity and latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interactive Demo</head><p>We provide an interactive demo in the code release to show the image editing use case with our anycost generator. Anycost generator provides a fast preview to enable interactive image editing. A screenshot of the demo interface is provided in <ref type="figure" target="#fig_4">Figure 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Training</head><p>We used similar training hyper-parameters as <ref type="bibr" target="#b44">[45]</ref>. For the first stage, i.e., training a vanilla StyleGAN2 model, we used exactly the same training configuration. For multiresolution training and adaptive-channel training, since we can distill from the pre-trained generator, we do not need to add the path length regularization <ref type="bibr" target="#b44">[45]</ref>, which accelerates the training.</p><p>For multi-resolution training, at reach iteration, we randomly sampled two resolutions from the four possible resolutions for both generator and discriminator training, which improves the data efficiency and stabilizes training. During adaptive-channel training, we randomly sampled one channel configuration at each iteration. For the uniform channel setting, we randomly sampled one of the four ratios using a uniform probability. For the flexible channel setting, we followed <ref type="bibr" target="#b49">[50]</ref> to use a "sandwich" rule for channel sampling (i.e., sample the full generator for 25% of the time, sample the smallest generator for 25% of the time, and randomly sample a generator at the rest of the time). We empirically find that it makes the convergence more stable and improves the FID of the full generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Evolutionary search</head><p>We use evolutionary search to find the best sub-generator under a certain resource budget. We use a population size P = 50 and max iterations T = 20. For each iteration, we only keep the top-10 best candidates in the current generation, and then generate a new generation consisting of 25 crossover candidates and 25 mutation candidates. For a crossover, we randomly sample two top candidates and cross them to generate a new one; for mutation, we keep a mutation probability of 0.1. For the new candidates, We directly evaluate the output difference w.r.t. the full generator or FIDs without further fine-tuning, as our anycost generator is well trained for any subset of weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Metrics</head><p>Fréchet Inception Distance (FID). For FID <ref type="bibr" target="#b29">[30]</ref> evaluation on high-resolution images (1024 for FFHQ <ref type="bibr" target="#b44">[45]</ref> and 512 for LSUN Car <ref type="bibr" target="#b74">[75]</ref>), we follow <ref type="bibr" target="#b44">[45]</ref> to compute the FID using 50k images (i.e., FID-50k). For FID evaluation on low-resolution images (≤ 256 on FFHQ), we follow <ref type="bibr" target="#b42">[43]</ref> to compute the FID using 70k images (i.e., FID-70k), so that we can compare with the results in the paper.</p><p>Perceptual path length (PPL). We follow <ref type="bibr" target="#b44">[45]</ref> to compute the PPL metric, using 100k random samples.</p><p>Attribute consistency. We trained our own attribute predictor to evaluate the attribute consistency (can be found in the code release). We randomly sampled 10k images with truncation ψ = 0.5 to compute the attribute consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Time</head><p>We discuss the training time on FFHQ <ref type="bibr" target="#b44">[45]</ref> dataset that consists of 70k samples. Our experiments are based on a PyTorch implementation of StyleGAN2 <ref type="bibr" target="#b44">[45]</ref> † . Given a normally trained StyleGAN2 model, we first train the model for 5 million images to support multi-resolution, and then train it for another 5 million images to support adaptive channel numbers. On 8 NVIDIA Titan RTX GPUs, the training takes 5.3 days in total, which is roughly 60% of the StyleGAN2 training time. Note that we just need to train the model once, and we can run it under various computational budgets by using a smaller sub-generator, while distillation or compression-based methods need to train each sub-generator architecture one by one.</p><p>The cost of evolutionary search is small compared to the training. We used 2048 samples to evaluate the difference between the full generator and sub-generators' outputs and find the one with minimal difference. Given a certain budget, the evolutionary search can be done in 4 hours on a single GPU. Therefore, the marginal cost to support extra devices is very small. We compare the GPU hours vs. #devices in <ref type="figure" target="#fig_5">Figure 10</ref>. We can save 3.8× GPU hours when deploying on 6 different devices. This is a practical case when we cover devices from smartphones, iPads, laptops, to cloud servers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Image Projection and Editing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Extracting Editing Directions</head><p>To extract the editing directions, We use the trained generator to randomly generate 100,000 {w, G(w)} pairs, and use a pre-trained attribute classifier to give the binary attribute predictions (e.g., young or old) <ref type="bibr" target="#b67">[68]</ref>. The samples are generated using truncation ψ = 0.7. We then compute the decision boundary between positive w's and negative w's. Please refer to <ref type="bibr" target="#b67">[68]</ref> for more details. We empirically find that, finding the editing directions is less precise compared to image projection. Therefore, we only used the largest generator to compute the directions, which works pretty well for other sub-generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Discussion on Projection vs. Editing</head><p>We notice that sometimes a better projected image may not lead to an easily editable latent code. For example, in <ref type="figure" target="#fig_7">Figure 11</ref>, we compare two projected latent codes (row 1 and row 2). The second projected code has lower LPIPS loss, but after editing, there are clear artifacts on the image. We hypothesize that the projected latent code may not be in the training domain of the generator <ref type="bibr" target="#b80">[81]</ref>. Therefore, the reconstruction loss alone cannot fully reflect the quality of the projected code in image editing scenario. We left the more in-depth discussion to the future work.</p><p>In practice, we empirically find that encoder-based initialization and 100 iterations of backward optimization can lead to both good visual similarity and editing ability at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Qualitative Comparison of Image Projection</head><p>We have shown the quantitative results for image projection in the main paper <ref type="table" target="#tab_8">(Table 6</ref>). Here we compare the qualitative results of baseline image projection and consistencyaware projection to show the necessity of applying consistency constraints.   <ref type="figure">Figure 12</ref>: Our consistent-aware encoder achieves better consistency between the full and the sub-generator's outputs. Vanilla encoder trained only for the full generator leads to visible differences in clothing colors, skin tones, and more fine-grained details such as eye shapes. Both encoders have achieved the same reconstruction performance for the full generator.Results are hand-picked to reflect the difference.</p><p>Encoder-based projection. We compare the baseline encoder optimized only for the full generator, and our consistency-aware encoder optimized for both full generator and sub-generators. Both encoders achieve the same level of reconstruction performance for the full generator (measured by LPIPS+MSE loss). Given the encoder's predicted w code, we compare the outputs of the full generator and the 8× faster sub-generator. The results are shown in <ref type="figure">Figure 12</ref>. We can see that our consistency-aware encoder results in a smaller gap between the full generator and subgenerators' outputs. Vanilla encoder trained only for the full generator leads to a poor reconstruction performance for the 8× faster generator, and a noticeable difference between the two outputs: there are differences in both macro properties like skin tones, clothing colors, and fine-grained details such as eye shapes. The comparison demonstrates the advantage for our consistency-aware encoder training. Note that the images are picked to clearly demonstrate the difference.</p><p>Optimization-based projection. Given the encoder's prediction, we further optimize w to improve the projection quality. We provide a visual comparison of vanilla optimization and consistency-aware optimization in <ref type="figure" target="#fig_0">Figure 13</ref>. Again, we compare the outputs of the full generator and 8× faster sub-generator. First, despite that the encoder's predicted w can reconstruct the input image reasonably well (the second column), additional optimization can help refine details (e.g., the gaze in the first example, the earring in the second example), which is critical for editing high-resolution real images. Second, the consistent-aware optimization provides better consistency between the full and sub-generators' outputs (e.g., the clothing in the first example, and the eye shape in the second examples, the skin tone and cap color in the third example). The improved consistency allows us to provide a more accurate, fast preview for the final output. Note that the images are picked to clearly demonstrate the difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Synthesis Results</head><p>Anycost GAN (uniform) on FFHQ. We provide more results of Anycost GAN (uniform) on FFHQ dataset <ref type="bibr" target="#b44">[45]</ref> in <ref type="figure" target="#fig_1">Figure 14</ref>.</p><p>Anycost GAN (flexible) on FFHQ. We provide more results of Anycost GAN (uniform) on FFHQ dataset <ref type="bibr" target="#b44">[45]</ref> in <ref type="figure" target="#fig_2">Figure 15</ref>.</p><p>Anycost GAN (uniform) on LSUN Car. We provide more results of Anycost GAN (uniform) on LSUN Car dataset <ref type="bibr" target="#b74">[75]</ref> in <ref type="figure">Figure 16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Projection Results</head><p>We provide additional projection results on FFHQ in <ref type="figure">Figure 17</ref>. The target images are taken from the test set of CelebA-HQ dataset <ref type="bibr" target="#b41">[42]</ref> dataset. Anycost generators produce consistent outputs at various computational budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Editing Results</head><p>We provide more editing results of FFHQ generator. Similarly, we compare the edited images from the full generator and the 8× computation-reduced sub-generator. The results are shown in <ref type="figure" target="#fig_3">Figure 18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Failure Cases</head><p>The major failure cases we observed are that, some fine details are missing or changed in smaller generators' outputs, like earrings and rim. We show some example images in <ref type="figure" target="#fig_4">Figure 19</ref>. It could be a problem when we try to edit the details of images. The failure case could be mitigated by using a slightly larger sub-generator to provide better consistency (e.g., 0.5× instead of 0.25× in this example).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>①Figure 3 :</head><label>3</label><figDesc>Sampling-based multi-resolution training Anycost GAN synthesizes realistic images at a wide range of resolutions and model capacities via(1)sampling-based multi-res training, (2) adaptive-channel training, and (3) generator-conditioned discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>(a) Intermediate layers do not output realistic images for StyleGAN2 baseline. (b) Multi-resolution training in our model enables photorealistic lower-resolution outputs at intermediate layers. (c) Without consistency loss, running the model at full and half-channel produces inconsistent images. (d) Adding consistency loss maintains output consistency (similar images per column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Anycost GAN outperforms existing compression baselines<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref> at various computation budgets, despite only training a single, flexible generator across computation budgets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Anycost GAN maintains consistency after image editing, providing a quick preview at 8× computation reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>The UI interface of our face editing demo. Please refer to our demo video for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Anycost GAN has a small marginal cost for supporting extra devices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>A better projected latent code (i.e., with lower LPIPS loss) is not necessarily easily editable. The second projection (row 2) has lower LPIPS loss, but after editing, there are clear artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 19 :Figure 13 :</head><label>1913</label><figDesc>Some fine details are changed when using a subgenerator for synthesis. Consistent-aware optimization leads to better image projection for sub-generators, reducing the output difference between the full generator and sub-generators. Results are hand-picked to reflect the difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :</head><label>1415161718</label><figDesc>Additional synthesis results for anycost generator (uniform) on FFHQ. Additional synthesis results for anycost generator (flexible) on FFHQ. Additional synthesis results for anycost generator (uniform) on LSUN Car. Additional projection results for FFHQ anycost generators. Zoom in for a better view. Editing results for FFHQ anycost generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>FID-70k on FFHQ of different multi-resolution training techniques. Our sampling-based technique can train one model that produces multiple resolution outputs with higher image quality (measured by FID<ref type="bibr" target="#b29">[30]</ref>) compared to single resolution training. The models are trained with half channels (Config-E) for faster ablation.</figDesc><table><row><cell>Resolution</cell><cell cols="2">1024 512</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>32</cell></row><row><cell cols="7">Single-resolution 3.25 4.17 3.76 4.04 3.32 2.41</cell></row><row><cell>MSG-GAN [41]</cell><cell>-</cell><cell>-</cell><cell cols="2">4.79 6.34</cell><cell>2.7</cell><cell>3.04</cell></row><row><cell>Ours (low res)</cell><cell>-</cell><cell>-</cell><cell cols="4">3.49 3.26 2.52 2.18</cell></row><row><cell>Ours (high res)</cell><cell cols="4">2.99 3.08 3.35 3.98</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">on 256×256 images using config-E (half channels). More</cell></row><row><cell cols="5">experimental details are provided in Section B.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Anycost GANs achieves similar or better FIDs/path lengths at various channel widths compared to StyleGAN2, despite training a single, flexible generator.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">FID-50k↓</cell><cell></cell><cell>Path length↓</cell></row><row><cell cols="2">Channels:</cell><cell cols="5">1.0× 0.75× 0.5× 0.25× 1.0× 0.5×</cell></row><row><cell>FFHQ</cell><cell cols="2">StyleGAN2 2.84</cell><cell>-</cell><cell>3.31</cell><cell>-</cell><cell>145.0 124.5</cell></row><row><cell>1024</cell><cell>Anycost</cell><cell cols="5">2.77 3.05 3.28 5.01 144.2 147.2</cell></row><row><cell>Car</cell><cell cols="2">StyleGAN2 2.32</cell><cell>-</cell><cell>3.19</cell><cell>-</cell><cell>415.5 471.2</cell></row><row><cell>512</cell><cell>Anycost</cell><cell cols="5">2.38 2.46 2.61 3.69 380.1 430.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The generated outputs of the full and sub-generators have high attribute consistency, validated by match rates. The chosen attributes have high inconsistency for two separately trained generators ("Separate").</figDesc><table><row><cell cols="4">Smiling BlackHair Eyeglass StraightHair Earrings</cell></row><row><cell>Separate 55.8% 62.7%</cell><cell>83.4%</cell><cell>55.8%</cell><cell>50.6%</cell></row><row><cell>0.5× ch 98.1% 97.0%</cell><cell>99.9%</cell><cell>95.9%</cell><cell>94.1%</cell></row><row><cell>0.25× ch 96.9% 95.5%</cell><cell>99.8%</cell><cell>93.6%</cell><cell>88.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Inference FPS &amp; speed up rates on different devices.</figDesc><table><row><cell cols="2">FLOPs reduction 1× 2×</cell><cell>4×</cell><cell>6×</cell><cell>8×</cell><cell>10×</cell></row><row><cell>Xeon CPU FPS</cell><cell cols="3">0.63 1.78 5.94 6.24</cell><cell>7.48</cell><cell>7.35</cell></row><row><cell>speed up rate</cell><cell cols="5">1× 2.8× 9.5× 10.0× 11.9× 11.7×</cell></row><row><cell>Nano GPU FPS</cell><cell cols="3">0.65 1.17 2.77 3.59</cell><cell>4.07</cell><cell>4.1</cell></row><row><cell>speed up rate</cell><cell cols="5">1× 1.8× 4.2× 5.5× 6.2× 6.3×</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Consistency-aware encoder gives accurate projection results for both the full generator and sub-generators. It largely reduces the gap between them. Here w = E(x).</figDesc><table><row><cell></cell><cell cols="5">(G(w), x) (G (w), x) (G (w), G(w))</cell></row><row><cell></cell><cell cols="5">LPIPS MSE LPIPS MSE LPIPS MSE</cell></row><row><cell>ALAE [63]</cell><cell>0.32 0.15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IDInvert [81]</cell><cell>0.22 0.06</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>pSp [66]</cell><cell>0.19 0.04</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">Ours (fullG-only) 0.13 0.03 0.17 0.04 0.07</cell><cell>0.012</cell></row><row><cell cols="5">Ours (full+subG) 0.13 0.03 0.14 0.03 0.02</cell><cell>0.003</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">† https://github.com/rosinality/stylegan2-pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We thank Taesung Park and Zhixin Shu for the helpful discussion. Part of the work is supported under NSF CAREER Award #1943349. We thank MIT-IBM Watson AI Lab for the support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2stylegan: How to embed images into the stylegan latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Im-age2stylegan++: How to edit the embedded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Compressing gans using knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeline</forename><surname>Aguinaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Yeh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameya</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kolten</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Feizi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00159</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mimicgan: Robust projection onto image manifolds with corruption mimicking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rushil</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer-Timo</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic photo manipulation with a generative image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural photo editing with introspective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Once for All: Train One Network and Specialize it for Efficient Deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno>2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A progressive refinement approach to fast radiosity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenchang</forename><forename type="middle">Eric</forename><surname>Michael F Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald P</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Autogan-distiller: Searching to compress generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08198</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ganalyze: Toward visual definitions of cognitive image properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lore</forename><surname>Goetschalckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling the interaction of light between diffuse surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cindy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">E</forename><surname>Goral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">P</forename><surname>Torrance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bennett</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battaile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="222" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image processing using multi-code gan prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Heterogeneous face attribute estimation: A deep multitask learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Design automation for efficient deep learning computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10616</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ganspace: Discovering interpretable gan controls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Härkönen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Bernhard Nessler, and Sepp Hochreiter</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Slimmable generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02244</idno>
		<title level="m">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transforming and projecting images into class-conditional generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<idno>2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the&quot;steerability&quot; of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The rendering equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kajiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Msg-gan: Multi-scale gradients for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Diverse image-to-image translation via disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gan compression: Efficient architectures for interactive conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Universal style transfer via feature transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="385" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mcunet: Tiny deep learning on iot devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">On the limited memory bfgs method for large scale optimization. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02793</idno>
		<title level="m">Generative adversarial networks for image and video synthesis: Algorithms and applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Face model compression by distilling knowledge from neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adversarial latent autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Alaluf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Nitzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stav</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00951</idno>
		<title level="m">Encoding in style: a stylegan encoder for image-to-image translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Co-evolutionary compression for unpaired image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Gan slimming: All-in-one gan compression by a unified optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shupeng</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Haq: Hardware-aware automated quantization with mixed precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Self-supervised gan compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01491</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Trained ternary quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenzhuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Indomain gan inversion for real image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
