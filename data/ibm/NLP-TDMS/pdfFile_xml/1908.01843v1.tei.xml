<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
							<email>cheng-ya14@mails.tsinghua.edu.cnfandywang</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent Marketing Solution</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Tencent Marketing Solution</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fact verification (FV) is a challenging task which requires to retrieve relevant evidence from plain text and use the evidence to verify given claims. Many claims require to simultaneously integrate and reason over several pieces of evidence for verification. However, previous work employs simple models to extract information from evidence without letting evidence communicate with each other, e.g., merely concatenate the evidence for processing. Therefore, these methods are unable to grasp sufficient relational and logical information among the evidence. To alleviate this issue, we propose a graph-based evidence aggregating and reasoning (GEAR) framework which enables information to transfer on a fully-connected evidence graph and then utilizes different aggregators to collect multievidence information. We further employ BERT, an effective pre-trained language representation model, to improve the performance. Experimental results on a large-scale benchmark dataset FEVER have demonstrated that GEAR could leverage multi-evidence information for FV and thus achieves the promising result with a test FEVER score of 67.10%. Our code is available at https://github. com/thunlp/GEAR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the rapid development of information extraction (IE), huge volumes of data have been extracted.</p><p>How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion <ref type="bibr" target="#b28">(Wang et al., 2017)</ref> and open domain question answering <ref type="bibr" target="#b2">(Chen et al., 2017a)</ref>. Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) "SUPPORTED" Example</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Claim</head><p>The Rodney King riots took place in the most populous county in the USA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evidence</head><p>(1) The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992.</p><p>(2) Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.</p><p>"REFUTED" Example Claim Giada at Home was only available on DVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evidence</head><p>(1) Giada at Home is a television show and first aired on October 18, 2008, on the Food Network.</p><p>(2) Food Network is an American basic cable and satellite television channel. <ref type="table">Table 1</ref>: Some examples of reasoning over several pieces of evidence together for verification. The italic words are the key information to verify the claim. Both of the claims require to reason and aggregate multiple evidence sentences for verification.</p><p>More specifically, given a claim, an FV system is asked to label it as "SUPPORTED", "REFUTED", or "NOT ENOUGH INFO", which indicate that the evidence can support, refute, or is not sufficient for the claim.</p><p>Existing FV methods formulate FV as a natural language inference (NLI) <ref type="bibr" target="#b0">(Angeli and Manning, 2014)</ref> task. However, they utilize simple evidence combination methods such as concatenating the evidence or just dealing with each evidence-claim pair. These methods are unable to grasp sufficient relational and logical information among the evidence. In fact, many claims require to simultaneously integrate and reason over several pieces of evidence for verification. As shown in Table 1, for both of the "SUPPORTED" example and "REFUTED" example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence.</p><p>To integrate and reason over information from multiple pieces of evidence, we propose a graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected evidence graph and encourage information propagation among the evidence. Then, we aggregate the pieces of evidence and adopt a classifier to decide whether the evidence can support, refute, or is not sufficient for the claim. Intuitively, by sufficiently exchanging and reasoning over evidence information on the evidence graph, the proposed model can make the best of the information for verifying claims. For example, by delivering the information "Los Angeles County is the most populous county in the USA" to "the Rodney King riots occurred in Los Angeles County" through the evidence graph, the synthetic information can support "The Rodney King riots took place in the most populous county in the USA". Furthermore, we adopt an effective pretrained language representation model BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> to better grasp both evidence and claim semantics.</p><p>We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) <ref type="bibr" target="#b25">(Thorne et al., 2018a)</ref>. Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">FEVER Shared Task</head><p>The FEVER shared task <ref type="bibr">(Thorne et al., 2018b)</ref> challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab 1 with a blind test set. <ref type="bibr" target="#b18">Nie et al. (2019)</ref>; <ref type="bibr" target="#b31">Yoneda et al. (2018)</ref> and <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> have achieved the top three results among 23 teams.</p><p>Existing methods mainly formulate FV as an NLI task. <ref type="bibr" target="#b25">Thorne et al. (2018a)</ref> simply concatenate all evidence together, and then feed the concatenated evidence and the given claim into the NLI model. <ref type="bibr" target="#b14">Luken et al. (2018)</ref> adopt the decomposable attention model (DAM) <ref type="bibr" target="#b20">(Parikh et al., 2016)</ref> to generate NLI predictions for each claimevidence pair individually and then aggregate all NLI predictions for final verification. Then, <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>; <ref type="bibr" target="#b31">Yoneda et al. (2018)</ref>; <ref type="bibr" target="#b11">Hidey and Diab (2018)</ref> adopt the enhanced sequential inference model (ESIM) <ref type="bibr" target="#b3">(Chen et al., 2017b)</ref>, a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, <ref type="bibr" target="#b15">Malon (2018)</ref> fine-tunes the generative pretraining transformer (GPT) <ref type="bibr" target="#b22">(Radford et al., 2018)</ref> for FV. Based on the methods mentioned above, <ref type="bibr" target="#b18">Nie et al. (2019)</ref> specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, <ref type="bibr" target="#b30">Yin and Roth (2018)</ref> propose the TWOWINGOS system which trains the evidence identification and claim verification modules jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Natural Language Inference</head><p>The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref> and Multi-NLI <ref type="bibr" target="#b29">(Williams et al., 2018)</ref>. These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results <ref type="bibr" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr" target="#b20">Parikh et al., 2016;</ref><ref type="bibr" target="#b23">Sha et al., 2016;</ref><ref type="bibr">Chen et al., 2017b,c;</ref><ref type="bibr" target="#b16">Munkhdalai and Yu, 2017;</ref><ref type="bibr" target="#b17">Nie and Bansal, 2017;</ref><ref type="bibr" target="#b5">Conneau et al., 2017;</ref><ref type="bibr" target="#b9">Gong et al., 2018;</ref><ref type="bibr" target="#b24">Tay et al., 2018;</ref><ref type="bibr" target="#b8">Ghaeini et al., 2018)</ref>. It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-trained Language Models</head><p>Pre-trained language representation models such as ELMo  and OpenAI GPT <ref type="bibr" target="#b22">(Radford et al., 2018)</ref> are proven to be effective on many NLP tasks. BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> employs bidirectional transformer and welldesigned pre-training tasks to fuse bidirectional context information and obtains the state-of-theart results on the NLI task. In our experiments, we find the fine-tuned BERT model outperforms other NLI-based models on the claim verification subtask of FEVER. Hence, we use BERT as the sentence encoder in our framework to better encoding semantic information of evidence and claims.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ERNet</head><p>Layer 0 Layer T … <ref type="figure">Figure 1</ref>: The pipeline of our method. The GEAR framework is illustrated in the claim verification section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We employ a three-step pipeline with components for document retrieval, sentence selection and claim verification to solve the task. In the document retrieval and sentence selection stages, we simply follow the method from <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> since their method has the highest score on evidence recall in the former FEVER shared task. And we propose our Graph-based Evidence Aggregating and Reasoning (GEAR) framework in the final claim verification stage. The full pipeline of our method is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Retrieval and Sentence Selection</head><p>In this section, we describe our document retrieval and sentence selection components. Additionally, we add a threshold filter after the sentence selection component to filter out those noisy evidence.</p><p>In the document retrieval step, we adopt the entity linking approach from <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>. Given a claim, the method first utilizes the constituency parser from AllenNLP  to extract potential entities from the claim. Then it uses the entities as search queries and finds relevant Wikipedia documents via the online MediaWiki API 2 . The seven highest-ranked results for each query are stored to form a candidate article set. Finally, the method drops the articles which are not in the offline Wikipedia dump and filters the articles by the word overlap between their titles and the claim.</p><p>The sentence selection component selects the most relevant evidence for the claim from all sentences in the retrieved documents. <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref> modify the ESIM model to compute the relevance score between the evidence and the claim. In the training phase, the model uses the hinge loss function max(0, 1 + s n −s p ) with the negative sampling strategy, where s p and s n denote the relevance scores of positive and negative samples. In the test phase, the final model ensembles the results from 10 models with different random seeds. Sentences with top-5 relevance scores are selected to form the final evidence set in the original method.</p><p>In addition to the original model <ref type="bibr" target="#b10">(Hanselowski et al., 2018)</ref>, we add a relevance score filter with a threshold τ . Sentences with relevance scores lower than τ are filtered out to alleviate the noises. Thus the final size of the retrieved evidence set is equal to or less than 5. We choose different values of τ and select the value based on the dev set result. The evaluation results of the document retrieval and sentence selection components are shown in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Claim Verification with GEAR</head><p>In this section, we describe our GEAR framework for claim verification. As shown in <ref type="figure">Figure 1</ref>, given a claim and the retrieved evidence, we first utilize a sentence encoder to obtain representations for the claim and the evidence. Then we build a fully-connected evidence graph and propose an evidence reasoning network (ERNet) to propagate information among evidence and reason over the graph. Finally, we utilize an evidence aggregator to infer the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Encoder</head><p>Given an input sentence, we employ BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> as our sentence encoder by extracting the final hidden state of the [CLS] token as the representation, where [CLS] is the special classification embedding in BERT.</p><p>Specifically, given a claim c and N pieces of retrieved evidence {e 1 , e 2 , ..., e N }, we feed each evidence-claim pair (e i , c) into BERT to obtain the evidence representation e i . We also feed the claim into BERT alone to obtain the claim presentation c. That is,</p><formula xml:id="formula_0">e i = BERT (e i , c) , c = BERT (c) .</formula><p>(1)</p><p>Note that we concatenate the evidence and the claim to extract the evidence representation because the evidence nodes in the reasoning graph need the information from the claim to guide the message passing process among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evidence Reasoning Network</head><p>To encourage the information propagation among evidence, we build a fully-connected evidence graph where each node indicates a piece of evidence. We also add self-loop to every node because each node needs the information from itself in the message propagation process. We use h t = {h t 1 , h t 2 , ..., h t N } to represent the hidden states of nodes at layer t, where h t i ∈ R F ×1 and F is the number of features in each node. The initial hidden state of each evidence node h 0 i is initialized by the evidence presentation: h 0 i = e i . Inspired by recent work on semi-supervised graph learning and relational reasoning <ref type="bibr" target="#b13">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b27">Velickovic et al., 2018;</ref><ref type="bibr" target="#b19">Palm et al., 2018)</ref>, we propose an evidence reasoning network (ERNet) to propagate information among the evidence nodes. We first use an MLP to compute the attention coefficients between a node i and its neighbor j (j ∈ N i ),</p><formula xml:id="formula_1">p ij = W t−1 1 (ReLU(W t−1 0 (h t−1 i h t−1 j ))),<label>(2)</label></formula><p>where N i denotes the set of neighbors of node i,</p><formula xml:id="formula_2">W t−1 0 ∈ R H×2F and W t−1 1 ∈ R 1×H</formula><p>are weight matrices, and · · denotes concatenation operation.</p><p>Then, we normalize the coefficients using the softmax function,</p><formula xml:id="formula_3">α ij = softmax j (p ij ) = exp(p ij ) k∈N i exp(p ik ) .<label>(3)</label></formula><p>Finally, the normalized attention coefficients are used to compute a linear combination of the neighbor features and thus we obtain the features for node i at layer t,</p><formula xml:id="formula_4">h t i = j∈N i α ij h t−1 j .<label>(4)</label></formula><p>By stacking T layers of ERNet, we assume that each evidence could grasp enough information by communicating with other evidence. We feed the final hidden states of evidence nodes {h T 1 , h T 2 , ..., h T N } into our evidence aggregator to make the final inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evidence Aggregator</head><p>We employ an evidence aggregator to gather information from different evidence nodes and obtain the final hidden state o ∈ R F ×1 . The aggregator may utilize different aggregating strategies and we suggest three aggregators in our framework:</p><p>Attention Aggregator. Here we use the representation of the claim c to attend the hidden states of evidence and get the final aggregated state o.</p><formula xml:id="formula_5">p j = W 1 (ReLU(W 0 (c h T j ))), α j = softmax(p j ) = exp(p j ) N k=1 exp(p k ) , o = N k=1 α k h T k ,<label>(5)</label></formula><p>where W 0 ∈ R H×2F and W 1 ∈ R 1×H . Max Aggregator. The max aggregator performs the element-wise Max operation among hidden states.</p><formula xml:id="formula_6">o = Max(h T 1 , h T 2 , ..., h T N ).<label>(6)</label></formula><p>Mean Aggregator. The mean aggregator performs the element-wise Mean operation among hidden states.</p><formula xml:id="formula_7">o = Mean(h T 1 , h T 2 , ..., h T N ).<label>(7)</label></formula><p>Once the final state o is obtained, we employ a one-layer MLP to get the final prediction l.</p><formula xml:id="formula_8">l = softmax(ReLU(Wo + b)),<label>(8)</label></formula><p>where W ∈ R C×F and b ∈ R C×1 are parameters, and C is the number of prediction labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We conduct our experiments on the large-scale dataset FEVER <ref type="bibr" target="#b25">(Thorne et al., 2018a)</ref>. The dataset consists of 185,455 annotated claims with a set of 5,416,537 Wikipedia documents from the June 2017 Wikipedia dump. We follow the dataset partition from the FEVER Shared Task <ref type="bibr">(Thorne et al., 2018b)</ref>. <ref type="table" target="#tab_2">Table 2</ref> shows the statistics of the dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>In this section, we describe the baseline systems in our experiments. We first introduce the top-3 systems from the FEVER shared task. As BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> has achieved promising performance on several NLP tasks, we also implement two baseline systems via fine-tuning BERT in the claim verification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Task Systems</head><p>We choose the top-3 models from the FEVER shared task as our baselines. The Athene UKP TU Darmstadt team (Athene) <ref type="bibr" target="#b10">(Hanselowski et al., 2018)</ref> combines five inference vectors from the ESIM model via attention mechanism to make the final prediction.</p><p>The UCL Machine Reading Group (UCL MRG) <ref type="bibr" target="#b31">(Yoneda et al., 2018)</ref> predicts the label of each evidence-claim pair and aggregates the results via a label aggregation component.</p><p>The UNC NLP team <ref type="bibr" target="#b18">(Nie et al., 2019)</ref> proposes the neural semantic matching network and uses the model jointly to solve all three subtasks. They also incorporate additional information such as pageview frequency and WordNet features. They have achieved best results in the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT Fine-tuning Systems</head><p>We implement two BERT fine-tuning systems with different evidence combination approaches. The BERT-Concat system concatenates all evidence into a single string while the BERT-Pair system encodes each evidence-claim pair independently and then aggregates the results. Both systems share the same document retrieval and sentence selection components proposed by us.</p><p>BERT-Concat. In the BERT-Concat system, we simply concatenate all evidence into a single sentence and utilize BERT to predict the relation between the concatenated evidence and the claim. In the training phase, we add the ground truth evidence into the retrieved evidence set with relevance score 1 and select five pieces of evidence with the highest scores. In the test phase, we concatenate the retrieved evidence for predicting.</p><p>BERT-Pair. In the BERT-Pair system, we utilize BERT to predict the label for each evidenceclaim pair. Concretely, we use each evidenceclaim pair as the input and the label of the claim as the prediction target. In the training phase, we select the ground truth evidence for SUPPORTED and REFUTED claims and the retrieved evidence for NEI claims. In the test phase, we predict labels for all retrieved evidence-claim pairs. Because different evidence-claim pairs may have inconsistent predicted labels, we then utilize an aggregator to obtain the final claim label. We find the aggregator only returning the predicted label from the most relevant evidence has the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyperparameter Settings</head><p>We utilize BERT BASE <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> in all of the BERT fine-tuning baselines and our GEAR framework. The learning rate is 2e-5.</p><p>For BERT-Concat, the maximum sequence length is 256 and the batch size is 16. We limit the max length for concatenated evidence to 240 and the max length for claims to 16. We train this model for two epochs based on dev results. For BERT-Pair, we set the maximum sequence length to 128 and batch size to 32. We train this model for one epoch. As for the GEAR framework, we use the fine-tuned BERT-Pair model to extract features and the batch size is 512.</p><p>In our ERNet, we set the batch size to 256, the number of features F to 768 and the dimension of weight matrices H to 64. The model is trained to minimize the negative log likelihood loss on the predicted label using the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 5e-3 and L2 weight decay of 5e-4. We use an early stopping strategy on the label accuracy of the validation set, with a patience of 20 epochs. We attempt to stack 0-3 ERNet layers and analyze the effect of different layer numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>Besides traditional evaluation metrics such as label accuracy and F1, we use other two metrics to evaluate our model.</p><p>FEVER score. The FEVER score is the label accuracy conditioned on providing at least one complete set of evidence. Claims labeled as "NEI" do not need the evidence.   OFEVER score. The document retrieval and sentence selection components are usually evaluated by the oracle FEVER (OFEVER) score, which is the upper bound of the FEVER score by assuming perfect downstream systems.</p><p>For all of the experiments with GEAR, the scores (label accuracy, FEVER score) we report on the dev set are mean values with 10 runs initialized by different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results and Analysis</head><p>In this section, we first present the evaluations of the document retrieval and sentence selection components. Then we evaluate our GEAR framework in several different aspects. Finally, we present a case study to demonstrate the effectiveness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Document Retrieval and Sentence Selection</head><p>We use the OFEVER metric to evaluate the document retrieval component. <ref type="table" target="#tab_4">Table 3</ref> shows the OFEVER scores of our model and models from other teams. After running the same model proposed by <ref type="bibr" target="#b10">Hanselowski et al. (2018)</ref>, we find our OFEVER score is slightly lower, which may due to the random factors.</p><p>Then we compare our sentence selection component with different thresholds, as shown in Table 4. We find the model with threshold 0 achieves the highest recall and OFEVER score. When the threshold increases, the recall value and the OFEVER score drop gradually while the precision and F1 score increase. The results are consistent with our intuition. If we do not filter out evidence, more claims could be provided with the full evidence set. If we increase the value of the threshold, more pieces of noisy evidence are filtered out, which contributes to the increase of precision and F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Claim Verification with GEAR</head><p>In this section, we evaluate our GEAR framework in different aspects. We first compare the label accuracy scores between our framework and baseline systems. Then we explore the effect of different thresholds from the upstream sentence filter. We also conduct additional experiments to check the effect of sentence embedding. As there are nearly 39% of claims require reasoning over multiple pieces of evidence, we construct a difficult dev subset and check the effectiveness of our ER-Net for evidence reasoning. Finally, we make an error analysis and provide the theoretical upperbound label accuracy of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Evaluation</head><p>We use the label accuracy metric to evaluate the effectiveness of different claim verification models. The second column of <ref type="table" target="#tab_10">Table 7</ref> shows the label accuracy of different models on the dev set. We find the BERT fine-tuning models outperform all of the models from the shared task, which shows the strong capacity of BERT in representation learning and semantic understanding. The BERT-Concat model has a slight improvement over BERT-Pair, which is 0.37%.</p><p>Our final model outperforms the best BERT-Concat baseline by 1.17%. As our framework provides a better way for evidence aggregating and reasoning, the improvement demonstrates that our framework has a better ability to integrate features from different evidence by propagating, analyzing and aggregating the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Sentence Thresholds</head><p>The rightmost column of <ref type="table" target="#tab_5">Table 4</ref> shows the results of our GEAR frameworks with different sentence selection thresholds. We choose the model with threshold τ = 10 −3 , which has the highest label accuracy, as our final model. When the threshold increases from 0 to 10 −3 , the label accuracy increases due to less noisy information. However, when the threshold increases from 10 −3 to 10 −1 ,   the label accuracy decreases because informative evidence is filtered out, and the model can not obtain sufficient evidence to make the right inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Sentence Embedding</head><p>The BERT model we used in the sentence encoding step is fine-tuned on the FEVER dataset for one epoch. We need to find out whether the finetuning process or simply incorporating the sentence embeddings from BERT makes the major contribution to the final result. We conduct an experiment using a BERT model without the finetuning process and we find the final dev label accuracy is close to the result from a random guess. Therefore, the fine-tuning process rather than sentence embeddings plays an important role in this task. We need the fine-tuning process to capture the semantic and logical relations between evidence and the claim. Sentence embeddings are more general and cannot perform well in this specific task. So that we cannot just use sentence embeddings from other methods (e.g., ELMo, CNN) to replace the sentence embeddings we used here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of ERNet</head><p>In our observation, more than half of the claims in the dev dataset only need one piece of evidence to make the right inference. To verify the effectiveness of our framework on reasoning over multiple pieces of evidence, we build a difficult dev sub-  We test our final model on the difficult subset and present the results in <ref type="table" target="#tab_7">Table 5</ref>. We find our models with ERNet perform better than models without ERNet and the minimal improvement between them is 1.27%. We can also discover from the table that models with 2 ERNet layers achieve the best results, which indicates that claims from the difficult subset require multi-step evidence propagation. This result demonstrates the ability of our framework to deal with claims which need multiple evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Analysis</head><p>In this section, we examine the effect of errors propagating from upstream components. We utilize an evidence-enhanced dev subset, which assumes all pieces of ground truth evidence are retrieved, to test the theoretical upper-bound score of our GEAR framework.</p><p>In our analysis, the main errors of our framework come from the upstream document retrieval and sentence selection components which can not extract sufficient evidence for inferring. For example, to verify the claim "Giada at Home was only available on DVD", we need the evidence "Giada at Home is a television show and first aired on October 18, 2008, on the Food Network." and "Food Network is an American basic cable and satellite television channel.". However, the entity linking Claim: Al Jardine is an American rhythm guitarist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truth evidence:</head><p>{Al Jardine, 0}, {Al Jardine, 1}</p><p>Retrieved evidence: {Al Jardine, 1}, {Al Jardine, 0}, {Al Jardine, 2}, {Al Jardine, 5}, {Jardine, 42}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evidence:</head><p>(1) He is best known as the band's rhythm guitarist, and for occasionally singing lead vocals on singles such as "Help Me, Rhonda" (1965), "Then I Kissed Her" <ref type="bibr">(1965)</ref> and "Come Go with Me" (1978).</p><p>(2) Alan Charles Jardine (born September 3, 1942) is an American musician, singer and songwriter who co-founded the Beach Boys.</p><p>(3) In 2010, Jardine released his debut solo studio album, A Postcard from California. (4) In 1988, Jardine was inducted into the Rock and Roll Hall of Fame as a member of the Beach Boys. (5) Ray Jardine American rock climber, lightweight backpacker, inventor, author and global adventurer.</p><p>Label: SUPPORTED <ref type="table">Table 8</ref>: A case of the claim that requires integrating multiple evidence to verify. The representation for evidence "{DocName, LineNum}" means the evidence is extracted from the document "DocName" and of which the line number is LineNum. method used in our document retrieval component could not retrieve the "Food Network" document only from parsing the content of the claim. Thus the claim verification component can not make the right inference with insufficient evidence.</p><p>To explore the effect of this issue, we test our models on an evidence-enhanced dev set, in which we add the ground truth evidence with relevance score 1 into the evidence set before the sentence threshold filter. It ensures that each claim in the evidence-enhanced set is provided with the ground truth evidence as well as the retrieved evidence.</p><p>The experimental results are shown in <ref type="table" target="#tab_8">Table 6</ref>. We can find that all scores in the table increase by more than 1.4% compared to the original dev set label accuracy in <ref type="table" target="#tab_10">Table 7</ref> because of the addition of the ground truth evidence. Because of the assumption of oracle upstream components, the results in <ref type="table" target="#tab_8">Table 6</ref> indicate the theoretical upper bound label accuracy of our framework.</p><p>The results show the challenges in the previous evidence retrieval task, which could not be solved by current models. <ref type="bibr" target="#b18">Nie et al. (2019)</ref> propose a two-hop evidence enhancement method which improves 0.08% on their final FEVER score. As the addition of the ground truth evidence leads to a  <ref type="figure">Figure 2</ref>: Attention map for the case in <ref type="table">Table 8</ref>. The first five rows indicate the attention weights from nodes 1 to 5 in the first ERNet layer and the last row shows the attention weights from the attention aggregator. more than 1.4% increase in our experiment, it is worthwhile to design a better evidence retrieval pipeline, which remains to be our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Full Pipeline</head><p>We present the evaluation of our full pipeline in this section. Note that there is a gap between the label accuracy and the final FEVER score due to the completeness of the evidence set. We find that a model which is good at predicting NEI instances tends to obtain higher FEVER score. So we choose our final model based on the dev FEVER score among all of our experiments. This model contains one layer of ERNet and uses the attention aggregator. The threshold of the sentence filter is 10 −3 . <ref type="table" target="#tab_10">Table 7</ref> presents the evaluations of the full pipeline. We find the test FEVER score of BERT fine-tuning systems outperform other shared task models by nearly 1%. Furthermore, our full pipeline outperforms the BERT-Concat baseline by 1.46% and achieves significant improvements. <ref type="table">Table 8</ref> shows an example in our experiments which needs multiple pieces of evidence to make the right inference. The ground truth evidence set contains the sentences from the article "Al Jardine" with line number 0 and 1. These two pieces of evidence are also ranked at top two in our retrieved evidence set. To verify whether "Al Jardine is an American rhythm guitarist", our model needs the evidence "He is best known as the bands rhythm guitarist" as well as the evidence "Alan Charles Jardine ... is an American musician". We plot the attention map from our final model with one layer of ERNet and the attention aggregator in <ref type="figure">Figure 2</ref>. We can find that all evidence nodes tend to attend the first and the second evidence nodes, which provide the most useful information in this case. The attention weights in other evidence nodes are pretty low, which indicates that our model has the ability to select useful information from multiple pieces of evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a novel Graph-based Evidence Aggregating and Reasoning (GEAR) framework on the claim verification subtask of FEVER. The framework utilizes the BERT sentence encoder, the evidence reasoning network (ERNet) and an evidence aggregator to encode, propagate and aggregate information from multiple pieces of evidence. The framework is proven to be effective and our final pipeline achieves significant improvements. In the future, we would like to design a multi-step evidence extractor and incorporate external knowledge into our framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>…</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of FEVER dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: Document retrieval evaluation on dev set (%).</cell></row><row><cell cols="3">('-' denotes a missing value)</cell><cell></cell><cell></cell></row><row><cell>τ</cell><cell cols="3">OFEVER Precision Recall</cell><cell>F1</cell><cell>GEAR LA</cell></row><row><cell>0</cell><cell>91.10</cell><cell>24.08</cell><cell cols="2">86.72 37.69</cell><cell>74.84</cell></row><row><cell>10 −4</cell><cell>91.04</cell><cell>30.88</cell><cell cols="2">86.63 45.53</cell><cell>74.86</cell></row><row><cell>10 −3</cell><cell>90.86</cell><cell>40.60</cell><cell cols="2">86.36 55.23</cell><cell>74.91</cell></row><row><cell>10 −2</cell><cell>90.27</cell><cell>53.12</cell><cell cols="2">85.47 65.52</cell><cell>74.89</cell></row><row><cell>10 −1</cell><cell>87.70</cell><cell>70.61</cell><cell cols="2">81.64 75.72</cell><cell>74.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Sentence selection evaluation and average la-</cell></row><row><cell>bel accuracy of GEAR with different thresholds on dev</cell></row><row><cell>set (%).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Label accuracy on the difficult dev set with different ERNet layers and evidence aggregators (%).</figDesc><table><row><cell>ERNet Layers</cell><cell cols="2">Aggregator</cell></row><row><cell></cell><cell cols="2">Attention Max Mean</cell></row><row><cell>0</cell><cell>77.12</cell><cell>76.95 76.30</cell></row><row><cell>1</cell><cell>77.74</cell><cell>77.66 77.62</cell></row><row><cell>2</cell><cell>77.82</cell><cell>77.66 77.73</cell></row><row><cell>3</cell><cell>77.70</cell><cell>77.55 77.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Label accuracy on the evidence-enhanced dev</cell></row><row><cell>set with different ERNet layers and evidence aggrega-</cell></row><row><cell>tors (%).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Evaluations of the full pipeline.</figDesc><table><row><cell>The results of</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/ competitions/18814</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.mediawiki.org/wiki/API: Main_page</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Naturalli: Natural logic inference for common sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="534" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural network-based sentence encoder with gated attention for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>ACL, Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Allennlp: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, Workshop for NLP Open Source Software</title>
		<meeting>ACL, Workshop for NLP Open Source Software</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dr-bilstm: Dependent reading bidirectional lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oladimeji</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1460" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ukp-athene: Multi-sentence textual entailment for claim verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hanselowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Team sweeper: Joint sentence extraction and fact checking with pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on FEVER</title>
		<meeting>the First Workshop on FEVER</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Qed: A fact verification system for the fever shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackson</forename><surname>Luken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanjiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on FEVER</title>
		<meeting>the First Workshop on FEVER</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="156" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Team papelo: Transformer networks at fever</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Malon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on FEVER</title>
		<meeting>the First Workshop on FEVER</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="109" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shortcut-stacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>ACL, Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Palm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3368" to="3378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Technical report</title>
		<meeting>Technical report</meeting>
		<imprint>
			<publisher>OpenAI</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reading and thinking: Re-read lstm unit for textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2870" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compare, compress and propagate: Enhancing neural architectures with alignment factorization for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1565" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fever: a large-scale dataset for fact extraction and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
	<note>Christos Christodoulopoulos, and Arpit Mittal</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Christos Christodoulopoulos, and Arpit Mittal. 2018b. The Fact Extraction and VERification (FEVER) shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on FEVER</title>
		<meeting>the First Workshop on FEVER</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Twowingos: A two-wing optimization strategy for evidential claim verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ucl machine reading group: Four factor framework for fact finding (hexaf)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on FEVER</title>
		<meeting>the First Workshop on FEVER</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
