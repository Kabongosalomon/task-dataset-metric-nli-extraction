<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TrackFormer: Multi-Object Tracking with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TrackFormer: Multi-Object Tracking with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatiotemporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the new concept of identity preserving track queries. Both decoder query types benefit from self-and encoder-decoder attention on global framelevel features, thereby omitting any additional graph optimization and matching or modeling of motion and appearance. TrackFormer represents a new tracking-by-attention paradigm and yields state-of-the-art performance on the task of multi-object tracking (MOT17) and segmentation (MOTS20). The code is available at https://github. com/timmeinhardt/trackformer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans need to focus their attention to track objects in space and time, for example, when playing a game of tennis, golf, or pong. This challenge is only increased when tracking not one, but multiple objects, in crowded and real world scenarios. Following this analogy, we demonstrate the effectiveness of Transformer <ref type="bibr" target="#b46">[47]</ref> attention for the task of multi-object tracking (MOT) in videos.</p><p>The goal in MOT is to follow the trajectories of a set of objects, e.g., pedestrians, while keeping their identities discriminated as they are moving throughout a video sequence. With progress in image-level object detectors <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b6">7]</ref>, most approaches follow the tracking-by-detection paradigm which consists of two-steps: (i) detecting objects in individual video frames, and (ii) associating sets of detections between frames, thereby creating individual object tracks over * Work done during an internship at Facebook AI Research. time. Traditional tracking-by-detection methods associate detections via temporally sparse <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> or dense <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17]</ref> graph optimization, or apply convolutional neural networks to predict matching scores between detections <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27]</ref> suggest a variation of the traditional paradigm, coined tracking-by-regression <ref type="bibr" target="#b11">[12]</ref>. In this approach, the object detector not only provides frame-wise detections, but replaces the data association step with a continuous regression of each track to the changing position of its object. These approaches achieve track association implicitly, but achieve top performance only by relying either on additional graph optimization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27]</ref> or motion and appearance models <ref type="bibr" target="#b3">[4]</ref>. This is largely due to the isolated and local bounding box regression which lacks any notion of object identity or global communication between tracks.</p><p>In this work, we introduce a tracking-by-attention paradigm which formulates MOT as a set prediction problem. Our paradigm not only applies attention for data association <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b10">11]</ref>, but performs tracking and detection in a unified way. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, this is achieved by evolving a set of track predictions from frame to frame forming trajectories over time.</p><p>We present TrackFormer, an end-to-end trainable Transformer <ref type="bibr" target="#b46">[47]</ref> encoder-decoder architecture which encodes frame-level features from a convolutional neural network (CNN) <ref type="bibr" target="#b15">[16]</ref> and decodes queries into bounding boxes associated with identities. The data association is performed through the nove and simple concept of track queries. Each query represents an object and follows it in space and time over the course of a video sequence in an autoregressive fashion. New objects entering the scene are detected by static object queries as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b57">58]</ref> and subsequently transform to future track queries. At each frame, the encoderdecoder processes the input image features, as well as the track and object queries, and outputs bounding boxes with assigned identities. Thereby, TrackFormer achieves detection and data association jointly via tracking-by-attention without relying on any additional track matching, graph optimization, or explicit modeling of motion and appearance. Our model is trained end-to-end and extends the recently proposed set prediction objective for object detection <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b57">58]</ref> to multi-object tracking.</p><p>We evaluate TrackFormer on the MOT17 <ref type="bibr" target="#b27">[28]</ref> benchmark where it achieves state-of-the-art performance for public detections. Furthermore, we demonstrate the flexibility of our model with an additional mask prediction head and show state-of-the-art results on the Multi-Object Tracking and Segmentation (MOTS20) challenge <ref type="bibr" target="#b47">[48]</ref>.</p><p>In summary, we make the following contributions:</p><p>• An end-to-end multi-object tracking approach which achieves detection and data association in a trackingby-attention paradigm.</p><p>• The concept of autoregressive track queries which embed an object's spatial position and identity, thereby tracking it in space and time.</p><p>• State-of-the-art results on two challenging multiobject tracking (MOT17) and segmentation (MOTS20) benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In light of the recent trend to look beyond data association of given detections, we categorize and review methods according to their respective tracking paradigm.</p><p>Tracking-by-detection approaches form trajectories by associating a given set of detections over time.</p><p>Graphs have been used for track association and longterm re-identification by formulating the problem as a maximum flow (minimum cost) optimization <ref type="bibr" target="#b2">[3]</ref> with distance based <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref> or learned costs <ref type="bibr" target="#b22">[23]</ref>. Other methods use association graphs <ref type="bibr" target="#b43">[44]</ref>, learned models <ref type="bibr" target="#b20">[21]</ref>, and motion information <ref type="bibr" target="#b19">[20]</ref>, general-purpose solvers <ref type="bibr" target="#b52">[53]</ref>, multicuts <ref type="bibr" target="#b45">[46]</ref>, weighted graph labeling <ref type="bibr" target="#b16">[17]</ref>, edge lifting <ref type="bibr" target="#b17">[18]</ref>, or trainable graph neural networks <ref type="bibr" target="#b5">[6]</ref>. However, graphbased approaches suffer from expensive optimization routines, limiting their practical application for online tracking.</p><p>Appearance driven methods capitalize on increasingly powerful image recognition backbones to track objects by relying on similarity measures given by twin neural networks <ref type="bibr" target="#b21">[22]</ref>, learned reID features <ref type="bibr" target="#b39">[40]</ref>, detection candidate selection <ref type="bibr" target="#b7">[8]</ref> or affinity estimation <ref type="bibr" target="#b9">[10]</ref>. Just like for reidentification, appearance models struggle in crowded scenarios with many object-object-occlusions.</p><p>Motion can be modelled for trajectory prediction <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b40">41]</ref> using a constant velocity assumption (CVA) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref> or the social force model <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b23">24]</ref>. Learning a motion model from data <ref type="bibr" target="#b22">[23]</ref> can also accomplish track association between frames <ref type="bibr" target="#b54">[55]</ref>. However, the projection of non-linear 3D motion into the 2D image domain still poses a challenging problem for many models.</p><p>Tracking-by-regression refrains from associating detections between frames but instead accomplishes tracking by regressing past object locations to the new positions in the current frame. Previous efforts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref> use regression heads on region-pooled object features. In <ref type="bibr" target="#b55">[56]</ref>, objects are represented as center points which allow for an association by a distance-based greedy matching algorithm. To overcome their lacking notion of object identity and global track reasoning, additional re-identification and motion models <ref type="bibr" target="#b3">[4]</ref>, as well as traditional <ref type="bibr" target="#b26">[27]</ref> and learned <ref type="bibr" target="#b5">[6]</ref> graph methods have been necessary to achieve top performance.</p><p>Tracking-by-segmentation not only predicts object masks but leverages the pixel-level information to mitigate issues from crowdedness and ambiguous background areas. Prior attempts have used category-agnostic image segmentation <ref type="bibr" target="#b29">[30]</ref>, applied Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> with 3D convolutions <ref type="bibr" target="#b47">[48]</ref> and mask pooling layers <ref type="bibr" target="#b35">[36]</ref>, or represented objects as unordered point clouds <ref type="bibr" target="#b48">[49]</ref>. However, the scarcity of annotated MOT segmentation data makes modern approaches still rely on bounding box predictions.</p><p>Attention for image recognition correlates each element of the input with respect to the others and is used in Transformers <ref type="bibr" target="#b46">[47]</ref> for image generation <ref type="bibr" target="#b31">[32]</ref> and object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b57">58]</ref>. For MOT, attention has only been used to associate a given set of object detections <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b10">11]</ref>, not tackling the detection and tracking problem jointly.</p><p>In contrast, TrackFormer casts the entire tracking objective into a single set prediction problem, applying attention not only as a post-processing matching step. It jointly reasons about track initialization, identity, and spatiotemporal trajectories. This allows us to refrain from any additional graph optimization, appearance or motion model by only relying on feature-level global attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TrackFormer</head><p>We present TrackFormer, an end-to-end multi-object tracking (MOT) approach based on an encoder-decoder Transformer <ref type="bibr" target="#b46">[47]</ref> architecture. This section describes how we cast MOT as a set prediction problem and introduce the tracking-by-attention paradigm. We then introduce the concept of track queries, and how these are trained for frame-to-frame data association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">MOT as a set prediction problem</head><p>Given a video sequence with K individual object identities, MOT describes the task of generating ordered tracks</p><formula xml:id="formula_0">T k = (b k t1 , b k t2 , .</formula><p>. . ) with bounding boxes b t and track identities k. The subset (t 1 , t 2 , . . . ) of total frames T indicates the time span between an object entering and leaving the the scene. These include all frames for which an object is occluded by either the background or other objects.</p><p>In order to cast MOT as a set prediction problem, we leverage an encoder-decoder Transformer architecture. Our model performs tracking online and yields per-frame object bounding boxes and class predictions associated with identities in four consecutive steps: (i) Frame-level feature extraction with a common CNN backbone, e.g., ResNet <ref type="bibr" target="#b15">[16]</ref>.</p><p>(ii) Encoding of frame features with self-attention in a Transformer encoder <ref type="bibr" target="#b46">[47]</ref>.</p><p>(iii) Decoding of queries with self-and encoder-decoder attention in a Transformer decoder.</p><p>(iv) Mapping of queries to box and class predictions using multilayer perceptrons (MLP).</p><p>Objects are implicitly represented in the decoder queries, which are embeddings used by the decoder to output bounding box coordinates and class predictions. The decoder alternates between two types of attention: (i) self-attention over all queries, which allows for joint reasoning about the objects in a scene and (ii) encoder-decoder attention, which gives queries global access to the visual information of the current frame. The output embeddings accumulate bounding box and class information over multiple consecutive decoding layers. The permutation invariance of Transformers requires additive positional and object encodings for the frame features and decoder queries, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Tracking with decoder queries</head><p>The total set of output embeddings is initialized with two types of query encodings: (i) static object queries, which allow the model to initialize tracks at any frame of the video, and (ii) autoregressive track queries, which are responsible for tracking objects across frames.</p><p>The simultaneous Transformer decoding of object and track queries allows our model to perform detection and tracking in a unified way, and thereby introduces a new tracking-by-attention paradigm. A detailed architecture overview where we illustrate the integration of track and object queries into the Transformer decoder is shown in appendix A.3.</p><p>Track initialization. New objects that appear in the scene are detected by a fixed number of N object output embeddings each initialized with a static and learned object encoding referred to as object queries <ref type="bibr" target="#b6">[7]</ref>. Intuitively, each object query learns to predict objects with certain spatial properties, such as bounding box size and position. The decoder self-attention relies on the object encoding to avoid duplicate detections and to reason about spatial and categorical relations of objects. The number of object queries is ought to exceed the maximum number of objects per frame.</p><p>Track queries. In order to achieve frame-to-frame track generation, we introduce the concept of track queries to the decoding step. Track queries follow objects through a video sequence carrying over their identity information while adapting to their changing position in an autoregressive manner.</p><p>For this purpose, each new object detection initializes a track query with the corresponding output embedding of the previous frame. The Transformer encoder-decoder performs attention on current frame features and decoder queries continuously updating the instance-specific representation of object identity and location in each track query embedding. Self-attention over the joint set of both query types allows for the detection of new objects while simultaneously avoiding re-detection of already tracked objects. TrackFormer thereby achieves implicit multi-frame attention over past frames.</p><p>In <ref type="figure" target="#fig_5">Figure 2</ref>, we provide a visual illustration of the track query concept. The initial detection in frame t = 0 spawns new track queries following their corresponding object to frame t and beyond. To this end, N object object queries (white) are decoded to output embeddings for potential track initializations. Each successful object detection {b 0 0 , b 1 0 , . . . } with a classification score above σ object , i.e., output embedding not predicting the background class (crossed), initializes a new track query embedding. As not all objects in a sequence might already appear on the first frame, the track identities K 0 = {0, 1, . . . } only represent a subset of all K. For the decoding step at any frame t &gt; 0, each track query initializes an additional output embedding associated to a different identity (colored). The joint set of N object +N track output embeddings is initialized by (learned) object and (temporally adapted) track queries, respectively.     The Transformer decoder transforms the entire set of output embeddings at once and yields bounding box and class predictions for frame t. The number of track queries N track changes between frames as new objects are detected or tracks are removed. Existing tracks followed by a track query can be removed either if their classification score drops below σ track or by non-maximum suppression (NMS) with an IoU threshold of σ NMS . The application of a comparatively high σ NMS only removes strongly overlapping duplicate bounding boxes which we found to be not resolvable by the decoder self-attention.</p><formula xml:id="formula_1">/ c G g f H b d V n E o C L R K z W H Y D r I B R A S 1 N N Y N u I g H z g E E n G N / m f u c R p K K x a O p J A j 7 H Q 0 E j S r A 2 0 s C + 7 B M Q G m T e n z U l F i q K J Q c 5 L S 8 a d 4 L E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M</formula><formula xml:id="formula_2">L i q 0 D b s E 5 U F W k S 4 B Z 5 8 = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i z o w m W F v q A N Z T K 5 a Y f O T M L M R C i h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s P D B z O u X f u v S d I G F X a d X + s t f W N</formula><formula xml:id="formula_3">L i q 0 D b s E 5 U F W k S 4 B Z 5 8 = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i z o w m W F v q A N Z T K 5 a Y f O T M L M R C i h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s P D B z O u X f u v S d I G F X a d X + s t f W N</formula><formula xml:id="formula_4">L i q 0 D b s E 5 U F W k S 4 B Z 5 8 = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i z o w m W F v q A N Z T K 5 a Y f O T M L M R C i h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s P D B z O u X f u v S d I G F X a d X + s t f W N</formula><p>Track query re-identification. The ability to decode an arbitrary number of track queries allows for an attentionbased short-term re-identification process. We keep decoding previously removed track queries for a maximum number of T track-reid frames. During this patience window, track queries are considered to be inactive and do not contribute to the trajectory until a classification score higher than σ track-reid triggers a re-identification. The spatial information embedded into each track query prevents their application for long-term occlusions with large object movement, but, nevertheless, allows for a short-term recovery from track loss. This is possible without any dedicated re-identification training; and furthermore, cements TrackFormer's holistic approach by relying on the same attention mechanism as for track initialization, identity preservation and trajectory forming through short-term occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">TrackFormer training</head><p>For track queries to follow objects to the next frame and work in interaction with object queries, TrackFormer requires dedicated frame-to-frame tracking training. This is accomplished by training on two adjacent frames, as indicated in <ref type="figure" target="#fig_5">Figure 2</ref>, and optimizing the entire MOT objective at once. The set prediction loss for frame t measures the set prediction of all output embeddings N = N object + N track with respect to the ground truth objects in terms of class prediction and bounding box similarity.</p><p>The set prediction loss is computed in two steps:</p><p>(i) Object detection on frame t − 1 with N object object queries (see t = 0 in <ref type="figure" target="#fig_5">Figure 2</ref>).</p><p>(ii) Tracking of objects from (i) and detection of new objects on frame t with all N queries.</p><p>The number of track queries N track depends on the number of successfully detected objects in frame t−1. During training, the MLP predictionsŷ = {ŷ j } N j=1 of the output embeddings from step (iv) are each assigned to one of the ground truth objects y or the background class. Each y i represents a bounding box b i , object class c i and identity k i .</p><p>Bipartite matching. The mapping j = π(i) from ground truth objects y i to the joint set of object and track query predictionsŷ j is determined either via track identity or costs based on bounding box similarity and object class. For the former, we denote the subset of ground truth track identities at frame t with K t ⊂ K. Each detection from step (i) is assigned to its respective ground truth track identity k from the set K t−1 ⊂ K. The corresponding output embeddings, i.e. track queries, inherently carry over the identity information to the next frame. The two ground truth track identity sets describe a hard assignment of the N track track query outputs to the ground truth objects in frame t: K t ∩ K t−1 : Match by track identity k. K t−1 \ K t : Match with background class.</p><p>K t \ K t−1 : Match by minimum cost mapping.</p><p>The second set of ground truth track identities K t−1 \ K t includes tracks which either have been occluded or left the scene at frame t. The last set K object = K t \ K t−1 of previously not yet tracked ground truth objects remains to be matched with the N object object queries. To achieve this, we follow <ref type="bibr" target="#b6">[7]</ref> and search for the injective minimum cost mappingσ in the following assignment problem,</p><formula xml:id="formula_5">σ = arg min σ ki∈Kobject C match (y i ,ŷ σ(i) ),<label>(1)</label></formula><p>with index σ(i) and pair-wise costs C match between ground truth y i and predictionŷ i . The problem is solved with a combinatorial optimization algorithm as in <ref type="bibr" target="#b44">[45]</ref>. Given the ground truth class labels c i and predicted class probabilitieŝ p i (c i ) for output embeddings i, the matching cost C match is defined as</p><formula xml:id="formula_6">C match = −p σ(i) (c i ) + C box (b i ,b σ(i) ).<label>(2)</label></formula><p>In <ref type="bibr" target="#b6">[7]</ref>, a class cost term without logarithmic probabilities yielded better empirical performance. The C box term penalizes bounding box differences by a linear combination of a 1 distance and a generalized intersection over union (IoU) <ref type="bibr" target="#b37">[38]</ref> cost C iou ,</p><formula xml:id="formula_7">C box = λ 1 ||b i −b σ(i) || 1 + λ iou C iou (b i ,b σ(i) ),<label>(3)</label></formula><p>with weighting parameters λ 1 , λ iou , ∈ . The scaleinvariant IoU term provides similar relative errors for different box sizes and mitigates inconsistency of the 1 distance. The optimal cost mappingσ determines the corresponding assignments in π(i).</p><p>Set prediction loss. The final MOT set prediction loss is computed over all N = N object + N track output predictions:</p><formula xml:id="formula_8">L MOT (y,ŷ, π) = N i=1 L query (y,ŷ i , π).<label>(4)</label></formula><p>The output embeddings which were not matched via track identity orσ are not part of the mapping π and will be assigned to the background class c i = 0. We indicate the ground truth object matched with prediction i by y π=i and define the loss per query</p><formula xml:id="formula_9">L query = − logp i (c π=i ) + L box (b π=i ,b i ), if i ∈ π − logp i (0), if i / ∈ π.</formula><p>The bounding box loss L box is computed in the same fashion as <ref type="formula" target="#formula_7">(3)</ref>, but we differentiate its notation as the cost term C box is generally not required to be differentiable.</p><p>Track augmentations. The two-step loss computation (see (i) and (ii)) for training track queries represents only a limited range of possible tracking scenarios. Therefore, we propose the following augmentations to enrich the set of potential track queries during training. These augmentations will be verified in our experiments. We use three types of augmentations similar to <ref type="bibr" target="#b55">[56]</ref> which lead to perturbations of object location and motion, missing detections, and simulated occlusions.</p><p>1. The frame t − 1 for step (i) is sampled from a range of frames around frame t, thereby generating challenging frame pairs where the objects have moved substantially from their previous position. Such a sampling allows for the simulation of camera motion and low frame rates from usually benevolent sequences.</p><p>2. We sample false negatives with a probability of p FN by removing track queries before proceeding with step (ii). The corresponding ground truth objects in frame t will be matched with object queries and trigger a new object detection. Keeping the ratio of false positives sufficiently high is vital for a joined training of both query types.</p><p>3. To improve the removal of tracks by assigning the background class in occlusion scenarios, we complement the set of track queries with additional false positives. These queries are sampled from output embeddings of frame t−1 that were classified as background. Each of the original track queries has a chance of p FP to spawn an additional false positive query. We chose these with a large likelihood of occluding with the respective spawning track query.</p><p>Another common augmentation for improved robustness, is to applying spatial jittering to input bounding boxes or center points <ref type="bibr" target="#b55">[56]</ref>. The nature of track queries, which encode spatial object information implicitly, does not allow for such an explicit perturbation in the spatial domain. We believe our randomization of the temporal range provides a more natural augmentation from video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present tracking results for Track-Former on two MOTChallenge benchmarks, namely, MOT17 <ref type="bibr" target="#b28">[29]</ref> and MOTS20 <ref type="bibr" target="#b47">[48]</ref>. Furthermore, we verify individual contributions in an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MOT benchmarks and metrics</head><p>Datasets. The MOT17 <ref type="bibr" target="#b28">[29]</ref> benchmark consists of a train and test set, each with 7 sequences and pedestrians annotated with full-body bounding boxes. To evaluate the tracking (data association) robustness independently, three sets of public detections with varying quality are provided, namely, DPM <ref type="bibr" target="#b13">[14]</ref>, Faster R-CNN <ref type="bibr" target="#b36">[37]</ref> and SDP <ref type="bibr" target="#b50">[51]</ref>.</p><p>MOTS20 <ref type="bibr" target="#b47">[48]</ref> provides mask annotations for 4 train and test sequences of MOT17. The corresponding bounding boxes are not full-body, but based on the visible segmentation masks, and only large objects are annotated.</p><p>Metrics. Different aspects of MOT are evaluated by a number of individual metrics <ref type="bibr" target="#b4">[5]</ref>. The community focuses on two compound metrics, namely, Multiple Object Tracking Accuracy (MOTA) and Identity F1 Score (IDF1) <ref type="bibr" target="#b38">[39]</ref>. While the former focuses on object coverage, the identity preservation of a method is measured by the latter. For MOTS, we report MOTSA which evaluates predictions with a ground truth matching based on mask IoU.</p><p>Public detections. The MOT17 <ref type="bibr" target="#b27">[28]</ref> benchmark is evaluated in a private and public detection setting. The latter allows for a comparison of tracking methods independent of the underlying object detection performance. MOT17 provides three sets of public detections with varying quality. In contrast to classic tracking-by-detection methods, Track-Former is not able to directly produce tracking outputs from detection inputs. Therefore, we report the results of Track-Former and CenterTrack <ref type="bibr" target="#b55">[56]</ref> in <ref type="table">Table 1</ref> by filtering the initialization of tracks with a minimum IoU requirement. For more implementation details and a discussion on the fairness of such a filtering, we refer to A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>TrackFormer follows the ResNet50 <ref type="bibr" target="#b15">[16]</ref> CNN feature extraction and Transformer encoder-decoder architecture presented in Deformable DETR <ref type="bibr" target="#b57">[58]</ref>. However, the Focal loss <ref type="bibr" target="#b24">[25]</ref> applied in <ref type="bibr" target="#b57">[58]</ref> emphasizes only the track queries, and ignores new object detections during training. Therefore, we resort to the original cross-entropy loss of DETR <ref type="bibr" target="#b6">[7]</ref> as in Section 3.3. Deformable DETR <ref type="bibr" target="#b57">[58]</ref> substantially reduces training time and improves detection performance for small objects which are very prominent in the MOT17 dataset. It <ref type="bibr" target="#b57">[58]</ref> achieves this by replacing the original attention over single scale feature maps with multi-scale deformable attention modules. For track queries, the deformable reference points for the current frame t are dynamically adjusted to the previous frame bounding box centers.</p><p>Queries and the background class. By design, TrackFormer can only detect a maximum of N object objects. To detect the maximum number of 52 objects per frame in MOT17 <ref type="bibr" target="#b27">[28]</ref>, we train TrackFormer with N object = 300 learned object queries. The number of possible track queries is adaptive and only practically limited by the ability of the decoder to discriminate them.</p><p>For optimal performance, the total number of queries must exceed the number of ground truth objects per frame by a large margin. To mitigate the resulting class imbalance, we follow <ref type="bibr" target="#b6">[7]</ref> and downweigh the class prediction loss for background class queries by a factor of 0.1. To facilitate the training of track removal, we do not apply downweighting for false positive track augmentations.</p><p>Simulate MOT from single images. The encoderdecoder multi-level attention mechanism requires substantial amounts of training data. Hence, we follow a similar approach as in <ref type="bibr" target="#b55">[56]</ref> and simulate MOT data from the Crowd-Human <ref type="bibr" target="#b42">[43]</ref> person detection dataset. The adjacent training frames t − 1 and t are generated by applying random spatial augmentations to a single image. To simulate high frame rates as in MOT17 <ref type="bibr" target="#b27">[28]</ref>, we only randomly resize and crop of up to 5% with respect to the original image size.</p><p>Training procedure. We follow <ref type="bibr" target="#b57">[58]</ref> and pretrain the model for 50 epochs without track queries on COCO <ref type="bibr" target="#b25">[26]</ref>. The tracking capabilities are learned by training on MOT17 frame pairs or simulating adjacent MOT frames from single images. As in <ref type="bibr" target="#b57">[58]</ref>, the backbone and encoder-decoder are trained with individual learning rates of 0.00001 and 0.0001, respectively. The MOT17 public detections model is trained for a total of 40 epochs with a learning rate drop by a factor of 10 after the first 10 epochs. The private detections model is pretrained for 50 epochs on CrowdHuman and then fine-tuned on MOT17 with reduced learning rates for additional 20 epochs. Excluding the COCO pretraining, we train the public detections model for around 2 days on 7 16GB RTX GPUs.</p><p>Mask training. TrackFormer predicts instance-level object masks with a segmentation head as in <ref type="bibr" target="#b6">[7]</ref> by generating spatial attention maps from the encoded image features and decoder output embeddings. Subsequent upscaling and convolution operations yield mask predictions for all output embeddings. Since the two datasets have several sequences in common, we adopt the private detection training pipeline from MOT17 including the pretraining on CrowdHuman. However, for our MOTS20 model, we retrain TrackFormer with the original DETR <ref type="bibr" target="#b6">[7]</ref> attention. This is due to the reduced memory consumption for single scale feature maps and inferior segmentation masks from sparse deformable attention maps. Furthermore, the beneficial effect of deformable attention vanishes on MOTS20 as it excludes small objects from segmentation. After training on MOT17, we freeze the model and train only the segmentation head on all COCO images containing persons. Finally, we fine-tune the entire model on the MOTS20 dataset. </p><formula xml:id="formula_10">Method MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ ID</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Benchmark results</head><p>MOT17. Following the training procedure described in Section 4.2, we evaluate TrackFormer on the MOT17 <ref type="bibr" target="#b27">[28]</ref> test set and report results in <ref type="table">Table 1</ref>.</p><p>For private detections, we achieve results comparable with modern state-of-the-art methods. This is due to the detection performance of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b57">58]</ref>, and thereby TrackFormer, which still lacks behind modern object detectors. As shown in <ref type="table">Table 2</ref>, TrackFormer applied for singleframe detection of pedestrians, i.e., many small objects with object-object occlusions, is merely on par with Faster R-CNN. Note, the superior CenterTrack detection performance, which only translates to 2.8 points higher MOTA compared to our method. This demonstrates TrackFormer as a powerful approach for tracking.  To further isolate the tracking performance, we compare results in a public detection setting in the lower two sections of <ref type="table">Table 1</ref>. In that case, TrackFormer achieves state-ofthe-art results both in terms of MOTA and IDF1 for online methods without pretraining on CrowdHuman <ref type="bibr" target="#b42">[43]</ref>. Our identity preservation performance is only surpassed by offline methods which benefit from the processing of entire sequences at once.</p><p>TrackFormer achieves top performance via global attention between encoded input pixels and decoder queries without relying on additional motion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> or appearance models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. Furthermore, the frame to frame association with track queries avoids any post-processing with heuristic greedy matching procedures <ref type="bibr" target="#b55">[56]</ref> or additional graph optimization <ref type="bibr" target="#b26">[27]</ref>.</p><p>MOTS20. In addition to object detection and tracking, TrackFormer is able to predict instance-level segmentation masks. As reported in <ref type="table" target="#tab_2">Table 3</ref>, we achieve state-of-theart MOTS results in terms of object coverage (MOTSA) and identity preservation (IDF1). All methods are evaluated in a private setting. A MOTS20 test set submission is only recently possible, hence we also provide the 4-fold cross-validation evaluation established in <ref type="bibr" target="#b47">[48]</ref> and report the mean best epoch results over all splits. TrackFormer surpasses all previous methods without relying on a dedicated tracking formulation for segmentation masks as in <ref type="bibr" target="#b48">[49]</ref>. In <ref type="figure" target="#fig_6">Figure 3</ref>, we present a qualitative comparison of Track-Former and Track R-CNN <ref type="bibr" target="#b47">[48]</ref> on two test sequences.  <ref type="table" target="#tab_2">Table 3</ref> can be clearly observed by the difference in pixel mask accuracy.  <ref type="table">Table 4</ref>. Ablation study on individual TrackFormer components. We report mean best epoch results in a private setting on a 7-fold split on the MOT17 <ref type="bibr" target="#b27">[28]</ref> training set. For the last row without (w\o) all components, we train only for object detection and associate tracks based on output embedding distance.</p><formula xml:id="formula_11">Method MOTA ↑ ∆ IDF1 ↑ ∆ TrackFormer 51.4 55.3 -----w\o -----------------</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>The ablation study on the MOT17 and MOTS20 training sequences are evaluated in a private detection setting with a 7-and 4-fold cross-validation split, respectively.</p><p>TrackFormer components. We ablate the impact of different TrackFormer components on the tracking performance in <ref type="table">Table 4</ref>. Our full system including pretraining on the CrowdHuman dataset provides a MOTA and IDF1 of 51.4 and 55.3, respectively. The baseline without (w\o) pretraining reduces this by -8.6 and -10.1 points which demonstrates the limitations of the MOT17 dataset. The attentionbased track query re-identification has a negligible effect on MOTA but improves IDF1 by 1.6 points.</p><p>If we further ablate our false positives (FP) and frame range track augmentations, we see another drop of -4.6 MOTA and -2.6 IDF1 points. Both augmentations provide the training which rich tracking scenarios and prevent an early overfitting. The false negative track augmentations are indispensable for a joint training of object and track queries, hence we refrain from ablating these.</p><p>Our final baseline is without (w\o) any tracking components, not using track queries and is only trained for object detection. Data association is performed with a greedy center distance matching as in <ref type="bibr" target="#b55">[56]</ref>. This leads to a dramatic drop of -13.6 in IDF1, as shown in the last row of <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mask training MOTA ↑ IDF1 ↑ TrackFormer × 61.9 56.0 61. <ref type="bibr" target="#b8">9</ref> 54.8 <ref type="table">Table 5</ref>. We demonstrate the effect of jointly training for tracking and segmentation on a 4-fold split on the MOTS20 <ref type="bibr" target="#b47">[48]</ref> train set. We evaluate with regular MOT metrics, i.e., matching to ground truth with bounding boxes instead of masks.</p><p>The version represents previous post-processing and matching methods and demonstrates the strength of jointly addressing track initialization, identity and trajectory forming in a unified TrackFormer configuration.</p><p>Mask information improves tracking. This final ablation is studying if segmentation mask prediction can improve tracking performance. <ref type="table">Table 5</ref> shows that a unified segmentation and tracking training procedure ican improve IDF1 by +1.2. In contrast to <ref type="bibr" target="#b6">[7]</ref>, we trained the entire model including the mask head and evaluate its bounding box tracking performance. The additional mask information did not improve track coverage (MOTA) but resolved ambiguous occlusion scenarios during training, thereby improving identity preservation (IDF1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a unified tracking-by-attention paradigm for detection and multi-object tracking with Transformers. Our end-to-end TrackFormer architecture introduces track query embeddings which follow objects over a sequence in an autoregressive manner. An encoderdecoder architecture transforms each track query to the changing position of its corresponding object. TrackFormer jointly tackles track initialization, identity and spatiotemporal trajectory forming solely by attention operations and does not rely on any additional matching, graph optimization, motion or appearance modeling. Our approach achieves state-of-the-art results for multi-object tracking as well as segmentation. We hope that this paradigm will foster future work in detection and multi-object tracking.</p><p>This section provides additional material for the main paper: §A contains further implementation details for Track-Former ( §A.1), a visualization of the Transformer encoderdecoder architecture ( §A.3), and parameters for multiobject tracking ( §A.4). §B contains a discussion related to public detection evaluation ( §B.1), and detailed persequence results for MOT17 and MOTS20 ( §B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details A.1. Backbone and training</head><p>We provide additional hyperparameters for TrackFormer.</p><p>This supports our implementation details reported in Section 4.2 of the main paper. The Deformable DETR <ref type="bibr" target="#b57">[58]</ref> encoder and decoder both apply 6 individual layers of feature-width 256. Each attention layer applies multi-headed self-attention <ref type="bibr" target="#b46">[47]</ref> with 8 attention heads. We do not use the "DC5" (dilated conv 5 ) version of the backbone as this will incur a large memory requirement related to the larger resolution of the last residual stage. We expect that using "DC5" or any other heavier, or higher-resolution, backbone to provide better accuracy and leave this for future work.</p><p>Our training hyperparameters mostly follow the original DETR <ref type="bibr" target="#b6">[7]</ref>. The weighting parameters of the individual box cost C box and loss L box are set to λ 1 = 5 and λ iou = 2. The probabilities for the track augmentation at training time are p FN = 0.4 and p FP = 0.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Dataset splits</head><p>All experiments evaluated on dataset splits (ablation studies and MOTS20 training set in <ref type="table" target="#tab_2">Table 3</ref>) apply the same training pipeline presented in Section 4.2 to each split. We average validation metrics over all splits and report the results from a single epoch (which yields the best mean MOTA / MOTSA) over all splits, i.e., we do not take the best epoch for each individual split. For our ablation on the MOT17 <ref type="bibr" target="#b27">[28]</ref> training set, we separate the 7 sequences into 7 splits each with a single sequence as validation set. Before training each of the 4 MOTS20 [48] splits, we pre-train the model on all MOT17 sequences excluding the corresponding split of the validation sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Transformer encoder-decoder architecture</head><p>To foster the understanding of TrackFormer's integration of track queries within the decoder self-attention block, we provide a simplified visualization of the encoder-decoder architecture in <ref type="figure">Figure A</ref> illustration in <ref type="bibr" target="#b6">[7]</ref>, we indicate track identities instead of spatial encoding with color-coded queries. The frame features (indicated in grey) are the final output of the CNN feature extractor and have the same number of channels as both query types. The entire Transformer architecture applies N and M independently supervised encoder and decoder layers, with spatial positional and object encoding as in <ref type="bibr" target="#b6">[7]</ref>. Track queries are fed autoregressively from the previous frame output embeddings of the last decoding layer (before the final feed-forward class and bounding box networks (FFN)). The object encoding is achieved by re-adding the object queries to the corresponding embeddings in the decoder key (K) and query (Q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Multi-object tracking parameters</head><p>In Section 3.2, we explain the process of track initialization and removal over a sequence. The corresponding hyperparameters were optimized by a grid search on the MOT17 training set cross-validation splits. The grid search yielded track initialization and removal thresholds of σ detection = 0.9 and σ track = 0.8, respectively. The lower σ track score prevents tracks from being removed too early and improves identity preservation performance. TrackFormer benefits from an NMS operation for the removal of strong occlusion cases with an intersection over union larger than σ NMS = 0.9.  <ref type="bibr">(IoU)</ref>. We report mean results over the three sets of public detections provided by <ref type="bibr" target="#b27">[28]</ref> and separate between online and offline approaches. The arrows indicate low or high optimal metric values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>For the track query re-identification, our search proposed an optimal inactive patience and score of T track-reid = 5 and σ track-reid = 0.8, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Public detections and track filtering</head><p>TrackFormer implements a new tracking-by-attention paradigm which requires track initializations to be filtered for an evaluation with public detections. Here, we provide a discussion on the comparability of TrackFormer with earlier methods and different filtering schemes.</p><p>Common tracking-by-detection methods directly process the MOT17 public detections and report their mean tracking performance over all three sets. This is only possible for methods that perform data association on a bounding box level. However, TrackFormer and point-based methods such as CenterTrack <ref type="bibr" target="#b55">[56]</ref> require a procedure for filtering track initializations by public detections in a comparable manner. Unfortunately, MOT17 does not provide a standardized protocol for such a filtering. The authors of Cen-terTrack <ref type="bibr" target="#b55">[56]</ref> filter detections based on bounding box center distances (CD). Each public detection can possibly initialize a single track but only if its center point falls in the bounding box area of the corresponding track.</p><p>In <ref type="table">Table A</ref>.1, we revisit our MOT17 test set results but with this public detections center distance (CD) filtering, while also inspecting the CenterTrack per-sequence results in <ref type="table">Table A</ref>. <ref type="bibr" target="#b2">3</ref>. We observe that this filtering does not reflect the quality differences in each set of public detections, i.e., DPM <ref type="bibr" target="#b13">[14]</ref> and SDP <ref type="bibr" target="#b50">[51]</ref> results are expected to be the worst and best, respectively, but their difference is small.</p><p>We hypothesize that a center distance filtering is not in accordance with the common public detection setting and propose a filtering based on Intersection over Union (IoU). For IoU filtering, public detections only initialize a track if they have an IoU larger than 0.5. The results in <ref type="table">Table A</ref>.1, show that for TrackFormer and CenterTrack, using IoU filtering performs worse compared to the CD filtering which is expected as this is a more challenging evaluation protocol. We believe IoU-based filtering (instead of CD-based) provides a fairer comparison to previous MOT methods which directly process public detections as inputs (IN). This is validated by the per-sequence results in <ref type="table">Table A</ref>.4, where IoU filtering shows differences across detectors that are more meaningfully correlated with detector performance, compared to the relatively uniform performance across detections with the CD based method in <ref type="table">Table A</ref>.3 (where DPM, FRCNN and SDP show very similar performance).</p><p>Consequently, we follow the IoU-based filtering protocol to compare with CenterTrack in our main paper. While our gain over CenterTrack seems similar across the two filtering techniques for MOTA (see <ref type="table">Table A</ref>.1), the gain in IDF1 is significantly larger under the more challenging IoU-based protocol, which suggests that CenterTrack benefits from the less challenging CD-based filtering protocol, while Track-Former does not rely on the filtering for achieving its high IDF1 tracking accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. MOT17 and MOTS20 sequence results</head><p>In <ref type="table">Table A</ref> Evaluation metrics In Section 4.1 we explained two compound metrics for the evaluation of MOT results, namely, Multi-Object Tracking Accuracy (MOTA) and Identity F1 score (IDF1). <ref type="bibr" target="#b4">[5]</ref> However, the MOTChallenge benchmark implements all CLEAR MOT <ref type="bibr" target="#b4">[5]</ref> evaluation metrics. In addition to MOTA and IDF1, we report the following additional CLEAR MOT metrics:</p><p>MT: Ground truth tracks covered for at least 80%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML:</head><p>Ground truth tracks covered for at most 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FP:</head><p>False positive bounding boxes not corresponding to any ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FN:</head><p>False negative ground truth boxes not covered by any bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Sw.:</head><p>Bounding box switching the corresponding ground truth identity. in the previous frame.</p><p>sMOTSA: Mask-based Multi-Object Tracking Accuracy (MOTA) which counts true positives instead of only masks with IoU larger than 0.5.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>TrackFormer performs joint object detection and tracking by attention with Transformers. Object and autoregressive track queries reason about track initialization, identity, and occlusion of spatiotemporal trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " b W c A v I s k I F H 6 M / / q 7 B F 2 2 u y 7 w 4 o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k l B 9 F j o x V O p a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 W x s b m 3 v 7 B b 2 i v s H h 0 f H p Z P T t o l T z X i L x T L W 3 Y A a L o X i L R Q o e T f R n E a B 5 J 1 g U p / 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P d Q b j U G p 7 F b c B c g 6 8 X J S h h z N Q e m r P 4 x Z G n G F T F J j e p 6 b o J 9 R j Y J J P i v 2 U 8 M T y i Z 0 x H u W K h p x 4 2 e L U 2 f k 0 i p D E s b a l k K y U H 9 P Z D Q y Z h o F t j O i O D a r 3 l z 8 z + u l G N 7 6 m V B J i l y x 5 a I w l Q R j M v + b D I X m D O X U E s q 0 s L c S N q a a M r T p F G 0 I 3 u r L 6 6 R d r X j X F f e + W q 6 5 e R w F O I c L u A I P b q A G d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t m 4 4 + c w Z / I H z + Q P I S o 1 n &lt; / l a t e x i t &gt; CNN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b W c A v I s k I F H 6 M / / q 7 B F 2 2 u y 7 w 4 o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k l B 9 F j o x V O p a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 W x s b m 3 v 7 B b 2 i v s H h 0 f H p Z P T t o l T z X i L x T L W 3 Y A a L o X i L R Q o e T f R n E a B 5 J 1 g U p / 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P d Q b j U G p 7 F b c B c g 6 8 X J S h h z N Q e m r P 4 x Z G n G F T F J j e p 6 b o J 9 R j Y J J P i v 2 U 8 M T y i Z 0 x H u W K h p x 4 2 e L U 2 f k 0 i p D E s b a l k K y U H 9 P Z D Q y Z h o F t j O i O D a r 3 l z 8 z + u l G N 7 6 m V B J i l y x 5 a I w l Q R j M v + b D I X m D O X U E s q 0 s L c S N q a a M r T p F G 0 I 3 u r L 6 6 R d r X j X F f e + W q 6 5 e R w F O I c L u A I P b q A G d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t m 4 4 + c w Z / I H z + Q P I S o 1 n &lt; / l a t e x i t &gt; CNN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b W c A v I s k I F H 6 M / / q 7 B F 2 2 u y 7 w 4 o = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k l B 9 F j o x V O p a D + g D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k E R r / 4 i b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k Q K g 6 7 7 7 W x s b m 3 v 7 B b 2 i v s H h 0 f H p Z P T t o l T z X i L x T L W 3 Y A a L o X i L R Q o e T f R n E a B 5 J 1 g U p / 7 n S e u j Y j V I 0 4 T 7 k d 0 p E Q o G E U r P d Q b j U G p 7 F b c B c g 6 8 X J S h h z N Q e m r P 4 x Z G n G F T F J j e p 6 b o J 9 R j Y J J P i v 2 U 8 M T y i Z 0 x H u W K h p x 4 2 e L U 2 f k 0 i p D E s b a l k K y U H 9 P Z D Q y Z h o F t j O i O D a r 3 l z 8 z + u l G N 7 6 m V B J i l y x 5 a I w l Q R j M v + b D I X m D O X U E s q 0 s L c S N q a a M r T p F G 0 I 3 u r L 6 6 R d r X j X F f e + W q 6 5 e R w F O I c L u A I P b q A G d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t m 4 4 + c w Z / I H z + Q P I S o 1 n &lt; / l a t e x i t &gt; CNN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V / W H V s l 4 A Y M V K s 6 i h 4 8 A z u g x W + w = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i y I 4 L J C X 9 C G M p n c t E N n J m F m I p T Q r 3 D j r 7 h x o Y h b c e f f O G m z q K 0 H B g 7 n 3 D v 3 3 h M k j C r t u j / W 2 v r G 5 t Z 2 a a e 8 u 7 d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F m j K C 0 &lt; / l a t e x i t &gt; Transformer Encoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V / W H V s l 4 A Y M V K s 6 i h 4 8 A z u g x W + w = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i y I 4 L J C X 9 C G M p n c t E N n J m F m I p T Q r 3 D j r 7 h x o Y h b c e f f O G m z q K 0 H B g 7 n 3 D v 3 3 h M k j C r t u j / W 2 v r G 5 t Z 2 a a e 8 u 7 d / c G g f H b d V n E o C L R K z W H Y D r I B R A S 1 N N Y N u I g H z g E E n G N / m f u c R p K K x a O p J A j 7 H Q 0 E j S r A 2 0 s C + 7 B M Q G m T e n z U l F i q K J Q c 5 L S 8 a d 4 L E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F m j K C 0 &lt; / l a t e x i t &gt; Transformer Encoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V / W H V s l 4 A Y M V K s 6 i h 4 8 A z u g x W + w = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e k m W A Q 3 l q Q g u i y I 4 L J C X 9 C G M p n c t E N n J m F m I p T Q r 3 D j r 7 h x o Y h b c e f f O G m z q K 0 H B g 7 n 3 D v 3 3 h M k j C r t u j / W 2 v r G 5 t Z 2 a a e 8 u 7 d / c G g f H b d V n E o C L R K z W H Y D r I B R A S 1 N N Y N u I g H z g E E n G N / m f u c R p K K x a O p J A j 7 H Q 0 E j S r A 2 0 s C + 7 B M Q G m T e n z U l F i q K J Q c 5 L S 8 a d 4 L E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F m j K C 0 &lt; / l a t e x i t &gt; Transformer Encoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 H S o 0 U S 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>z a 3 t 0 k 5 5 d 2 / / 4 N A + O m 6 r O J U E W i R m s e w G W A G j A l q a a g b d R A L m A Y N O M L 7 N / c 4 j S E V j 0 d S T B H y O h 4 J G l G B t p I F 9 2 S c g N M i 8 P 2 t K L F Q U S w 5 y W l 4 0 7 o D E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F X J 6 C q &lt; / l a t e x i t &gt; Transformer Decoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 H S o 0 U S 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>z a 3 t 0 k 5 5 d 2 / / 4 N A + O m 6 r O J U E W i R m s e w G W A G j A l q a a g b d R A L m A Y N O M L 7 N / c 4 j S E V j 0 d S T B H y O h 4 J G l G B t p I F 9 2 S c g N M i 8 P 2 t K L F Q U S w 5 y W l 4 0 7 o D E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F X J 6 C q &lt; / l a t e x i t &gt; Transformer Decoder &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 H S o 0 U S 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>z a 3 t 0 k 5 5 d 2 / / 4 N A + O m 6 r O J U E W i R m s e w G W A G j A l q a a g b d R A L m A Y N O M L 7 N / c 4 j S E V j 0 d S T B H y O h 4 J G l G B t p I F 9 2 S c g N M i 8 P 2 t K L F Q U S w 5 y W l 4 0 7 o D E o R E H d s W t u j M 4 q 8 Q r S A U V a A z s 7 3 4 Y k 5 S b r w j D S v U 8 N 9 F + h q W m h I E Z k i p I M B n j I f Q M F Z i D 8 r P Z W V P n 3 C i h Y / Y x T 2 h n p i 5 2 Z J g r N e G B q e R Y j 9 S y l 4 v / e b 1 U R z d + R k W S a h B k P i h K m a N j J 8 / I C a k E o t n E E E w k N b s 6 Z I Q l J i Y P V T Y h e M s n r 5 J 2 r e p d V d 2 H W q X u F n G U 0 C k 6 Q x f I Q 9 e o j u 5 R A 7 U Q Q U / o B b 2 h d + v Z e r U + r M 9 5 6 Z p V 9 J y g P 7 C + f g F X J 6 C q &lt; / l a t e x i t &gt;TransformerDecoder TrackFormer casts multi-object tracking as a set prediction problem performing joint detection and tracking-by-attention. The architecture consists of a CNN for image feature extraction, a Transformer<ref type="bibr" target="#b46">[47]</ref> encoder for image feature encoding and a Transformer decoder which applies self-and encoder-decoder attention to produce output embeddings with bounding box and class information. At frame t = 0, the decoder transforms Nobject object queries (white) to output embeddings either initializing new autoregressive track queries or predicting the background class (crossed). On subsequent frames, the decoder processes the joint set of Nobject + Ntrack queries to follow or remove (blue) existing tracks as well as initialize new tracks (purple).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>We compare TrackFormer segmentation results with the popular Track R-CNN [48] on selected MOTS20 [48] test sequences. The superiority of TrackFormer in terms of MOTSA in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>. 1 .Figure A. 1 .</head><label>11</label><figDesc>In comparison to the original The TrackFormer encoder-decoder architecture. We indicate the tensor dimensions in squared brackets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Sw. ↓</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Private</cell></row><row><cell></cell><cell>TubeTK [31]</cell><cell>63.0</cell><cell>58.6 735 468 27060 177483 4137</cell></row><row><cell>Online</cell><cell cols="2">CTracker [34] CenterTrack [56] 67.8 66.6</cell><cell>57.4 759 570 22284 160491 5529 64.7 816 579 18498 160332 3039</cell></row><row><cell></cell><cell>TrackFormer</cell><cell>65.0</cell><cell>63.9 1074 324 70443 123552 3528</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Public</cell></row><row><cell></cell><cell>jCC [20]</cell><cell>51.2</cell><cell>54.5 493 872 25937 247822 1802</cell></row><row><cell></cell><cell>FWT [17]</cell><cell>51.3</cell><cell>47.6 505 830 24101 247921 2648</cell></row><row><cell>Offline</cell><cell>eHAF [44] TT [55]</cell><cell>51.8 54.9</cell><cell>54.7 551 893 33212 236772 1834 63.1 575 897 20236 233295 1088</cell></row><row><cell></cell><cell>MPNTrack [6]</cell><cell>58.8</cell><cell>61.7 679 788 17413 213594 1185</cell></row><row><cell></cell><cell>Lif T [18]</cell><cell>60.5</cell><cell>65.6 637 791 14966 206619 1189</cell></row><row><cell></cell><cell>FAMNet [10]</cell><cell>52.0</cell><cell>48.7 450 787 14138 253616 3072</cell></row><row><cell>Online</cell><cell cols="2">Tracktor++ [4] GSM [27] CenterTrack [56] 60.5 56.3 56.4</cell><cell>55.1 498 831 8866 235449 1987 57.8 523 813 14379 230174 1485 55.7 580 777 11599 208577 2540</cell></row><row><cell></cell><cell>TrackFormer</cell><cell>62.5</cell><cell>60.7 702 632 32828 174921 3917</cell></row><row><cell></cell><cell cols="2">DPM [14]</cell><cell>0.61 42308 36557</cell></row><row><cell></cell><cell cols="2">FRCNN [37]</cell><cell>0.72 10081 25963</cell></row><row><cell></cell><cell cols="2">SDP [51]</cell><cell>0.81 7599 18865</cell></row><row><cell></cell><cell cols="3">CenterTrack [56] 0.77 7662 9900</cell></row><row><cell></cell><cell cols="2">TrackFormer</cell><cell>0.73 13178 15441</cell></row><row><cell cols="4">Table 2. Detection performance on the MOT17 test set. Both</cell></row><row><cell cols="4">TrackFormer and CenterTrack were pretrained on CrowdHu-</cell></row><row><cell cols="4">man [43] and evaluated only for single frame detection.</cell></row></table><note>Table 1. Comparison of modern multi-object tracking methods evaluated on the MOT17 [28] test set. We report private as well as public detections results and separate between online and of- fline approaches. TrackFormer achieves state-of-the-art results in terms of MOTA among all public tracking methods. Both Track- Former and CenterTrack filter track initializations by requiring a minimum IoU with public detections. For a detailed discussion on the fairness of such a filtering, we refer to B.1 and Table A.1.Method AP ↑ FP ↓ FN ↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MethodTbD sMOTSA ↑ IDF1 ↑ FP ↓ FN ↓ ID Sw. ↓</figDesc><table><row><cell cols="5">Train set (4-fold cross-validation)</cell><cell></cell><cell></cell></row><row><cell>MHT DAM [21]</cell><cell>×</cell><cell>48.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FWT [17]</cell><cell>×</cell><cell>49.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MOTDT [8]</cell><cell>×</cell><cell>47.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>jCC [20]</cell><cell>×</cell><cell>48.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TrackRCNN [48]</cell><cell></cell><cell>52.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MOTSNet [36]</cell><cell></cell><cell>56.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointTrack [49]</cell><cell></cell><cell>58.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TrackFormer</cell><cell></cell><cell>58.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Test set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Track R-CNN [48]</cell><cell></cell><cell>40.6</cell><cell cols="4">42.4 1261 12641 567</cell></row><row><cell>TrackFormer</cell><cell></cell><cell>54.9</cell><cell cols="4">63.6 2233 7195 278</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Comparison of modern multi-object tracking and seg-</cell></row><row><cell>mentation methods evaluated on the MOTS20 [48] train and test</cell></row><row><cell>sets. Methods indicated with TbD originally perform tracking-</cell></row><row><cell>by-detection without segmentation. Hence, they are evaluated on</cell></row><row><cell>SDP [52] public detections and predict masks with an additional</cell></row><row><cell>Mask R-CNN [15] fine-tuned on MOTS20. TrackFormer achieves</cell></row><row><cell>state-of-the-art results in terms of MOTSA and IDF1 on both sets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>IN IoU CD MOTA ↑ IDF1 ↑Table A.1. Comparison of modern multi-object tracking methods evaluated on the MOT17 [28] test set for different public detection processing. Public detections are either directly processed as input (IN) or applied for filtering of track initializations by center distance (CD) or intersection over union</figDesc><table><row><cell></cell><cell></cell><cell>Offline</cell><cell></cell><cell></cell></row><row><cell>MHT DAM [21] jCC [20] FWT [17] eHAF [44] TT [55] MPNTrack [6] Lif T [18]</cell><cell>× × × × × × ×</cell><cell></cell><cell></cell><cell>50.7 51.2 51.3 51.8 54.9 58.8 60.5</cell><cell>47.2 54.5 47.6 54.7 63.1 61.7 65.6</cell></row><row><cell></cell><cell></cell><cell>Online</cell><cell></cell><cell></cell></row><row><cell cols="2">MOTDT [8] FAMNet [10] Tracktor++ [4] GSM Tracktor [27] × × × × CenterTrack [56] TrackFormer CenterTrack [56] TrackFormer</cell><cell>× ×</cell><cell>× ×</cell><cell>50.9 52.0 56.3 56.4 60.5 62.5 61.5 63.4</cell><cell>52.7 48.7 55.1 57.8 55.7 60.7 59.6 60.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Sequence sMOTSA ↑ IDF1 ↑ MOTSA ↑ FP ↓ FN ↓ ID Sw. ↓Table A.2. We present TrackFormer tracking and segmentation results on each individual sequence of the MOTS20 [48] test set. MOTS20 is evaluated in a private detections setting. The arrows indicate low or high optimal metric values.</figDesc><table><row><cell>MOTS20-01</cell><cell>59.8</cell><cell>68.0</cell><cell>79.6</cell><cell>255 364</cell><cell>16</cell></row><row><cell>MOTS20-06</cell><cell>63.9</cell><cell>65.1</cell><cell>78.7</cell><cell cols="2">595 1335 158</cell></row><row><cell>MOTS20-07</cell><cell>43.2</cell><cell>53.6</cell><cell>58.5</cell><cell>834 4433</cell><cell>75</cell></row><row><cell>MOTS20-12</cell><cell>62.0</cell><cell>76.8</cell><cell>74.6</cell><cell>549 1063</cell><cell>29</cell></row><row><cell>ALL</cell><cell>54.9</cell><cell>63.6</cell><cell>69.9</cell><cell cols="2">2233 7195 278</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.4, we provide per-sequence MOT17<ref type="bibr" target="#b27">[28]</ref> test set results for public detection filtering via Intersection over Union (IoU). Futhermore, we present per-sequence Track-Former results on the MOTS20<ref type="bibr" target="#b47">[48]</ref> test set inTable A.2. Sequence Public detection MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ ID Sw. ↓Table A.3. We report the original per-sequence CenterTrack [56] MOT17 [28] test set results with Center Distance (CD) public detection filtering. The results do not reflect the varying object detection performance of DPM, FRCNN and SDP, respectively. The arrows indicate low or high optimal metric values. Sequence Public detection MOTA ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ ID Sw. ↓</figDesc><table><row><cell>MOT17-01 DPM [14]</cell><cell>41.6</cell><cell>44.2</cell><cell>5</cell><cell>8</cell><cell>496</cell><cell>3252</cell><cell>22</cell></row><row><cell>MOT17-03 DPM</cell><cell>79.3</cell><cell>71.6</cell><cell>94</cell><cell>8</cell><cell>1142</cell><cell>20297</cell><cell>191</cell></row><row><cell>MOT17-06 DPM</cell><cell>54.8</cell><cell>42.0</cell><cell>54</cell><cell>63</cell><cell>314</cell><cell>4839</cell><cell>175</cell></row><row><cell>MOT17-07 DPM</cell><cell>44.8</cell><cell>42.0</cell><cell>11</cell><cell>16</cell><cell>1322</cell><cell>7851</cell><cell>147</cell></row><row><cell>MOT17-08 DPM</cell><cell>26.5</cell><cell>32.2</cell><cell>11</cell><cell>37</cell><cell>378</cell><cell>15066</cell><cell>88</cell></row><row><cell>MOT17-12 DPM</cell><cell>46.1</cell><cell>53.1</cell><cell>16</cell><cell>45</cell><cell>207</cell><cell>4434</cell><cell>30</cell></row><row><cell>MOT17-14 DPM</cell><cell>31.6</cell><cell>36.6</cell><cell>13</cell><cell>78</cell><cell>636</cell><cell>11812</cell><cell>196</cell></row><row><cell>MOT17-01 FRCNN [37]</cell><cell>41.0</cell><cell>42.1</cell><cell>6</cell><cell>9</cell><cell>571</cell><cell>3207</cell><cell>25</cell></row><row><cell>MOT17-03 FRCNN</cell><cell>79.6</cell><cell>72.7</cell><cell>93</cell><cell>7</cell><cell>1234</cell><cell>19945</cell><cell>180</cell></row><row><cell>MOT17-06 FRCNN</cell><cell>55.6</cell><cell>42.9</cell><cell>57</cell><cell>59</cell><cell>363</cell><cell>4676</cell><cell>190</cell></row><row><cell>MOT17-07 FRCNN</cell><cell>45.5</cell><cell>41.5</cell><cell>13</cell><cell>15</cell><cell>1263</cell><cell>7785</cell><cell>156</cell></row><row><cell>MOT17-08 FRCNN</cell><cell>26.5</cell><cell>31.9</cell><cell>11</cell><cell>36</cell><cell>332</cell><cell>15113</cell><cell>89</cell></row><row><cell>MOT17-12 FRCNN</cell><cell>46.1</cell><cell>52.6</cell><cell>15</cell><cell>45</cell><cell>197</cell><cell>4443</cell><cell>30</cell></row><row><cell>MOT17-14 FRCNN</cell><cell>31.6</cell><cell>37.6</cell><cell>13</cell><cell>77</cell><cell>780</cell><cell>11653</cell><cell>202</cell></row><row><cell>MOT17-01 SDP [51]</cell><cell>41.8</cell><cell>44.3</cell><cell>7</cell><cell>8</cell><cell>612</cell><cell>3112</cell><cell>27</cell></row><row><cell>MOT17-03 SDP</cell><cell>80.0</cell><cell>72.0</cell><cell>93</cell><cell>8</cell><cell>1223</cell><cell>19530</cell><cell>181</cell></row><row><cell>MOT17-06 SDP</cell><cell>55.5</cell><cell>43.8</cell><cell>56</cell><cell>61</cell><cell>354</cell><cell>4712</cell><cell>181</cell></row><row><cell>MOT17-07 SDP</cell><cell>45.2</cell><cell>42.4</cell><cell>13</cell><cell>15</cell><cell>1332</cell><cell>7775</cell><cell>147</cell></row><row><cell>MOT17-08 SDP</cell><cell>26.6</cell><cell>32.3</cell><cell>11</cell><cell>36</cell><cell>350</cell><cell>15067</cell><cell>91</cell></row><row><cell>MOT17-12 SDP</cell><cell>46.0</cell><cell>53.0</cell><cell>16</cell><cell>45</cell><cell>221</cell><cell>4426</cell><cell>30</cell></row><row><cell>MOT17-14 SDP</cell><cell>31.7</cell><cell>37.1</cell><cell>13</cell><cell>76</cell><cell>749</cell><cell>11677</cell><cell>205</cell></row><row><cell>All</cell><cell>61.5</cell><cell>59.6</cell><cell>621</cell><cell cols="3">752 14076 200672</cell><cell>2583</cell></row><row><cell>MOT17-01 DPM [14]</cell><cell>48.2</cell><cell>38.4</cell><cell>5</cell><cell>8</cell><cell>266</cell><cell>3012</cell><cell>60</cell></row><row><cell>MOT17-03 DPM</cell><cell>73.8</cell><cell>70.4</cell><cell>90</cell><cell>14</cell><cell>6102</cell><cell>21083</cell><cell>236</cell></row><row><cell>MOT17-06 DPM</cell><cell>55.6</cell><cell>54.6</cell><cell>58</cell><cell>76</cell><cell>499</cell><cell>4533</cell><cell>195</cell></row><row><cell>MOT17-07 DPM</cell><cell>53.6</cell><cell>46.3</cell><cell>11</cell><cell>17</cell><cell>686</cell><cell>7047</cell><cell>112</cell></row><row><cell>MOT17-08 DPM</cell><cell>35.0</cell><cell>34.9</cell><cell>12</cell><cell>28</cell><cell>544</cell><cell>13024</cell><cell>164</cell></row><row><cell>MOT17-12 DPM</cell><cell>49.9</cell><cell>57.6</cell><cell>21</cell><cell>36</cell><cell>508</cell><cell>3789</cell><cell>44</cell></row><row><cell>MOT17-14 DPM</cell><cell>39.2</cell><cell>42.4</cell><cell>19</cell><cell>57</cell><cell>947</cell><cell>9958</cell><cell>338</cell></row><row><cell>MOT17-01 FRCNN [37]</cell><cell>49.5</cell><cell>40.7</cell><cell>8</cell><cell>7</cell><cell>363</cell><cell>2831</cell><cell>66</cell></row><row><cell>MOT17-03 FRCNN</cell><cell>75.5</cell><cell>71.4</cell><cell>91</cell><cell>12</cell><cell>6490</cell><cell>18907</cell><cell>243</cell></row><row><cell>MOT17-06 FRCNN</cell><cell>59.0</cell><cell>56.7</cell><cell>64</cell><cell>50</cell><cell>644</cell><cell>3962</cell><cell>224</cell></row><row><cell>MOT17-07 FRCNN</cell><cell>52.8</cell><cell>45.2</cell><cell>11</cell><cell>16</cell><cell>867</cell><cell>6980</cell><cell>131</cell></row><row><cell>MOT17-08 FRCNN</cell><cell>34.2</cell><cell>35.0</cell><cell>13</cell><cell>30</cell><cell>552</cell><cell>13201</cell><cell>142</cell></row><row><cell>MOT17-12 FRCNN</cell><cell>48.0</cell><cell>56.5</cell><cell>18</cell><cell>38</cell><cell>532</cell><cell>3932</cell><cell>40</cell></row><row><cell>MOT17-14 FRCNN</cell><cell>38.8</cell><cell>42.9</cell><cell>20</cell><cell>50</cell><cell>1596</cell><cell>9238</cell><cell>485</cell></row><row><cell>MOT17-01 SDP [51]</cell><cell>55.7</cell><cell>43.0</cell><cell>8</cell><cell>5</cell><cell>391</cell><cell>2396</cell><cell>69</cell></row><row><cell>MOT17-03 SDP</cell><cell>77.5</cell><cell>71.3</cell><cell>103</cell><cell>12</cell><cell>7159</cell><cell>16063</cell><cell>302</cell></row><row><cell>MOT17-06 SDP</cell><cell>58.5</cell><cell>56.5</cell><cell>78</cell><cell>58</cell><cell>724</cell><cell>3950</cell><cell>214</cell></row><row><cell>MOT17-07 SDP</cell><cell>55.8</cell><cell>46.1</cell><cell>14</cell><cell>14</cell><cell>958</cell><cell>6370</cell><cell>141</cell></row><row><cell>MOT17-08 SDP</cell><cell>36.4</cell><cell>35.6</cell><cell>15</cell><cell>26</cell><cell>720</cell><cell>12525</cell><cell>193</cell></row><row><cell>MOT17-12 SDP</cell><cell>50.7</cell><cell>59.8</cell><cell>21</cell><cell>31</cell><cell>666</cell><cell>3559</cell><cell>44</cell></row><row><cell>MOT17-14 SDP</cell><cell>42.4</cell><cell>42.8</cell><cell>22</cell><cell>47</cell><cell>1614</cell><cell>8561</cell><cell>474</cell></row><row><cell>All</cell><cell>62.5</cell><cell>60.7</cell><cell>702</cell><cell cols="3">632 32828 174921</cell><cell>3917</cell></row></table><note>Table A.4. We report TrackFormer results on each individual sequence and set of public detections evaluated on the MOT17 [28] test set. We apply our minimum Intersection over Union (IoU) public detection filtering. The arrows indicate low or high optimal metric values.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We are grateful for discussions with Jitendra Malik, Karttikeya Mangalam, and David Novotny.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-target tracking by continuous energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Engin Turetken, and Pascal Fua. Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a neural solver for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Brasó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple target tracking in world coordinate with single, minimally calibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4836" to="4845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motchallenge: A benchmark for single-camera multiple target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improvements to frank-wolfe optimization for multi-detector multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lifted disjoint paths with application in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Hornakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Swoboda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Mach. Learn</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A linear programming approach for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidney</forename><forename type="middle">S</forename><surname>Fels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning by tracking: siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning an image-based motion context for multiple people tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Fenzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Everybody needs somebody: Modeling social and grouping behavior on a linear programming multiple people tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2999" to="3007" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gsm: Graph similarity model for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiankun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Joint Conf. Art. Int</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">Mot16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Track, then decide: Category-agnostic visionbased multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljoša</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. Rob. Aut</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tubetk: Adopting tubes to track multi-object in a one-step training model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning multiobject tracking and segmentation from automatic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idoia</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog., 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning pedestrian dynamics from the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Heterogeneous association graph fusion for target association in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Segment as points for efficient online multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis., 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Who are you with and where are you going? IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2129" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multiple target tracking using spatio-temporal markov chain monte carlo data association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Long-term tracking with deep tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
