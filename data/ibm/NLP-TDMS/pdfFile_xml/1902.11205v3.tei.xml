<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Optimizing Diversity and Relevance in Neural Response Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
							<email>yizzhang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
							<email>chrisbkt@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
							<email>mgalley@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly Optimizing Diversity and Relevance in Neural Response Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although recent neural conversation models have shown great potential, they often generate bland and generic responses. While various approaches have been explored to diversify the output of the conversation model, the improvement often comes at the cost of decreased relevance <ref type="bibr" target="#b19">(Zhang et al., 2018)</ref>. In this paper, we propose a SPACEFUSION model to jointly optimize diversity and relevance that essentially fuses the latent space of a sequenceto-sequence model and that of an autoencoder model by leveraging novel regularization terms. As a result, our approach induces a latent space in which the distance and direction from the predicted response vector roughly match the relevance and diversity, respectively. This property also lends itself well to an intuitive visualization of the latent space. Both automatic and human evaluation results demonstrate that the proposed approach brings significant improvement compared to strong baselines in both diversity and relevance. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of neural response generation is advancing rapidly both in terms of research and commercial applications <ref type="bibr" target="#b22">Zhou et al., 2018;</ref><ref type="bibr" target="#b18">Yoshino et al., 2019;</ref><ref type="bibr" target="#b20">Zhang et al., 2019)</ref>. Nevertheless, vanilla sequence-to-sequence (S2S) models often generate bland and generic responses <ref type="bibr" target="#b11">(Li et al., 2016a)</ref>. <ref type="bibr" target="#b11">Li et al. (2016a)</ref> encourage diversity by re-ranking the beam search results according to their mutual information with the conversation context. However, as beam search itself often produces lists of nearly identical sequences, this method can require a large beam width <ref type="bibr">(e.g. 200)</ref>. As a result, re-ranking can be extremely <ref type="bibr">1</ref> An implementation of our model is available at https: //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: "I would love to play this game". See <ref type="table" target="#tab_3">Table 2</ref> for more details. [Context] Anyone want to start this game? <ref type="figure">Figure 1</ref>: Illustration of one context and its multiple responses in the latent space induced by our model. Distance and direction from the predicted response vector given the context roughly match the relevance and diversity, respectively. Based on the example in <ref type="table" target="#tab_3">Table 2</ref>. 2 time-consuming, raising difficulties for real-time applications. This highlights the need to improve the diversity of candidates before re-ranking, and the need to optimize for diversity during training rather than just at the decoding stage.</p><p>While various approaches have been explored to diversify the output of conversation models, the improvement often comes at the cost of decreased response relevance along other dimensions. For instance, <ref type="bibr" target="#b21">Zhao et al. (2017)</ref> present an approach to enhancing diversity by mapping diverse responses to a probability distribution using a conditional variational autoencoder (CVAE). Despite the improved response diversity, this approach reduces response relevance as measured against the baseline. One possible reason for this diversityrelevance trade-off is that such probabilistic approaches are not explicitly encouraged to induce a disentangled representation in latent space for controlling diversity and relevance independently. Consider a Gaussian distribution, which is widely used for CVAE. A Gaussian distribution naturally brings frequent responses near its mean and such responses are often generic and boring. To generate diverse and interesting responses, one needs to sample a little distance from the mean. But doing so naturally leads to infrequent and thus even irrelevant responses.</p><p>In this paper, we propose a novel geometrical approach that explicitly encourages a structured latent space in which the distance and direction from a predicted response vector roughly match the relevance and diversity, respectively, as illustrated in <ref type="figure">Figure 1</ref>. To induce such a latent space, we leverage two different models: 1) a S2S model, producing the predicted response vector (the black dot at the center in <ref type="figure">Figure 1</ref>), and 2) an autoencoder (AE) model, yielding the vectors for potential responses (the colored dots). In order to make the S2S and AE share the same latent space (the cloud), we use the same decoder for both and train them jointly end-to-end with novel regularization terms. As this fuses the two latent spaces, we refer to our model as SPACEFUSION.</p><p>Regularization is necessary because only sharing the decoder, as in <ref type="bibr" target="#b13">(Luan et al., 2017)</ref>, does not necessarily align the latent spaces obtained by S2S and AE respectively or impose a disentangled structure onto the space. We introduce two regularization terms to tackle this issue. 1) interpolation term: we encourage a smooth semantic transition along the path between the predicted response vector and each target response vector (arrowed lines in <ref type="figure">Figure 1</ref>). This term effectively prevents semantically different responses from aligning in the same direction, essentially scattering them over different directions. 2) fusion term: we want the vectors from the two models to be distributed in a homogeneous manner, rather than forming two separate clusters ( <ref type="figure" target="#fig_4">Figure 5</ref>) that can potentially make sampling non-trivial. With the resulting latent space, we can control relevance and diversity by respectively adjusting distance and direction from a predicted response vector, without sacrificing each other greatly.</p><p>Our approach also lends itself well to the intuitive visualization of latent space. Since our model allows us to geometrically find not only the predicted response vector but also the target response vector as in <ref type="figure" target="#fig_4">Figure 5</ref>, we can visually interpret the structure of latent space and identify major issues thereof. We devote Section 5.1 to show comprehensive examples for visualization-based analysis.</p><p>Automatic and human evaluations demonstrate that the proposed approach improves both the diversity and relevance of the responses, compared to strong baselines on two datasets with one-tomany context-response mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Grounded conversation models utilize extra context inputs besides conversation history, such as persona <ref type="bibr" target="#b12">(Li et al., 2016b)</ref>, textual knowledge <ref type="bibr">(Ghazvininejad et al., 2017;</ref>, dialog act <ref type="bibr" target="#b21">(Zhao et al., 2017)</ref> and emotion <ref type="bibr" target="#b9">(Huber et al., 2018)</ref>. Our approach does not depend on such extra input and thus is complementary to this line of studies.</p><p>Variational autoencoder (VAE) models explicitly model the uncertainty of responses in latent space. <ref type="bibr" target="#b1">Bowman et al. (2016)</ref> used VAE with Long-Short Term Memory (LSTM) cells to generate sentences. The basic idea of VAE is to encode the input x into a probability distribution (e.g. Gaussian) z instead of a point encoding. However, it suffers from the vanishing latent variable problem <ref type="bibr" target="#b1">(Bowman et al., 2016;</ref><ref type="bibr" target="#b21">Zhao et al., 2017)</ref> when applied to text generation tasks. <ref type="bibr" target="#b1">Bowman et al. (2016)</ref>; <ref type="bibr" target="#b3">Fu et al. (2019)</ref> proposed to tackle this problem with word dropping and specific KL annealing methods. <ref type="bibr" target="#b21">Zhao et al. (2017)</ref> proposed to add a bag-of-word loss, complementary to KL annealing. Applying this to a CVAE conversation model, they showed that even greedy decoding can generate diverse responses. However, as VAE/CVAE conversation models can be limited to a simple latent representations such as standard Gaussian distribution, <ref type="bibr" target="#b8">Gu et al. (2018)</ref> proposed to enrich the latent space by leveraging a Gaussian mixture prior. Our work takes a geometrical approach that is fundamentally different from probabilistic approaches to tackle the limitations of parameteric distributions in representation and difficulties in training.</p><p>Decoding and ranking encourage diversity during the decoding stage. As "vanilla" beam search often produces lists of nearly identical sequences, <ref type="bibr" target="#b16">Vijayakumar et al. (2016)</ref> propose to include a dissimilarity term in the objective of beam search decoding. <ref type="bibr" target="#b11">Li et al. (2016a)</ref> re-ranked the results obtained by beam search based on mutual information with the context using a separately trained response-to-context S2S model.</p><p>Multi-task learning is another line of studies related to the present work (see Section 3.2). <ref type="bibr" target="#b15">Sennrich et al. (2016)</ref> use multi-task learning to improve neural machine translation by utilizing monolingual data, which usually far exceeds the amount of parallel data. A similar idea is applied by <ref type="bibr" target="#b13">Luan et al. (2017)</ref> to conversational modeling, involving two tasks: 1) a S2S model that learns a context-to-response mapping using conversation data, and 2) an AE model that utilizes speakerspecific non-conversational data. The decoders of S2S and AE were shared, and the two tasks were trained alternately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The SPACEFUSION Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem statement</head><p>Let D = [(x 0 , y 0 ), (x 1 , y 1 ), · · · , (x n , y n )] denote a conversational dataset, where x i and y i are a context and its response, respectively. x i consists of one or more utterances. Our aim is to train a model on D to generate relevant and diverse responses given a context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusing latent spaces</head><p>We design our model to induce a latent space where different responses for a given context are in different directions around the predicted response vector, as illustrated in <ref type="figure">Figure 1</ref>. Then we can obtain diverse responses by varying the direction and keep their relevance by sampling near the predicted response vector.</p><p>To fulfill this goal, we first produce the predicted response representation z S2S and target response representations z AE using an S2S model and an AE model, respectively, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Both encoders are implemented using stacked Gated Recurrent Unit (GRU) <ref type="bibr" target="#b2">(Cho et al., 2014)</ref> cells followed by a noise layer that adds multivariate Gaussian noise ∼ N (0, σ 2 I). We then explicitly encourage smooth semantic transition along the path from z S2S to z AE by imposing any interpolation between them to generate the same response via the following loss term:</p><formula xml:id="formula_0">L interp = − 1 |y| log p(y|z interp )<label>(1)</label></formula><p>where z interp = uz S2S + (1 − u)z AE and u ∼ U (0, 1) is a uniformly distributed random vari- able. |y| is the number of words in y. Note that it is this regularization term that effectively prevents significantly different responses from aligning in the same direction, essentially scattering them over different directions. In order for this interpolation loss to work, we share the same decoder for both AE and S2S models as in <ref type="bibr" target="#b13">(Luan et al., 2017)</ref>. The decoder consists of stacked GRU cells followed by a softmax layer. It is worth mentioning that z interp is not just randomly drawn from a single line but from a richer probabilistic region as both z interp and z S2S are stochastic due to the random component . Now, we want vectors from both the AE and S2S models to be distributed in a homogeneous manner scattered over the entire space while keeping the distance between z S2S and z AE as small as possible for any (context-response) pair in the training data. This objective is represented in the following regularization term:</p><formula xml:id="formula_1">L fuse = i∈batch d(z S2S (x i ), z AE (y i )) n − i,j∈batch,i =j d(z S2S (x i ), z S2S (x j )) n 2 − n − i,j∈batch,i =j d(z AE (y i ), z AE (y j )) n 2 − n<label>(2)</label></formula><p>where n is the batch size and d(a, b) is the root mean square of the difference between a and b.</p><p>For each batch, we basically disperse vectors obtained by the same model and pull the predicted response vectors to the corresponding target response vectors. In practice, we found that the performance is better if the Euclidean distance is clipped to a prescribed maximum value. 3 Finally, with weight parameters α and β, the loss function is defined as:</p><formula xml:id="formula_2">L = − 1 |y| log p(y|z S2S ) − 1 |y| log p(y|z AE ) + αL interp + βL fuse<label>(3)</label></formula><p>As L interp and L fuse encourage the path between z S2S and z AE to be smooth and short while scattering vectors over the entire space, they effectively fuse the z S2S latent space and the z AE latent space. Accordingly we refer this approach as SPACEFU-SION with path regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>In contrast to previous multi-task conversation model <ref type="bibr" target="#b13">(Luan et al., 2017)</ref>, where S2S and AE are trained alternately, our approach trains S2S and AE at the same time by minimizing the loss function of Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>Like <ref type="bibr" target="#b21">Zhao et al. (2017)</ref>; Bowman et al. <ref type="formula" target="#formula_0">(2016)</ref>, for a given context, we sample different latent vectors to obtain multiple hypotheses. This is done by adding a random vector r that is uniformly sampled from a hypersphere of radius |r| to the prediction z S2S (x).</p><formula xml:id="formula_3">z(x, r) = z S2S (x) + r<label>(4)</label></formula><p>where |r| is tuned on the validation set to optimize the trade-off between relevance and diversity. z(x, r) is then fed to the decoder as the initial state of GRU cells. We then generate responses using greedy decoding. 4 4 Experiment Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We used the following datasets. Some of their key features are presented in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Switchboard:</head><p>We use the version offered by <ref type="bibr" target="#b21">Zhao et al. (2017)</ref>, which is an extension of the original version by Godfrey and Holliman (1997). <ref type="bibr" target="#b21">Zhao et al. (2017)</ref> collected multiple references for the test set using information retrieval (IR) techniques followed by human filtering, and randomly split the data into 2316/60/62 conversations for 4 Although we use greedy decoding in this work, other decoding techniques, such as beam search, can be applied.  train/validate/test, respectively. Each conversation has multiple turns and thus multiple (x, y) pairs, as listed in <ref type="table" target="#tab_1">Table 1</ref>. As our approach does not utilize extra information except conversation history, we removed the meta data (e.g. gender, age, prompt) from this dataset.</p><p>Reddit: As the Switchboard dataset is relatively small and multiple references are synthetically constructed, we have developed another multireference dataset by extracting posts and comments on Reddit.com during 2011 collected by a third party. 5 As each Reddit post and comment may have multiple comments, it is a natural source of multi-reference responses. We further filtered the data based on the number of replies to obtain the final conversation dataset in which each context has at least 10 different responses, and on average the number of responses is 24.1 for a given context. The size is significantly larger than Switchboard, as listed in <ref type="table" target="#tab_1">Table 1</ref>. The conversations are randomly shuffled before being split into train/valid/test subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model setup</head><p>Both encoders and the shared decoder consist of two GRU cells, each with 128 hidden units. The variance of the noise layer in each decoder is σ 2 = 0.1 2 . The word embedding dimension is 128. The weight parameters (see <ref type="table" target="#tab_5">Equation 3</ref>) are set as α = 1 and β = 30. For both datasets, the inference radius |r| (see <ref type="formula" target="#formula_3">Equation 4</ref>) is set to 1.5 which optimizes F1 score on the validation set. All models are trained using the Adam method (Kingma and Ba, 2014) with a learning rate of 0.001 on both datasets until convergence (around 4 epochs for Reddit and 10 epochs for Switchboard).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Automatic evaluation</head><p>For a given context x, we have N r reference responses and generate the same number of hypothe-ses. <ref type="bibr">6</ref> We define the following metrics based on 4gram BLEU <ref type="bibr" target="#b14">(Papineni et al., 2002)</ref>, as suggested by <ref type="bibr" target="#b21">Zhao et al. (2017)</ref>.</p><formula xml:id="formula_4">Precision = 1 N r Nr i=1 max j∈[1,Nr] BLEU(r j , h i ) Recall = 1 N r Nr j=1 max i∈[1,Nr] BLEU(r j , h i ) F1 = 2 precision · recall precision + recall</formula><p>We use Precision as an approximate surrogate metric for relevance and Recall for diversity. It should be noted that recall is not equivalent to other diversity metrics, e.g., distinct <ref type="bibr" target="#b11">(Li et al., 2016a)</ref> and entropy <ref type="bibr" target="#b19">(Zhang et al., 2018)</ref>, which only depend on hypotheses. One potential issue of these metrics is that even randomly generated responses may yield a high diversity score. F1 is the harmonic average of these two and is used to measure the overall response quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Human evaluation</head><p>We conduct a human evaluation using crowdworkers. For each hypothesis, given its context, we ask three annotators to individually measure the quality, on a scale of 1 to 5, in terms of two aspects: relevance and interest. Interestingness is treated as an estimation of the diversity, as these two are often correlated. The hypotheses from all systems are shuffled before being provided to annotators. System names are invisible to the annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Baselines</head><p>We compare the proposed model with the following baseline models:</p><p>S2S+Sampling: We consider a vanilla version of S2S model. The dimensions are similar to our model: both encoder and decoder consist of two stacked GRU cells with 128 hidden units, and the word embedding size is 128. As in the baseline in <ref type="bibr" target="#b21">Zhao et al. (2017)</ref>, we applied softmax sampling at inference time to generate multiple hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVAE+BOW:</head><p>For the CVAE conversation model, we use the original implementation and <ref type="bibr">6</ref> We set the number of hypotheses equal to the number of references to encourage precision and recall have comparable impact on F1 hyperparameters of <ref type="bibr" target="#b21">Zhao et al. (2017)</ref> with the bag-of-words (BOW) loss. The number of trainable model parameters is 15.4M, which is much larger than our model (3.2M).</p><p>MTask: Since our approach utilizes a multi-task learning scheme, we also compare it against a vanilla multi-task learning model, MTask, similar to <ref type="bibr" target="#b13">(Luan et al., 2017)</ref>, to illustrate the effect of space fusion. The model architecture and hyperparameters are identical to the proposed model except that the loss function is L = − log p(y|z S2S ) − log p(y|z AE ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">In-depth analysis of latent space</head><p>In this section, we undertake an in-depth analysis to verify whether the latent space induced by our method manifests desirable properties, namely: 1) disentangled space structure between relevance and diversity, 2) homogeneous space distribution in which semantics changes smoothly without holes. We first provide a qualitative investigation based on real examples. Then, we present a set of corpus-level quantitative analyses focused on geometric properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Qualitative examples</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we investigate three different directions from the context "Anyone want to start this game?" , which is a real example taken from Reddit. The three different directions correspond to clearly different semantics: "No I don't", "when?" and "Yes I do." If we generate a response with the vector predicted by the S2S model (u = 0), our model outputs "I would love to play this game" which is highly relevant to the context. Now as we move along each direction, we can see our model gradually transforms the response toward the corresponding responses of each direction. For instance, towards "No I don't", our model gradually transforms the response to "I am not interested in the game" (u = 0.18) and then "I am not interested." (u = 0.21). In contrary, towards "Yes I do", the response transforms to "I would love to play it." (u = 0.15). Besides the positive or negative directions, the same transition applies to other directions such as "When?". This example clearly shows that there is a rough correspondence context x: Anyone want to start this game? response at u = 0: I would love to play this game.   <ref type="figure">Fig. 1 for a</ref>   between geometric properties and semantic properties in the latent space induced by our method as shown in <ref type="figure">Figure 1</ref>-the relevance of the response decreases as we move away from the predicted response vector and different directions are associated with semantically different responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Direction vs. diversity</head><p>In order to quantitatively verify the correspondence between direction and diversity, we visualize the distribution of cosine similarities among multiple references for each context for a set of 1000 random samples drawn from the test dataset. Specifically, for a context x k and its associated reference responses [y k,0 , y k,1 , · · · ], we compute the cosine similarity between z AE (y k,i ) − z S2S (x k ) and z AE (y k,j ) − z S2S (x k ). In <ref type="figure" target="#fig_2">Figure 3</ref>, we compare the distribution of our model with that of MTask, which does not employ our regularization terms. While our method yields a bell shaped curve with average cosine similarity being close to zero (0.38), the distribution of MTask is extremely skewed with average cosine similarity being close to 1 (0.95). This indicates that the directions of the reference responses are more evenly distributed in our latent space whereas everything is packed in a narrow band in the MTask's space. This essentially makes the inference process simple and robust in that one can choose arbitrary directions to generate diverse responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Distance vs. relevance</head><p>In order to quantitatively verify the correspondence between distance and relevance, we visu-0.25 0.00 0.25 0.50 0.75 1.00 cos similarity 0% 5% 10% 15% 20% 25% w. regularization w/o regularization alize the perplexity of reference responses along the path from the associated z S2S (u = 0) to the z AE (u = 1) corresponding to the predicted response. In <ref type="figure" target="#fig_3">Figure 4</ref>, we compare our model with MTask, which as already noted, does not employ our regularization terms. While our model shows a gradual increase in perplexity, there is a huge bump for MTask's line. This clearly indicates that there is a rough correspondence between distance and relevance in our latent space whereas even a slight change can lead to an irrelevant response in the MTask's space. We further illustrate the smooth change in relevance according to distance for a specific example in <ref type="table" target="#tab_5">Table 3</ref>. Given the context "Anyone want to start this game?", our model is able to transition from the predicted response "I would love to play this game" to a one of reference responses "Yes I do". The relevance smoothly descreases, generating intermediate responses such as "I would love to play it." In contrary, the MTask model tends to produce irrelevant or ungrammatical responses as it moves away from the predicted response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Homogeneity and Convexity</head><p>Other desirable properties, with which we want to equip our latent space are homogeneity and convexity. If the space is not homogeneous, we have to sample differently depending on the regional traits. If the space is not convex, we have to worry about running into the holes that are not properly associated with valid semantic meanings. In order to verify homogeneity and convexity, we visualize our latent space in a 2D space produced by the multidimensional scaling (MDS) algorithm <ref type="bibr" target="#b0">(Borg and Groenen, 2003)</ref>, which approximately preserves pairwise distance. For comparison, we also provide a visualization for MTask. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, our latent space offers great homogeneity and convexity regardless of which model is used to produce a dot (i.e. z S2S or z AE ). In contrary, MTask's latent space forms two separate clusters for z S2S and z AE with a large gap in-between where no training samples were mapped to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Automatic evaluation</head><p>We let each system generate 100 hypotheses {h j } for each context x i in the test dataset. Assuming x i has N r,i references, we pick the top N r,i distinct hypotheses ranked by log p(h j |x i )+λ|h j |. Similar to <ref type="bibr" target="#b11">(Li et al., 2016a;</ref><ref type="bibr" target="#b17">Wu et al., 2016)</ref>, we takes |h j | into consideration, as BLEU is sensitive to length. For fair comparison, λ is tuned such that the average hypothesis length becomes roughly the same for all systems and approaches the average length of the references. <ref type="bibr">7</ref> The automatic evaluation results are reported in <ref type="table" target="#tab_7">Table 4</ref>. On both datasets, the proposed system consistently outperforms the baselines by a large margin in Precision, Recall, and F1.</p><p>Examples of system outputs and human references can be found in <ref type="table" target="#tab_9">Table 5</ref> and <ref type="table" target="#tab_12">Table 6</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Human evaluation</head><p>We randomly sampled 500 contexts from the Reddit test dataset and picked the top 1 hypothesis generated for each context ranked by log p(h j |x i ) + λ|h j |. As in the automatic evaluation, we tuned λ such that all systems have roughly  the same average hypothesis length. We also randomly select one reference for each context and compare them with the systems (labeled "human" in <ref type="table" target="#tab_10">Table 7)</ref> As illustrated in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a SPACEFUSION model to jointly optimize diversity and relevance that leverages novel regularization terms to essentially fuse the latent space of a S2S model with that of an autoen-context A: Are they doing a lot of recycling out in ***? B: Well at my workplace they are. We have places for aluminum cans and we have everybody been issued a separate trash can for recyclable paper. Let's see them get that word straight.</p><p>A: Uh-huh B: As far as the community goes, it's pretty much voluntary. Nothing's been done. ***. We have just private garbage services. SPACE</p><p>• They're not recycling as a matter of fact. FUSION</p><p>• And it's a waste of time • You know the paper is a big recycling program.</p><p>• But they just put it in the trash • Or if you have a recycling bag or anything like that. CVAE</p><p>• Um -hum. +BOW</p><p>• Oh that would be nice.</p><p>• Is that right?</p><p>• It's a it's a luxury • Well that's interesting. MTask</p><p>• Um -hum.</p><p>• That's right.</p><p>• I don't know how much money you're going to do it.</p><p>• That's right.</p><p>• Yeah S2S+</p><p>• That's great. sampling • What do you feel about this uh -huh.</p><p>• It's just kind of a glass of town and it's even irritating to me. I don't know • Somebody and it's so much better.</p><p>• It now and it's hard to go. human</p><p>• Yeah it is to some degree inconvenient i'd have to say that coder model. This fused latent space exhibits desirable properties such as smooth semantic interpolation between two points. The distance and direction from the predicted response vector roughly match relevance and diversity, respectively. These properties also enable intuitive visualization of the latent space. Both automatic and human evaluation results demonstrate that the proposed approach brings significant improvement compared to strong baselines in terms of both diversity and relevance. In future work, we will provide theoretical justification of the effectiveness of the proposed regularization terms. We expect that this technique will find application as an efficient "mixing board" for conversation that draws on multiple sources of information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>SPACEFUSION model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Distribution of the directions from a given context to its multiple responses, measured by the cosine similarity between z AE (y k,i ) − z S2S (x k ) and z AE (y k,j ) − z S2S (x k ). Histogram calculated based on 1000 x k from Reddit test data and visualized with bin width of 0.02.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Perplexity of z interp on the Reddit test dataset as a function of u for simple multi-task model (without regularization, dashed line) and SPACEFUSION (with regularization, solid line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>MDS visualization of the two latent spaces: z s2s (red dots) and z AE (blue dots) of 1000 randomly picked (x, y) pairs from the Reddit test dataset. Left: multi-task model (without regularization); right: SPACEFUSION (with regularization).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Key features of the datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Yes I do." 0.18 I am not interested in the game. 0.15 I'd be interested in the game 0.15 I'd love to play it. 0.21 I am not interested.</figDesc><table><row><cell>u</cell><cell>towards "No I don't."</cell><cell>u towards "0.31 When is it? towards "when?" u 0.27 Yes I do.</cell></row><row><cell cols="2">0.30 No I don't.</cell><cell>0.40 When will you?</cell></row><row><cell></cell><cell></cell><cell>1.00 When?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Semantic interpolation along different directions y. Results decoded from z interp See</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Semantic interpolation with and without regularization. Results decoded from z interp .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance of each model on automatic measures. The highest score in each row is in bold for each dataset. Note that our BLEU scores are normalized to [0, 100].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>context Everything about this movie is awesome! SPACE• I love this movie. FUSION• It's so awesome!!! I have no idea how to watch this movie. I can't wait for the trailer.• I don't think i'm a fan of the movie.• I would love to see this.• I want to watch this movie.</figDesc><table><row><cell>CVAE</cell><cell>• Smartphones of the best games!.</cell></row><row><cell>+BOW</cell><cell>• I'm in the same boat! I feel the same</cell></row><row><cell></cell><cell>way about this</cell></row><row><cell></cell><cell>• I don't know why but can't tell if the</cell></row><row><cell></cell><cell>New York will be the only one.</cell></row><row><cell></cell><cell>• Caves would never say yes, but I'd love</cell></row><row><cell></cell><cell>to know.</cell></row><row><cell></cell><cell>• I know where this movie is.</cell></row><row><cell>MTask</cell><cell>• This is so funny.</cell></row><row><cell></cell><cell>• I think I saw this.</cell></row><row><cell></cell><cell>• I don't know how many people do you.</cell></row><row><cell></cell><cell>• I would.</cell></row><row><cell></cell><cell>• I remember watching was was disap-</cell></row><row><cell></cell><cell>pointed.</cell></row><row><cell>S2S+</cell><cell>• Whoa man, this is amazing!!!</cell></row><row><cell>sampling</cell><cell>• Man, I love Tom Cruise and I love the</cell></row><row><cell></cell><cell>show.</cell></row><row><cell></cell><cell>• Is this a bill on the right?</cell></row><row><cell></cell><cell>• More like samurai jack.</cell></row><row><cell></cell><cell>• I weep for the hivemind.</cell></row><row><cell>human</cell><cell>• I love this movie.</cell></row><row><cell></cell><cell>• Watched this yesterday! so amazing.</cell></row></table><note>• My favorite Muppet movie by far.• Seriously. Good times. Great oldies.• Now you have got the song into my head.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Example system outputs and human reference on Reddit dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">, the proposed model</cell></row><row><cell cols="4">outperforms all systems except human, consistent</cell></row><row><cell cols="3">with our automatic evaluation results.</cell><cell></cell></row><row><cell></cell><cell cols="2">relevance interest</cell><cell>average</cell></row><row><cell>SPACEFUSION</cell><cell>2.72</cell><cell>2.53</cell><cell>2.63</cell></row><row><cell>CVAE+BOW</cell><cell>2.51</cell><cell>2.37</cell><cell>2.44</cell></row><row><cell>Multi-Task</cell><cell>2.34</cell><cell>2.14</cell><cell>2.24</cell></row><row><cell>S2S+Sampling</cell><cell>2.58</cell><cell>2.43</cell><cell>2.50</cell></row><row><cell>human</cell><cell>3.59</cell><cell>3.41</cell><cell>3.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Performance of each model on human evaluation. The highest score, except human, in each row is in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Example system outputs and human reference on Switchboard dataset. Controversial or offensive words are replaced by ***.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This value is set as 0.3 for the present experiments</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://files.pushshift.io/reddit/ comments/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Approximately 10 words/tokens for Switchboard and 12 for Reddit</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modern multidimensional scaling: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingwer</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Groenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Measurement</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="280" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10145</idno>
		<title level="m">Cyclical annealing schedule: A simple approach to mitigating kl vanishing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Grounded response generation task at dstc7</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural approaches to conversational ai. Foundations and Trends R in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="127" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01932</idno>
		<title level="m">Wen-tau Yih, and Michel Galley. 2017. A knowledge-grounded neural conversation model</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Switchboard-1 release 2. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holliman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">926</biblScope>
			<biblScope unit="page">927</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">DialogWAE: Multimodal response generation with conditional wasserstein auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwoo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12352</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emotional dialogue generation using image-grounded language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">277</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task learning for speaker-role adaptation in neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazaros</forename><surname>Polymenakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulaka</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03461</idno>
		<title level="m">Dialog system technology challenge 7</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating informative and diverse conversational responses via adversarial information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1813" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Consistent dialogue generation with self-supervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05759</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The design and implementation of xiaoice, an empathetic social chatbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08989</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
