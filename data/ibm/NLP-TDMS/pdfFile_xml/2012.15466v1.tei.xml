<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLEAR: Contrastive Learning for Sentence Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-31">31 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
							<email>zhuofeng@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
							<email>sinongwang@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
							<email>mkhabsa@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology, Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
							<email>haom@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CLEAR: Contrastive Learning for Sentence Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-31">31 Dec 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained language models have proven their unique powers in capturing implicit language features. However, most pre-training approaches focus on the word-level training objective, while sentence-level objectives are rarely studied. In this paper, we propose Contrastive LEArning for sentence Representation (CLEAR), which employs multiple sentence-level augmentation strategies in order to learn a noise-invariant sentence representation. These augmentations include word and span deletion, reordering, and substitution. Furthermore, we investigate the key reasons that make contrastive learning effective through numerous experiments. We observe that different sentence augmentations during pre-training lead to different performance improvements on various downstream tasks.Our approach is shown to outperform multiple existing methods on both SentEval and GLUE benchmarks. * Work done while the author was an intern at Facebook AI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning a better sentence representation model has always been a fundamental problem in Natural Language Processing (NLP). Taking the mean of word embeddings as the representation of sentence (also known as mean pooling) is a common baseline in the early stage.</p><p>Later on, pre-trained models such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> propose to insert a special token (i.e., <ref type="bibr">[CLS]</ref> token) during the pre-training and take its embedding as the representation for the sentence.</p><p>Because of the tremendous improvement brought by BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, people seemed to agree that CLS-token embedding is better than averaging word embeddings. Nevertheless, a recent paper Sentence-BERT <ref type="bibr" target="#b21">(Reimers and Gurevych, 2019)</ref> observed that averaging of all output word vectors outperforms the CLS-token embedding marginally. Sentence-BERT's results suggest that models like BERT learn a better representation at the token level. One natural question is how to better learn sentence representation.</p><p>Inspired by the success of contrastive learning in computer vision <ref type="bibr">(Zhuang et al., 2019;</ref><ref type="bibr" target="#b24">Tian et al., 2019;</ref><ref type="bibr" target="#b7">He et al., 2020;</ref><ref type="bibr" target="#b1">Chen et al., 2020;</ref><ref type="bibr" target="#b19">Misra and Maaten, 2020)</ref>, we are interested in exploring whether it could also help language models generate a better sentence representation.</p><p>The key method in contrastive learning is augmenting positive samples during the training. However, data augmentation for text is not as fruitful as for image. The image can be augmented easily by rotating, cropping, resizing, or cutouting, etc. <ref type="bibr" target="#b1">(Chen et al., 2020)</ref>. In NLP, there are minimal augmentation ways that have been researched in literature <ref type="bibr" target="#b6">(Giorgi et al., 2020;</ref><ref type="bibr" target="#b5">Fang and Xie, 2020)</ref>. The main reason is that every word in a sentence may play an essential role in expressing the whole meaning. Additionally, the order of the words also matters.</p><p>Most existing pre-trained language models <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref> are adding different kinds of noises to the text and trying to restore them at the word-level.</p><p>Sentence-level objectives are rarely studied. BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> combines the word-level loss, masked language modeling (MLM) with a sentence-level loss, next sentence prediction (NSP), and observes that MLM+NSP is essential for some downstream tasks. RoBERTa  drops the NSP objective during the pre-training but achieves a much better performance in a variety of downstream tasks. ALBERT <ref type="bibr" target="#b13">(Lan et al., 2019)</ref> proposes a self-supervised loss for Sentence-Order Prediction (SOP), which models the transformer encoder f (·) inter-sentence coherence. Their work shows that coherence prediction is a better choice than the topic prediction, the way NSP uses. DeCLUTR <ref type="bibr" target="#b6">(Giorgi et al., 2020)</ref> is the first work to combine Contrastive Learning (CL) with MLM into pre-training. However, it requires an extremely long input document, i.e., 2048 tokens, which restricts the model to be pre-trained on limited data. Further, DeCLUTR trains from existing pre-trained models, so it remains unknown whether it could also achieve the same performance when it trains from scratch.</p><formula xml:id="formula_0">transformer encoder f (·) Original sentence s · · · Tok i · · · Tok 1 Tok j · · · TokN · · · Tok ′ i · · · Tok ′ 1 [CLS] Tok ′ j · · · Tok ′ N · · · · · · · · · · · · · · · · · · E[CLS] E1 Ei Ej EN h1 hi hj hN h[CLS] z 1 s 1 = AUG(s, seed 1 ) · · · Tok ′′ i · · · Tok ′′ 1 [CLS] Tok ′′ j · · · Tok ′′ N · · · · · · · · · · · · · · · · · · E[CLS] E1 Ei Ej EN h1 hi hj hN h[CLS] z 2 s 2 = AUG(s, seed 2 ) g(·) g(·) maximize agreement AUG ∼ A AUG ∼ A</formula><p>Drawing from the recent advances in pretrained language models and contrastive learning, we propose a new framework, CLEAR, combining word-level MLM objective with sentence-level CL objective to pre-train a language model. MLM objective enables the model capture word-level hidden features while CL objective ensures the model with the capacity of recognizing similar meaning sentences by training an encoder to minimize the distance between the embeddings of different augmentations of the same sentence. In this paper, we present a novel design of augmentations that can be used to pre-train a language model at the sentence-level. Our main findings and contributions can be summarized as follows:</p><p>• We proposed and tested four basic sentence augmentations: random-words-deletion, spans-deletion, synonym-substitution, and reordering, which fills a large gap in NLP about what kind of augmentations can be used in contrastive learning.</p><p>• We showed that model pre-trained by our proposed method outperforms several strong baselines (including RoBERTa and BERT) on both GLUE  and Sen-tEval <ref type="bibr" target="#b3">(Conneau and Kiela, 2018)</ref> benchmark. For example, we showed +2.2% absolute improvement on 8 GLUE tasks and +5.7% absolute improvement on 7 SentEval semantic textual similarity tasks compared to RoBERTa model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are three lines of literatures that are closely related to our work: sentence representation, largescale pre-trained language representation models, contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Representation</head><p>Learning the representation of sentence has been studied by many existing works. Applying various pooling strategies onto word embeddings as the representation of sentence is a common baseline <ref type="bibr" target="#b8">(Iyyer et al., 2015;</ref><ref type="bibr" target="#b22">Shen et al., 2018;</ref><ref type="bibr" target="#b21">Reimers and Gurevych, 2019)</ref>.</p><p>Skip-Thoughts  trains an encoderdecoder model trying to reconstruct surrounding sentences. Quick-Thoughts (Logeswaran and Lee, 2018) trains a encoder-only model with the ability to select the correct context of the sentence out of other contrastive sentences. Later on, many pre-trained language models such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> propose to use the manually-inserted token (the [CLS] token) as the representation of the whole sentence and become the new state-of-the-art in a variety of downstream tasks. One recent paper Sentence-BERT <ref type="bibr" target="#b21">(Reimers and Gurevych, 2019)</ref> compares the average BERT embeddings with the CLStoken embedding and surprisingly finds that computing the mean of all output vectors at the last layer of BERT outperforms the CLS-token marginally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Large-scale Pre-trained Language Representation Models</head><p>The deep pre-trained language models have proven their powers in capturing implicit language features even with different model architectures, pre-training tasks, and loss functions. Two of the early works that are GPT <ref type="bibr" target="#b20">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>: GPT uses a leftto-right Transformer while BERT designs a bidirectional Transformer. Both created an incredible new state of the art in a lot of downstream tasks. Following this observation, recently, a tremendous number of research works are published in the pre-trained language model domain. Some extend previous models to a sequence-to-sequence structure <ref type="bibr" target="#b23">(Song et al., 2019;</ref><ref type="bibr" target="#b15">Liu et al., 2020)</ref>, which enforces the model's capability on language generation. The others <ref type="bibr" target="#b29">(Yang et al., 2019;</ref><ref type="bibr" target="#b2">Clark et al., 2020)</ref> explore the different pre-training objectives to either improve the model's performance or accelerate the pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning</head><p>Contrastive Learning has become a rising domain because of its significant success in various computer vision tasks and datasets. Several researchers <ref type="bibr">(Zhuang et al., 2019;</ref><ref type="bibr" target="#b24">Tian et al., 2019;</ref><ref type="bibr" target="#b19">Misra and Maaten, 2020;</ref><ref type="bibr" target="#b1">Chen et al., 2020)</ref> proposed to make the representations of the different augmentation of an image agree with each other and showed positive results. The main difference between these works is their various definition of image augmentation.</p><p>Researchers in the NLP domain have also started to work on finding suitable augmentation for text. CERT <ref type="bibr" target="#b5">(Fang and Xie, 2020)</ref> applies the back-translation to create augmentations of original sentences, while DeCLUTR (Giorgi et al., 2020) regards different spans inside one document are similar to each others. Our model differs from CERT in adopting an encoder-only structure, which decreases noise brought by the decoder. Further, unlike DeCLUTR, which only tests one augmentation and trains the model from an existing pre-trained model, we pre-train all models from scratch, which provides a straightforward comparison with the existing pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section proposes a novel framework and several sentence augmentation methods for contrastive learning in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Contrastive Learning Framework</head><p>Borrow from SimCLR <ref type="bibr" target="#b1">(Chen et al., 2020)</ref>, we propose a new contrastive learning framework to learn the sentence representation, named as CLEAR. There are four main components in CLEAR, as outlined in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>• An augmentation component AUG(·) which apply the random augmentation to the original sentence. For each original sentence s, we generate two random augmentations s 1 = AUG(s, seed 1 ) and s 2 = AUG(s, seed 2 ), where seed 1 and seed 2 are two random seeds.</p><p>Note that, to test each augmentation's effect solely, we adopt the same augmentation to generate s 1 and s 2 . Testing the mixing augmentation models requests more computational resources, which we plan to leave for future work. We will detail the proposed augmentation set A at Section 3.3.</p><p>• A transformer-based encoder f (·) that learns the representation of the input augmented sentences</p><formula xml:id="formula_1">H 1 = f ( s 1 ) and H 2 = f ( s 2 ).</formula><p>Any encoder that learns the sentence representation can be used here to replace our encoder. We choose the current start-of-theart (i.e., transformer <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>) to learn sentence representation and use the representation of a manually-inserted token as the vector of the sentence (i.e., <ref type="bibr">[CLS]</ref>, as used in BERT and RoBERTa).</p><p>• A nonlinear neural network projection head g(·) that maps the encoded augmentations H 1 and H 2 to the vector z 1 = g(H 1 ), z 2 = g(H 2 ) in a new space. According to observations in SimCLR <ref type="bibr" target="#b1">(Chen et al., 2020)</ref>, adding Tok 5 · · · Tok N Tok <ref type="bibr">[del]</ref> Tok <ref type="bibr">[del]</ref> Tok <ref type="bibr">[del]</ref> Tok <ref type="bibr">[del]</ref> · · · · · ·  a nonlinear projection head can significantly improve representation quality of images.</p><p>• A contrastive learning loss function defined for a contrastive prediction task, i.e., trying to predict positive augmentation pair ( s 1 , s 2 ) in the set {s}. We construct the set {s} by randomly augmenting twice for all the sentences in a minibatch (assuming a minibatch is a set {s} size N ), getting a set {s} with size 2N . The two variants from the same original sentence form the positive pair, while all other instances from the same minibatch are regarded as negative samples for them. The contrastive learning loss has been tremendously used in previous work <ref type="bibr" target="#b28">(Wu et al., 2018;</ref><ref type="bibr" target="#b1">Chen et al., 2020;</ref><ref type="bibr" target="#b6">Giorgi et al., 2020;</ref><ref type="bibr" target="#b5">Fang and Xie, 2020)</ref>. The loss function for a positive pair is defined as:</p><formula xml:id="formula_2">l(i, j)=− log exp (sim(z i , z j )/τ ) 2N k=1 ½ [k =i] exp (sim(z i , z k )/τ )<label>(1)</label></formula><p>where ½ [k =i] is the indicator function to judge whether k = i, τ is a temperature parameter, sim(u, v) = u ⊤ v/( u 2 v 2 ) denotes the cosine similarity of two vector u and v. The overall contrastive learning loss is defined as the sum of all positive pairs' loss in a minibatch:</p><formula xml:id="formula_3">L CL = 2N i=1 2N j=1 m(i, j)l(i, j)<label>(2)</label></formula><p>where m(i, j) is a function returns 1 when i and j is a positive pair, returns 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Combined Loss for Pre-training</head><p>Similar to <ref type="bibr" target="#b6">(Giorgi et al., 2020)</ref>, for the purpose of grabbing both token-level and sentence-level features, we use a combined loss of MLM objective and CL objective to get the overall loss:</p><formula xml:id="formula_4">L total = L MLM + L CL<label>(3)</label></formula><p>where L MLM is calculated through predicting the random-masked tokens in set {s} as described in BERT and RoBERTa <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref>. Our pre-training target is to minimize the L total .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design Rationale for Sentence Augmentations</head><p>The data augmentation is crucial for learning the representation of image <ref type="bibr" target="#b24">(Tian et al., 2019;</ref><ref type="bibr" target="#b9">Jain et al., 2020)</ref>. However, in language modeling, it remains unknown whether data (sentence) augmentation would benefit the representation learning and what kind of data augmentation could apply to the text. To answer these questions, we explore and test four basic augmentations (shown in <ref type="figure" target="#fig_1">Figure 2</ref>) and their combinations in our experiment. We do believe there exist more potential augmentations, which we plan to leave for future exploration.</p><p>One type of augmentation we consider is deletion, which bases on the hypothesis that some deletion in a sentence wouldn't affect too much of the original semantic meaning. In some case, it may happen that deleting some words leads the sentence to a different meaning (e.g., the word not). However, we believe including proper noise can benefit the model to be more robust. We consider two different deletions, i.e., word deletion and span deletion.</p><p>• Word deletion (shown in <ref type="figure" target="#fig_1">Figure 2a</ref>) randomly selects tokens in the sentence and replace them by a special token <ref type="bibr">[DEL]</ref>, which is similar to the token [MASK] in BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>.</p><p>• Span deletion (shown in <ref type="figure" target="#fig_1">Figure 2b</ref>) picks and replaces the deletion objective on the spanlevel. Generally, span-deletion is a special case of word-deletion, which puts more focus on deleting consecutive words.</p><p>To avoid the model easily distinguishing the two augmentations from the remaining words at the same location, we eliminate the consecutive token [DEL] into one token.</p><p>Reordering (shown in <ref type="figure" target="#fig_1">Figure 2c</ref>) is another widely-studied augmentation that can keep the original sentence's features. BART  has explored restoring the original sentence from the random reordered sentence. We randomly sample several pairs of span and switch them pairwise to construct the reordering augmentation in our implementation.</p><p>Substitution (shown in <ref type="figure" target="#fig_1">Figure 2d</ref>) has been proven efficient in improving model's robustness <ref type="bibr" target="#b10">(Jia et al., 2019)</ref>. Following their work, we sample some words and replace them with synonyms to construct one augmentation. The synonym list comes from a vocabulary they used. In our pre-training corpus, there are roughly 40% tokens with at least one similar-meaning token in the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>This section presents empirical experiments that compare the proposed methods with various baselines and alternative approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Model configuration: We use the Transformer (12 layers, 12 heads and 768 hidden size) as our primary encoder <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref>. Models are pre-trained for 500K updates, with minibatches containing 8,192 sequences of maximum length 512 tokens. For the first 24,000 steps, the learning rate is warmed up to a peak value of 6e−4, then linearly decayed for the rest. All models are optimized by Adam (Kingma and Ba, 2014) with β 1 = 0.9, β 2 = 0.98, ǫ = 1e−6, and L 2 weight decay of 0.01. We use 0.1 for dropout on all layers and in attention. All of the models are pre-trained on 256 NVIDIA Tesla V100 32GB GPUs. Pre-training data: We pre-train all the models on a combination of BookCorpus  and English Wikipedia datasets, the data BERT used for pre-training. For more statistics of the dataset and processing details, one can refer to BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>. Hyperparameters for MLM: For calculating MLM loss, we randomly mask 15% tokens of the input text s and use the surrounding tokens to predict them. To fill the gap between fine-tuning and pre-training, we also adopt the 10%-randomreplacement and 10%-keep-unchanged setting in BERT for the masked tokens. Hyperparameters for CL: To compute CL loss, we set up different hyperparameters:</p><p>• For Word Deletion (del-word), we delete 70% tokens.</p><p>• For Span Deletion (del-span), we delete 5 spans (each with 5% length of the input text).</p><p>• For Reordering (reorder), we randomly pick 5 pairs of spans (each with roughly 5% length as well) and switch spans pairwise. • For Substitution (subs), we randomly select 30% tokens and replace each token with one of their similar-meaning tokens.</p><p>Some of the above hyperparameters are slightlytuned on the WiKiText-103 dataset <ref type="bibr" target="#b18">(Merity et al., 2016)</ref> (trained for 100 epochs, evaluated on the GLUE dev benchmark). For example, we find 70% deletion model perform best out of {30%, 40%, 50%, 60%, 70%, 80%, 90%} deletion models. For models using mixed augmentations, like MLM+2-CL-objective in <ref type="table" target="#tab_2">Table 1</ref>, they use the same optimized hyperparameters as in the single model. For instance, our notation MLM+subs+delspan represents a model combining the MLM loss with CL loss: for MLM, it masks 15% tokens; for CL, it substitutes 30% tokens first and then deletes 5 spans to generate augmented sentences.</p><p>Note that the hyperparameters we used might not be the most optimized ones. Yet, it is unknown whether optimized hyperparameters on a 1-CL-objective model perform consistently on a 2-CL-objective model. Additionally, it is also unclear whether the optimized hyperparameters for WiKiText-103 are still the optimized ones on BookCorpus and English Wikipedia datasets. However, it is hard to tune every possible hyperparameter due to the extensive computation resource requirement for pre-training. We will leave these questions to explore in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GLUE Results</head><p>We mainly evaluate all the models by the General Language Understanding Evaluation (GLUE) benchmark development set . GLUE is a benchmark containing several different types of NLP tasks: natural language inference task (MNLI, QNLI, and RTE), similarity task (QQP, MRPC, STS), sentiment analysis task (SST), and linguistic acceptability task(CoLA). It provides a comprehensive evaluation for pretrained language models.</p><p>To fit the different downstream tasks' requirements, we follow the RoBERTa's hyperparamters to finetune our model for various tasks. Specifically, we add an extra fully connected layer and then finetune the whole model on different training sets.</p><p>The primary baselines we include are BERTbase and RoBERTa-base. The results for BERTbase are from huggingface's reimplementation 1 . A more fair comparison comes from RoBERTa-base since we use the same hyperparameters RoBERTabase used for MLM loss. Note that our models are all combining two-loss, it is still unfair to compare a MLM-only model with a MLM+CL model. To answer this question, we set two other baselines in Section 5.1 to make a more strict comparison: one combines two MLM losses, the other adopts a double batch size. As we can see in <ref type="table" target="#tab_2">Table 1</ref>, our proposed several models outperform the baselines on GLUE. Note that different tasks adopt different evaluation matrices, our two best models MLM+delword and MLM+del-span+reorder both improve the best baseline RoBERTa-base by 2.2% on average score. Besides, a more important observation is that all best performance for each task comes from our proposed model. On CoLA and RTE, our best model exceeds the baseline by 7.0% and 8.0% correspondingly. Further, we also find that different downstream tasks benefit from different augmentations. We will make a more specific analysis in Section 5.2.</p><p>One notable thing is that we don't show the result of MLM+subs, MLM+reorder, and MLM+subs+reorder in <ref type="table" target="#tab_2">Table 1</ref>. We observe that the pre-training for these three models either converges quickly or suffers from a gradient explosion problem, which indicates that these three augmentations are too easy to distinguish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SentEval Results for Semantic Textual Similarity Tasks</head><p>SentEval is a popular benchmark for evaluating general-purpose sentence representations <ref type="bibr" target="#b3">(Conneau and Kiela, 2018)</ref>. The specialty for this benchmark is that it doesn't do the fine-tuning like in GLUE. We evaluate the performance of our proposed methods for common Semantic Textual Similarity (STS) tasks on SentEval. Note that some previous models (e.g., Sentence-BERT (Reimers and Gurevych, 2019)) on the SentEval leaderboard trains on the specific datasets such as Stanford NLI <ref type="bibr" target="#b0">(Bowman et al., 2015)</ref> and MultiNLI <ref type="bibr" target="#b27">(Williams et al., 2017)</ref>, which makes it hard for a direct comparison. To make it easier, we compare one of our proposed models with RoBERTa-base directly on SentEval. According to Sentence-BERT, using the mean of all output vectors in the last layer is more effective than using the CLS-token output. We test both pooling strategies for each model.</p><p>From <ref type="table" target="#tab_3">Table 2</ref>, we observe that mean-pooling strategy does not show much advantages. In many of the cases, CLS-pooling is better than the meanpooling for our proposed models. The underlying reason is that the contrastive learning directly updates the representation of [CLS] token. Besides that, we find adding the CL loss makes the model especially good at the Semantic Textual Similarity (STS) task, beating the best baseline by a large margin (+5.7%). We think it is because the pretraining of contrastive learning is to find the similar sentence pairs, which aligns with STS task. This could explain why our proposed models show such large improvements on STS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>This section discusses an ablation study to compare the CL loss and MLM loss and shows some observations about what different augmentation learns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>Our proposed CL-based models outperforms MLM-based models, one remaining question is, where does our proposed model benefit from? Does it come from the CL loss, or is it from the larger batch size (since to calculate CL loss, one needs to store extra information per batch)?</p><p>To answer this question, we set up two extra baselines: Double MLM RoBERTa-base adopts the MLM+MLM loss, each MLM is performed on different mask for the same original sentence; the other Double-batch RoBERTa-base uses single MLM loss with a double-size batch. Due to the limitation of computational resource, we conduct the ablation study on a smaller pre-training corpus, i.e., WiKiText-103 dataset <ref type="bibr" target="#b18">(Merity et al., 2016)</ref>. All the models listed in <ref type="table" target="#tab_4">Table 3</ref> are pre-trained for 500 epochs on 64 NVIDIA Tesla V100 32GB GPUs. Three of our proposed models are reported in the table. The general performance for the variants doesn't show much difference compared with the original RoBERTa-base, with a +0.4% increase on the average score on Double-batch RoBERTa-base, which confirms the idea that a larger batch benefits the representation training as proposed by previous work . Yet, the best-performed baseline is still not as good as our best-proposed model. It tells us the proposed model does not solely benefit from a larger batch; CL loss also helps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Different Augmentation Learns Different Features</head><p>In <ref type="table" target="#tab_2">Table 1</ref>, we find an interesting phenomenon: different proposed models are good at specific tasks. One example is MLM+subs+del-span helps the model be good at dealing with similarity and paraphrase tasks. On QQP and STS, it achieves the highest score; on MRPC, it ranks second. We infer the outperformance of MLM+subs+del-span in this kind of task is because synonym substitution helps translate the original sentence to similar meaning sentences while deleting different spans makes more variety of similar sentences visible. Combining them enhances the model's capacity to deal with many unseen sentence pairs.</p><p>We also notice that MLM+del-span achieves good performance on inference tasks (MNLI, QNLI, RTE). The underlying reason is, with a span deletion, the model has already been pretrained well to infer the other similar sentences. The ability to identify similar sentence pairs helps to recognize the contradiction. Therefore, the gap between the pre-trained task and this downstream task narrows.</p><p>Overall, we observe that different augmentation learns different features. Some specific augmentations are especially good at some certain downstream tasks. Designing a task-specific augmentation or exploring meta-learning to adaptively select different CL objectives is a promising future direction.</p><p>In this work, we presented an instantiation for contrastive sentence representation learning. By carefully designing and testing different data augmentations and combinations, we prove the proposed methods' effectiveness on GLUE and SentEval benchmark under the diverse pre-training corpus.</p><p>The experiment results indicate that the pretrained model would be more robust when leveraging adequate sentence-level supervision. More importantly, we reveal that different augmentation learns different features for the model. Finally, we demonstrate that the performance improvement comes from both the larger batch size and the contrastive loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed contrastive learning framework CLEAR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Four sentence augmentation methods in proposed contrastive learning framework CLEAR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Word Deletion: Tok1, Tok2, and Tok4 are deleted, the sentence after augmentation will be: [Tok[del], Tok3, Tok[del], Tok5, . . . , TokN ].</figDesc><table><row><cell></cell><cell cols="5">sentence after word deletion</cell><cell></cell></row><row><cell cols="2">Tok [del]</cell><cell>Tok 3</cell><cell>Tok [del]</cell><cell>Tok 5</cell><cell>· · ·</cell><cell>Tok N</cell></row><row><cell>Tok [del]</cell><cell>Tok [del]</cell><cell>Tok 3</cell><cell>Tok [del]</cell><cell>Tok 5</cell><cell>· · ·</cell><cell>Tok N</cell></row><row><cell cols="7">Tok 4 original sentence Tok 3 Tok 5 · · · Tok N (a) Tok 4 Tok 2 Tok 1 Tok 3 Tok 2 Tok 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Span Deletion: The span [Tok1, Tok2, Tok3, Tok4] is deleted, the sentence after augmentation will be: [Tok[del], Tok5, . . . , TokN ]. . . . , TokN ].</figDesc><table><row><cell cols="3">sentence after span deletion</cell></row><row><cell>Tok [del]</cell><cell>Tok 5</cell><cell>Tok N</cell></row><row><cell></cell><cell>Tok 5</cell><cell>Tok N</cell></row><row><cell cols="2">original sentence</cell><cell></cell></row><row><cell cols="3">· · · (b) Tok 4 Tok 5 Tok N Tok 2 sentence after reordering Tok 3 Tok 1 Tok 1 Tok 3 Tok 2 Tok 5 · · · Tok N original sentence (c) Reordering: Two spans [Tok1, Tok2] and Tok 4 Tok 1 [Tok4] are reordered, the sentence after aug-mentation will be: [Tok4, Tok3, Tok1, Tok2, Tok 1 (d) Synonym Substitution: Tok2, Tok3, and Tok 4 Tok 5 · · · Tok ′ 2 Tok ′ 3 Tok ′ N sentence after similar word subsitution Tok 3 Tok 2 Tok 5 · · · Tok N original sentence TokN are substituted by their synonyms Tok ′ 2 , Tok5, Tok 4 Tok ′ 3 , and Tok ′ N , respectively. The sentence after augmentation will be: [Tok1, Tok ′ 2 , Tok ′ 3 , Tok4,</cell></row><row><cell>Tok5, . . . , Tok ′ N ].</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance of competing methods evaluated on GLUE dev set. Following GLUE's setting, unweighted average accuracy on the matched and mismatched dev sets is reported for MNLI. The unweighted average of accuracy and F1 is reported for MRPC and QQP. The unweighted average of Pearson and Spearman correlation is reported for STS-B. The Matthews correlation is reported for CoLA. For all other tasks we report accuracy.</figDesc><table><row><cell>Method</cell><cell cols="8">MNLI QNLI QQP RTE SST-2 MRPC CoLA STS</cell><cell>Avg</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base (Devlin et al., 2019)</cell><cell>84.0</cell><cell>89.0</cell><cell>89.1</cell><cell>61.0</cell><cell>93.0</cell><cell>86.3</cell><cell>57.3</cell><cell cols="2">89.5 81.2</cell></row><row><cell>RoBERTa-base (Liu et al., 2019)</cell><cell>87.2</cell><cell>93.2</cell><cell>88.2</cell><cell>71.8</cell><cell>94.4</cell><cell>87.8</cell><cell>56.1</cell><cell cols="2">89.4 83.5</cell></row><row><cell>MLM+1-CL-objective</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLM+ del-word</cell><cell>86.8</cell><cell>93.0</cell><cell>90.2</cell><cell>79.4</cell><cell>94.2</cell><cell>89.7</cell><cell>62.1</cell><cell cols="2">90.5 85.7</cell></row><row><cell>MLM+ del-span</cell><cell>87.3</cell><cell>92.8</cell><cell>90.1</cell><cell>79.8</cell><cell>94.4</cell><cell>89.9</cell><cell>59.8</cell><cell cols="2">90.3 85.6</cell></row><row><cell>MLM+2-CL-objective</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLM+ subs+ del-word</cell><cell>87.3</cell><cell>93.1</cell><cell>90.0</cell><cell>73.3</cell><cell>93.7</cell><cell>90.2</cell><cell>62.1</cell><cell cols="2">90.1 85.0</cell></row><row><cell>MLM+ subs+ del-span</cell><cell>87.0</cell><cell>93.4</cell><cell>90.3</cell><cell>74.4</cell><cell>94.3</cell><cell>90.5</cell><cell>63.3</cell><cell cols="2">90.5 85.5</cell></row><row><cell>MLM+ del-word+ reorder</cell><cell>87.0</cell><cell>92.7</cell><cell>89.5</cell><cell>76.5</cell><cell>94.5</cell><cell>90.6</cell><cell>59.1</cell><cell cols="2">90.4 85.0</cell></row><row><cell>MLM+ del-span+ reorder</cell><cell>86.7</cell><cell>92.9</cell><cell>90.0</cell><cell>78.3</cell><cell>94.5</cell><cell>89.2</cell><cell>64.3</cell><cell cols="2">89.8 85.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of competing methods evaluated on SentEval. All results are pre-trained on BookCorpus and English Wikipedia datasets for 500k steps.</figDesc><table><row><cell>Method</cell><cell cols="7">SICK-R STS-B STS12 STS13 STS14 STS15 STS16</cell><cell>Avg</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa-base-mean</cell><cell>74.1</cell><cell>65.6</cell><cell>47.2</cell><cell>38.3</cell><cell>46.7</cell><cell>55.0</cell><cell>49.5</cell><cell>53.8</cell></row><row><cell>RoBERTa-base-[CLS]</cell><cell>75.9</cell><cell>71.9</cell><cell>47.4</cell><cell>37.5</cell><cell>47.9</cell><cell>55.1</cell><cell>57.6</cell><cell>56.1</cell></row><row><cell>MLM+1-CL-objective</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLM+ del-word-mean</cell><cell>75.9</cell><cell>69.0</cell><cell>50.6</cell><cell>40.0</cell><cell>50.2</cell><cell>58.9</cell><cell>52.4</cell><cell>56.7</cell></row><row><cell>MLM+ del-span-mean</cell><cell>71.0</cell><cell>62.6</cell><cell>49.3</cell><cell>41.7</cell><cell>48.9</cell><cell>58.1</cell><cell>52.3</cell><cell>54.8</cell></row><row><cell>MLM+ del-word-[CLS]</cell><cell>77.1</cell><cell>71.6</cell><cell>50.6</cell><cell>44.5</cell><cell>48.3</cell><cell>58.4</cell><cell>56.1</cell><cell>58.1</cell></row><row><cell>MLM+ del-span-[CLS]</cell><cell>62.7</cell><cell>57.4</cell><cell>34.4</cell><cell>20.4</cell><cell>24.3</cell><cell>32.0</cell><cell>31.5</cell><cell>37.5</cell></row><row><cell>MLM+2-CL-objective</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLM+ del-word+ reorder-mean</cell><cell>75.8</cell><cell>66.2</cell><cell>51.1</cell><cell>45.7</cell><cell>51.8</cell><cell>61.3</cell><cell>57.0</cell><cell>58.4</cell></row><row><cell>MLM+ del-span+ reorder-mean</cell><cell>75.4</cell><cell>67.8</cell><cell>48.3</cell><cell>50.3</cell><cell>54.9</cell><cell>60.4</cell><cell>56.8</cell><cell>59.1</cell></row><row><cell>MLM+ subs+ del-word-mean</cell><cell>73.6</cell><cell>63.4</cell><cell>44.6</cell><cell>39.8</cell><cell>50.1</cell><cell>55.5</cell><cell>49.6</cell><cell>53.8</cell></row><row><cell>MLM+ subs+ del-span-mean</cell><cell>75.5</cell><cell>67.0</cell><cell>48.3</cell><cell>45.0</cell><cell>54.6</cell><cell>60.9</cell><cell>58.5</cell><cell>58.5</cell></row><row><cell>MLM+ del-word+ reorder-[CLS]</cell><cell>71.9</cell><cell>63.8</cell><cell>41.9</cell><cell>30.9</cell><cell>37.4</cell><cell>48.9</cell><cell>52.1</cell><cell>49.6</cell></row><row><cell>MLM+ del-span+ reorder-[CLS]</cell><cell>75.0</cell><cell>68.7</cell><cell>49.4</cell><cell>54.3</cell><cell>57.6</cell><cell>64.0</cell><cell>61.4</cell><cell>61.5</cell></row><row><cell>MLM+ subs+ del-word-[CLS]</cell><cell>73.6</cell><cell>62.9</cell><cell>44.5</cell><cell>35.8</cell><cell>47.6</cell><cell>55.8</cell><cell>59.6</cell><cell>54.3</cell></row><row><cell>MLM+ subs+ del-span-[CLS]</cell><cell>75.6</cell><cell>72.5</cell><cell>49.0</cell><cell>48.9</cell><cell>57.4</cell><cell>63.6</cell><cell>65.6</cell><cell>61.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study for several methods evaluated on GLUE dev set. All results are pre-trained on wiki-103 data for 500 epochs.</figDesc><table><row><cell>Method</cell><cell cols="8">MNLI-m QNLI QQP RTE SST-2 MRPC CoLA STS</cell><cell>Avg</cell></row><row><cell>RoBERTa-base</cell><cell>80.4</cell><cell>87.5</cell><cell>87.4</cell><cell>61.4</cell><cell>91.4</cell><cell>82.4</cell><cell>38.9</cell><cell cols="2">81.9 76.4</cell></row><row><cell>MLM-variant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Double-batch RoBERTa-base</cell><cell>80.3</cell><cell>88.0</cell><cell>87.1</cell><cell>59.9</cell><cell>91.9</cell><cell>82.1</cell><cell>43.0</cell><cell cols="2">82.0 76.8</cell></row><row><cell>Double MLM RoBERTA-base</cell><cell>80.5</cell><cell>87.6</cell><cell>87.3</cell><cell>57.4</cell><cell>90.4</cell><cell>77.7</cell><cell>42.2</cell><cell cols="2">83.0 75.8</cell></row><row><cell>MLM+CL-objective</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MLM+ del-span</cell><cell>80.6</cell><cell>88.8</cell><cell>87.3</cell><cell>62.1</cell><cell>92.1</cell><cell>77.8</cell><cell>44.1</cell><cell cols="2">81.4 76.8</cell></row><row><cell>MLM+ del-span + reorder</cell><cell>81.1</cell><cell>88.7</cell><cell>87.5</cell><cell>58.1</cell><cell>90.0</cell><cell>80.4</cell><cell>43.3</cell><cell cols="2">87.4 77.1</cell></row><row><cell>MLM+ subs + del-word + reorder</cell><cell>80.5</cell><cell>87.7</cell><cell>87.3</cell><cell>59.6</cell><cell>90.4</cell><cell>80.2</cell><cell>45.1</cell><cell cols="2">87.1 77.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://huggingface.co/transformers/v1.1.0/examples.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Senteval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05449</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12766</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>John M Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03659</idno>
		<title level="m">Declutr: Deep contrastive learning for unsupervised textual representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing</title>
		<meeting>the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04973</idno>
		<title level="m">Contrastive code representation learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Certified robustness to adversarial word substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerem</forename><surname>Göksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00986</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08210</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02893</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunderstand-ingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentencebert: Sentence embeddings using siamese bertnetworks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Baseline needs more love: On simple wordembedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09843</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<editor>Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins</editor>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
