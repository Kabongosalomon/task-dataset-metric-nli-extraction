<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse and Dense Data with CNNs: Depth Completion and Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
							<email>maximilian.jaritz@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria RITS Team</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Valeo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><surname>De Charette</surname></persName>
							<email>raoul.de-charette@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria RITS Team</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Wirbel</surname></persName>
							<email>emilie.wirbel@valeo.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Valeo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Perrotton</surname></persName>
							<email>xavier.perrotton@valeo.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Valeo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fawzi</forename><surname>Nashashibi</surname></persName>
							<email>fawzi.nashashibi@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria RITS Team</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse and Dense Data with CNNs: Depth Completion and Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks are designed for dense data, but vision data is often sparse (stereo depth, point clouds, pen stroke, etc.). We present a method to handle sparse depth data with optional dense RGB, and accomplish depth completion and semantic segmentation changing only the last layer. Our proposal efficiently learns sparse features without the need of an additional validity mask. We show how to ensure network robustness to varying input sparsities. Our method even works with densities as low as 0.8% (8 layer lidar), and outperforms all published stateof-the-art on the Kitti depth completion benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most computer vision algorithms rely now on convolutional neural networks (CNNs) which are designed for dense data, rather than sparse data which is rarely considered. However, vision data can become sparse when reprojected into a different dense coordinate space (e.g. Lidar data reprojected into camera image plane). Corrupted data may also be seen as sparse data, either because of noise or invalid measurement (e.g. lack of reflectivity for Lidar, camera saturation). Finally, some processes are inherently sparse such as stereo disparity that relies on saliency, sparse by nature.</p><p>A main objective of processing sparse data is to complete missing information. This is known as data completion (aka inpainting for images) and upscaling is one instance of this problem. Classically, this was achieved through sophisticated interpolation of valid data, which failed at completing large holes in data. Machine learning on the other hand can complete large chunks of missing data from learned appearance priors. <ref type="figure">Figure 1</ref>: Our method handles sparse depth input data with or without additional dense RGB, to accomplish depth completion or semantic segmentation (with minor adaptation). In robotics, this task has interesting applications, as sensors have different resolutions and field of views, i.e. different densities in a common reference frame. As an example, the reprojection of a Velodyne 64 layers Lidar only covers 5.9% pixels of the whole image space in the Kitti dataset and even less density with fewer layers <ref type="bibr" target="#b0">1</ref> . Apart from depth completion, the long-term goal of processing sparse data is its fusion with dense data which would benefit all vision tasks in general.</p><p>Naively applying CNN to sparse data does not work as they are sensitive to missing data. It is commonly claimed that learning all combination of missing data is virtually impossible. Instead, sparse convolution <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b16">17]</ref> was proposed recently which allows for a feature representation that is invariant to missing data using an additional validity mask input. While this led to an important performance gain, we show that a mask input might be redundant and that using a normal (i.e. dense) CNN architecture with ad-hoc training process can yield significantly better performance while remaining generic. Intuitively our results demonstrate that with proper training, CNNs can learn where valid input data are and how to rely on them to build features invariant to the missing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>We propose a new method to leverage sparse data processing and demonstrate its performance on depth completion and semantic segmentation tasks. <ref type="figure">Fig. 1</ref> illustrates our approach: using a common network structure, different kind of data are used to perform inference tasks which require data completion and benefit from data fusion. Using an encoder-decoder scheme, NASNet <ref type="bibr" target="#b25">[26]</ref> with minor changes and a sparse training strategy, we show that our method can efficiently handle sparse inputs of various densities without the need of retraining or any additional mask input. In addition to being lighter and easier to design, our method preserves better sharp edges. We extend our research to inputs of different densities and prove that dense RGB and sparse depth data can be efficiently fused together in a late fusion manner. The experiments demonstrate the performance of our approach on depth completion and semantic segmentation for both single sparse input (depth) or sparse + dense inputs (depth + RGB). Tests are conducted on both synthetic and real data and we carry out an additional ablation study to prove robustness at lower data density. Results on the Kitti Depth Completion benchmark show we outperform state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We present the state-of-the-art of pixel-wise inference tasks, focusing on Depth Prediction and Semantic Segmentation.</p><p>In the targeted tasks the expected result is a dense output in the image plane and our research relies on past works for dense or sparse processing. Consequently, we first provide the reader with an overview of inference from dense inputs, and then detail inference from sparse inputs only or the fusion of sparse + dense, at the core of this work.</p><p>Dense Inputs Pixelwise inference tasks such as semantic segmentation seek an output of equal resolution as the input, which led to Fully Convolutional Networks (FCNs). The downsampled CNN features in FCNs are upsampled with bilinear upsampling or transposed convolution <ref type="bibr" target="#b12">[13]</ref>. Skip connections between the equivalent feature maps of the downsampling (encoder) and upsampling (decoder) part of the network allow to keep details <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1]</ref>. To preserve spatial context, Pyramid Pooling in PSPNet <ref type="bibr" target="#b24">[25]</ref> concatenates upsampled local and global multiscale features maps. An alternative to downsampling the features is to dilate the convolutional kernels <ref type="bibr" target="#b1">[2]</ref>. This way, skip connections can be omitted as the resolution stays the same.</p><p>Multiscale networks are commonly used for semantic segmentation <ref type="bibr" target="#b12">[13]</ref> or depth prediction task <ref type="bibr" target="#b5">[6]</ref>. From a single image, depth is inferred with learned priors on object and scenes, for example with VGG <ref type="bibr" target="#b5">[6]</ref> or deeper ResNet-50 <ref type="bibr" target="#b11">[12]</ref> networks. Unlike semantic segmentation that requires costly annotation, depth can be easily learned. Either in a supervised manner with sparse Lidar measurements <ref type="bibr" target="#b10">[11]</ref> or for example in a self-supervised fashion with stereo by computing the loss between the right camera image and the reprojected version of the left image using depth <ref type="bibr" target="#b6">[7]</ref>.</p><p>Sparse Inputs The nature of sparse data varies with the sensor and scenario, as can be seen in <ref type="figure" target="#fig_0">fig 2.</ref> For instance, Lidar data exhibit structured sparsity due to the discrete polar scanning behavior ( <ref type="figure" target="#fig_0">fig. 2a</ref>). Stereo vision or structured light sensors deliver dense data with patches of missing data ( <ref type="figure" target="#fig_0">fig. 2b</ref>). Artificially altered data typically has uniformly distributed data ( <ref type="figure" target="#fig_0">fig. 2c</ref>). Classically, sparse 2D inputs are considered for inpainting of uniform sparsity using local interpolation <ref type="bibr" target="#b9">[10]</ref> or guided optimization <ref type="bibr" target="#b20">[21]</ref> for patches sparsity.</p><p>As CNNs are designed to operate on dense data, a common strategy is to transform sparse data to a 2D or 3D grid with holes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. A validity mask can be given as additional input to express valid or missing data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b16">17]</ref>. In <ref type="bibr" target="#b21">[22]</ref> -which established the groundwork of CNN sparse depth completion -only valid data locations are considered in the convolution and the activation is biased accordingly. As filters do not distinguish between a dense pattern and a pattern with missing values, the network is sparsity independent. Meanwhile, it induces a blurry output due to the extension of the validity domain (see discussion in 3.2). In <ref type="bibr" target="#b16">[17]</ref> the mask is input as block-wise coordinates which leverage the loss of spatial information in <ref type="bibr" target="#b21">[22]</ref>, but cannot accommodate to the variety of sparsities.</p><p>Other alternatives either act directly on the data manifold <ref type="bibr" target="#b7">[8]</ref> to avoid the extension of the valid domain or apply order-invariant operations <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparse + Dense Inputs</head><p>The problem of inputs with different densities/sparsities is yet little addressed. For the inference tasks, the sparse depth is either used to guide the RGB inference <ref type="bibr" target="#b23">[24]</ref> or just combined <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, surface normals and occlusion boundaries are inferred from RGB only which provides a coarse geometric representation of the scene which is then completed via a global optimization guided by the sparse depth input. Alternatively, <ref type="bibr" target="#b14">[15]</ref> concatenates RGB and sparse depth to a 4-channel input to an encoder-decoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our aim is to propose a method that efficiently handles sparse depth data with or without additional dense RGB data. To study and handle sparsity, we focus on the task of depth completion of sparse depth data but our proposal can also accomplish semantic segmentation with minor adjustments, as we shall demonstrate, and copes with inputs of varying density. Multiple inputs efficiency is proven using sparse depth with dense RGB, which further improves performance.</p><p>The proposed method uses a network architecture adapted from NASNet <ref type="bibr" target="#b25">[26]</ref> (sec. 3.1), with an encoderdecoder for larger receptive field. We chose not to use a validity mask after an extensive analysis (sec. 3.2) which led to a lighter network with better performance.</p><p>A sparse training strategy is proposed to be more robust to varying input density (sec. 3.3). Finally, a late fusion scheme is used when using multiple inputs (sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>For dense data, state-of-the-art methods in semantic segmentation use encoder-decoder networks or dilated convolutions <ref type="bibr" target="#b1">[2]</ref>. While the latter significantly reduces the number of parameters, they are ill-conditioned for sparse data as dilated kernel with zeros between weights can miss available pixels. In <ref type="bibr" target="#b21">[22]</ref>, a network without any downsampling is used with at most 11x11 kernels, implying a rather small receptive field. As a consequence, <ref type="bibr" target="#b21">[22]</ref> perform similarly as classical local interpolation <ref type="bibr" target="#b9">[10]</ref> and work less efficiently on very sparse data. As in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref> we prefer encoder-decoder with larger receptive field, thus enabling better data completion.</p><p>The encoder part of our network is an adaptation of NASNet <ref type="bibr" target="#b25">[26]</ref> which is flexible and very efficient in terms of parameters vs. performance. We use the mobile version to fit real-time constraints and slightly modify it by removing batch normalization after the first strided convolution layer when the input is sparse. The latter is necessary because zero values of missing pixels falsify the mean computation of the batchnorm layer.  <ref type="figure">Figure 3</ref>: Saturation of the validity mask for different input densities (3a) with max pooling as <ref type="bibr" target="#b21">[22]</ref> (here 3 convs: stride 1, kernel 3). For density ≥ 0.3, the mask is saturated after first conv. Even at 0.1 density the spatial location of valid pixels is already mostly lost in deeper layers (3b).</p><p>We build our own custom decoder with transposed convolutions for upsampling, normal convolutions, and copy and concatenate skip connections like in <ref type="bibr" target="#b18">[19]</ref> between the encoder and decoder stages of equivalent resolution.</p><p>Numerous choices in this paper were guided by experiments with a small custom encoder-decoder exhibiting acceptable performance on sparse depth but that failed at extracting good features from dense RGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Analysis of Validity Mask</head><p>A validity mask <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref> is a binary matrix of same size as the input data, with ones indicating available input data and zeros elsewhere. To propagate the mask through the network, <ref type="bibr" target="#b21">[22]</ref> use a max pooling of similar kernel size k and stride s than feature convolution (k, s). Intuitively, the mask expresses whether the current filter contains at least one valid pixel. The authors further normalize convolution by the number of valid pixels in the filter and rescale valid outputs. The drawback of such an approach is that the scaling tends to over-activate highly-upscaled features at lower density. This problem is similar in spirit to the problem of extension of the valid domain <ref type="bibr" target="#b7">[8]</ref>. As a consequence, the mask is almost entirely valid at deeper layers.</p><p>In <ref type="figure">fig. 3a</ref>, we can see that the mask saturation (the percentage considered as valid) increases with input density as expected, but reaches almost full saturation after only a few layers. This means that the validity information is quickly lost in the later layers which is visible in <ref type="figure">fig. 3b</ref> showing an example of such a validity mask and how it is transformed after a few layers. Another consequence is that the network tends to produce blurry outputs as seen in <ref type="figure" target="#fig_3">fig. 5</ref>. We interpret that this is a consequence of the normalization phase on the number of valid pixels, which processes a mask with only one valid pixel in the same way as a fully valid mask.</p><p>In an attempt to leverage the latter issue, we tested average pooling to preserve the ratio of valid/invalid pixels in the filter. However, this did not improve results either.</p><p>We tested to let the network learn how to use the validity mask by concatenating the actual validity mask channel-wise to the features before each convolution. While it improves performance on small networks, we observed no improvement with validity masks for large networks such as NASNet.</p><p>Tests confirmed our analysis that without any validity mask large networks still manage to learn sparsity invariant features on their own while preserving spatial information about the validity pixels. Consequently, we do not use any validity mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sparse Data Training</head><p>Varying density Existing research surprisingly only use fixed density when training although we found that varying synthetic densities within range of ]0, 1] naturally helps networks to be invariant to different densities, as discussed in sec. 4.2.1.</p><p>Another interesting proposal from <ref type="bibr" target="#b4">[5]</ref> is to apply rectangles cut-out on data. While this should force the network to use farther away features, it barely improved results in our experiment.</p><p>Losses For depth completion, we compute the loss only on unobserved pixels available in the ground truth <ref type="bibr" target="#b1">2</ref> . Other strategies such as computing the loss on all pixels (unobserved and observed) or using a weighted sum of observed and unobserved pixels losses, were proven to work less well. The interest of our choice is to favor learning prediction of the unknown pixels, over learning to reproduce already measured data.</p><p>In accordance with <ref type="bibr" target="#b21">[22]</ref> we found the L 1 loss to reach slightly better results than L 2 for depth prediction. We train using inverse depth in [1/km] which corresponds to the inverse mean average error (iMAE) in the Kitti Benchmark.</p><p>The final depth d is obtained by reversing the inverse depth d inv where d inv &gt; 0 and setting the output value to the maximum representable depth d max where the network regresses to d inv = 0 (non-activation). It reads:</p><formula xml:id="formula_0">d = d −1 inv , for d inv &gt; 0 d max , for d inv = 0<label>(1)</label></formula><p>As optimizer we use Adam with learning rate of 0.001, β 1 = 0.9, β 2 = 0.999 and = 10 −8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Sparse Depth + RGB Fusion</head><p>Because the same scene is sensed with a camera and a depth sensor, we want our network to learn to use dense and sparse information jointly for better prediction. A naive strategy consists of averaging separate predictions from each modality. An alternative is to apply an early fusion like <ref type="bibr" target="#b1">2</ref> In Kitti dataset the ground truth is also sparse as it results of Lidar augmentation using left-right consistency. Read <ref type="bibr" target="#b21">[22]</ref> for details. in <ref type="bibr" target="#b14">[15]</ref>, where modalities are simply concatenated channelwise and fed to the network ( <ref type="figure" target="#fig_2">fig. 4a</ref>). However, modalities have different representations (RGB intensities, distance values) and in order to reason from both at the same time, it appears preferable to transform them to a similar feature space before fusing them (known as late fusion <ref type="figure" target="#fig_2">fig. 4b</ref>). A joint representation can be enforced by using element-wise addition of features coming from modality specific encoder networks <ref type="bibr" target="#b2">[3]</ref>. However, we chose instead to use channel-wise concatenation with a following convolution to allow the two branches to provide information of distinct nature as in <ref type="bibr" target="#b22">[23]</ref>.</p><p>In practice, for the task of depth completion using additional dense RGB data requires a particular attention to the choice of architecture and fusion strategy to extract robust enough features from RGB so that fusion actually improves the results. The performance obtained support our choices as for the early vs. late benchmark (sec. 4.2.1 and 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate our proposal we carried out two experimental tasks: depth completion (sec. 4.2) and semantic segmentation (sec. 4.3). For both tasks we used either sparse depth (sD) Lidar input, dense RGB input, or a fusion of both and tested on both synthetic and real public datasets described in sec. 4.1.</p><p>For the depth completion task our proposed method can handle input of varying density, reaching better results than others or when trained with a fixed density. We reached above state-of-the-art on 3 out 4 metrics on the real Kitti Depth Completion Benchmark and our Lidar ablation study shows consistent depth maps reconstructed from only 8 lidar layers. Finally, for semantic segmentation we prove that our method can handle sparse depth only and significantly improves when fusing RGB and sparse depth compared to the RGB only baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Synthia This dataset built with the Unity game engine provides RGB, depth and semantics for urban and highway scenarios <ref type="bibr" target="#b19">[20]</ref> with pedestrian and cars. We use summer sequences 1, 2, 4 and 6 for training and sequence 5 for testing (all views). With our split the training/validation/testing sets contain 28484/1500/6296 frames cropped (bottom/center) and rescaled to 320x160 pixels.</p><p>Uniform sparse depth input is simulated by setting different ratios of pixels to zero, which we call pixel density. A density of 0.1 means that 10% of the pixels are available and 90% are not.</p><p>Kitti The Kitti Depth dataset provides RGB, raw lidar data (64 layers HDL-64E) projected into the image plane, and sparse ground truth from the accumulation of lidar point clouds with stereo consistency check <ref type="bibr" target="#b21">[22]</ref>. Comparison against other methods (sec. 4.2.2) is performed at full-resolution (1216x352), but cropped (bottom/center) and rescaled to 608x160 to reduce training time elsewhere.</p><p>Depth being sparse, we use max pooling to downsample it to avoid loosing any points (common resize methods would take zeros into account and corrupt the output).</p><p>Cityscapes Cityscapes <ref type="bibr" target="#b3">[4]</ref> provides RGB and stereo disparity from German cities, with 20000 coarse and 3000 fine semantic annotations for training. We resized the data from 2048x1024 to 512x256 for time matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Depth completion</head><p>We first evaluate on synthetic data (Synthia) and then on real data (Kitti). The metrics are from the Kitti Benchmark: Mean Average Error (MAE, L 1 mean over all pixels), Root Mean Square Error (RMSE, L 2 over all pixels), both in mm, as well as their inverse depth counterparts iMAE and iRMSE in 1/km between predictionŷ and ground truth y averaged over the number of evaluated pixels N .</p><p>For the experiment on synthetic data in table 1 we also report the δ-metric as in <ref type="bibr" target="#b5">[6]</ref>, which is the percentage of relative errors inside a certain threshold = {1.05, 1.10, 1.25, 1.50}:   is probably because the network can combine the learned visual features and learned geometric features from RGB and very precise sparse depth information. Late fusion clearly outperforms early fusion confirming that one network branch per modality is needed to map modalities to a similar feature space before fusion. Early fusion performs approximately as good as sparse depth only suggesting that the network simply ignores the less informative input modality. Qualitative results in <ref type="figure" target="#fig_3">fig. 5</ref> exhibit sharp and precise completion for sD and even better for RGB+sD (notice the pedestrians). While <ref type="bibr" target="#b21">[22]</ref> performs decently, <ref type="bibr" target="#b9">[10]</ref> exhibits sharp but chaotic results due to the very low density. Note that sparse depth is much more important as modality than RGB, even at this low sparsity level, because the input is effectively a subset of the ground truth. This effect is even stronger in the case of the Kitti benchmark where the ground truth is constructed with consecutive Lidar measurements. Another consequence is that the network cannot be trained to perform depth completion outside of the field of view of the depth sensor, because ground truth is never available in this area. This is why in section 4.2.2 we conduct an ablation study: Lidar layers are removed to test the robustness to lower density but also the extrapolation capabilities of the inference.</p><formula xml:id="formula_1">δ = 1 N i (δ i &lt; ), δ i = max ŷ i y i , y î y i<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varying Density</head><p>We further investigate the influence of different input densities at test time and plot results in <ref type="figure">fig. 6</ref> for sparsity invariant CNNs <ref type="bibr" target="#b21">[22]</ref> trained as in the paper with a fixed density (here of 0.1), our method trained at fixed density of 0.1, and our method trained at varying density randomly chosen in ]0, 1] per image. Despite being trained at fixed density, the performance of <ref type="bibr" target="#b21">[22]</ref> is almost perfectly stable over different densities at test time, except for the lowest of 0.02. Our method trained at fixed density of 0.1 achieves much better results for densities {0.05, 0.1, 0.3} that are close or equal to the training density. However, the error increases drastically beyond, including the case where more information is given (higher density). The network thus specializes in densities seen during training. However, when we train our network at varying density between 0 and 1, it gets very robust and we obtain better results than <ref type="bibr" target="#b21">[22]</ref> at all densities even at the lowest density of 0.02. Results demonstrate that our method with varying training density could perform under a large variety of sensor densities which has great implications for Lidar applications. For the other experiments, where training and test have same densities we use a fixed density to guarantee the best results at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Real Data (Kitti)</head><p>Depth Completion Benchmark In table 2 we report the best methods from the Kitti benchmark. At the time of submission, we rank first on all published methods. Accounting for anonymous submissions, we rank first on all metrics but the RMSE (third). <ref type="figure" target="#fig_5">Fig. 7</ref> displays the visual output (recolored) from the benchmark website (test set). Classical morphological interpolation as IP Basic <ref type="bibr" target="#b9">[10]</ref> is favored  <ref type="figure">Figure 6</ref>: Test results on Synthia for depth completion with sparse depth only input at different density of sparse convolution <ref type="bibr" target="#b21">[22]</ref> trained at fixed density of 0.1 (blue), our network trained at fixed density of 0.1 (orange) and our network trained at varying densities between 0 and 1.   by the dense input of the 64 layers lidar. It produces very sharp output but fails logically to reconstruct shapes when the density is low. Sparse Convolution <ref type="bibr" target="#b21">[22]</ref> does better on shapes by integrating learned priors, but the output is rather blurry and the network is unable to predict outside of the field of view of the lidar (top part). These are expected counterparts as explained in sec. 3.2. HMSNetv2, an anonymous method at the time of submission, reconstructs shapes well, but produces a slightly blurrier output than ours which might help to decrease the RMSE at the expense of the other metrics. The effect is noticeable in the zoom inset. Because the ground truth is sparse and only inside the lidar field of view the network is never supervised regarding its prediction at the top of the image. This is better understood looking at results from the validation set ( <ref type="figure" target="#fig_3">fig. 5</ref>) as we can display the ground truth along with the results. We use full resolution 1216x352, batch size 8 and train 20 epochs.</p><p>Lidar Ablation Study Because the ground truth is obtained from lidar, the depth modality is much more important than RGB. To compensate for that and to give a clue which depth map precision can be obtained with fewer layer lidars, the input can be reduced from the original: we subsampled the 64 layers Velodyne data to simulate 8, 16 and 32 layers 3 .</p><p>In <ref type="figure" target="#fig_6">fig. 8a</ref> we can see that by decreasing the number of input layers the dense depth map prediction deteriorates as expected. Interestingly, the RGB input always improves especially at lower densities. Qualitative results in <ref type="figure" target="#fig_6">fig. 8</ref> show remarkable output even with only 8 layers -i.e. 0.008 density -(note the bike in the foreground).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic Segmentation</head><p>In this section we investigate how sparse depth can improve semantic segmentation by evaluating our method on synthetic data (Synthia, 13 classes) and real data  Note that our goal is not to improve state-of-the-art results, but rather to compare against a RGB-only baseline. In the literature semantic segmentation is usually carried out with RGB only and we prove here our method outperforms the baseline using additional sparse depth data.</p><p>First, to adapt our network to semantic segmentation we modify the last layer to output the probabilities for all classes (Softmax) instead of the 1-channel depth regression, and train with a cross entropy loss. We report the mean Intersection over Union (IoU), first computed per class and then averaged over classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Synthetic Data (Synthia)</head><p>For sparse depth, we use a pixel density of 0.3 close to the 64 Velodyne lidar (0.27 inside the FOV) and trained for 30 epochs with a batch size of 16. The results in table 3a indicate naturally that texture and intensity from RGB data carry more semantic information than depth. As for depth completion, our RGB+sD late fusion is proved best while early fusion fails to integrate depth features and reaches only slightly better output as RGB only. In <ref type="figure">fig. 9</ref>, sparse depth only shows very acceptable results but fails as expected on lane markings and far away buildings. With our late RGB+sD fusion the network better reconstructs the shape of cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Real Data (Cityscapes)</head><p>On real data (see table 3b), we obtain similar results relatively to Synthia although the depth data is different in Cityscapes: almost dense, unscaled disparity from stereo camera instead of metric distance. This proves the robustness of our proposed method. We trained 50 epochs on coarse labels and then 50 epochs on fine ground truth using a batch size of 16. Qualitative results in <ref type="figure">fig. 9</ref> are similar to synthetic data with satisfying result although thin structures  <ref type="figure">Figure 9</ref>: Qualitative results for semantic segmentation on Synthia (synthetic, 0.3 sD density) and Cityscapes (real). Note markings that cannot be predicted when using only sD input.</p><p>like light poles are never segmented. The network uses together dense RGB and sparse depth for better segmentation.</p><p>Our findings demonstrate that sparse depth can directly be input into a network for semantic segmentation and possibly other tasks such as object detection without first generating the dense depth map like is commonly done in RGB-D networks as for example in <ref type="bibr" target="#b8">[9]</ref>. Additionally, our method works with various densities and sparsity types <ref type="figure" target="#fig_0">(Fig. 2)</ref> given that the latter does not change between train and test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a method to handle sparse depth data, using a modified NASNet <ref type="bibr" target="#b25">[26]</ref> with our decoder, a sparse training strategy and a late fusion scheme for dense RGB + sparse depth. Following our study of sparse data and validity mask, we do not use any additional mask proving that the network learns sparsity invariant features by itself.</p><p>Our results on depth completion outperform all published methods on the Kitti benchmark and are qualitatively remarkable with only 8 layers lidar. Changing only the last layer, we also performed semantic segmentation on synthetic and real datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different sparsity patterns from sensors such as Lidar 2a, stereo camera 2b or synthetic data 2c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of early fusion (4a) as opposed to our late fusion framework (4b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results for depth completion on Synthia (synthetic) at 0.02 density and Kitti (real) validation set with 64 layers Lidar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results from public Kitti Depth Completion Benchmark (recolored), with an inset zoom on a parked bike. (* anonymous)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Depth completion with simulated fewer layer lidars (downsampling of 64 layers input). Even with only 8 layers our method completes the depth map remarkably.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Depth completion on Synthia at input depth density of 0.02. For iMAE lower is better, for δ-metric higher is better, indicated numbers are thresholds. While sparse depth is clearly the most important modality for depth prediction, late fusion can considerably improve the results.</figDesc><table><row><cell>Synthia</cell><cell>Kitti</cell></row><row><cell>RGB input</cell><cell></cell></row><row><cell>sD input</cell><cell></cell></row><row><cell>Sparse Conv [22]</cell><cell></cell></row><row><cell>IP-Basic [10]</cell><cell></cell></row><row><cell>Ours sD</cell><cell></cell></row><row><cell>Ours RGB + sD</cell><cell></cell></row><row><cell>Ground truth</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">: Depth completion performance on the Kitti bench-</cell></row><row><cell>mark. (* anonymous)</cell><cell></cell></row><row><cell>RGB input</cell><cell>sD input</cell></row><row><cell>Sparse Conv [22]</cell><cell>IP-Basic [10]</cell></row><row><cell>HMSNetv2*</cell><cell>Ours RGB+sD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation on Synthia (3a, 0.3 uniform sparse depth) and Cityscapes (3b, stereo depth) exhibit similar performance. RGB being the main modality but fusion with sparse depth always improves performance.</figDesc><table><row><cell>(Cityscapes, 19 classes). We do not use the Kitti segmen-</cell></row><row><cell>tation benchmark, because the corresponding lidar point</cell></row><row><cell>clouds are not provided and the number of training images</cell></row><row><cell>is very small (200).</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">3.0%, 1.6%, 0.8% pixels density simulating 32, 16, 8 layers Lidars. 1 arXiv:1808.00769v2 [cs.CV] 31 Aug 2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We subsampled every 2nd, 4th, and 8th layer to simulate 32, 16 and 8 layers, untwisted data linearly using the car speed from IMU, and projected into the camera image plane.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The proposed method is proven to efficiently fuse dense RGB and sparse depth. This should benefit all vision tasks using inputs with various densities. Our future work will target application to 3D object detection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6526" to="6534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00036</idno>
		<title level="m">defense of classical image processing: Fast depth completion on the cpu</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stückler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sbnet: Sparse blocks network for fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1801.02108</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneidre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adapnet: Adaptive semantic segmentation in adverse environmental conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4644" to="4651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
