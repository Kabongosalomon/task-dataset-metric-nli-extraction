<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural data-to-text generation: A comparison between pipeline and end-to-end architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-27">27 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><forename type="middle">Castro</forename><surname>Ferreira</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tilburg center for Cognition and Communication (TiCC)</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Federal University of Minas Gerais (UFMG)</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Van Der Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tilburg center for Cognition and Communication (TiCC)</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Van Miltenburg</surname></persName>
							<email>c.w.j.vanmiltenburg@tilburguniversity.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Tilburg center for Cognition and Communication (TiCC)</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
							<email>e.j.krahmer@tilburguniversity.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Tilburg center for Cognition and Communication (TiCC)</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural data-to-text generation: A comparison between pipeline and end-to-end architectures</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-27">27 Nov 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. By contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in between. This study introduces a systematic comparison between neural pipeline and endto-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of the encoder-decoder Gated-Recurrent Units (GRU) and Transformer, two state-of-the art deep learning methods. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-to-text Natural Language Generation (NLG) is the computational process of generating meaningful and coherent natural language text to describe non-linguistic input data <ref type="bibr" target="#b11">(Gatt and Krahmer, 2018)</ref>. Practical applications can be found in domains such as weather forecasts <ref type="bibr" target="#b20">(Mei et al., 2016)</ref>, health care <ref type="bibr" target="#b26">(Portet et al., 2009)</ref>, feedback for car drivers <ref type="bibr" target="#b2">(Braun et al., 2018)</ref>, diet management <ref type="bibr" target="#b0">(Anselma and Mazzei, 2018)</ref>, election results <ref type="bibr" target="#b18">(Leppänen et al., 2017)</ref> and sportscasting news (van der <ref type="bibr">Lee et al., 2017)</ref>.</p><p>Traditionally, most of data-to-text applications have been designed in a modular fashion, in which 1 https://github.com/ThiagoCF05/DeepNLG/ the non-linguistic input data (be it, say, numerical weather information or game statistics) are converted into natural language (e.g., weather forecast, game report) through several explicit intermediate transformations. The most prominent example is the 'traditional' pipeline architecture <ref type="bibr" target="#b27">(Reiter and Dale, 2000)</ref> that performs tasks related to document planning, sentence planning and linguistic realization in sequence. Many of the traditional, rule-based NLG systems relied on modules because (a) these modules could be more easily reused across applications, and (b) because going directly from input to output using rules was simply too complex in general (see <ref type="bibr" target="#b11">Gatt and Krahmer 2018</ref> for a discussion of different architectures).</p><p>The emergence of neural methods changed this: provided there is enough training data, it does become possible to learn a direct mapping from input to output, as has also been shown in, for example, neural machine translation. As a result, in NLG more recently, neural end-to-end data-totext models have been proposed, which directly learn input-output mappings and rely much less on explicit intermediate representations <ref type="bibr" target="#b32">(Wen et al., 2015;</ref><ref type="bibr" target="#b6">Dušek and Jurcicek, 2016;</ref><ref type="bibr" target="#b20">Mei et al., 2016;</ref><ref type="bibr" target="#b17">Lebret et al., 2016;</ref><ref type="bibr" target="#b12">Gehrmann et al., 2018)</ref>.</p><p>However, the fact that neural end-to-end approaches are possible does not necessarily entail that they are better than (neural) pipeline models. On the one hand, cascading of errors is a known problem of pipeline models in general (an error in an early module will impact all later modules in the pipeline), which (almost by definition) does not apply to end-to-end models. On the other hand, it is also conceivable that developing dedicated neural modules for specific tasks leads to better performance on each of these successive tasks, and combining them might lead to better, and more reusable, output results. In fact, this has never been systematically studied, and this is the main goal of the current paper.</p><p>We present a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of output text from RDF input triples, relying on an augmented version of the WebNLG corpus <ref type="bibr" target="#b10">(Gardent et al., 2017b)</ref>. Using two state-of-the-art deep learning techniques, GRU <ref type="bibr" target="#b5">(Cho et al., 2014)</ref> and Transformer <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>, we develop both a neural pipeline and an end-to-end architecture. The former, which also makes use of the Neu-ralREG approach <ref type="bibr" target="#b3">(Castro Ferreira et al., 2018a)</ref>, tackles standard NLG tasks (discourse ordering, text structuring, lexicalization, referring expression generation and textual realization) in sequence, while the latter does not address these individual tasks, but directly tries to learn how to map RDF triples into corresponding output text.</p><p>Using a range of evaluation techniques, including both automatic and human measures, combined with a qualitative analysis, we provide answers to our two main research questions: (RQ1) How well do deep learning methods perform as individual modules in a data-to-text pipeline architecture? And (RQ2) How well does a neural pipeline architecture perform compared to a neural end-to-end one? Our results show that adding supervision during the data-to-text generation process, by distinguishing separate modules and combining them in a pipeline, leads to better results than full end-to-end approaches. Moreover, the pipeline architecture offers somewhat better generalization to unseen domains and compares favorably to the current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The experiments presented in this work were conducted on the WebNLG corpus <ref type="bibr">(Gardent et al., 2017a,b)</ref>, which consists of sets of Subject, Predicate, Object RDF triples and their target texts. In comparison with other popular NLG benchmarks <ref type="bibr" target="#b1">(Belz et al., 2011;</ref><ref type="bibr" target="#b24">Novikova et al., 2017;</ref><ref type="bibr" target="#b21">Mille et al., 2018)</ref>, WebNLG is the most semantically varied corpus, consisting of 25,298 texts describing 9,674 sets of up to 7 RDF triples in 15 domains. Out of these domains, 5 are exclusively present in the test set, being unseen during the training and validation processes. <ref type="figure" target="#fig_0">Figure 1</ref> depicts an example of a set of 3 RDF triples and its related text.</p><p>To evaluate the intermediate stages between  the triples and the target text, we use the augmented version of the WebNLG corpus <ref type="bibr" target="#b4">(Castro Ferreira et al., 2018b)</ref>, which provides gold-standard representations for traditional pipeline steps, such as discourse ordering (i.e., the order in which the source triples are verbalized in the target text), text structuring (i.e., the organization of the triples into paragraph and sentences), lexicalization (i.e., verbalization of the predicates) and referring expression generation (i.e., verbalization of the entities).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pipeline Architecture</head><p>Based on <ref type="bibr" target="#b27">Reiter and Dale (2000)</ref>, we propose a pipeline architecture which converts a set of RDF triples into text in 5 sequential steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discourse Ordering</head><p>Originally designed to be performed when the document is planned, Discourse Ordering is the process of determining the order in which the communicative goals should be verbalized in the target text. In our case, the communicative goals are the RDF triples received as input by the model. Given a set of linearized triples, this step determines the order in which they should be verbalized. For example, given the triple set in <ref type="figure" target="#fig_0">Figure 1</ref> in the linearized format: Our discourse ordering model would ideally return the set club club manager, which later is used to retrieve the input triples on the predicted order. In case of triples with the same predicates, as club, our implementation will randomly retrieve the triples.</p><formula xml:id="formula_0">&lt;TRIPLE&gt; A.C.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Structuring</head><p>Text Structuring is the step which intends to organize the ordered triples into paragraphs and sentences. Since the WebNLG corpus only contains single-paragraph texts, this step will be only evaluated on sentence planning, being closer to the Aggregation task of the original architecture <ref type="bibr" target="#b27">(Reiter and Dale, 2000)</ref>. However, it can be easily extended to predict paragraph structuring in multiparagraph datasets.</p><p>Given a linearized set of ordered triples, this step works by generating the predicates segmented by sentences based on the tokens &lt;SNT&gt; and &lt;/SNT&gt;. For example, given the ordered triple set in <ref type="figure" target="#fig_0">Figure 1</ref> in the same linearized format as in Discourse Ordering, the module would generate &lt;SNT&gt; club club &lt;/SNT&gt; &lt;SNT&gt; manager &lt;/SNT&gt;, where predicates are replaced by the proper triples for the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexicalization</head><p>Lexicalization involves finding the proper phrases and words to express the content to be included in each sentence <ref type="bibr" target="#b27">(Reiter and Dale, 2000)</ref>. In this study, given a linearized ordered set of triples segmented by sentences, the Lexicalization step aims to predict a template which verbalizes the predicates of the triples. For our example based on <ref type="figure" target="#fig_0">Figure 1</ref>, given the ordered triple set segmented by sentences in the following format: The template format not only selects the proper phrases and words to verbalize the predicates, but also does indications for the further steps. The general tags ENTITY-[0-9] indicates where references should be realized. The number in an entity tag indicates the entity to be realized based on its occurrence in the ordered triple set. For instance, ENTITY-3 refers to the entity Calcio Catania, the third mentioned entity in the ordered triple set.</p><p>Information for the further textual realization step is stored in the tags VP, which contains the aspect, mood, tense, voice and number of the subsequent lemmatized verb, and DT, which depicts the form of the subsequent lemmatized determiner. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Referring Expression Generation</head><p>Referring Expression Generation (REG) is the pipeline task responsible for generating the references to the entities of the discourse <ref type="bibr" target="#b15">(Krahmer and van Deemter, 2012)</ref>. As previously explained, the template created in the previous step depicts where and to which entities such references should be generated. Given our example based in <ref type="figure" target="#fig_0">Figure 1</ref>, the result of the REG step for the template predicted in the previous step would be: To perform the task, we used the Neu-ralREG algorithm <ref type="bibr" target="#b3">(Castro Ferreira et al., 2018a)</ref>. Given a reference to be realized, this algorithm works by encoding the template before (pre-context) and after (post-contex) the reference using two different Bidirectional LSTMs <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref>. Attention vectors are then computed for both vectors and concatenated together with the embedding of the entity. Finally, this representation is decoded into the referring expression to the proper entity in the given context. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Textual Realization</head><p>Textual Realization aims to perform the last steps of converting the non-linguistic data into text.</p><p>In our pipeline architecture this includes setting the verbs (e.g., VP[aspect=simple, tense=past, voice=passive, person=3rd, num-ber=singular] locate → was located) and determiners (DT[form=undefined] a American national → an American national) to their right formats. Both verbs and determiners are solved in a rule-based strategy, where the implications are extracted from the training set. This step will not be individually evaluated as the other ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">End-to-End Architecture</head><p>Our end-to-end architecture is similar to novel data-to-text models <ref type="bibr" target="#b32">(Wen et al., 2015;</ref><ref type="bibr" target="#b6">Dušek and Jurcicek, 2016;</ref><ref type="bibr" target="#b20">Mei et al., 2016;</ref><ref type="bibr" target="#b17">Lebret et al., 2016;</ref><ref type="bibr" target="#b12">Gehrmann et al., 2018)</ref>, which aim to convert a non-linguistic input into natural language without explicit intermediate representations, making use of Neural Machine Translation techniques. In this study, our end-to-end architecture intends to directly convert an unordered (linearized) set of RDF triples into text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Models Set-Up</head><p>Both pipeline steps and the end-to-end architecture were modelled using two deep learning encoder-decoder approaches: Gated-Recurrent Units (GRU; <ref type="bibr" target="#b5">Cho et al. 2014)</ref> and Transformer <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. These models differ in the way they encode their input. GRUs encode the input data by going over the tokens one-by-one, while Transformers (which do not have a recurrent structure) may encode the entire source sequence as a whole, using position embeddings to keep track of the order. We are particularly interested in the capacity of such approaches to learn order and structure in the process of text generation. The model settings are explained in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment 1: Learning the pipeline</head><p>Most of the data-to-text pipeline applications have their steps implemented using rule-based or statistical data-driven models. However, these techniques have shown to be outperformed by deep neural networks in other Computational Linguistics subfields and in particular pipeline steps like Referring Expression Generation. NeuralREG <ref type="bibr" target="#b3">(Castro Ferreira et al., 2018a)</ref>, for instance, outperforms other techniques in generating references and co-references along a single-paragraph text.</p><p>Given this context, our first experiment intends to analyze how well deep learning methods perform particular steps of the pipeline architecture, like Discourse Ordering, Text Structuring, Lexicalization and Referring Expression Generation, in comparison with simpler data-driven baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data</head><p>We used version 1.5 of the augmented WebNLG corpus <ref type="bibr">(Castro Ferreira et al., 2018b) 4</ref> to evaluate the steps of our pipeline approach. Based on its intermediate representations, we extracted goldstandards to train and evaluate the different steps.</p><p>Discourse Ordering We used pairs of RDF triple sets and their ordered versions to evaluate our Discourse Ordering approaches. For the cases in the training set where a triple set was verbalized in more than one order, we added one entry per verbalization taking the proper order as the target. To make sure the source set followed a pattern, we ordered the input according to the alphabetic order of its predicates, followed by the alphabetical order of its subjects and objects in case of similar predicates. In total, our Discourse Ordering data consists of 13,757, 1,730 and 3,839 ordered triple sets for 5,152, 644 and 1,408 training, development and test input triple sets, respectively.</p><p>Text Structuring 14,010, 1,752 and 3,955 structured triple sets were extracted for 10,281, 1,278 and 2,774 training, development and test ordered triple sets, respectively.</p><p>Lexicalization 18,295, 2,288 and 5,012 lexicalization templates were used for 12,814, 1,601 and 3,463 training, development and test structured triple sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression Generation</head><p>To evaluate the performance of the REG models, we extracted 67,144, 8,294 and 19,210 reference instances from training, development and test part of the corpus. Each instance consists of the cased tokenized referring expression, the identifier of the target entity and the uncased tokenized pre-and post-contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Metrics</head><p>Discourse Ordering and Text Structuring approaches were evaluated based on their accuracy to predict one of the gold-standards given the input (many of the RDF triple sets in the corpus were verbalized in more than one order and structure). Referring Expression Generation approaches were also evaluated based on their accuracy to predict the uncased tokenized gold-standard referring expressions. Lexicalization was evaluated based on the BLEU score of the predicted templates in their uncased tokenized form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baselines</head><p>We proposed random and majority baselines for the steps of Discourse Ordering, Text Structuring and Lexicalization. In comparison with Neural-REG, we used the OnlyNames baseline, also introduced in Castro <ref type="bibr" target="#b3">Ferreira et al. (2018a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discourse Ordering</head><p>The random baseline returns the triple set in a random order, whereas the majority one returns the most frequent order of the input predicates in the training set. For unseen sets of predicates, the majority model returns the triple set in the same order as the input.</p><p>Text Structuring The random baseline for this step chooses a random split of triples in sentences, inserting the tags &lt;SNT&gt; and &lt;/SNT&gt; in aleatory positions among them. The majority baseline returns the most frequent sentence intervals in the training set based on the input predicates. In case of an unseen set, the model looks for sentence intervals in subsets of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Lexicalization Pseudocode</head><p>Require: struct, model 1: start, end ← 0, Lexicalization Algorithm 1 depicts our baseline approach for Lexicalization. As in Text Structuring, given a set of input triples structured in sentences, the random and majority models return a random and the most frequent template that describes the input predicates, respectively (line 6). If the set of predicates is unseen, the model returns a template that describes a subset of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression</head><p>Generation We used OnlyNames, a baseline introduced in Castro Ferreira et al. (2018a), in contrast to NeuralREG. Given an entity to be referred to, this model returns the entity Wikipedia identifier with underscores replaced by spaces (Massimo Drago → Massimo Drago). <ref type="table">Table 1</ref> shows the results for our models for each of the 4 evaluated pipeline steps. In general, the results show a clear pattern in all of these steps: both neural models (GRU and Transformer) introduced higher results on domains seen during training, but their performance drops substantially on unseen domains in comparison with the baselines (Random and Majority). The only exception is found in Text Structuring, where the neural models outperforms the Majority baseline on unseen domains, but are still worse than the Random baseline. Between both neural models, recurrent networks seem to have an advantage over the Transformer in Discourse Ordering and Text Structuring, whereas the latter approach performs better than the former one in Lexicalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head><p>In this experiment, we contrast our pipeline with our end-to-end implementation and state-of-theart models for RDF-to-text. The models were evaluated in automatic and human evaluations, followed by a qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Approaches</head><p>Pipeline We evaluated 4 implementations of our pipeline architecture, where the output of the previous step is fed into the next one. We call these implementations Random, Majority, GRU and Transformer, where each one has its steps solved by one of the proposed baselines or deep learning implementations. In Random and Majority, the referring expressions were generated by the OnlyNames baseline, whereas for GRU and Transformer, NeuralREG was used for the seen entities, OnlyNames for the unseen ones and special rules to realize dates and numbers.</p><p>End-to-End We aimed to convert a set of RDFtriples into text using a GRU and a Transformer implementation without explicit intermediate representations in-between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Models for Comparison</head><p>To ground this study with related work, we compared the performance of the proposed approaches with 4 state-of-the-art RDF-to-text models.</p><p>Melbourne is the approach which obtained the highest performance in the automatic evaluation of the WebNLG Challenge. The approach consists of a neural encoder-decoder approach, which encodes a linearized triple set, with predicates split on camel case (e.g. floorArea → floor area) and entities represented by general (e.g., ENTITY-1) and named entity recognition (e.g., PERSON) tags, into a template where references are also represented with general tags. The referring expressions are later generated in the template simply by replacing these general tags with an approach similar to OnlyNames.</p><p>UPF-FORGe obtained the highest ratings in the human evaluation of the WebNLG challenge, having a performance similar to texts produced by humans. It also follows a pipeline architecture, which maps predicate-argument structures onto sentences by applying a series of rule-based graphtransducers <ref type="bibr" target="#b22">(Mille et al., 2019)</ref>. <ref type="bibr" target="#b19">Marcheggiani and Perez (2018)</ref> proposed a graph convolutional network that directly encodes the input triple set in contrast with previous model that first linearize the input to then decode it into text. <ref type="bibr" target="#b23">Moryossef et al. (2019)</ref> proposed an approach which converts an RDF triple set into text in two steps: text planning, a non-neural method where the input will be ordered and structured, followed by a neural realization step, where the ordered and structured input is converted into text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Evaluation</head><p>Automatic Evaluation We evaluated the textual outputs of each system using the BLEU <ref type="bibr" target="#b25">(Papineni et al., 2002)</ref> and METEOR <ref type="bibr" target="#b16">(Lavie and Agarwal, 2007)</ref> metrics. The evaluation was done on the entire test data, as well as only in their seen and unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head><p>We conducted a human evaluation, selecting the same 223 samples used in the evaluation of the WebNLG challenge <ref type="bibr" target="#b10">(Gardent et al., 2017b)</ref>. For each sample, we used the original texts and the ones generated by our 6 approaches and by the Melbourne and UPF-FORGe reference systems, totaling 2,007 trials. Each trial displayed the triple set and the respective text. The goal of the participants was to rate the trials based on the fluency (i.e., does the text flow in a natural, easy to read manner?) and semantics (i.e., does the text clearly express the data?) of the text in a 1-7 Likert scale.</p><p>We recruited 35 raters from Mechanical Turk to participate in the experiment. We first familiarized them with the set-up of the experiment, depicting a trial example in the introduction page accompanied by an explanation. Then each participant had to rate 60 trials, randomly chosen by the system, making sure that each trial was rated at least once. 5</p><p>Qualitative Analysis To have a better understanding of the positive and negative aspects of each model, we also performed a qualitative analysis, where the second and third authors of this study analyzed the original texts and the ones generated by the previous 8 models for 75 trials ex-   tracted from the human evaluation sample for each combination between size and domain of the corpus. The trials were displayed in a similar way to the human evaluation, where the annotators did not know which model produced the text. The only difference was the additional display of the predicted structure by the pipeline approaches (a fake structure was displayed for the other models). Both annotators analyzed grammaticality aspects, like whether the texts had mistakes involving the determiners, verbs and references, and semantic ones, like whether the text followed the predicted order and structure, and whether it verbalizes less or more information than the input triples. 6 <ref type="table" target="#tab_7">Table 2</ref> depicts the results of automatic and human evaluations, whereas <ref type="table" target="#tab_8">Table 3</ref> shows the results of the qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Results</head><p>Automatic Evaluation In terms of BLEU, our neural pipeline models (GRU and Transformer) outperformed all the reference approaches in all domains, whereas our end-to-end GRU and Random pipeline obtained the best results on seen and unseen domains, respectively. Regarding METEOR, which includes synonymy matching to score the inputs, reference methods introduced the best scores in all domains. In seen and unseen domains, our neural GRU pipeline and reference approach UPF-FORGe obtained the best results, respectively.</p><p>Human Evaluation In all domains, neural GRU pipeline and UPF-FORGe were rated the highest in fluency by the participants of the evaluation. In seen ones, both our neural pipeline approaches (GRU and Transformer) were rated the best, whereas UPF-FORGe was considered the most fluent approach in unseen domains.</p><p>UPF-FORGe was also rated the most semantic approach in all domains, followed by neural GRU and Majority pipeline approaches. For seen domains, similar to the fluency ratings, both our neural pipeline approaches were rated the highest, whereas UPF-FORGe was considered the most semantic approach in unseen domains.</p><p>Qualitative Analysis In general, UPF-FORGe emerges as the system which follows the input the best: 91% of the evaluated trials verbalized the input triples. Moreover, the annotators did not find any grammatical mistakes in the output of this approach.</p><p>When focusing on the neural pipeline approaches, we found that in all the steps up to Text Structuring, the recurrent networks retained more information than the Transformer. However, 68% of the Transformer's text trials contained all the input triples, against 67% of the GRU's trials. As in Experiment 1, we see that recurrent networks as GRUs are better in ordering and structuring the discourse, but is outperformed by the Transformer in the Lexicalization step. In terms of fluency, we did not see a substantial difference between both kinds of approaches.</p><p>Regarding our end-to-end trials, different from the pipeline ones, less than a half verbalized all the input triples. Moreover, the end-to-end outputs also constantly contained more information than there were in the non-linguistic input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>This study introduced a systematic comparison between pipeline and end-to-end architectures for data-to-text generation, exploring the role of deep neural networks in the process. In this section we answer the two introduced research questions and additional topics based on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How well do deep learning methods perform as individual modules in a data-to-text pipeline?</head><p>In comparison with Random and Majority baselines, we observed that our deep learning implementations registered a higher performance in the pipeline steps on domains seen during training, but their performance dropped considerably on unseen domains, being lower than the baselines.</p><p>In the comparison between our GRU and Transformer, the former seems to be better at ordering and structuring the non-linguistic input, whereas the latter performs better in verbalizing an ordered and structured set of triples. The advantage of GRUs over the Transformer in Discourse Ordering and Text Structuring may be its capacity to implicitly take order information into account. On the other hand, the Transformer could have had difficulties caused by the task's design, where triples and sentences were segmented by tags (e.g. &lt;TRIPLE&gt; and &lt;SNT&gt;), rather than positional embeddings, which suits this model better. In sum, more research needs to be done to set this point.</p><p>How well does a neural pipeline architecture perform compared to a neural end-to-end one? Our neural pipeline approaches were superior to the end-to-end ones in most tested circumstances: the former generates more fluent texts which better describes data on all domains of the corpus. The difference is most noticeable for unseen domains, where the performance of end-to-end approaches drops considerably. This shows that endto-end approaches do not generalize as well as the pipeline ones. In the qualitative analysis, we also found that end-to-end generated texts have the problem of describing non-linguistic representations which are not present in the input, also known as Hallucination <ref type="bibr" target="#b28">(Rohrbach et al., 2018)</ref>.</p><p>The example in <ref type="figure" target="#fig_1">Figure 2</ref> shows the advantage of our pipeline approaches in comparison with the end-to-end ones. It depicts the texts produced by the proposed approaches for an unseen set during training of 4 triples, where 2 out of the 4 predicates are present in the WebNLG training set (e.g., birthPlace and occupation). In this context, the pipeline approaches managed to generate a semantic text based on the two seen predicates, whereas the end-to-end approaches hallucinated texts which have no semantic relation with  the non-linguistic input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>We compared the proposed approaches with 4 state-of-the-art RDF-to-text systems. Except for <ref type="bibr" target="#b19">Marcheggiani and Perez (2018)</ref>, all the others are not end-to-end approaches, already directing the field to pipeline architectures. UPF-FORGe is a proper pipeline system with several sequential steps, Melbourne first generates a delexicalized template to later realize the referring expressions, and <ref type="bibr" target="#b23">Moryossef et al. (2019)</ref> splits the process up into Planning, where ordering and structuring are merged, and Realization. Besides the approach of Marcheggiani and Perez (2018), the ADAPT system, introduced in the WebNLG challenge <ref type="bibr" target="#b10">(Gardent et al., 2017b)</ref>, is another full end-to-end approach to the task. It obtained the highest results in the seen part of the WebNLG corpus (BLEU = 60.59; METEOR = 0.44). However, the results drastically dropped on the unseen part of the dataset (BLEU = 10.53; METEOR = 0.19). Such results correlate with our findings showing the difficult of end-to-end approaches to generalize to new domains.</p><p>By obtaining the best results in almost all the evaluated metrics, UPF-FORGe emerges as the best reference system, showing again the advantage of generating text from non-linguistic data in several explicit intermediate representations.</p><p>However, it is important to observe that the advantage of UPF-FORGe over our pipeline approaches is the fact that it was designed taking the seen and unseen domains of the corpus into account. So in practice, there was no "unseen" domains for UPF-FORGe. In a fair comparison between this reference system with our neural pipeline approaches in only seen domains, we may see that ours are rated higher in almost all the evaluated metrics.</p><p>General Applicability Besides our findings, the results for other benchmarks, such as <ref type="bibr" target="#b7">Elder et al. (2019)</ref>, suggest that pipeline approaches can work well in the context of neural data-to-text generation. Concerning our pipeline approach specifically, although it was designed to convert RDF triples to text, we assume it can be adapted to other domains (and languages) where communicative goals can be linearized and split in units, as in the E2E dataset <ref type="bibr" target="#b24">(Novikova et al., 2017)</ref>. In future work, we plan to study this in more detail.</p><p>Conclusion In a systematic comparison, we show that adding supervision during the data-totext process leads to more fluent text that better describes the non-linguistic input data than full end-to-end approaches, confirming the trends in related work in favor of pipeline architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of a set of triples (top) and the corresponding text (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example of a set of triples from an unseen domain during training (top) and the corresponding texts produced by our pipeline (e.g., GRU and Transformer) and end-to-end approaches (e.g., E2E GRU and E2E Trans.) (bottom). In the top set of triples, predicates seen during training are highlighted in italic, whereas the unseen ones are underlined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Massimo Drago played for the club SSD Potenza Calcio and his own club was Calcio Catania. He is currently managing AC Cesena.</figDesc><table><row><cell>A.C. Cesena</cell><cell>manager</cell><cell>Massimo Drago</cell></row><row><cell>Massimo Drago</cell><cell>club</cell><cell>S.S.D. Potenza Calcio</cell></row><row><cell>Massimo Drago</cell><cell>club</cell><cell>Calcio Catania</cell></row><row><cell></cell><cell>↓</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Massimo Drago VP[...] play for DT[...] the club SSD Potenza Calcio and his own club VP[...] be Calcio Catania . He VP[...] be currently VP[...] manage AC Cesena .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>BC 5.38 BC 4.32 CD 4.70 CD 5.15 BCD 3.81 C Melbourne 5.04 CD 5.23 BC 4.65 CD 4.94 C</figDesc><table><row><cell></cell><cell>All</cell><cell>Seen</cell><cell cols="2">Unseen All</cell><cell>Seen</cell><cell>Unseen</cell></row><row><cell></cell><cell></cell><cell>BLEU</cell><cell></cell><cell></cell><cell>METEOR</cell><cell></cell></row><row><cell>Random</cell><cell>41.68</cell><cell>41.72</cell><cell>41.51</cell><cell>0.20</cell><cell>0.27</cell><cell>-</cell></row><row><cell>Majority</cell><cell>43.82</cell><cell>44.79</cell><cell>41.13</cell><cell>0.33</cell><cell>0.41</cell><cell>0.22</cell></row><row><cell>GRU</cell><cell>50.55</cell><cell>55.75</cell><cell>38.55</cell><cell>0.33</cell><cell>0.42</cell><cell>0.22</cell></row><row><cell>Transformer</cell><cell>51.68</cell><cell>56.35</cell><cell>38.92</cell><cell>0.32</cell><cell>0.41</cell><cell>0.21</cell></row><row><cell>E2E GRU</cell><cell>33.49</cell><cell>57.20</cell><cell>6.25</cell><cell>0.25</cell><cell>0.41</cell><cell>0.09</cell></row><row><cell>E2E Transformer</cell><cell>31.88</cell><cell>50.79</cell><cell>5.88</cell><cell>0.25</cell><cell>0.39</cell><cell>0.09</cell></row><row><cell>Melbourne</cell><cell>45.13</cell><cell>54.52</cell><cell>33.27</cell><cell>0.37</cell><cell>0.41</cell><cell>0.33</cell></row><row><cell>UPF-FORGe</cell><cell>38.65</cell><cell>40.88</cell><cell>35.70</cell><cell>0.39</cell><cell>0.40</cell><cell>0.37</cell></row><row><cell cols="2">(Marcheggiani and Perez, 2018) -</cell><cell>55.90</cell><cell>-</cell><cell>-</cell><cell>0.39</cell><cell>-</cell></row><row><cell>(Moryossef et al., 2019)</cell><cell>47.40</cell><cell>-</cell><cell>-</cell><cell>0.39</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>Fluency</cell><cell></cell><cell></cell><cell>Semantic</cell><cell></cell></row><row><cell>Random</cell><cell>4.55 E</cell><cell>4.79 D</cell><cell>4.07 D</cell><cell>4.44 D</cell><cell>4.73 D</cell><cell>3.86 C</cell></row><row><cell>Majority</cell><cell cols="5">5.00 CD 5.25 CD 4.49 CD 5.02 BC 5.41 BC</cell><cell>4.25 BC</cell></row><row><cell>GRU</cell><cell>5.31 B</cell><cell cols="4">5.51 AB 4.91 BC 5.21 BC 5.48 AB</cell><cell>4.67 B</cell></row><row><cell>Transformer</cell><cell cols="3">5.03 BC 5.53 AB 4.05 D</cell><cell>4.87 C</cell><cell>5.49 AB</cell><cell>3.64 C</cell></row><row><cell>E2E GRU</cell><cell cols="3">4.73 DE 5.40 BC 3.45 E</cell><cell>4.47 D</cell><cell>5.21 CD</cell><cell>3.03 D</cell></row><row><cell>E2E Transformer</cell><cell cols="5">5.02 5.33 BC</cell><cell>4.15 C</cell></row><row><cell>UPF-FORGe</cell><cell>5.46 B</cell><cell cols="3">5.43 BC 5.51 AB 5.31 B</cell><cell>5.35 BC</cell><cell>5.24 A</cell></row><row><cell>Original</cell><cell>5.76 A</cell><cell>5.82 A</cell><cell>5.63 A</cell><cell>5.74 A</cell><cell>5.80 A</cell><cell>5.63 A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>(1) BLEU and METEOR scores of the models in the automatic evaluation, and (2) Fluency and Semantic obtained in the human evaluation. In the first part, best results are boldfaced and second best ones are underlined. In the second part, ranking was determined by pair-wise Mann-Whitney statistical tests with p &lt; 0.05.</figDesc><table><row><cell></cell><cell></cell><cell>Semantic</cell><cell></cell></row><row><cell></cell><cell cols="3">Ord. Struct. Txt. Ovr. Keep.</cell></row><row><cell>Random</cell><cell>1.00</cell><cell>1.00 0.43 0.05</cell><cell>0.41</cell></row><row><cell>Majority</cell><cell>1.00</cell><cell>1.00 0.75 0.01</cell><cell>0.69</cell></row><row><cell>GRU</cell><cell>0.77</cell><cell>0.73 0.67 0.01</cell><cell>0.81</cell></row><row><cell>Transformer</cell><cell>0.75</cell><cell>0.69 0.68 0.08</cell><cell>0.80</cell></row><row><cell>E2E GRU</cell><cell>-</cell><cell>-0.47 0.41</cell><cell>-</cell></row><row><cell>E2E Trans.</cell><cell>-</cell><cell>-0.39 0.53</cell><cell>-</cell></row><row><cell>Melbourne</cell><cell>-</cell><cell>-0.73 0.19</cell><cell>-</cell></row><row><cell>UPF-FORGe</cell><cell>-</cell><cell>-0.91 0.00</cell><cell>-</cell></row><row><cell>Original</cell><cell>-</cell><cell>-0.99 0.12</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Grammaticality</cell><cell></cell></row><row><cell></cell><cell>Verb</cell><cell>Det. Reference</cell><cell></cell></row><row><cell>Random</cell><cell>0.95</cell><cell>0.91 0.89</cell><cell></cell></row><row><cell>Majority</cell><cell>1.00</cell><cell>1.00 0.99</cell><cell></cell></row><row><cell>GRU</cell><cell>1.00</cell><cell>0.99 0.80</cell><cell></cell></row><row><cell>Transformer</cell><cell>0.95</cell><cell>1.00 0.93</cell><cell></cell></row><row><cell>E2E GRU</cell><cell>0.97</cell><cell>1.00 0.91</cell><cell></cell></row><row><cell>E2E Trans.</cell><cell>0.95</cell><cell>0.97 0.79</cell><cell></cell></row><row><cell>Melbourne</cell><cell>0.96</cell><cell>0.87 0.77</cell><cell></cell></row><row><cell>UPF-FORGe</cell><cell>1.00</cell><cell>1.00 1.00</cell><cell></cell></row><row><cell>Original</cell><cell>0.95</cell><cell>0.95 0.92</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Qualitative analysis. The first part shows the percentage of trials that keeps the input predicates over Discourse Ordering (Ord.), Text Structuring (Struct.) and in the final text (Txt.). It also shows the ratio of text trials with more predicates than in the input (Ovr.) and the pipeline texts which keep the decisions of previous steps (Keep.). The second part shows the number of trials without verb, determiner and reference mistakes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>who was born in Willington, who was born in New York, was born in New York and is competing in the competing in the U.S.A. The construction of the city is produced in Mandesh.</figDesc><table><row><cell></cell><cell cols="3">Ace Wilder background "solo singer"</cell></row><row><cell></cell><cell>Ace Wilder</cell><cell>birthPlace</cell><cell>Sweden</cell></row><row><cell></cell><cell>Ace Wilder</cell><cell>birthYear</cell><cell>1982</cell></row><row><cell></cell><cell>Ace Wilder</cell><cell>occupation</cell><cell>Songwriter</cell></row><row><cell></cell><cell></cell><cell>↓</cell></row><row><cell>GRU</cell><cell cols="3">Ace Wilder, born in Sweden, performs as Songwriter.</cell></row><row><cell>Transformer</cell><cell cols="3">Ace Wilder (born in Sweden) was Songwriter.</cell></row><row><cell cols="4">E2E GRU The test pilot E2E Trans. Test pilot Elliot See was born in Dallas and died in St. Louis.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Both kind of tags with their respective information are treated as a single token.3 NeuralREG works with the Wikipedia representation of the entities (e.g., Massimo Drago) in the templates instead of general tags (e.g., ENTITY-1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/ThiagoCF05/webnlg</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The raters had an average age of 32.29 and 40% were female. 17 participants indicated they were fluent in English, while 18 were native. The experiment, which obtained ethical clearance from the university, took around 20-30 minutes to be completed and each rater received a pay in U.S. dollars for participation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Inter-annotator agreement for the evaluated aspects ranged from 0.26 (Reference) to 0.93 (Input Triples), with an average Krippendorff α of 0.67.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is part of the research program "Discussion Thread Summarization for Mobile Devices" (DISCOSUMO) which is financed by the Netherlands Organization for Scientific Research (NWO). We also acknowledge the three reviewers for their insightful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Models Set-Up General Settings We used the implementation of Nematus <ref type="bibr" target="#b29">(Sennrich et al., 2017)</ref> for both models. We trained each architecture (i.e., GRU and Transformer) three times. For testing, we ensembled the settings which obtained the best results in the development sets in each training execution for GRUs, whereas for the Transformer, we selected the setting which obtained the best result in the respective development set.</p><p>Models were trained using stochastic gradient descent with Adam (Kingma and Ba, 2015) (β 1 = 0.9, β 2 = 0.98, ǫ = 10 −9 ) for a maximum of 200,000 updates. They were evaluated on the development sets after every 5,000 updates and early stopping was applied with patience 30 based on cross-entropy. Encoder, decoder and softmax embeddings were tied, whereas decoding was performed with beam search of size 5 to predict sequences with length up to 100 tokens.</p><p>GRU Settings Bidirectional GRUs with attention were used as described in <ref type="bibr" target="#b29">Sennrich et al. (2017)</ref>. Source and target word embeddings were 300D each, whereas hidden units were 512D. We applied layer normalization as well as dropout with a probability of 0.1 in both source and target word embeddings and 0.2 for hidden units.</p><p>Transformer Settings Both encoder and decoder consisted of N = 6 identical layers. Word embeddings and hidden units were 512D each, whereas the inner dimension of feed-forward sublayers were 2048D. The multi-head attention sublayers consisted of 8 heads each. Dropout of 0.1 were applied to the sums of word embeddings and positional encodings, to residual connections, to the feed-forward sub-layers and to attention weights. At training, models had 8000 warm-up steps and label smoothing of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Segmentation</head><p>In the lexicalization step of the pipeline and in the end-to-end architecture, byte-pair encoding (BPE) <ref type="bibr" target="#b30">(Sennrich et al., 2016)</ref> was used to segment the tokens of the target template and text, respectively. The model was trained to learn 20,000 merge operations with a threshold of 50 occurrences.</p><p>NeuralREG To generate referring expressions in the pipeline architecture, we used the concatenative-attention version of the NeuralREG algorithm <ref type="bibr" target="#b3">(Castro Ferreira et al., 2018a)</ref>. We follow most of the settings in the original paper, except for the number of training epochs, minibatches, dropout, beam search and early stop of the neural networks, which we respectively set to 60, 80, 0.2, 5 and 10. Another difference is in the input of the model: while NeuralREG in the original paper generates referring expressions based on templates where only the references are delexicalized, here the algorithm generates referring expressions based on a template where verbs and determiners are also delexicalized as previously explained.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing and testing the messages produced by a virtual dietitian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Anselma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Mazzei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The first surface realisation shared task: Overview and evaluation results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation<address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Saferdrive: An NLG-based behaviour change support system for drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="588" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NeuralREG: An end-to-end approach to referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Thiago Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ákos</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1959" to="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching the WebNLG corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Thiago Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wubben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="171" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence generation for spoken dialogue via deep syntax trees and strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurcicek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Designing a symbolic intermediate representation for neural surface realization</title>
		<idno type="DOI">10.18653/v1/W19-2308</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</title>
		<meeting>the Workshop on Methods for Optimizing and Evaluating Neural Language Generation<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="65" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Creating training corpora for NLG micro-planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL&apos;17</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The WebNLG challenge: Generating text from RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation, INLG&apos;17</title>
		<meeting>the 10th International Conference on Natural Language Generation, INLG&apos;17<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end content and plan selection for data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falcon</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computational generation of referring expressions: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kees Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="218" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for MT evaluation with high levels of co</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation, StatMT&apos;07</title>
		<meeting>the Second Workshop on Statistical Machine Translation, StatMT&apos;07<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the bio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;16</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;16<address><addrLine>Austin, Texas; Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
	<note>Proceedings of the 10th International Conference on Natural Language Generation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Data-driven news generation for automated journalism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Leppänen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Munezero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Granroth-Wilding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Toivonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3528</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep graph convolutional encoders for structured data to text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What to talk about and how? selective generation using LSTMs with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1086</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, HLT-NAACL&apos;16</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, HLT-NAACL&apos;16<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
	<note>California</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The first multilingual surface realisation shared task (SR&apos;18): Overview and evaluation results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-3601</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Multilingual Surface Realisation</title>
		<meeting>the First Workshop on Multilingual Surface Realisation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A portable grammar-based NLG system for verbalization of structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatia</forename><surname>Dasiopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing</title>
		<meeting>the 34th ACM/SIGAPP Symposium on Applied Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1054" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Step-by-step: Separating planning from realization in neural data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2267" to="2277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The E2E dataset: New challenges for end-to-end generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, ACL&apos;02</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics, ACL&apos;02<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic generation of textual summaries from neonatal intensive care data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Portet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayajulu</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Freer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cindy</forename><surname>Sykes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">78</biblScope>
			<biblScope unit="page" from="789" to="816" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object hallucination in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4035" to="4045" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadejde</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1086</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL&apos;16</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1236</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation fo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;15</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP&apos;15<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
