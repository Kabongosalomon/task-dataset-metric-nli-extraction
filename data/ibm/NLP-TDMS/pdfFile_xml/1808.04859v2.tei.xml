<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GestureGAN for Hand Gesture-to-Gesture Translation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-10-22">2018. October 22-26. 2018. October 22-26. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<email>wei.wang@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
							<email>danxu@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Texas State University</orgName>
								<address>
									<settlement>San Marcos</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<email>niculae.sebe@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GestureGAN for Hand Gesture-to-Gesture Translation in the Wild</title>
					</analytic>
					<monogr>
						<title level="m">2018 ACM Multimedia Conference (MM &apos;18)</title>
						<meeting> <address><addrLine>Seoul, Republic of Korea</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2018-10-22">2018. October 22-26. 2018. October 22-26. 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3240508.3240704</idno>
					<note>Republic of Korea. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Computer vision</term>
					<term>Machine learning</term>
					<term>KEYWORDS Generative Adversarial Networks</term>
					<term>Image Translation</term>
					<term>Hand Gesture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hand gesture-to-gesture translation in the wild is a challenging task since hand gestures can have arbitrary poses, sizes, locations and self-occlusions. Therefore, this task requires a high-level understanding of the mapping between the input source gesture and the output target gesture. To tackle this problem, we propose a novel hand Gesture Generative Adversarial Network (Ges-tureGAN). GestureGAN consists of a single generator G and a discriminator D, which takes as input a conditional hand image and a target hand skeleton image. GestureGAN utilizes the hand skeleton information explicitly, and learns the gesture-to-gesture mapping through two novel losses, the color loss and the cycleconsistency loss. The proposed color loss handles the issue of "channel pollution" while back-propagating the gradients. In addition, we present the Fréchet ResNet Distance (FRD) to evaluate the quality of generated images. Extensive experiments on two widely used benchmark datasets demonstrate that the proposed Gesture-GAN achieves state-of-the-art performance on the unconstrained hand gesture-to-gesture translation task. Meanwhile, the generated images are in high-quality and are photo-realistic, allowing them to be used as data augmentation to improve the performance of a hand gesture classifier. Our model and code are available at https://github.com/Ha0Tang/GestureGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Hand gesture-to-gesture translation in the wild is a task that converts the hand gesture of a given image to a target gesture with a different pose, size and location while preserving the identity information. This task has many applications, such as human-computer interactions, entertainment, virtual reality and data augmentation. However, this task is difficult since it needs (i) handling complex backgrounds with different illumination conditions, objects and occlusions; (ii) a high-level semantic understanding of the mapping between the input and output gestures.</p><p>Recently, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">[7]</ref> have shown the potential to solve this challenging task. GAN is a generative model based on game theory, which has achieved impressive performance in many applications, such as high-quality image generation <ref type="bibr" target="#b11">[12]</ref>, video generation <ref type="bibr" target="#b55">[56]</ref> and audio generation <ref type="bibr" target="#b29">[30]</ref>. To generate specific kinds of images, videos and audios, Mirza et al. <ref type="bibr" target="#b27">[28]</ref> proposed the Conditional GAN (CGAN), which comprises a vanilla GAN and other external information, such as class labels <ref type="bibr" target="#b2">[3]</ref>, text descriptions <ref type="bibr" target="#b36">[37]</ref>, images <ref type="bibr" target="#b9">[10]</ref> and object keypoints <ref type="bibr" target="#b36">[37]</ref>.</p><p>In this paper, we focus on the image-to-image translation task using CGAN. Image-to-image translation tasks can be divided into two types: paired <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41]</ref> and unpaired <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b61">62]</ref>. However, existing image-to-image translation frameworks are inefficient in the multi-domain image-to-image translation task. For instance, given m image domains, pix2pix <ref type="bibr" target="#b9">[10]</ref> and BiCycleGAN <ref type="bibr" target="#b62">[63]</ref> need to train A 2 m =m(m−1)=Θ(m 2 ) models. CycleGAN <ref type="bibr" target="#b61">[62]</ref>, DiscoGAN <ref type="bibr" target="#b12">[13]</ref> and DualGAN <ref type="bibr" target="#b57">[58]</ref> need to train C 2 m = m(m−1) 2 =Θ(m 2 ) models, or m(m−1) generator/discriminator pairs since one model has 2 different generator/discriminator pairs for these methods. Combo-GAN <ref type="bibr" target="#b0">[1]</ref> requires m=Θ(m) models. StarGAN <ref type="bibr" target="#b2">[3]</ref> needs one model. However, for some specific image-to-image translation applications such as hand gesture-to-gesture translation, m could be arbitrary large since gestures in the wild can have arbitrary poses, sizes, appearances, locations and self-occlusions.</p><p>To address these limitations, several works have been proposed to generate images based on object keypoints. For instance, Reed et al. <ref type="bibr" target="#b35">[36]</ref> present an extension of Pixel Convolutional Neural Networks (PixelCNN) to generate images based on keypoints and text description. Siarohin et al. <ref type="bibr" target="#b40">[41]</ref> introduce a deformable Generative Adversarial Network for pose-based human image generation. Ma et al. <ref type="bibr" target="#b24">[25]</ref> propose a two-stage reconstruction pipeline that learns generates novel person images. However, the aforementioned methods always have the "channel pollution" problem that is frequently occurring in generative models such as PG <ref type="bibr" target="#b1">2</ref>  <ref type="bibr" target="#b23">[24]</ref> leading to blurred generated images. To solve this issue, in this paper, we propose a novel Generative Adversarial Network, i.e., GestureGAN which treats each channel independently. It allows generating high-quality hand gesture images with arbitrary poses, sizes and locations in the wild, and thus reducing the dependence on environment and preprocessing operations. GestureGAN only consists of one generator and one discriminator, taking a conditional hand gesture image and a target hand skeleton image as inputs. In addition, to better learn the mapping between inputs and outputs, we propose two novel losses, i.e., color loss and cycle-consistency loss. Note that the color loss can handle the problem of "channel pollution", making the generated images sharper and having higher quality. Furthermore, we propose the Fréchet ResNet Distance (FRD), which is a novel evaluation metric to evaluate the generated image of GANs. Extensive experiments on two public benchmark datasets demonstrate that GestureGAN can generate high-quality images with convincing details. Thus, these generated images can augment the training data and improve the performance of hand gesture classifiers.</p><p>Overall, the contributions of this paper are as follows: • We propose a novel Generative Adversarial Network, i.e., Ges-tureGAN, which can generate target hand gesture with arbitrary poses, sizes and locations in the wild. In addition, we present a novel color loss to learn the hand gesture-to-gesture mappings, handling the problem of "channel pollution". • We propose an efficient Fréchet ResNet Distance (FRD) metric to evaluate the similarity of the real and generated images, which is more consistent with human judgment. FRD measures the similarity between the real image and the generated image in a high-level semantic feature space. • Qualitative and quantitative results demonstrate the superiority of the proposed GestureGAN over the state-of-the-art models on the unconstrained hand gesture-to-gesture translation task. In addition, the generated hand gesture images are of high-quality and are photo-realistic, thus allowing them to be used to boost the performance of hand gesture classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Generative Adversarial Network (GAN) <ref type="bibr" target="#b6">[7]</ref> is an unsupervised learning method and has been proposed by Goodfellow et al. Recently, GAN has shown outstanding results in various applications, e.g., image generation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>, image editing <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>, video generation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b47">48]</ref>, texture synthesis <ref type="bibr" target="#b16">[17]</ref>, music generation <ref type="bibr" target="#b56">[57]</ref> and feature learning <ref type="bibr" target="#b53">[54]</ref>. Recent approaches employ the idea of GAN for conditional image generation, such as image-to-image translation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49]</ref>, text-to-image translation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b58">59]</ref>, image inpainting <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, image blending <ref type="bibr" target="#b51">[52]</ref>, image super-resolution <ref type="bibr" target="#b15">[16]</ref>, as well as the applications of other domains like semantic segmentation <ref type="bibr" target="#b22">[23]</ref>, object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b49">50]</ref>, human parsing <ref type="bibr" target="#b20">[21]</ref>, face aging <ref type="bibr" target="#b19">[20]</ref> and 3D vision <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">53]</ref>. The key point success of GANs in computer vision and graphics is the adversarial loss, which allows the model to generate images that are indistinguishable from real images, and this is exactly the goal that many computer vision and graphics tasks aim to optimize. Image-to-Image Translation frameworks use input-output data to learn a parametric mapping between inputs and outputs, e.g., Isola et al. <ref type="bibr" target="#b9">[10]</ref> build the pix2pix model, which uses a conditional GAN to learn a translation function from input to output image domains. Taigman et al. <ref type="bibr" target="#b45">[46]</ref> propose the Domain Transfer Network (DTN) which learns a generative function between one domain and another domain. Zhu et al. <ref type="bibr" target="#b61">[62]</ref> introduce the CycleGAN framework, which achieves unpaired image-to-image translation using the cycle-consistency loss. Moreover, Zhu et al. <ref type="bibr" target="#b62">[63]</ref> present the Bi-cycleGAN model based on CycleGAN <ref type="bibr" target="#b61">[62]</ref> and pix2pix <ref type="bibr" target="#b9">[10]</ref>, which targets multi-modal image-to-image translation. However, existing image-to-image translation models are inefficient and ineffective. For example, with m image domains, Cy-cleGAN <ref type="bibr" target="#b61">[62]</ref>, DiscoGAN <ref type="bibr" target="#b12">[13]</ref>, and DualGAN <ref type="bibr" target="#b57">[58]</ref> need to train 2C 2 m =m(m−1)=Θ(m 2 ) generators and discriminators, while pix2pix <ref type="bibr" target="#b9">[10]</ref> and BicycleGAN <ref type="bibr" target="#b62">[63]</ref> have to train A 2 m =m(m−1)=Θ(m 2 ) generator/discriminator pairs. Recently, Anoosheh et al. proposed Com-boGAN <ref type="bibr" target="#b0">[1]</ref>, which only need to train m generator/discriminator pairs for m different image domains, having a complexity of Θ(m). Additionally, Choi et al. <ref type="bibr" target="#b2">[3]</ref> propose StarGAN, in which a single generator and discriminator can perform unpaired image-to-image translations for multiple domains. Although the computational complexity of StarGAN is Θ(1), this model has only been validated on the face attributes modification task with clear background and face cropping. More importantly, for some specific image-to-image translation tasks such as hand gesture-to-gesture translation task, the image domains could be arbitrary large, e.g., gesture in the wild can have arbitrary poses, sizes, appearances, locations and self-occlusions. The aforementioned approaches are not effective for solving these specific situations. Keypoint/Skeleton Guided Image-to-Image Translation. To fix these limitations, several recent works have been proposed to generate person, bird or face images based on object keypoints <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref> or human skeleton <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b55">56]</ref>. For instance, Di et al. <ref type="bibr" target="#b4">[5]</ref> propose the Gender Preserving Generative Adversarial Network (GPGAN) to synthesize faces based on facial landmarks. Reed et al. <ref type="bibr" target="#b36">[37]</ref> propose the Generative Adversarial What-Where Network (GAWWN), which generates birds conditioned on both text descriptions and object location. Ma et al. propose the Pose Guided Person Generation Network (PG 2 ) <ref type="bibr" target="#b23">[24]</ref> and a two-stage reconstruction pipeline <ref type="bibr" target="#b24">[25]</ref>, which achieve person-to-person image translation using a conditional image and a target pose image (note that in these two models images are pre-cropped). Reed et al. <ref type="bibr" target="#b35">[36]</ref> present an extension of Pixel Convolutional Neural Networks (PixelCNN) to generate images parts based on keypoints and text description. Sun et al. <ref type="bibr" target="#b44">[45]</ref> propose a two-stage framework to perform head inpainting conditioned on the generated facial landmark in the first stage. Korshunova et al. <ref type="bibr" target="#b14">[15]</ref> use facial keypoints to define the affine transformations of the alignment and realignment steps for face swap. Wei et al. <ref type="bibr" target="#b50">[51]</ref> propose a Conditional MultiMode Network (CMM-Net) for landmark-guided smile generation. Qiao et al. <ref type="bibr" target="#b33">[34]</ref> present the Geometry-Contrastive Generative Adversarial Network (GCGAN) to generate facial expressions conditioned on geometry information of facial landmarks. Song et al. <ref type="bibr" target="#b43">[44]</ref> propose the Geometry-Guided Generative Adversarial Network (G2GAN) for facial expression synthesis guided by fiducial points. Yan et al. <ref type="bibr" target="#b55">[56]</ref> propose a method to generate human motion sequence with simple background using CGAN and human skeleton information. Siarohin et al. <ref type="bibr" target="#b40">[41]</ref> introduce PoseGAN for pose-based human image generation using human skeleton. The typical problem with the aforementioned generative models is that they suffer from "channels pollution" and thus they tend to generate blurry results with artifacts. To handle this problem, we propose GestureGAN, which allows generating high-quality hand gesture image with arbitrary poses, sizes and locations in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL DESCRIPTION 3.1 GestureGAN Objective</head><p>The goal of vanilla GAN is to train a generator G which learns the mapping between random noise z and image y. The mapping function of GAN, G(z) → y is learned via the following objective function,</p><formula xml:id="formula_0">L G AN (G, D) = E y [log D(y)] + E z [log(1−D(G(z)))] .<label>(1)</label></formula><p>Conditional GANs learn the mapping G(x, z) → y, where x is the input conditional image. Generator G is trained to generate image y that cannot be distinguished from "real" image y by an adversarially trained discriminator D, while the discriminator D is trained as well as possible to detect the "fake" images generated by the generator G.</p><p>The objective function of the conditional GAN is defined as follows,</p><formula xml:id="formula_1">L cG AN (G, D) = E x,y [log D(x, y)] + E x, z [log(1 − D(x, G(x, z)))] ,<label>(2)</label></formula><p>where generator G tries to minimize this objective while the discriminator D tries to maximize it. Thus, the solution is G * = arg min G max D L cGAN (G, D). In this paper, we try to learn two mappings through one generator. The framework of the proposed Ges-tureGAN is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Adversarial Loss. In order to learn the gesture-to-gesture mapping, we employ the hand skeleton information explicitly. We exploit OpenPose <ref type="bibr" target="#b41">[42]</ref> to detect 21 hand keypoints denoted as (p i , q i |i = 1, 2, · · · , 21), where p i and q i represent pixel coordinates of keypoints. For each keypoint (p i , q i ), c i ∈[0, 1] represents the confidence that the keypoint is correctly localized. Thus, the adversarial losses of the two mappings G([x, K y ], z 1 ) → y and G([y, K x ], z 2 ) → x are defined respectively, as follows:</p><formula xml:id="formula_2">L Ky (G, D, K y ) =E [x, Ky ],y log D([x, K y ], y) + E [x, Ky ], z 1 log(1 − D([x, K y ], G([x, K y ], z 1 ))) ,<label>(3)</label></formula><formula xml:id="formula_3">L Kx (G, D, K x ) =E [y, Kx ], x [log D([y, K x ], x )] + E [y, Kx ], z 2 [log(1 − D([y, K x ], G([y, K x ], z 2 )))] ,<label>(4)</label></formula><p>where K y and K x are the hand keypoints of image y and x respectively; [·, ·] represents the concatenate operation. K y and K x are defined by setting the pixels around the corresponding keypoint (p i , q i ) to 1 (white) with the radius of 4 and 0 (black) elsewhere. In other words, each keypoint is actually represented with pixels in a circle with a radius of 4. Therefore, the total adversarial loss based on hand keypoint can be defined as,</p><formula xml:id="formula_4">L K (G, D, K x , K y ) = L Ky (G, D, K y ) + L Kx (G, D, K x ).<label>(5)</label></formula><p>In addition, to explore the influence of the confidence score c i to the generated image, we define a confidence keypoint image K in which the pixels around the corresponding keypoint (p i , q i ) are set to c i in a radius of 4 pixels and 0 (black) elsewhere. Thus, Equation 5 can be expressed as</p><formula xml:id="formula_5">L K (G, D, K x , K y ) = L K y (G, D, K y ) + L K x (G, D, K x )</formula><p>. Moreover, following OpenPose <ref type="bibr" target="#b41">[42]</ref>, we connect the 21 keypoints (hand joints) to obtain the hand skeleton, denoted as S x and S y . The hand skeleton image visually contains richer hand structure information than the hand keypoint image. Next, the adversarial loss based on hand skeleton can be derived from Equation 5, i.e.,</p><formula xml:id="formula_6">L S (G, D, S x , S y ) = L S y (G, D, S y ) + L S x (G, D, S x ).</formula><p>In hand skeleton image S x and S y , the hand joints are connected by the lines with the width of 4 and with white color. Next, corresponding to the confidence keypoint image, we have also defined an adversarial loss using confidence hand skeleton as</p><formula xml:id="formula_7">L S (G, D, S x , S y ) = L S y (G, D, S y ) + L S x (G, D, S x ),</formula><p>where the line connections in S x and S y are filled with the confidence score of later point, e.g., if the hand skeleton connects points 1 and 2, thus this line connection is filled with the confidence of point 2, i.e., c 2 with the width of 4. Improved Pixel Loss. Previous work indicated that mixing the adversarial loss with a traditional loss such as L 1 loss <ref type="bibr" target="#b9">[10]</ref> or L 2 loss <ref type="bibr" target="#b31">[32]</ref> between the generated image and the ground truth image improves the quality of generated images. The definition of L 1 and L 2 losses are:</p><formula xml:id="formula_8">L L {1, 2} (G, S x , S y ) =E [x, Sy ],y, z 1 ∥y − G([x, S y ], z 1 ) ∥ {1, 2} + E [y, Sx ], x, z 2 ∥x − G([y, S x ], z 2 ) ∥ {1, 2} .<label>(6)</label></formula><p>However, we observe that the existing image-to-image translation models such as PG 2 <ref type="bibr" target="#b23">[24]</ref> cannot retain the holistic color of the input images. An example is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, where PG 2 is affected by the pollution issue and produces more unrealistic regions. Therefore, to remedy this limitation we introduce a novel channel-wise color loss. Traditional generative models convert a whole image to another, which leads to the "channel pollution" problem. However, the color loss treats r , д and b channels independently and generates only one channel each time, and then these three channels are combined to produce the final image. Intuitively, since the generation of a three-channel image space is much more complex than the generation of a single-channel image space, leading to higher possibility of artifacts, we independently generate each channel. The objective of r , д and b channel losses can be defined as follows,</p><formula xml:id="formula_9">L Col or c {1, 2} (G, S x , S y ) =E [x c , Sy ],y c , z 1 ∥y c − G([x c , S y ], z 1 ) ∥ {1, 2} + E [y c , Sx ], x c , z 2 ∥x c − G([y c , S x ], z 2 ) ∥ {1, 2} ,<label>(7)</label></formula><p>where c∈{r , д, b}, x r , x д and x b denote the r , д and b channels of image x respectively and similar to y r , y д and y b , ∥· ∥ 1 and ∥· ∥ 2 represent L 1 and L 2 distance losses. Thus, the color L 1 and L 2 losses can be expressed as,</p><formula xml:id="formula_10">L Col or {1, 2} (G, S x , S y )=L Col or r {1, 2} +L Col or д {1, 2} +L Col or b {1, 2} .<label>(8)</label></formula><p>When back-propagating the gradients of the L 1 loss, the partial derivatives are constants, i.e., ±1. Therefore, the error from other channels will not influence the current one as the derivative is a constant. However, for the original L 2 loss, the derivative is not a fixed constant. Actually, the derivative for the variables in one channel is always influenced by the errors from other channels. We have listed the gradients of red channel of Equations 6 and 8. Let y represent the generated target image G([x, S y ], z 1 ), we have,</p><formula xml:id="formula_11">∂ ∂ y io, jo r L L 2 (G, S x , S y ) = ∂ ∂ y io, jo r i, j (y i, j r − y i, j r ) 2 + i, j (y i, j д − y i, j д ) 2 + i, j (y i, j b − y i, j b ) 2 = y io, jo r − y io, jo r i, j (y i, j r − y i, j r ) 2 + i, j (y i, j д − y i, j д ) 2 + i, j (y i, j b − y i, j b ) 2 . (9) ∂ ∂ y io, jo r L Col or 2 (G, S x , S y ) = ∂ ∂ y io, jo r i, j (y i, j r − y i, j r ) 2 + i, j (y i, j д − y i, j д ) 2 + i, j (y i, j b − y i, j b ) 2 = y io, jo r − y io, jo r i, j (y i, j r − y i, j r ) 2 .<label>(10)</label></formula><p>Therefore, we can calculate the gradient of original L 2 and color L 2 losses,</p><formula xml:id="formula_12">▽L L 2 (G, S y ) = ∂ ∂ y io, jo r L L 2 (G, S y )+ ∂ ∂ y io, jo д L L 2 (G, S y ) + ∂ ∂ y io, jo b L L 2 (G, S y ).<label>(11)</label></formula><formula xml:id="formula_13">▽L Col or 2 (G, S y ) = ∂ ∂ y io, jo r L Col or 2 (G, S y )+ ∂ ∂ y io, jo д L Col or 2 (G, S y ) + ∂ ∂ y io, jo b L Col or 2 (G, S y ).<label>(12)</label></formula><p>Clearly, in Equation 9 the red channel in original L 2 loss is polluted by green and blue channels. As a consequence, the error from other channels will also influence the red channel. On the contrary, if we compute the loss for each channel independently, we can avoid such influence as shown in <ref type="figure" target="#fig_0">Equation 10</ref>. Cycle-Consistency Loss. It is worth noting that the CycleGAN <ref type="bibr" target="#b61">[62]</ref> is different from pix2pix framework <ref type="bibr" target="#b9">[10]</ref> as the training data in Cy-cleGAN is unpaired. The CycleGAN introduces the cycle-consistency loss to enforce forward-backward consistency. The cycle-consistency loss can be regarded as "pseudo" pairs of training data even though we do not have the corresponding data in the target domain which corresponds to the input data from the source domain. However, in this paper we introduce the cycle-consistency loss for the paired image-to-image translation task. The cycle loss ensures the consistency between source images and the reconstructed image, and it can be expressed as,</p><formula xml:id="formula_14">L cyc (G, S x , S y ) =E x,y, Sx , Sy , z 1 , z 2 x − G(G([x, S y ], z 1 ), S x , z 2 ) 1 + E x,y, Sx , Sy , z 1 , z 2 y − G(G([y, S x ], z 2 ), S y , z 1 ) 1 .<label>(13)</label></formula><p>Similar to StarGAN <ref type="bibr" target="#b2">[3]</ref> we use the same generator G two times, with the first time to convert an original image into the target one, then to recover the original image from the generated image. Identity Preserving Loss. To preserve the person identity after image synthesis, we propose the identity preserving loss, which can be expressed as follows,</p><formula xml:id="formula_15">L id e nt i t y (G, S x , S y ) =E x, Sy , z 1 F (y) − F (G([x, S y ], z 1 )) 1 + E y, Sx , z 2 | |F (x ) − F (G([y, S x ], z 2 ))| | 1 ,<label>(14)</label></formula><p>where F is a feature extractor. The feature extractor employs a VGG network <ref type="bibr" target="#b42">[43]</ref> originally pretrained for face recognition. We minimize the difference between the feature maps which are generated from the real and the generated images via the pretrained CNN for identity preservation.</p><p>Overall Loss. The final objective of the proposed GestureGAN is,</p><formula xml:id="formula_16">L =L S (G, D, S x , S y ) + λ 1 L Col or {1, 2} (G, S x , S y )+ λ 2 L cyc (G, S x , S y ) + λ 3 L id e nt i t y (G, S x , S y ),<label>(15)</label></formula><p>where, λ 1 , λ 2 and λ 3 are three hyper-parameters controlling the relative importance of these four losses. In our experiments, we follow the same setup of pix2pix <ref type="bibr" target="#b9">[10]</ref>. Instead of using the random noise vector z, we provide noise only in the form of dropout in generator G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>Generator. We adopt the "U-shaped" network <ref type="bibr" target="#b9">[10]</ref> as our generator. U-net has skip connections, which concatenate all channels at layer l with those at layer n−l, where n is the total number of layers.</p><p>Discriminator. We employ PatchGAN <ref type="bibr" target="#b9">[10]</ref> as our discriminator architecture. The goal of PatchGAN is to classify each small patch in an image as real or fake. We run PatchGAN convolutationally across an image, then average all results to calculate the ultimate output of discriminator D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>We follow the standard optimization method from <ref type="bibr" target="#b6">[7]</ref> to optimize the proposed GestureGAN, i.e., we alternate between one gradient descent step on discriminator D, and one step on generator G. In addition, as suggested in the original GAN paper <ref type="bibr" target="#b6">[7]</ref>, we train to maximize log D([x, S y ], y) rather than to minimize log(1 − D([x, S y ], y)). Moreover, in order to slow down the rate of D relative to G we divide the objective by 2 while optimizing D,</p><formula xml:id="formula_17">L(D) = 1 2 L bc e (D([x, S y ], y), 1) + L bc e (D([x, S y ], G([x, S y ], z 1 )), 0) + 1 2 [L bc e (D([y, S x ], x ), 1) + L bc e (D([y, S x ], G([y, S x ], z 2 )), 0)] ,<label>(16)</label></formula><p>where L bce denotes the Binary Cross Entropy loss function. We also employ dual discriminators as in Xu et al. <ref type="bibr" target="#b54">[55]</ref>, Nguyen et al. <ref type="bibr" target="#b28">[29]</ref> and CycleGAN <ref type="bibr" target="#b61">[62]</ref>, which have demonstrated that they improve the ability of discriminator to generate more photo-realistic images. Thus Equations 16 is modified as:</p><formula xml:id="formula_18">L(D 1 , D 2 ) = 1 2 L bc e (D 1 ([x, S y ], y), 1) + L bc e (D 1 ([x, S y ], G([x, S y ], z 1 )), 0) + 1 2 [L bc e (D 2 ([y, S x ], x ), 1) + L bc e (D 2 ([y, S x ], G([y, S x ], z 2 )), 0)] .<label>(17)</label></formula><p>We employ the minibatch SGD algorithm and apply Adam optimizer <ref type="bibr" target="#b13">[14]</ref> as solver. The momentum terms β 1 and β 2 of Adam are 0.5 and 0.999, respectively. The initial learning rate for Adam is 0.0002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Setup</head><p>Datasets. We evaluate the proposed GestureGAN on two public hand gesture datasets: NTU Hand Digit <ref type="bibr" target="#b37">[38]</ref> and Creative Senz3D <ref type="bibr" target="#b26">[27]</ref>, which include different hand gestures. NTU Hand Digit dataset <ref type="bibr" target="#b37">[38]</ref> contains 10 hand gestures (e.g., decimal digits from 0 to 9) color images and depth maps collected with a Kinect sensor under cluttered background. The total images in this dataset are 10 gestures × 10 subjects × 10 times = 1000 images. All images are in 640×480 resolution. In our experiment, we only use the RGB images. We randomly select 84,636 pairs, each of which is comprised of two images of the same person but different gestures. 9,600 pairs are randomly selected for the testing subset and the rest of 75,036 pairs as the training set. Creative Senz3D dataset <ref type="bibr" target="#b26">[27]</ref> includes static hand gestures performed by 4 people, each performing 11 different gestures repeated 30 times each in the front of a Creative Senz3D camera. The overall number of images of this dataset is 4 subjects × 11 gestures × 30 times = 1320. All images are in resolution 640×480. In our experiment, we only use the RGB images. We randomly select 12,800 pairs and 135,504 pairs as the testing and training set, each pair being composed of two images of the same person but different gestures. Implementation Details. For both datasets, we do left-right flip for data augmentation and random crops are disabled in this experiment as was done in PG 2 <ref type="bibr" target="#b23">[24]</ref>. For the embedding method, skeleton images are fed into an independent encoder similar to PG 2 <ref type="bibr" target="#b23">[24]</ref>, then we extract the fully connected layer feature vector to concatenate it with the image embedding at the bottleneck fully connected layer. For optimization, models are trained with a mini-batch size of 8 for 20 epochs on both datasets. Hyper-parameters are set empirically with λ 1 =100, λ 2 =10. λ 3 is 0.1 in the beginning and is gradually increased to 0.5. At inference time, we follow the same settings of PG 2 <ref type="bibr" target="#b23">[24]</ref>, Ma et al. <ref type="bibr" target="#b24">[25]</ref> and PoseGAN <ref type="bibr" target="#b40">[41]</ref> to randomly select the target keypoint or skeleton. GestureGAN is implemented using the public deep learning framework PyTorch. To speed up the training and testing processes, we use a Nvidia TITAN Xp GPU with 12G memory. Evaluation Metrics. Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), Inception Score (IS) <ref type="bibr" target="#b38">[39]</ref>, Fréchet Inception Distance (FID) <ref type="bibr" target="#b8">[9]</ref> and the proposed Fréchet ResNet Distance (FRD) are employed to evaluate the quality of generated images. FRD approach provides an alternative method to quantify the quality of synthesis and is similar to the Fréchet Inception Distance (FID) proposed by <ref type="bibr" target="#b8">[9]</ref>. FID is a measure of similarity between two datasets of images. The authors in <ref type="bibr" target="#b8">[9]</ref> have shown that the FID is more robust to noise than IS <ref type="bibr" target="#b38">[39]</ref> and correlates well with the human judgment of visual quality <ref type="bibr" target="#b8">[9]</ref>. To calculate FID <ref type="bibr" target="#b8">[9]</ref> between two image domains x and y, they first embed both into a feature space F given by an Inception model. Then viewing the feature space as a continuous multivariate Gaussian as suggested in <ref type="bibr" target="#b8">[9]</ref>, the Fréchet distance between the two Gaussians to quantify the quality of the data and the definition of FID can be expressed as:</p><formula xml:id="formula_19">FID(x, y) = ∥µ x − µ y ∥ 2 2 +Tr( x + y − 2( x y ) 1 2 ),<label>(18)</label></formula><p>where (µ x , x ) and (µ y , y ) are the mean and the covariance of the data distribution and model distribution, respectfully. Unlike FID, which regards the datasets x and y as a whole, the proposed FRD is inspired from feature matching methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>, and separately calculates the Fréchet distance between generated images and real images from the semantical level. In this way, images from two domains do not affect each other when computing the Fréchet distance. Moreover, for FID the number of samples should be greater than the dimension of the coding layer, while the proposed FRD does not have this limitation. We denote x i and y i as images in the x and y domains, respectively. For calculating FRD, we first embed both images x i and y i into a feature space F with 1000×1 dimension given by a ResNet pretrained model <ref type="bibr" target="#b7">[8]</ref>. We then calculate the Fréchet distance between two feature maps f (x i ) and f (y i ). The Fréchet distance F (f (x i ), f (y i )) is defined as the infimum over all reparameterizations α and β of [0, 1] of the maximum over all t ∈ [0, 1] of the distance in F between f (x i )(α(t)) and f (y i )(β(t)), where α and β are continuous, non-decreasing surjections of the range [0, 1]. The proposed FRD is a measure of similarity between the feature vector of the real image f (y i ) and the feature vector of the generated image f (x i ) by calculating the Fréchet distance between them. The Fréchet distance is defined as the minimum cord-length sufficient to join a point traveling forward along f (y i ) and one traveling forward along f (x i ), although the rate of travel for each point may not necessarily be uniform. Thus, the definition of FRD between two image domain x and y is,</p><formula xml:id="formula_20">FRD(x, y) = 1 N N 1 inf α , β max t ∈[0, 1] d f (x i )(α (t )), f (y i )(β (t )) ,<label>(19)</label></formula><p>where d is the distance function of F , N is the total number of images in x and y domains.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative &amp; Quantitative Results</head><p>Comparison against Baselines. We compare the proposed Ges-tureGAN with the most related four works, i.e., PG 2 <ref type="bibr" target="#b23">[24]</ref>, Yan et al. <ref type="bibr" target="#b55">[56]</ref>, PoseGAN <ref type="bibr" target="#b40">[41]</ref> and Ma et al. <ref type="bibr" target="#b24">[25]</ref>. PG 2 <ref type="bibr" target="#b23">[24]</ref> and Ma et al. <ref type="bibr" target="#b24">[25]</ref> try to generate a person image with different poses based on a conditional person image and a target keypoint image. Yan et al. <ref type="bibr" target="#b55">[56]</ref> and PoseGAN <ref type="bibr" target="#b40">[41]</ref> explicitly employ human skeleton information to generate person images. Note that Yan et al. <ref type="bibr" target="#b55">[56]</ref> adopt a CGAN to generate motion sequences based on appearance information and skeleton information by exploiting frame level smoothness.</p><p>We re-implemented this model to generate a single frame for fair comparison. These four methods are paired image-to-image models and comparison results are shown in <ref type="figure" target="#fig_2">Figure 3</ref> and <ref type="table" target="#tab_0">Table 1</ref>. As we can see in <ref type="figure" target="#fig_2">Figure 3</ref>, GestureGAN produces sharper images with convincing details compared with other baselines. Moreover, it is obvious that our results in <ref type="table" target="#tab_0">Table 1</ref> are consistently much better than baseline methods on both datasets. Generated Results of Each Epoch. <ref type="figure" target="#fig_4">Figure 5</ref> (left) illustrates the convergence loss L of the proposed GestureGAN in <ref type="bibr">Equation 15</ref>. Note that the proposed GestureGAN ensures a very fast yet stable convergence.</p><p>Analysis of the Model Components. In <ref type="figure" target="#fig_3">Figure 4</ref> we conduct ablations studies of our model. We gradually remove components of the proposed GestureGAN, i.e., Dual Discriminators (D), Identity Loss (I), Color Loss (P) and Cycle-consistency Loss (C). We find that removing the color loss and the cycle-consistency loss   substantially degrades results, meaning that the color loss and the cycle-consistency loss are critical to our results. In addition, the results without using the identity loss and the dual discriminators slightly degrade performance. We also provide quantitative results in <ref type="table" target="#tab_1">Table 2</ref>, and we can see that the full version of GestureGAN produces more photo-realistic results that other variants on two measurements except IS. The reason could be that the datasets we used only include human images which do not fit into ImageNet classes <ref type="bibr" target="#b3">[4]</ref>. Moreover, PG 2 <ref type="bibr" target="#b23">[24]</ref> and other super-resolution works such as <ref type="bibr" target="#b10">[11]</ref> also show the fact that sharper results have a lower quantitative value. User Study. Similar to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b61">62]</ref>, we have also provided a user study. We follow the same settings as in <ref type="bibr" target="#b9">[10]</ref> to conduct the Amazon Mechanical Turk (AMT) perceptual studies. The results of NTU Hand Digit <ref type="bibr" target="#b21">[22]</ref> and Senz3D <ref type="bibr" target="#b26">[27]</ref> datasets compared with the baseline models PG 2 <ref type="bibr" target="#b23">[24]</ref>, Ma et al <ref type="bibr" target="#b24">[25]</ref>, Yan et al. <ref type="bibr" target="#b55">[56]</ref> and PoseGAN <ref type="bibr" target="#b40">[41]</ref> are shown in <ref type="table" target="#tab_2">Table 3</ref>. Note that the proposed GestureGAN consistently achieves the best performance compared with baselines. FID vs. FRD. We also compare the performance between FID and the proposed FRD. The results shown in <ref type="table" target="#tab_3">Table 4</ref> and we can observe that FRD is more consistent with the human judgment in <ref type="table" target="#tab_2">Table 3</ref> than the FID metric. Moreover, we observe that the difference in  FRD between GestureGAN and the other methods is not as obvious as in the results from the user study in <ref type="table" target="#tab_2">Table 3</ref>. The reason is that FRD calculates the Fréchet distance between the feature maps extracted from the real image and the generated image using CNNs which are trained with semantic labels. Thus, these feature maps are employed to reflect the semantic distance between the images. The semantic distance between the images is not very large considering they are all hands. On the contrary, the user study measures the generation quality from a perceptual level. The difference on the perceptual level is more obvious than on the semantic level, i.e., the generated images with small artifacts show minor difference on the feature level, while being judged with a significant difference from the real images by humans. Data Augmentation. The generated images are high-quality and are photo-realistic, and these images can be used to improve the performance of a hand gesture classifier. The intuition is that if the generated images are realistic, the classifiers trained on both the real images and the generated images will be able to boost the accuracy of the real images. In this situation, the generated images work as augmented data. We employ a pretrained ResNet-50 model <ref type="bibr" target="#b7">[8]</ref> and feed the generated images to fine-tune it. For both datasets, we make a split of 70%/30% between training and testing sets. Specifically, the NTU Hand Digit dataset has 700 and 300 images for training and testing set. For the Senz3D dataset, the numbers of training and testing set are 924 and 396. The recognition results for the NTU Hand Digit and the Senz3D datasets are 15% and 34.34%, respectively. The term "real/real" in <ref type="table" target="#tab_4">Table 5</ref> represents the result without data augmentation. After adding the generated images by different methods to the training set, the performance improves significantly. Results compared with PG 2 <ref type="bibr" target="#b23">[24]</ref>, Yan et al. <ref type="bibr" target="#b55">[56]</ref>, Ma et al. <ref type="bibr" target="#b24">[25]</ref> and PoseGAN <ref type="bibr" target="#b40">[41]</ref> are shown in <ref type="table" target="#tab_4">Table 5</ref>. Clearly, GestureGAN achieves the best result compared with baselines. Influence of Gesture Size and Distance. We have also investigated the influence of the gesture size and the distance between source and target gestures. The training samples for the source and the target gesture from the same person are randomly paired and both gestures have different sizes and distances. Thus, the model  is able to learn a robust translation w.r.t different hand size and distance. We show a qualitative example in <ref type="figure" target="#fig_5">Figure 6</ref> in which the source images have different sizes and the target gestures have different locations. Note that GestureGAN can generate the target gesture from different hand sizes and distances with high quality.</p><p>Influence of Gesture Pairs. To evaluate the influence of gesture pairs, we searched all the translations between every possible category combinations including the translation within each category. In <ref type="figure" target="#fig_4">Figure 5</ref> (right), we show the MSE for the translation from the source to the target gesture types on NTU dataset. Note that the MSE for the generation of different gesture pairs has a small variance, showing that the influence of different gesture pairs is very low. This proves that our model is stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we focus on a challenging task of hand gesture-togesture translation in the wild. To this end, we propose a novel Generative Adversarial Network (GAN), i.e., GestureGAN, which can generate hand gestures with different poses, sizes and locations in the wild. We also propose two novel losses to learn the mapping from the source gesture to the target gesture, i.e., the color loss and the cycle-consistency loss. It is worth noting that the proposed color loss handles the "channel pollution" problem while back-propagating the gradients, which frequently occurs in the existing generative models. In addition, we present the Fréchet ResNet Distance (FRD) metric to evaluate the quality of generated images. Experimental results show that GestureGAN achieves state-of-theart performance. Lastly, the generated images of GestureGAN are of high-quality and are photo-realistic, and they can thus be used to improve the performance of hand gesture classifiers. Future work will focus on designing a GAN model which can handle the situation where the background is total different between source and target gestures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pipeline of the proposed GestureGAN model. GestureGAN consists of a single generator G and a discriminator D, which takes as input a conditional hand image and a target hand skeleton image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the "channel pollution" issue on different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison with PG 2 [24], Ma et al. [25], Yan et al. [56] and PoseGAN [41] on the NTU Hand Digit (Top) and the Senz3D (Bottom) datasets. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison using different components of GestureGAN on the NTU Hand Digit (Top) and the Senz3D (Bottom) datasets. All: full version of GestureGAN, D: Dual discriminators strategy, I: Identity preserving loss, P: Color loss, C: Cycle-consistency loss. "-" means removing. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Convergence loss L in Equation 15 (Left) and MSE of different gesture pairs on the NTU dataset (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Two samples with different hand sizes and distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results of different models on the NTU Hand Digit and Senz3D datasets. For PSNR and IS measures, higher is better. For MSE evaluation, lower is better.</figDesc><table><row><cell>Model</cell><cell cols="3">NTU Hand Digit [22] MSE PSNR</cell><cell>IS</cell><cell>MSE</cell><cell>Senz3D [27] PSNR</cell><cell>IS</cell></row><row><cell>PG 2 [24] (NIPS 2017)</cell><cell>116.1049</cell><cell>28.2403</cell><cell cols="2">2.4152</cell><cell>199.4384</cell><cell>26.5138</cell><cell>3.3699</cell></row><row><cell cols="2">Yan et al. [56] (ACM MM 2017) 118.1239</cell><cell>28.0185</cell><cell cols="2">2.4919</cell><cell>175.8647</cell><cell>26.9545</cell><cell>3.3285</cell></row><row><cell>Ma et al. [25] (CVPR 2018)</cell><cell>113.7809</cell><cell>30.6487</cell><cell cols="2">2.4547</cell><cell>183.6457</cell><cell>26.9451</cell><cell>3.3874</cell></row><row><cell>PoseGAN [41] (CVPR 2018)</cell><cell>113.6487</cell><cell>29.5471</cell><cell cols="2">2.4017</cell><cell>176.3481</cell><cell>27.3014</cell><cell>3.2147</cell></row><row><cell>GestureGAN (Ours)</cell><cell cols="7">105.7286 32.6091 2.5532 169.9219 27.9749 3.4107</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study: quantitative results with different components of GestureGAN on the NTU Hand Digit and Senz3D datasets. For PSNR and IS measures, higher is better. For MSE evaluation, lower is better. All: full version of GestureGAN, D: Dual discriminators strategy, I: Identity preserving loss, P: Color loss, C: Cycle-consistency loss. "-" means removing.</figDesc><table><row><cell>Component</cell><cell cols="3">NTU Hand Digit [22] MSE PSNR</cell><cell>IS</cell><cell>MSE</cell><cell>Senz3D [27] PSNR</cell><cell>IS</cell></row><row><cell>All</cell><cell cols="7">105.7286 32.6091 2.5532 169.9219 27.9749 3.4107</cell></row><row><cell>All -D (Dual Discriminators Strategy)</cell><cell>118.7830</cell><cell>28.0189</cell><cell cols="2">2.5071</cell><cell>198.0646</cell><cell>26.7237</cell><cell>3.2740</cell></row><row><cell>All -D -I (Identity Preserving Loss)</cell><cell>198.7054</cell><cell>25.8474</cell><cell cols="5">2.5438 1319.3957 18.3892 4.0784</cell></row><row><cell>All -D -I -P (Color Loss)</cell><cell>406.1478</cell><cell>22.1564</cell><cell cols="4">2.5842 1745.3214 14.6598</cell><cell>3.4519</cell></row><row><cell cols="2">All -D -I -P -C (Cycle-Consistency Loss) 707.6053</cell><cell cols="5">20.2684 2.6114 2064.8428 15.5426</cell><cell>3.2064</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of AMT perceptual studies (%) on the NTU Hand Digit and Senz3D datasets.</figDesc><table><row><cell>Method</cell><cell cols="2">NTU Hand Digit [22] Senz3D [27]</cell></row><row><cell>PG 2 [24] (NIPS 2017)</cell><cell>3.5%</cell><cell>2.8%</cell></row><row><cell>Yan et al. [56] (ACM MM 2017)</cell><cell>2.6%</cell><cell>2.3%</cell></row><row><cell>Ma et al. [25] (CVPR 2018)</cell><cell>7.1%</cell><cell>6.9%</cell></row><row><cell>PoseGAN [41] (CVPR 2018)</cell><cell>9.3%</cell><cell>8.6%</cell></row><row><cell>GestureGAN (Ours)</cell><cell>26.1%</cell><cell>22.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of FID and the proposed FRD metrics on the NTU Hand Digit and Senz3D datasets. For both FID and FRD, lower is better.</figDesc><table><row><cell>Method</cell><cell cols="2">NTU Hand Digit [22] FID FRD</cell><cell>Senz3D [27] FID FRD</cell></row><row><cell>PG 2 [24] (NIPS 2017)</cell><cell>24.2093</cell><cell>2.6319</cell><cell>31.7333 3.0933</cell></row><row><cell cols="2">Yan et al. [56] (ACM MM 2017) 31.2841</cell><cell>2.7453</cell><cell>38.1758 3.1006</cell></row><row><cell>Ma et al. [25] (CVPR 2018)</cell><cell>6.7661</cell><cell>2.6184</cell><cell>26.2713 3.0846</cell></row><row><cell>PoseGAN [41] (CVPR 2018)</cell><cell>9.6725</cell><cell>2.5846</cell><cell>24.6712 3.0467</cell></row><row><cell>GestureGAN (Ours)</cell><cell>7.5860</cell><cell>2.5223</cell><cell>18.4595 2.9836</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of hand gesture recognition accuracy (%) on the NTU Hand Digit and Senz3D datasets.</figDesc><table><row><cell>Method</cell><cell cols="2">NTU Hand Digit [22] Senz3D [27]</cell></row><row><cell>real/real</cell><cell>15.000%</cell><cell>34.343%</cell></row><row><cell>PG 2 [24] (NIPS 2017)</cell><cell>93.667%</cell><cell>98.737%</cell></row><row><cell>Yan et al. [56] (ACM MM 2017)</cell><cell>95.333%</cell><cell>99.495%</cell></row><row><cell>Ma et al. [25] (CVPR 2018)</cell><cell>95.864%</cell><cell>99.054%</cell></row><row><cell>PoseGAN [41] (CVPR 2018)</cell><cell>96.128%</cell><cell>99.549%</cell></row><row><cell>GestureGAN (Ours)</cell><cell>96.667%</cell><cell>99.747%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We want to thank the Nvidia Corporation for the donation of the TITAN Xp GPUs used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ComboGAN: Unrestrained Scalability for Image Domain Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asha</forename><surname>Anoosheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<title level="m">Began: Boundary equilibrium generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GP-GAN: gender preserving GAN for synthesizing faces from landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eye In-Painting with Exemplar Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cristian Canton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast faceswap using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In ICCV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative Face Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face aging with contextual generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defa</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renda</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-domain human parsing via adversarial feature and label adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defa</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pose Guided Person Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangled Person Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Head-mounted gesture controlled interface for human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvise</forename><surname>Memo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer Multimedia Tools and Applications</publisher>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformation-grounded image generation network for novel 3d view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Invertible Conditional GANs for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Álvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengchun</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01822</idno>
		<title level="m">Geometry-Contrastive Generative Adversarial Network for Facial Expression Synthesis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aäron van den Oord, Nal Kalchbrenner, Victor Bapst, Matt Botvinick, and Nando de Freitas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Generating interpretable images with controllable structure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Honglak Lee. 2016. Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Scott E Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust part-based hand gesture recognition using kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1110" to="1120" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural Face Editing with Intrinsic Image Disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deformable GANs for Pose-based Human Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hand Keypoint Detection in Single Images using Multiview Bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03474</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometry Guided Adversarial Facial Expression Synthesis. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Natural and Effective Obfuscation by Head Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Novel Feature Matching Strategy for Large Scale Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Every Smile is Unique: Landmark-Guided Diverse Smile Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alameda-Pineda</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricci</forename><surname>Elisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fua</forename><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebe</forename><surname>Nicu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07195</idno>
		<title level="m">Gp-gan: Towards realistic high-resolution image blending</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generativeadversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Controllable Invariance through Adversarial Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06090</idno>
		<title level="m">Face Transfer with Generative Adversarial Network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Skeleton-aided Articulated Motion Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Chia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Yu</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><forename type="middle">Tan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Packing and padding: Coupled multi-index for accurate image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bayes merging of multiple vocabularies for scalable image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
