<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lookahead Optimizer: k steps forward, 1 step back</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
							<email>michael@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
							<email>jlucas@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
							<email>hinton@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lookahead Optimizer: k steps forward, 1 step back</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of "fast weights" generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Lookahead Optimizer:</head><p>Require: Initial parameters φ 0 , objective function L Require: Synchronization period k, slow weights step size α, optimizer A for t = 1, 2, . . . do Synchronize parameters θ t,0 ← φ t−1 for i = 1, 2, . . . , k do sample minibatch of data d ∼ D θ t,i ← θ t,i−1 + A(L, θ t,i−1 , d) end for Perform outer update φ t ← φ t−1 + α(θ t,k − φ t−1 ) end for return parameters φ</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite their simplicity, SGD-like algorithms remain competitive for neural network training against advanced second-order optimization methods. Large-scale distributed optimization algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">45]</ref> have shown impressive performance in combination with improved learning rate scheduling schemes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b34">35]</ref>, yet variants of SGD remain the core algorithm in the distributed systems. The recent improvements to SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad <ref type="bibr" target="#b6">[7]</ref> and Adam <ref type="bibr" target="#b17">[18]</ref>, and (2) accelerated schemes, such as Polyak heavy-ball <ref type="bibr" target="#b32">[33]</ref> and Nesterov momentum <ref type="bibr" target="#b28">[29]</ref>. Both approaches make use of the accumulated past gradient information to achieve faster convergence. However, to obtain their improved performance in neural networks often requires costly hyperparameter tuning <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this work, we present Lookahead, a new optimization method, that is orthogonal to these previous approaches. Lookahead first updates the "fast weights" <ref type="bibr" target="#b11">[12]</ref> k times using any standard optimizer in its inner loop before updating the "slow weights" once in the direction of the final fast weights. We show that this update reduces the variance. We find that Lookahead is less sensitive to suboptimal hyperparameters and therefore lessens the need for extensive hyperparameter tuning. By using Lookahead with inner optimizers such as SGD or Adam, we achieve faster convergence across different deep learning tasks with minimal computational overhead.</p><p>Empirically, we evaluate Lookahead by training classifiers on the CIFAR <ref type="bibr" target="#b18">[19]</ref> and ImageNet datasets <ref type="bibr" target="#b4">[5]</ref>, observing faster convergence on the ResNet-50 and ResNet-152 architectures <ref type="bibr" target="#b10">[11]</ref>. We also trained LSTM language models on the Penn Treebank dataset <ref type="bibr" target="#b23">[24]</ref> and Transformer-based <ref type="bibr" target="#b41">[42]</ref> neural machine translation models on the WMT 2014 English-to-German dataset. For all tasks, using Lookahead leads to improved convergence over the inner optimizer and often improved generalization performance while being robust to hyperparameter changes. Our experiments demonstrate that Lookahead is robust to changes in the inner loop optimizer, the number of fast weight updates, and the slow weights learning rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we describe the Lookahead algorithm and discuss its properties. Lookahead maintains a set of slow weights φ and fast weights θ, which get synced with the fast weights every k updates. The fast weights are updated through applying A, any standard optimization algorithm, to batches of training examples sampled from the dataset D. After k inner optimizer updates using A, the slow weights are updated towards the fast weights by linearly interpolating in weight space, θ − φ. We denote the slow weights learning rate as α. After each slow weights update, the fast weights are reset to the current slow weights value. Psuedocode is provided in Algorithm 1. <ref type="bibr" target="#b0">1</ref> Standard optimization methods typically require carefully tuned learning rates to prevent oscillation and slow convergence. This is even more important in the stochastic setting <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">43]</ref>. Lookahead, however, benefits from a larger learning rate in the inner loop. When oscillating in the high curvature directions, the fast weights updates make rapid progress along the low curvature directions. The slow weights help smooth out the oscillations through the parameter interpolation. The combination of fast weights and slow weights improves learning in high curvature directions, reduces variance, and enables Lookahead to converge rapidly in practice. <ref type="figure" target="#fig_0">Figure 1</ref> shows the trajectory of both the fast weights and slow weights during the optimization of a ResNet-32 model on CIFAR-100. While the fast weights explore around the minima, the slow weight update pushes Lookahead aggressively towards an area of improved test accuracy, a region which remains unexplored by SGD after 20 updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slow weights trajectory</head><p>We can characterize the trajectory of the slow weights as an exponential moving average (EMA) of the final fast weights within each inner-loop, regardless of the inner optimizer. After k inner-loop steps we have:</p><formula xml:id="formula_0">φ t+1 = φ t + α(θ t,k − φ t ) (1) = α[θ t,k + (1 − α)θ t−1,k + . . . + (1 − α) t−1 θ 0,k ] + (1 − α) t φ 0<label>(2)</label></formula><p>Intuitively, the slow weights heavily utilize recent proposals from the fast weight optimization but maintain some influence from previous fast weights. We show that this has the effect of reducing variance in Section 3.1. While a Polyak-style average has further theoretical guarantees, our results match the claim that "an exponentially-decayed moving average typically works much better in practice" <ref type="bibr" target="#b24">[25]</ref>.  Fast weights trajectory Within each inner-loop, the trajectory of the fast weights depends on the choice of underlying optimizer. Given an optimization algorithm A that takes in an objective function L and the current mini-batch training examples d, we have the update rule for the fast weights:</p><formula xml:id="formula_1">θ t,i+1 = θ t,i + A(L, θ t,i−1 , d).<label>(3)</label></formula><p>We have the choice of maintaining, interpolating, or resetting the internal state (e.g. momentum) of the inner optimizer. We evaluate this tradeoff on the CIFAR dataset (where every choice improves convergence) in Appendix D.1 and maintain internal state for the other experiments.</p><p>Computational complexity Lookahead has a constant computational overhead due to parameter copying and basic arithmetic operations that is amortized across the k inner loop updates. The number of operations is O( k+1 k ) times that of the inner optimizer. Lookahead maintains a single additional copy of the number of learnable parameters in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Selecting the Slow Weights Step Size</head><p>The step size in the direction (θ t,k − θ t,0 ) is controlled by α. By taking a quadratic approximation of the loss, we present a principled way of selecting α. Proposition 1 (Optimal slow weights step size). For a quadratic loss function L(x) = 1 2 x T Ax−b T x, the step size α * that minimizes the loss for two points θ t,0 and θ t,k is given by:</p><formula xml:id="formula_2">α * = arg min α L(θ t,0 + α(θ t,k − θ t,0 )) = (θ t,0 − θ * ) T A(θ t,0 − θ t,k ) (θ t,0 − θ t,k ) T A(θ t,0 − θ t,k )</formula><p>where θ * = A −1 b minimizes the loss.</p><p>Proof is in the appendix. Using quadratic approximations for the curvature, which is typical in second order optimization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>, we can derive an estimate for the optimal α more generally. The full Hessian is typically intractable so we instead use aforementioned approximations, such as the diagonal approximation to the empirical Fisher used by the Adam optimizer <ref type="bibr" target="#b17">[18]</ref>. This approximation works well in our numerical experiments if we clip the magnitude of the step size. At each slow weight update, we compute:</p><formula xml:id="formula_3">α * = clip( (θ t,0 − (θ t,k −Â −1∇ L(θ t,k )) TÂ (θ t,0 − θ t,k ) (θ t,0 − θ t,k ) TÂ (θ t,0 − θ t,k ) , α low , 1)</formula><p>whereÂ is the empirical Fisher approximation and θ t,k −Â −1∇ L(θ t,k ) approximates the optimum θ * . We prove Proposition 1 and elaborate on assumptions in the appendix B.2. Setting α low &gt; 0 improves the stability of our algorithm. We evaluate the performance of this adaptive scheme versus a fixed scheme and standard Adam on a ResNet-18 trained on CIFAR-10 with two different learning rates and show the results in <ref type="figure" target="#fig_1">Figure 2</ref>. Additional hyperparameter details are given in appendix C. Both the fixed and adaptive Lookahead offer improved convergence.</p><p>In practice, a fixed choice of α offers similar convergence benefits and tends to generalize better. Fixing α avoids the need to maintain an estimate of the empirical Fisher, which incurs a memory and computational cost when the inner optimizer does not maintain such an estimate e.g. SGD. We thus use a fixed α for the rest of our deep learning experiments.  We analyze Lookahead on a noisy quadratic model to better understand its convergence guarantees. While simple, this model is a proxy for neural network optimization and effectively optimizing it remains a challenging open problem <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47]</ref>. In this section, we will show under equal learning rates that Lookahead will converge to a smaller steady-state risk than SGD. We will then show through simulation of the expected dynamics that Lookahead is able to converge to this steady-state risk more quickly than SGD for a range of hyperparameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model definition</head><p>We use the same model as in Schaul et al. <ref type="bibr" target="#b36">[37]</ref> and Wu et al. <ref type="bibr" target="#b42">[43]</ref>.</p><formula xml:id="formula_4">L(x) = 1 2 (x − c) T A(x − c),<label>(4)</label></formula><p>with c ∼ N (x * , Σ). We assume that both A and Σ are diagonal and that, without loss of generality, x * = 0. While it is trivial to assume that A is diagonal 2 the co-diagonalizable noise assumption is non-trivial but is common -see Wu et al. <ref type="bibr" target="#b42">[43]</ref> and Zhang et al. <ref type="bibr" target="#b46">[47]</ref> for further discussion. We use a i and σ 2 i to denote the diagonal elements of A and Σ respectively. Taking the expectation over c, the expected loss of the iterates θ (t) is,</p><formula xml:id="formula_5">L(θ (t) ) = E[L(θ (t) )] = 1 2 E[ i a i (θ (t) i 2 + σ 2 i )] = 1 2 i a i (E[θ (t) i ] 2 + V[θ (t) i ] + σ 2 i ). (5)</formula><p>Analyzing the expected dynamics of the SGD iterates and the slow weights gives the following result.</p><p>Proposition 2 (Lookahead steady-state risk). Let 0 &lt; γ &lt; 2/L be the learning rate of SGD and Lookahead where L = max i a i . In the noisy quadratic model, the iterates of SGD and Lookahead with SGD as its inner optimizer converge to 0 in expectation and the variances converge to the following fixed points:</p><formula xml:id="formula_6">V * SGD = γ 2 A 2 Σ 2 I − (I − γA) 2 (6) V * LA = α 2 (I − (I − γA) 2k ) α 2 (I − (I − γA) 2k ) + 2α(1 − α)(I − (I − γA) k ) V * SGD<label>(7)</label></formula><p>Remarks For the Lookahead variance fixed point, the first product term is always smaller than 1 for α ∈ (0, 1), and thus Lookahead has a variance fixed point that is strictly smaller than that of the SGD inner-loop optimizer for the same learning rate. Evidence of this phenomenon is present in deep neural networks trained on the CIFAR dataset, shown in <ref type="figure" target="#fig_0">Figure 10</ref>.</p><p>In Proposition 2, we use the same learning rate for both SGD and Lookahead. To fairly evaluate the convergence of the two methods, we compare the convergence rates under hyperparameter settings that achieve the same steady-state risk. In <ref type="figure" target="#fig_3">Figure 3</ref> we show the expected loss after 1000 updates (computed analytically) for both Lookahead and SGD. This shows that there exists (fixed) settings of the Lookahead hyperparameters that arrive at the same steady state risk as SGD but do so more quickly. Moreover, Lookahead outperforms SGD across the broad spectrum of α values we simulated. Details, further simulation results, and additional discussion are presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deterministic quadratic convergence</head><p>In the previous section we showed that on the noisy quadratic model, Lookahead is able to improve convergence of the SGD optimizer under setting with equivalent convergent risk. Here we analyze the quadratic model without noise using gradient descent with momentum <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b8">9]</ref> and show that when the system is under-damped, Lookahead is able to improve on the convergence rate.</p><p>As before, we restrict our attention to diagonal quadratic functions (which in this case is entirely without loss of generality). Given an initial point θ 0 , we wish to find the rate of contraction, that is, the smallest ρ satisfying ||θ t − θ * || ≤ ρ t ||θ 0 − θ * ||. We follow the approach of <ref type="bibr" target="#b30">[31]</ref> and model the optimization of this function as a linear dynamical system allowing us to compute the rate exactly. Details are in Appendix B. For Lookahead, we fix k = 20 lookahead steps and α = 0.5 for the slow weights step size. Lookahead is able to significantly improve on the convergence rate in the underdamped regime where oscillations are observed.</p><p>As in Lucas et al. <ref type="bibr" target="#b22">[23]</ref>, to better understand the sensitivity of Lookahead to misspecified conditioning we fix the momentum coefficient of classical momentum and explore the convergence rate over varying condition number under the optimal learning rate. As expected, Lookahead has slightly worse convergence in the over-damped regime where momentum is set too low and CM is slowly, monotonically converging to the optimum. However, when the system is under-damped (and oscillations occur) Lookahead is able to significantly improve the convergence rate by skipping to a better parameter setting during oscillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Our work is inspired by recent advances in understanding the loss surface of deep neural networks. While the idea of following the trajectory of weights dates back to Ruppert <ref type="bibr" target="#b35">[36]</ref>, Polyak and Juditsky <ref type="bibr" target="#b33">[34]</ref>, averaging weights in neural networks has not been carefully studied until more recently. Garipov et al. <ref type="bibr" target="#b7">[8]</ref> observe that the final weights of two independently trained neural networks can be connected by a curve with low loss. Izmailov et al. <ref type="bibr" target="#b13">[14]</ref> proposes Stochastic Weight Averaging (SWA), which averages the weights at different checkpoints obtained during training. Parameter averaging schemes are used to create ensembles in natural language processing tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref> and in training Generative Adversarial Networks <ref type="bibr" target="#b43">[44]</ref>. In contrast to previous approaches, which generally focus on generating a set of parameters at the end of training, Lookahead is an optimization algorithm which performs parameter averaging during the training procedure to achieve faster convergence. We elaborate on differences with SWA and present additional experimental results in appendix D.3.</p><p>The Reptile algorithm, proposed by Nichol et al. <ref type="bibr" target="#b29">[30]</ref>, samples tasks in its outer loop and runs an optimization algorithm on each task within the inner loop. The initial weights are then updated in the direction of the new weights. While the functionality is similar, the application and setting are starkly different. Reptile samples different tasks and aims to find parameters which act as good initial values for new tasks sampled at test time. Lookahead does not sample new tasks for each outer loop and aims to take advantage of the geometry of loss surfaces to improve convergence.</p><p>Katyusha <ref type="bibr" target="#b0">[1]</ref>, an accelerated form of SVRG <ref type="bibr" target="#b16">[17]</ref>, also uses an outer and inner loop during optimization.  challenges first in the form of additional memory overhead as the number of inner-loop steps increases and also in finding the best linear combination. Scieur et al. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> propose a method by which to find a good linear combination and apply this approach to deep learning problems and report both improved convergence and generalization. However, their method requires on the order of k times more memory than Lookahead. Lookahead can be seen as a simple version of Anderson acceleration wherein only the first and last iterates are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We completed a thorough evaluation of the Lookahead optimizer on a variety of deep learning tasks against well-calibrated baselines. We explored image classification on CIFAR-10/CIFAR-100 <ref type="bibr" target="#b18">[19]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref>. We also trained LSTM language models on the Penn Treebank dataset <ref type="bibr" target="#b23">[24]</ref> and Transformer-based <ref type="bibr" target="#b41">[42]</ref> neural machine translation models on the WMT 2014 English-to-German dataset. For all of our experiments, every algorithm consumed the same amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CIFAR-10 and CIFAR-100</head><p>The CIFAR-10 and CIFAR-100 datasets for classification consist of 32 × 32 color images, with 10 and 100 different classes, split into a training set with 50,000 images and a test set with 10,000 images. We ran all our CIFAR experiments with 3 seeds and trained for 200 epochs on a ResNet-18 <ref type="bibr" target="#b10">[11]</ref> with batches of 128 images and decay the learning rate by a factor of 5 at the 60th, 120th, and 160th epochs. Additional details are given in appendix C.</p><p>We summarize our results in <ref type="figure" target="#fig_5">Figure 5</ref>. <ref type="bibr" target="#b2">3</ref> We also elaborate on how Lookahead contrasts with SWA and present results demonstrating lower validation error with Pre-ResNet-110 and Wide-ResNet-28-10 [46] on CIFAR-100 in appendix D.3. Note that Lookahead achieves significantly faster convergence throughout training even though the learning rate schedule is optimized for the inner optimizerfuture work can involve building a learning rate schedule for Lookahead. This improved convergence is important for better anytime performance in new datasets where hyperparameters and learning rate schedules are not well-calibrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ImageNet</head><p>The 1000-way ImageNet task <ref type="bibr" target="#b4">[5]</ref> is a classification task that contains roughly 1.28 million training images and 50,000 validation images. We use the official PyTorch implementation 4 and the ResNet-50 and ResNet-152 <ref type="bibr" target="#b10">[11]</ref> architectures. Our baseline algorithm is SGD with an initial learning rate of 0.1 and momentum value of 0.9. We train for 90 epochs and decay our learning rate by a factor of 10 at the 30th and 60th epochs. For Lookahead, we set k = 5 and slow weights step size α = 0.5.</p><p>Motivated by the improved convergence we observed in our initial experiment, we tried a more aggressive learning rate decay schedule where we decay the learning rate by a factor of 10 at the 30th, 48th, and 58th epochs. Using such a schedule, we reach 75% single crop top-1 accuracy on ImageNet in just 50 epochs and reach 75.5% top-1 accuracy in 60 epochs. The results are shown in <ref type="figure">Figure 6</ref>.    To test the scalability of our method, we ran Lookahead with the aggressive learning rate decay on ResNet-152. We reach 77% single crop top-1 accuracy in 49 epochs (matching what is reported in He et al. <ref type="bibr" target="#b10">[11]</ref>) and 77.96% top-1 accuracy in 60 epochs. Other approaches for improving convergence on ImageNet can require hundreds of GPUs, or tricks such as ramping up the learning rate and adaptive batch-sizes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. The fastest convergence we are aware of uses an approximate second-order method to train a ResNet-50 to 75% top-1 accuracy in 35 epochs with 1,024 GPUs <ref type="bibr" target="#b31">[32]</ref>. In contrast, Lookahead requires changing one single line of code and can easily scale to ResNet-152.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Language modeling</head><p>We trained LSTMs <ref type="bibr" target="#b12">[13]</ref> for language modeling on the Penn Treebank dataset. We followed the model setup of Merity et al. <ref type="bibr" target="#b26">[27]</ref> and made use of their publicly available code in our experiments. We did not include the fine-tuning stages. We searched over hyperparameters for both Adam and SGD (without momentum) to find the model which gave the best validation performance. We then performed an additional small grid search on each of these methods with Lookahead. Each model was trained for 750 epochs. We show training curves for each model in <ref type="figure" target="#fig_7">Figure 7a</ref>.</p><p>Using Lookahead with Adam we were able to achieve the fastest convergence and best training, validation, and test perplexity. The models trained with SGD took much longer to converge (around 700 epochs) and were unable to match the final performance of Adam. Using Polyak weight averaging <ref type="bibr" target="#b33">[34]</ref> with SGD, as suggested by Merity et al. <ref type="bibr" target="#b26">[27]</ref> and referred to as ASGD, we were able to improve on the performance of Adam but were unable to match the performance of Lookahead. Full results are given in <ref type="table" target="#tab_4">Table 3</ref> and additional details are in appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Neural machine translation</head><p>We trained Transformer based models <ref type="bibr" target="#b41">[42]</ref> on the WMT2014 English-to-German translation task on a single Tensor Processing Unit (TPU) node. We took the base model from Vaswani et al. <ref type="bibr" target="#b41">[42]</ref> and trained it using the proposed warmup-then-decay learning rate scheduling scheme and, additionally, the same scheme wrapped with Lookahead. We found Lookahead speedups the early stage of the training over Adam and the later proposed AdaFactor <ref type="bibr" target="#b39">[40]</ref> optimizer. All the methods converge to similar training loss and BLEU score at the end, see <ref type="figure" target="#fig_7">Figure 7b</ref> and <ref type="table" target="#tab_5">Table 4</ref>.   Our NMT experiments further confirms Lookahead improves the robustness of the inner loop optimizer. We found Lookahead enables a wider range of learning rate {0.02, 0.04, 0.06} choices for the Transformer model that all converge to similar final losses. Full details are given in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Empirical analysis</head><p>Robustness to inner optimization algorithm, k, and α We demonstrate empirically on the CIFAR dataset that Lookahead consistently delivers fast convergence across different hyperparameter settings. We fix slow weights step size α = 0.5 and k = 5 and run Lookahead on inner SGD optimizers with different learning rates and momentum; results are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. In general, we observe that Lookahead can train with higher learning rates on the base optimizer with little to no tuning on k and α. This agrees with our discussion of variance reduction in Section 3.1. We also evaluate robustness to the Lookahead hyperparameters by fixing the inner optimizer and evaluating runs with varying updates k and step size α; these results are shown in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inner loop and outer loop evaluation</head><p>To get a better understanding of the Lookahead update, we also plotted the test accuracy for every update on epoch 65 in <ref type="figure" target="#fig_0">Figure 10</ref>. We found that within each inner loop the fast weights may lead to substantial degradation in task performance-this reflects our analysis of the higher variance of the inner loop update in section 3.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present Lookahead, an algorithm that can be combined with any standard optimization method. Our algorithm computes weight updates by looking ahead at the sequence of "fast weights" generated by another optimizer. We illustrate how Lookahead improves convergence by reducing variance and show strong empirical results on many deep learning benchmark datasets and architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Noisy quadratic analysis</head><p>Here we present the details of the noisy quadratic analysis, and the proof of Proposition 2.</p><p>Stochastic dynamics of SGD From Wu et al. <ref type="bibr" target="#b42">[43]</ref>, we can compute the dynamics of SGD with learning rate γ as follows:</p><formula xml:id="formula_7">E[x (t+1) ] = (I − γA) E[x (t) ] (8) V[x (t+1) ] = (I − γA) 2 V[x (t) ] + γ 2 A 2 Σ (9)</formula><p>Stochastic dynamics of Lookahead SGD We now compute the dynamics of the slow weights of Lookahead. Lemma 1. The Lookahead slow weights have the following trajectories:</p><formula xml:id="formula_8">E[φ t+1 ] = [1 − α + α(I − γA) k ] E[φ t ]<label>(10)</label></formula><formula xml:id="formula_9">V[φ t+1 ] = [1 − α + α(I − γA) k ] 2 V[φ t ] + α 2 k−1 i=0 (I − γA) 2i γ 2 A 2 Σ (11)</formula><p>Proof. The expectation trajectory follows from SGD,</p><formula xml:id="formula_10">E[φ t+1 ] = (1 − α) E[φ t ] + α E[θt,k] = (1 − α) E[φ t ] + α(I − γA) k E[φ t ] = [1 − α + α(I − γA) k ] E[φ t ]</formula><p>For the variance, we can write</p><formula xml:id="formula_11">V[φ t+1 ] = (1 − α) 2 V[φ t ] + α 2 V[θt,k] + 2α(1 − α)cov(φ t , θ t,k ).</formula><p>We proceed by computing the covariance term recursively. For simplicity, we work with a single element, θ, of the vector θ (as A is diagonal, each element evolves independently).</p><formula xml:id="formula_12">cov(θ t,k−1 , θ t,k ) = E[(θt,k−1 − E[θt,k−1])(θt,k − E[θt,k])] = E[(θt,k−1 − E[θt,k−1])(θt,k − (1 − γa) E[θt,k−1])] = E[θt,k−1θt,k] − (1 − γa) E[θt,k−1] 2 = E[(1 − γa)θ 2 t,k−1 ] − (1 − γa) E[θt,k−1] 2 = (1 − γa) V[θt,k−1]</formula><p>A similar derivation yields cov(φ t , θ t,k ) = (I − γA) k V[φt]. After substituting the SGD variance formula and some rearranging we have,</p><formula xml:id="formula_13">V[φ t+1 ] = [1 − α + α(I − γA) k ] 2 V[φ t ] + α 2 k−1 i=0 (I − γA) 2i γ 2 A 2 Σ</formula><p>We now proceed with the proof of Proposition 2.</p><p>Proof. First note that if the learning rate is chosen as specified, then each of the trajectories is a contraction map. By Banach's fixed point theorem, they each have a unique fixed point. Clearly the expectation trajectories contract to zero in each case.</p><p>For the variance we can solve for the fixed points directly. For SGD,</p><formula xml:id="formula_14">V * SGD = (1 − γA) 2 V * SGD + γA 2 Σ, ⇒ V * SGD = γ 2 A 2 Σ I − (I − γA) 2 .</formula><p>For Lookahead, we have,</p><formula xml:id="formula_15">V * LA = [1 − α + α(I − γA) k ] 2 V * LA + α 2 k−1 i=0 (I − γA) 2i γ 2 A 2 Σ ⇒ V * LA = α 2 k−1 i=0 (I − γA) 2i I − [(1 − α)I + α(I − γA) k ] 2 γ 2 A 2 Σ ⇒ V * LA = α 2 (I − (I − γA) 2k ) I − [(1 − α)I + α(I − γA) k ] 2 γ 2 A 2 Σ I − (I − γA) 2</formula><p>where for the final equality, we used the identity k 0 a i = (1 − a k )/(1 − a). Some standard manipulations of the denominator on the first term lead to the final solution,</p><formula xml:id="formula_16">V * LA = α 2 (I − (I − γA) 2k ) α 2 (I − (I − γA) 2k ) + 2α(1 − α)(I − (I − γA) k ) γ 2 A 2 Σ 2 I − (I − γA) 2</formula><p>For the same learning rate, Lookahead will achieve a smaller loss as the variance is reduced more. However, the convergence speed of the expectation term will be slower as we must compare 1 − α + α(I − γA) k to (I − γA) k and the latter is always smaller for α &lt; 1. In our experiments, we observe that Lookahead typically converges much faster than its inner optimizer. We speculate that the learning rate for the inner optimizer is set sufficiently high such that the variance reduction term is more important-this is the more common regime for neural networks that attain high validation accuracy, as higher initial learning rates are used to overcome the short-horizon bias <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Comparing convergence rates</head><p>In <ref type="figure" target="#fig_3">Figure 3</ref> we compared the convergence rates of SGD and Lookahead. We specified the eigenvalues of A according to the worst-case model from Li <ref type="bibr" target="#b20">[21]</ref> (also used by Wu et al. <ref type="bibr" target="#b42">[43]</ref>) and set Σ = A −1 . We computed the expected loss (Equation 5) for learning rates in the range (0, 1) for SGD and Lookahead with α ∈ (0, 1], with k = 5, at time T = 1000 (by unrolling the above dynamics). We computed the variance fixed point for each learning rate under each optimizer and use this value to compute the optimal loss. Finally, we plot the difference between the expected loss at T and the final loss, as a function of the final loss. This allows us to compare the convergence performance between SGD and Lookahead optimization settings which converge to the same solution.</p><p>Further convergence plots In <ref type="figure" target="#fig_0">Figure 11</ref> we present additional plots comparing the convergence performance between SGD and Lookahead. In (a) we show the convergence of Lookahead for a single choice of α, where our method is able to outperform SGD even for this fixed value. In (b) we show the convergence after only a few updates. Here SGD outperforms Lookahead for some smaller choices of α. This is because SGD is able to make progress on the expectation more rapidly and reduces this part of the loss quickly -this is related to the short-horizon bias phenomenon <ref type="bibr" target="#b42">[43]</ref>. However, even with only a few updates there are choices of α which are able to outperform SGD.</p><p>We also measured the optimal expected loss after some finite time horizon for both Lookahead and SGD in <ref type="figure" target="#fig_0">Figure 12</ref>. We performed a fine-grained grid search over the learning rate for SGD and both the learning rate and α for Lookahead (keeping k = 5 fixed). We evaluated 100 learning rates equally spaced on a log-scale in the range [10 −4 , 10 −1 ]. For Lookahead, we additionally evaluated 50 α values equally spaced on a log-scale in the range [10 −4 , 1]. For every time horizon, there exists settings of Lookahead that outperform SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Deterministic quadratic convergence analysis</head><p>Here we present additional details on the quadratic convergence analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Lookahead as a dynamical system</head><p>As in the main text, we will assume that the optimum lies at θ * = 0 for simplicity, but the argument easily generalizes. Here we consider the more general case of a quadratic function f (x) = 1 2 x T Ax. We use η to denote the CM learning rate and β for it's momentum coefficient.  First we can stack together a full set of fast weights and write the following,</p><formula xml:id="formula_17">    θ t,0 θ t−1,k . . . θ t−1,1     = LB (k−1) T     θ t−1,0 θ t−2,k . . . θ t−2,1    </formula><p>Here, L represents the Lookahead interpolation, B represents the update corresponding to classical momentum in the inner-loop and T is a transition matrix which realigns the fast weight iterates.</p><p>Each of these matrices takes the following form,</p><formula xml:id="formula_18">L =        αI 0 · · · 0 (1 − α)I I 0 · · · · · · 0 0 I . . . . . . . . . . . . . . . . . . 0 . . . 0 · · · 0 I 0        B =        (1 + β)I − ηA −βI 0 · · · 0 I 0 · · · · · · 0 0 I . . . . . . . . . . . . . . . . . . 0 . . . 0 · · · 0 I 0        T =            I − ηA βI −βI 0 · · · 0 I 0 · · · · · · 0 . . . 0 I . . . · · · . . . . . . . . . . . . . . . 0 . . . . . . . . . · · · 0 I 0 0 0 · · · 0 0 I 0           </formula><p>Each matrix consists of four blocks. The bottom left block is always an identity matrix that shifts the iterates along one index. The bottom right column is all zeros with the top-right column being non-zero only for L which applies the Lookahead interpolation. The top left row is used to apply the Lookahead/CM updates in each matrix.</p><p>After computing the appropriate product of these matrices, we can use standard solvers to compute the eigenvalues which bound the convergence of the linear dynamical system (see e.g. Lessard et al. <ref type="bibr" target="#b19">[20]</ref> for an exposition). Finally, note that because this linear dynamical systems corresponds to k updates (or one slow-weight update) we must compute the k th root of the eigenvalues to recover the correct convergence bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Optimal slow weight step size</head><p>We present the proof of Proposition 1 for the optimal slow weight step size α * .</p><p>Proof. We compute the derivative with respect to α</p><formula xml:id="formula_19">∇ α L(θ t,0 + α(θ t,k − θ t,0 )) = (θ t,k − θ t,0 ) T A(θ t,0 + α(θ t,k − θ t,0 )) − (θ t,k − θ t,0 ) T b</formula><p>Setting the derivative to 0 and using b = Aθ * :</p><formula xml:id="formula_20">α[(θ t,k − θ t,0 ) T A(θ t,k − θ t,0 )] = (θ t,k − θ t,0 ) T A(θ * − θ t,0 ) (12) =⇒ α * = arg min α L(θ t,0 + α(θ t,k − θ t,0 )) = (θ t,0 − θ * ) T A(θ t,0 − θ t,k ) (θ t,0 − θ t,k ) T A(θ t,0 − θ t,k )<label>(13)</label></formula><p>We approximate the optimal θ * = θ t,k −Â −1∇ L(θ t,k ), since the Fisher can be viewed as an approximation to the Hessian <ref type="bibr" target="#b24">[25]</ref>. Stochastic gradients are computed on mini-batches used in training so as to not incur additional computational cost. Because the algorithm with fixed α performs so well, we only did preliminary experiments with an adaptive α. We note that the approximation is greedy and incorporating priors on noise and curvature is an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental setup</head><p>Here we present additional details on the experiments appearing in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 CIFAR-classification</head><p>We run every experiment with three random seeds using the publicly available setup from <ref type="bibr" target="#b5">[6]</ref>.</p><p>A reviewer helpfully noted that this implementation of ResNet-18 has wider channels and more parameters than the original. For future work, it would be better to follow the original ResNet architecture for CIFAR classification. However, we observed consistently better convergence with Lookahead across different architectures and have results with other architectures in Appendix D.3. Our plots show the mean value with error bars of one standard deviation. We use a standard training procedure that is the same as that of Zagoruyko and Komodakis <ref type="bibr" target="#b45">[46]</ref>. That is, images are zero-padded with 4 pixels on each side and then a random 32 × 32 crop is extracted and mirrored horizontally 50% of the time. Inputs are normalized with per-channel means and standard deviations. Lookahead is evaluated on the slow weights of its inner optimizer. To make this evaluation fair, we evaluate the training loss at the end of each epoch by iterating through the training set again, without performing any gradient updates.</p><p>We conduct hyperparameter searches over learning rates and weight decay values, making choices based on final validation performance. For SGD, we set the momentum to 0.9 and sweep over the learning rates {0.01, 0.03, 0.05, 0.1, 0.2, 0.3} and weight decay values of {0.0003, 0.001, 0.003}. The best choice was a learning rate of 0.05 and weight decay of 0.001. We found AdamW <ref type="bibr" target="#b21">[22]</ref> to perform better than Adam and refer it to as Adam throughout our CIFAR experiment section. For Lookahead, we set the inner optimizer SGD learning rate to 0.1 and do a grid search over α = {0.2, 0.5, 0.8} and k = {5, 10}, with α = 0.8 and k = 5 performing best (though the choice is fairly robust). We report the verison of Lookahead that resets momentum in our CIFAR experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 ImageNet</head><p>For the baseline algorithm, we used SGD with a heavy ball momentum of 0.9. We swept over learning rates in the set: {0.01, 0.02, 0.05, 0.1, 0.2, 0.3} and selected a learning rate of 0.1 because it had the highest final validation accuracy.</p><p>We directly wrapped Lookahead around the settings provided in the official PyTorch repository repository with k = 5 and α = 0.5 (where SGD has a learning rate of 0.1 in the inner loop).</p><p>Observing the improved convergence of our algorithm, we tested Lookahead with the aggressive learning rate decay schedule (decaying at the 30th, 48th, and 58th epochs). We run our experiments on 4 Nvidia P100 GPUs with a batch size of 256 and weight decay of 1e-4. We used the same settings in the ResNet-152 experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Language modeling</head><p>For the language modeling task we used the model and code provided by Merity et al. <ref type="bibr" target="#b26">[27]</ref>. We used the default settings suggested in this codebase at the time of usage which we report here. The LSTM we trained had 3 layers each containing 1150 hidden units. We used word embeddings of dimension 400. Within each hidden layer we apply dropout with probability 0.3 and the input embedding layers use dropout with probability 0.65. We applied dropout to the embedding layer itself with probability 0.1. We used the weight drop method proposed in Merity et al. <ref type="bibr" target="#b26">[27]</ref> with probability 0.5. We adopt the regularization proposed in section 4.6 in Merity et al. <ref type="bibr" target="#b26">[27]</ref>: RNN activations have L2 regularization applied to them with a scaling of 2.0, and temporal activation regularization is applied with scaling 1.0. Finally, all weights receive a weight decay of 1.2e-6.</p><p>We trained the model using variable sequence lengths and batch sizes of 80. We apply gradient clipping of 0   <ref type="figure" target="#fig_0">Figure 14</ref>: Evolution of test accuracy on CIFAR-10 and ImageNet.</p><p>For this task, α = 0.5 or α = 0.8 and k = 5 or k = 10 worked best. As in our other experiments, we found that Lookahead was largely robust to different choices of k and α. We expect that we could achieve even better results with Lookahead if we jointly optimized the hyperparameters of Lookahead and the underlying optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Neural machine translation</head><p>For this task, we trained on a single TPU core that has 8 workers each with a minibatch size of 2048. We use the default hyperparameters for Adam <ref type="bibr" target="#b41">[42]</ref> and AdaFactor <ref type="bibr" target="#b39">[40]</ref> in the experiments. For Lookahead, we did a minor grid search over the learning rate {0.02, 0.04, 0.06} and k = {5, 10} while setting α = 0.5. We found learning rate 0.04 and k = 10 worked best. After we train those models for 250k steps, they can all reach around 27 BLEU on Newstest2014 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Inner Optimizer State</head><p>Throughout our paper, we maintain the state of our inner optimizer for simplicity. For SGD with heavy-ball momentum, this corresponds to preserving the momentum. Here, we present a sensitivity study by comparing the convergence of Lookahead when maintaining the momentum, interpolating the momentum, and resetting the momentum. All three improve convergence versus SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Validation Accuracy</head><p>We present curves corresponding to the evolution of validation accuracy during training on CIFAR and ImageNet datasets. Though not the focus of our work, we find that faster convergence in training loss does in fact correspond to better validation performance on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Comparison to Stochastic Weight Averaging</head><p>In this subsection, we elaborate on differences between Stochastic Weight Averaging (SWA) <ref type="bibr" target="#b13">[14]</ref> and Lookahead, showing that they serve different purposes but can be complementary. First, SWA and the general family of methods that perform tail averaging <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34]</ref> requires a choice of when to begin averaging. A choice that is either too early or too late can be detrimental to performance. This is illustrated in <ref type="figure" target="#fig_0">Figure 15</ref>, where we plot comparisons of the test accuracy of three runs of Lookahead and SWA, using SGD in the inner loop of both algorithms. We use the suggested schedule from the SWA paper, which has higher learning rates than is typical at the end of training. SWA achieves better performance when initialized from epoch 10 compared to epoch 1, due to poor performance from the earlier models. In contrast, Lookahead is used from initialization and does not have this tail averaging start decision. While SWA performs better during the intermediate stages of training (since it is loosely approximating an ensemble by averaging the weights of multiple models), Lookahead with its variance reduction properties achieves better final performance, even with the modified learning rate schedule. Lookahead also computes an exponential moving average of its fast weights rather than the artithmetic average of SWA, which increases the emphasis on recent proposals of weights <ref type="bibr" target="#b24">[25]</ref>.  <ref type="figure" target="#fig_0">Figure 15</ref>: Test Accuracy on CIFAR-100 with SWA and Lookahead (PreResNet-110). We follow exactly the hyperparameter settings in their repository and also run Lookahead with α = 0.8 and k = 10. Note that the learning rate schedule uses a learning rate that is higher than is typical at the end of training.</p><p>We do believe that Lookahead is complementary to SWA and traditional techniques for ensembling models. To this end, we perform three runs with Wide ResNet-28-10 [46] on CIFAR-100 with SWA and compare two choices of the inner loop algorithm of SWA. The inner loop algorithms are SGD (as in <ref type="bibr" target="#b13">[14]</ref>) and Lookahead wrapped around SGD. The runs with Lookahead achieve higher test accuracy throughout training and in the weight averaged network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Left) Visualizing Lookahead (k = 10) through a ResNet-32 test accuracy surface at epoch 100 on CIFAR-100. We project the weights onto a plane defined by the first, middle, and last fast (inner-loop) weights. The fast weights are along the blue dashed path. All points that lie on the plane are represented as solid, including the entire Lookahead slow weights path (in purple). Lookahead (middle, bottom right) quickly progresses closer to the minima than SGD (middle, top right) is able to. (Right) Pseudocode for Lookahead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>CIFAR-10 training loss with fixed and adaptive α. The adaptive α is clipped between [α low , 1]. (Left) Adam learning rate = 0.001. (Right) Adam learning rate = 0.003.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparing expected optimization progress between SGD and Lookahead(k = 5) on the noisy quadratic model. Each vertical slice compares the convergence of optimizers with the same final loss values. For Lookahead, convergence rates for 100 evenly spaced α values in the range (0, 1] are overlaid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Quadratic convergence rates (1 − ρ) of classical momentum versus Lookahead wrapping classical momentum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Performance comparison of the different optimization algorithms. (Left) Train Loss on CIFAR-100. (Right) CIFAR ResNet-18 validation accuracies with various optimizers. We do a grid search over learning rate and weight decay on the other optimizers (details in appendix C). Lookahead and Polyak are wrapped around SGD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) Training Loss on Transformer. Adam and AdaFactor both use a linear warmup scheme described in Vaswani et al.<ref type="bibr" target="#b41">[42]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Optimization performance on Penn Treebank and WMT-14 machine translation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>CIFAR-10 Train Loss: Different momentum We fix Lookahead parameters and evaluate on different inner optimizers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Convergence of SGD and Lookahead on the noisy quadratic model. (a): We show the convergence of Lookahead with a single fixed choice of α = 0.4. (b): We compare the early stage performance of Lookahead to SGD over a range of α values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Expected loss of SGD and Lookahead with (constant-through-time) hyperparameters tuned to be optimal at a finite time horizon. At each finite horizon (x-axis) we perform a grid search to find the best expected loss of each optimizer. Lookahead dominates SGD over all time horizons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>For Adam, we do a grid search on learning rate of {1e-4, 3e-4, 1e-3, 3e-3, 1e-2} and weight decay values of {0.1, 0.3, 1, 3}. The best choice was a learning rate of 3e-4 and weight decay of 1. For Polyak averaging, we compute the moving average of SGD use the best weight decay from SGD and sweep over the learning rates {0.05, 0.1, 0.2, 0.3, 0.5}. The best choice was a learning rate of 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 :</head><label>16</label><figDesc>Test Accuracy on CIFAR-100 with SWA and Lookahead (Wide ResNet-28-10). Following Izmailov et al, SWA is started at epoch 161. We plot the accuracy throughout training (left) and the accuracy of the SWA network (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Katyusha checkpoints parameters during optimization. Within each inner loop step, the parameters are pulled back towards the latest checkpoint. Lookahead computes the pullback only at the end of the inner loop and the gradient updates do not utilize the SVRG correction (though this would be possible). While Katyusha has theoretical guarantees in the convex optimization setting, the SVRG-based update does not work well for neural networks [4]. SGD 95.23 ± .19 78.24 ± .18 POLYAK 95.26 ± .04 77.99 ± .42 ADAM 94.84 ± .16 76.88 ± .39 LOOKAHEAD 95.27 ± .06 78.34 ± .05 CIFAR Final Validation Accuracy.</figDesc><table><row><cell>Train Loss</cell><cell>0.10 0.15 0.20 0.25 0.30</cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR-10 Train Loss</cell><cell></cell><cell cols="2">Polyak Adam SGD Lookahead</cell><cell>OPTIMIZER</cell><cell>CIFAR-10 CIFAR-100</cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100 Epoch</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell></row></table><note>Anderson acceleration [2] and other related extrapolation techniques [3] have a similar flavor to Lookahead. These methods keep track of all iterates within an inner loop and then compute some linear combination which extrapolates the iterates towards their fixed point. This presents additional</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>TOP 1 75.13 74.43 EPOCH 50 -TOP 5 92.22 92.15 EPOCH 60 -TOP 1 75.49 75.15 EPOCH 60 -TOP 5 92.53 92.56</figDesc><table><row><cell>OPTIMIZER</cell><cell>LA</cell><cell>SGD</cell></row><row><cell>EPOCH 50 -</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Top-1 and Top-5 single crop validation accuracies on ImageNet.Figure 6: ImageNet training loss. The asterisk denotes the aggressive learning rate decay schedule, where LR is decayed at iteration 30, 48, and 58. We report validation accuracies for this schedule.</figDesc><table><row><cell></cell><cell>3.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Adam + warmup</cell></row><row><cell></cell><cell>3.25</cell><cell></cell><cell></cell><cell></cell><cell>AdaFactor</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Lookahead + Adam + warmup</cell></row><row><cell></cell><cell>3.00</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Lookahead + Adam</cell></row><row><cell>Training Loss</cell><cell>2.25 2.50 2.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.50</cell><cell>0</cell><cell>10000</cell><cell>20000</cell><cell>30000</cell><cell>40000</cell><cell>50000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Inner Loop (Fast Weights) Steps</cell><cell></cell></row></table><note>(a) Training perplexity of LSTM models trained on the Penn Treebank dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>LSTM training, validation, and test perplexity on the Penn Treebank dataset.</figDesc><table><row><cell cols="2">OPTIMIZER TRAIN</cell><cell>VAL.</cell><cell>TEST</cell></row><row><cell>SGD</cell><cell>43.62</cell><cell>66.0</cell><cell>63.90</cell></row><row><cell>LA(SGD)</cell><cell>35.02</cell><cell cols="2">65.10 63.04</cell></row><row><cell>ADAM</cell><cell>33.54</cell><cell cols="2">61.64 59.33</cell></row><row><cell>LA(ADAM)</cell><cell>31.92</cell><cell cols="2">60.28 57.72</cell></row><row><cell>POLYAK</cell><cell>-</cell><cell cols="2">61.18 58.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Transformer Base Model trained for 50k steps on WMT English-to-German. "Adam-" denote Adam without learning rate warm-up.</figDesc><table><row><cell>OPTIMIZER</cell><cell cols="2">NEWSTEST13 NEWSTEST14</cell></row><row><cell>ADAM</cell><cell>24.6</cell><cell>24.6</cell></row><row><cell>LA(ADAM)</cell><cell>24.68</cell><cell>24.70</cell></row><row><cell>LA(ADAM-)</cell><cell>24.3</cell><cell>24.4</cell></row><row><cell>ADAFACTOR</cell><cell>24.17</cell><cell>24.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1. The slow weights step recovers the outer loop variance and restores the test accuracy.</figDesc><table><row><cell>Train Loss</cell><cell>0.4 0.6 0.8 1.0</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Train Loss on CIFAR-100</cell><cell></cell><cell cols="2">SGD Baseline =0.5, k=5 =0.8, k=5 =0.5, k=10 =0.8, k=10</cell><cell>α K 5 10</cell><cell>0.5 78.24 ± .02 78.19 ± .22</cell><cell>0.8 78.27 ± .04 77.94 ± .22</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100 Epoch</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Visualizing Lookahead accuracy for 60 fast weight updates. We plot the test accuracy after every update (the training accuracy and loss behave similarly). The inner loop update tends to degrade both the training and test accuracy, while the interpolation recovers the original performance.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Per Update Test Accuracy on Epoch 65</cell><cell></cell><cell></cell></row><row><cell>0.938</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.936</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.928 0.930 0.932 0.934 Test accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.924 0.926</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fast weights Slow weights</cell></row><row><cell>0</cell><cell>10</cell><cell>20</cell><cell>30 Fast weight updates</cell><cell>40</cell><cell>50</cell><cell>60</cell></row><row><cell>Figure 10:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>All settings have higher validation accu- racy than SGD (77.72%) Figure 9: CIFAR-100 train loss and final test accuracy with various k and α.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>.25 to all optimizers. During training, if validation loss has not decreased for 15 epochs then we reduce the learning rate by half. Before applying Lookahead, we completed a grid search over the Adam and SGD optimizers to find competitive baseline models. For SGD we did not apply momentum and searched learning rates in the range {50, 30, 10, 5, 2.5, 1, 0.1}. For Adam we kept the default momentum values of (β 1 , β 2 ) = (0.9, 0.999) and searched over learning rates in the range {0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}. We chose the best model by picking the model which achieved the best validation performance at any point during training.After picking the best SGD/Adam hyperparameters we trained the models again using Lookahead with the best baseline optimizers for the inner-loop. We tried using k = {5, 10, 20} inner-loop updates and α = {0.2, 0.5, 0.8} interpolation coefficients. Once again, we reported Lookahead's final performance by choosing the parameters which gave the best validation performance during training.MAINTAIN 95.15 ± .08 INTERPOLATE 95.16 ± .13 RESET 94.91 ± .05</figDesc><table><row><cell>Train Loss</cell><cell>0.15 0.20 0.25 0.30 0.35 0.40</cell><cell></cell><cell></cell><cell></cell><cell cols="6">Inner Optimizer State Ablation Maintain Momentum Interpolate Momentum Reset Momentum SGD Baseline</cell><cell>OPTIMIZER</cell><cell>CIFAR-10</cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100 Epoch</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>CIFAR Final Validation Accuracy.</figDesc><table><row><cell></cell><cell></cell><cell cols="9">Figure 13: Evaluation of maintaining, interpolating, and resetting momentum on CIFAR-10</cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR-10 Test Accuracy</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy</cell><cell>0.75 0.80 0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Lookahead SGD Adam Polyak</cell></row><row><cell></cell><cell>0.70</cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100 Epoch</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our open source implementation is available at https://github.com/michaelrzhang/lookahead.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Classical momentum's iterates are invariant to translations and rotations (see e.g. Sutskever et al.<ref type="bibr" target="#b40">[41]</ref>) and Lookahead's linear interpolation is also invariant to such changes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We refer to SGD with heavy ball momentum<ref type="bibr" target="#b32">[33]</ref> as SGD.<ref type="bibr" target="#b3">4</ref> Implementation available at https://github.com/pytorch/examples/tree/master/imagenet.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We'd like to thank Roger Grosse, Guodong Zhang, Denny Wu, Silviu Pitis, David Madras, Jackson Wang, Harris Chan, and Mufan Li for helpful comments on earlier versions of this work. We are also thankful for the many helpful comments from anonymous reviewers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The first direct acceleration of stochastic gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeyuan Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katyusha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 49th Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1200" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Iterative procedures for nonlinear integral equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald G Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="560" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Extrapolation methods: theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Brezinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaglia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the ineffectiveness of variance reduced optimization for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10026</idno>
		<title level="m">Loss surfaces, mode connectivity, and fast ensembling of dnns</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Why momentum really works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00006</idno>
		<ptr target="http://distill.pub/2017/momentum" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David C</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<title level="m">Averaging weights leads to wider optima and better generalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2007</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haidong</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11205</idno>
		<title level="m">Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis and design of optimization algorithms via integral quadratic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Packard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="95" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sharpness in rates of convergence for cg and symmetric lanczos methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren-Cang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Fixing weight decay regularization in adam</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00325</idno>
		<title level="m">Aggregated momentum: Stability through passive damping</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1193</idno>
		<title level="m">New insights and perspectives on the natural gradient method</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2408" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing lstm language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geneviève</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Neural networks: tricks of the trade</title>
		<imprint>
			<publisher>springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7700</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate o (1/kˆ2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yurii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Dokl. akad. nauk Sssr</title>
		<imprint>
			<biblScope unit="volume">269</biblScope>
			<biblScope unit="page" from="543" to="547" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive restart for accelerated gradient schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Candes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of computational mathematics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="715" to="732" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Second-order optimization method for large mini-batch: Training resnet-50 on imagenet in 35 epochs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Osawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohei</forename><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichiro</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Naruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rio</forename><surname>Yokota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Matsuoka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient estimations from a slowly convergent robbins-monro process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ruppert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
		<respStmt>
			<orgName>Cornell University Operations Research and Industrial Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">No more pesky learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Scieur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09639</idno>
		<title level="m">Nonlinear acceleration of deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Scieur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00370</idno>
		<title level="m">Nonlinear acceleration of cnns</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04235</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Understanding short-horizon bias in stochastic meta-optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02021</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Yap Georgios Piliouras Vijay Chandrasekhar Yasin Yazıcı, Chuan-Sheng Foo. The unusual effectiveness of averaging in gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan Winkler Kim-Hui</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04498</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet training in minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Conference on Parallel Processing</title>
		<meeting>the 47th International Conference on Parallel Processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushant</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04164</idno>
		<title level="m">Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
