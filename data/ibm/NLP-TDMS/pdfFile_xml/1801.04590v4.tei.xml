<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frame-Recurrent Video Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
							<email>msajjadi@tue.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems 2 Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
							<email>ravitejavemu@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems 2 Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems 2 Google</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Frame-Recurrent Video Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to generate high-quality images. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as a large number of separate multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, increasing the computational cost, and 2) each output frame is estimated independently conditioned on the input frames, limiting the system's ability to produce temporally consistent results.</p><p>In this work, we propose an end-to-end trainable framerecurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Super-resolution is a classic problem in image processing that addresses the question of how to reconstruct a highresolution (HR) image from its downscaled low-resolution (LR) version. With the rise of deep learning, super-resolution has received significant attention from the research community over the past few years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. While high-frequency details need to be reconstructed exclusively from spatial statistics in the case of single image superresolution, temporal relationships in the input can be ex- * This work was done when Mehdi S. M. Sajjadi was interning at Google.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head><p>Our result HR frame ploited to improve reconstruction for video super-resolution. It is therefore imperative to combine the information from as many LR frames as possible to reach the best video superresolution results.</p><p>The latest state-of-the-art video super-resolution methods approach the problem by combining a batch of LR frames to estimate a single HR frame, effectively dividing the task of video super-resolution into a large number of separate multiframe super-resolution subtasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref>. However, this approach is computationally expensive since each input frame needs to be processed several times. Furthermore, generating each output frame separately reduces the system's ability to produce temporally consistent frames, resulting in unpleasing flickering artifacts.</p><p>In this work, we propose an end-to-end trainable framerecurrent video super-resolution (FRVSR) framework to address the above issues. Instead of estimating each video frame separately, we use a recurrent approach that passes the previously estimated HR frame as an input for the following iteration. Using this recurrent architecture has several benefits. Each input frame needs to be processed only once, reducing the computational cost. Furthermore, information from past frames can be propagated to later frames via the HR estimate that is recurrently passed through time. Passing the previous HR estimate directly to the next step helps the model to recreate fine details and produce temporally consistent videos.</p><p>To analyze the performance of the proposed framework, we compare it with strong single image and video superresolution baselines using identical neural networks as building blocks. Our extensive set of experiments provides insights into how the performance of FRVSR varies with the number of recurrent steps used during training, the size of the network, and the amount of noise, aliasing or compression artifacts present in the LR input. The proposed approach clearly outperforms the baselines under various settings both in terms of quality and efficiency. Finally, we also compare FRVSR with several existing video super-resolution approaches and show that it significantly outperforms the current state of the art on a standard benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Our contributions</head><p>• We propose a recurrent framework that uses the HR estimate of the previous frame for generating the subsequent frame, leading to an efficient model that produces temporally consistent results.</p><p>• Unlike existing approaches, the proposed framework can propagate information over a large temporal range without increasing computations.</p><p>• Our system is end-to-end trainable and does not require any pre-training stages.</p><p>• We perform an extensive set of experiments to analyze the proposed framework and relevant baselines under various different settings.</p><p>• We show that the proposed framework significantly outperforms the current state of the art in video super-resolution both qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Video super-resolution</head><p>Let I LR t ∈ [0, 1] H×W ×C denote the t-th LR video frame obtained by downsampling the original HR video frame I HR t ∈ [0, 1] sH×sW ×C by scale factor s. Given a set of consecutive LR video frames, the goal of video super-resolution is to generate HR estimates I est t that approximate the original HR frames I HR t under some metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Related work</head><p>Super-resolution is a classic ill-posed inverse problem with approaches ranging from simple interpolation methods such as Bilinear, Bicubic and Lanczos <ref type="bibr" target="#b8">[9]</ref> to example-based super-resolution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>, dictionary learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43]</ref>, and self-similarity approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>. We refer the reader to Milanfar <ref type="bibr" target="#b29">[30]</ref> and Nasrollahi and Moeslund <ref type="bibr" target="#b30">[31]</ref> for extensive overviews of prior art up to recent years.</p><p>The recent progress in deep learning, especially in convolutional neural networks, has shaken up the field of superresolution. After Dong et al. <ref type="bibr" target="#b4">[5]</ref> reached state-of-the-art results with shallow convolutional neural networks, many others followed up with deeper network architectures, advancing the field tremendously <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. Parallel efforts have studied alternative loss functions for more visually pleasing reconstructions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref>. Agustsson and Timofte <ref type="bibr" target="#b0">[1]</ref> provide a recent survey on the current state of the art in single image super-resolution.</p><p>Video and multi-frame super-resolution approaches combine information from multiple LR frames to reconstruct details that are missing in individual frames which can lead to higher quality results. Classical video and multi-frame super-resolution methods are generally formulated as optimization problems that are computationally very expensive to solve <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Most of the existing deep learning-based video superresolution methods divide the task of video super-resolution into multiple separate sub-tasks, each of which generates a single HR output frame from multiple LR input frames. Kappeler et al. <ref type="bibr" target="#b19">[20]</ref> warp video frames I LR t−1 and I LR t+1 onto the frame I LR t using the optical flow method of Drulea and Nedevschi <ref type="bibr" target="#b7">[8]</ref>, concatenate the three frames and pass them through a convolutional neural network that produces the output frame I est t . Caballero et al. <ref type="bibr" target="#b2">[3]</ref> follow the same approach but replace the optical flow model with a trainable motion compensation network. Makansi et al. <ref type="bibr" target="#b28">[29]</ref> follow an approach similar to <ref type="bibr" target="#b2">[3]</ref> but combine warping and mapping to HR space into a single step.</p><p>Tao et al. <ref type="bibr" target="#b38">[39]</ref> rely on a batch of up to 7 input LR frames to estimate a single HR frame. After computing the motion from neighboring input frames to I LR t , they map the frames onto high-resolution grids. In a final step, they run an encoder-decoder style network with a Conv-LSTM in the core yielding I est t . Liu et al. <ref type="bibr" target="#b27">[28]</ref> process up to 5 LR frames using different numbers of input frames (I LR t ), (I LR t−1 , I LR t , I LR t+1 ), and (I LR t−2 , . . . , I LR t+2 ) simultaneously to produce separate HR estimates that are aggregated in a final step with dynamic weights to produce a single output I est t . While a number of the above mentioned methods are endto-end trainable, the authors often note that they first pre-train each component before fine-tuning the system as a whole in a final step <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Huang et al. <ref type="bibr" target="#b16">[17]</ref> use a bidirectional recurrent architecture for video super-resolution with shallow networks but do not use any explicit motion compensation in their model. Recurrent architectures have also been used for other tasks such as video deblurring <ref type="bibr" target="#b22">[23]</ref> and stylization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>. While Kim et al. <ref type="bibr" target="#b22">[23]</ref> and Chen et al. <ref type="bibr" target="#b3">[4]</ref> pass on a feature representation to the next step, Gupta et al. <ref type="bibr" target="#b14">[15]</ref> pass the previous output frame to the next step to produce temporally consistent stylized videos in concurrent work. A recurrent approach for video super-resolution was proposed by Farsiu et al. <ref type="bibr" target="#b9">[10]</ref> more than a decade ago with motivations similar to ours. However, this approach uses an approximation of the Kalman filter for frame estimation and is constrained to translational motion.  <ref type="figure">Figure 2</ref>: Overview of the proposed FRVSR framework (left) and the loss functions used for training (right). After computing the flow F LR in LR space using FNet, we upsample it to F HR . We then use F HR to warp the HR-estimate of the previous frame I est t−1 onto the current frame. Finally, we map the warped previous outputĨ est t−1 to LR-space using the space-to-depth transformation and feed it to the super-resolution network SRNet along with the current input frame I LR t . For training the networks (shown in red), we apply a loss on I est t as well as an additional loss on the warped previous LR frame to aid FNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>After presenting an overview of the FRVSR framework in Sec. 3.1 and defining the loss functions used for training in Sec. 3.2, we justify our design choices in Sec. 3.3 and give details on the implementation and training procedure in Sec. 3.4 and 3.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">FRVSR Framework</head><p>The proposed framework is illustrated in <ref type="figure">Fig. 2</ref>. Trainable components (shown in red) include the optical flow estimation network FNet and the super-resolution network SRNet. To produce the HR estimate I est t , our model makes use of the current LR input frame I LR t , the previous LR input frame I LR t−1 , and the previous HR estimate I est t−1 . 1. Flow estimation: As a first step, FNet estimates the flow between the low-resolution inputs I LR t−1 and I LR t yielding the normalized low-resolution flow map</p><formula xml:id="formula_0">F LR = FNet(I LR t−1 , I LR t ) ∈ [−1, 1] H×W ×2<label>(1)</label></formula><p>that assigns a position in I LR t−1 to each pixel location in I LR t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Upscaling flow:</head><p>Treating the flow map F LR as an image, we upscale it using bilinear interpolation with scaling factor s which results in an HR flow-map</p><formula xml:id="formula_1">F HR = UP(F LR ) ∈ [−1, 1] sH×sW ×2 .<label>(2)</label></formula><p>3. Warping previous output: We use the high-resolution flow map F HR to warp the previously estimated image I est t−1 according to the optical flow from the previous frame onto the current frame.</p><formula xml:id="formula_2">Ĩ est t−1 = WP(I est t−1 , F HR )<label>(3)</label></formula><p>We implemented warping as a differentiable function using bilinear interpolation similar to Jaderberg et al. <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Mapping to LR space:</head><p>We map the warped previous outputĨ est t−1 to LR space using the space-to-depth transformation</p><formula xml:id="formula_3">S s : [0, 1] sH×sW ×C → [0, 1] H×W ×s 2 C<label>(4)</label></formula><p>which extracts shifted low-resolution grids from the image and places them into the channel dimension, see <ref type="figure" target="#fig_2">Fig. 3</ref> for an illustration. The operator can be formally described as</p><formula xml:id="formula_4">S s (I) i,j,k = I si+k%s, sj+(k/s)%s, k/s 2<label>(5)</label></formula><p>with zero-based indexing, modulus % and integer division /.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Super-Resolution:</head><p>In the final step, we concatenate the LR mapping of the warped previous outputĨ est t−1 with the current low-resolution input frame I LR t in the channel dimension, and feed the result I LR t ⊕ S s (Ĩ est t−1 ) to the super-resolution network SRNet.</p><p>Summary: The final estimate I est t of the framework is the output of the super-resolution network SRNet:</p><formula xml:id="formula_5">SRNet(I LR t ⊕S s (WP(I est t−1 , UP(FNet(I LR t−1 , I LR t ))))) (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss functions</head><p>We use two loss terms to train our model, see <ref type="figure">Fig. 2</ref>, right. The loss L sr is applied on the output of SRNet and is backpropagated through both SRNet and FNet:</p><formula xml:id="formula_6">L sr = ||I est t − I HR t || 2 2<label>(7)</label></formula><p>Since we do not have a ground truth optical flow for our video dataset, we calculate the spatial mean squared error on the warped LR input frames leading to the auxiliary loss term L flow to aid FNet during training.</p><formula xml:id="formula_7">L flow = || WP(I LR t−1 , F LR ) − I LR t || 2 2<label>(8)</label></formula><p>The total loss used for training is L = L sr + L flow .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Justifications</head><p>The proposed FRVSR framework is motivated by the following ideas:</p><p>• Processing the input video frames more than once leads to high computational cost. Hence, we avoid the sliding window approach and process each input frame only once.</p><p>• Having direct access to the previous output can help the network to produce a temporally consistent estimate for the following frame. Furthermore, through a recurrent architecture, the network can effectively use a large number of previous LR frames to estimate the HR frame (see Sec. 4.6) without tradeoffs in computational efficiency. For this reason, we warp the previous HR estimate and feed it to the super-resolution network.</p><p>• All computationally intensive operations should be performed in LR space. To this end, we map the previous HR estimate to LR space using the space-to-depth transformation, the inverse of which has been previously used by Shi et al. <ref type="bibr" target="#b35">[36]</ref> for upsampling. Running SRNet in LR space has the additional advantages of reducing the memory footprint and increasing the receptive field when compared to a superresolution network that would operate in HR space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation</head><p>The proposed model in <ref type="figure">Fig. 2</ref> is a flexible framework that leaves the choice for a specific network architecture open.</p><p>For our experiments, we use fully convolutional architectures for both FNet and SRNet, see <ref type="figure" target="#fig_3">Fig. 4</ref> for details. The design of our optical flow network FNet follows a simple encoderdecoder style architecture to increase the receptive field of the convolutions. For SRNet, we follow the residual architecture used by Sajjadi et al. <ref type="bibr" target="#b34">[35]</ref>, but replace the upsampling layers with transposed convolutions. Our choice of network architectures strikes a balance between quality and complexity. More recent methods for each subtask, especially more complex optical flow estimation methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref> can be easily incorporated and will lead to even better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and Inference</head><p>Our training dataset consists of 40 high-resolution videos (720p, 1080p and 4k) downloaded from vimeo.com. We downsample the original videos by a factor of 2 to have a clean high-resolution ground truth and extract patches of size 256×256 to generate the HR videos. To produce the input LR videos, we apply Gaussian blur to the HR frames and downscale them by sampling every 4-th pixel in each dimension for s = 4. Unless specified otherwise, we use a Gaussian blur with standard deviation σ = 1.5 (see Sec. 4.2).</p><p>To train the recurrent system, we extract clips of 10 consecutive frames from the videos using FFmpeg. We avoid cuts or large scene changes in the clips by making sure that the clips do not contain keyframes. All losses are backpropagated through both networks SRNet and FNet as well as through time, i.e., even the optical flow network for the first frame in a clip receives gradients from the super-resolution loss on the 10th frame. The model directly estimates the full RGB video frames, so no post-processing is necessary.</p><p>To estimate the first frame I est 1 in each clip, we initialize the previous estimate with a black image I est 0 = 0 at both training and testing time. The network will then simply upsample the input frame I LR 1 independently without additional prior data, similar to a single image super-resolution network. This has the additional benefit of encouraging the network to learn how to upsample single images independently early on during training instead of only relying on copying the previously generated imageĨ est t−1 . Our architecture is fully end-to-end trainable and does not require component-wise pre-training. Initializing the networks with the Xavier method <ref type="bibr" target="#b13">[14]</ref>, we train the model on 2 million batches of size 4 using the Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with a fixed learning rate of 10 −4 . Note that each sample in the batch is a set of 10 consecutive video frames, i.e., 40 video frames are passed through the networks in each iteration.</p><p>As training progresses, the optical flow estimation gradually improves which gives the super-resolution network higher-quality data to work with, helping it to rely more and more on the warped previous estimateĨ est t−1 . At the same time, the super-resolution network automatically learns to ignore the previous imageĨ est t−1 when the optical flow network cannot find a good correspondence between I LR t−1 and I LR t , e.g., for the very first video frame in each batch or for occluded areas. These cases can be detected by the network through a comparison of the low frequencies inĨ est t−1 with those in I LR t . In areas where they do not match, the network ignores the details inĨ est t−1 and simply upscales the current input frame independently. Once the model has been trained, it can be run on videos of arbitrary size and length due to the fully convolutional nature of the networks. To super-resolve a video, the network is applied frame by frame in a single feedforward pass. Benchmarks for runtimes of different model sizes are reported in Sec. <ref type="bibr" target="#b3">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>For a fair evaluation of the proposed framework on equal ground, we compare our model with two baselines that use the same optical flow and super-resolution networks. After presenting the baselines in Sec. 4.1, we extensively investigate the performance of FRVSR along with the baselines in Sec. 4.2-4.7. All experiments are done for the challenging case of 4x upsampling. For evaluation, we use a dataset of ten 3-5s high-quality 1080p video clips downloaded from youtube.com, which we refer to as YT10. Finally, we compare our models with current state-of-the-art methods on the standard Vid4 benchmark dataset <ref type="bibr" target="#b26">[27]</ref> in Sec. 4.8. Following Caballero et al. <ref type="bibr" target="#b2">[3]</ref>, we compute video PSNR on the brightness channel (ITU-R BT.601 YCbCr standard) using the mean squared error over all pixels in the video.</p><p>For more results and video samples, we refer the reader to our homepage at msajjadi.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>SISR: For the single image super-resolution baseline, we omit optical flow estimation from FRVSR and disregard any prior information, feeding only I LR t into SRNet. VSR: To compare with the sliding window approach for video super-resolution, we include this baseline in which a fixed number of input frames are processed to produce a single output frame. Following Kappeler et al. <ref type="bibr" target="#b19">[20]</ref> and Caballero et al. <ref type="bibr" target="#b2">[3]</ref>, we warp the previous and next input frames onto the current frame, concatenate all three frames and feed them to SRNet. Note that this model is computationally more expensive than FRVSR since it runs FNet <ref type="figure">Figure 5</ref>: Performance for different blur sizes on YT10. For all blur sizes, FRVSR gives the best results. The best PSNR of FRVSR (σ = 1.5) is 1.00 dB and 0.39 dB higher than the best of SISR (σ = 2.0) and VSR (σ = 1.5), respectively. twice for each frame while the computation for SRNet is almost identical to that of FRVSR.</p><p>As with FRVSR, both baselines are trained starting from a Xavier initialization <ref type="bibr" target="#b13">[14]</ref> using the Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with a fixed learning rate of 10 −4 . We trained the SISR network for 500K steps and VSR for 2 million steps, both using a batch size of 16. All networks are trained using the same dataset, and their losses on a validation dataset have converged at the end of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Blur size</head><p>As mentioned in Sec. 3.5, we apply Gaussian blur to the HR frames before downsampling them to generate the LR input for the network. While a smaller blur kernel results in aliasing, excessive blur leads to loss of high-frequency information in the input, making it harder to reconstruct finer details. To analyze how different approaches perform for blurry or aliased inputs, we trained SISR, VSR and FRVSR on video frames that have been downscaled using different values of standard deviation for the Gaussian blur ranging from σ = 0 to σ = 5, see <ref type="figure">Fig. 5</ref>. The proposed framework FRVSR significantly outperforms SISR and VSR on all blur sizes. It is interesting to note that SISR, which relies on a single LR image for upsampling, benefits the most from larger blur kernels compared to VSR and FRVSR which perform best with σ = 1.5. This is due to the fact that video super-resolution methods are able to blend information from multiple frames and therefore benefit from sharper inputs. In the remaining experiments, we use a value of σ = 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training clip length</head><p>Since FRVSR is a recurrent network, it can be trained on video clips of any length. To test the effect of the clip length used to train the network on its performance, we trained the same model using video clips of length 2, 5 and 10, yielding average video PSNR values of 31.60, 32.01 and 32.10 on YT10, respectively. These results show that the PSNR has already started to saturate with a clip length of 5 and going beyond 10 may not yield significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Degraded inputs</head><p>To see how different models perform under input degradations, we trained and evaluated FRVSR and the baselines using noisy and compressed input frames. <ref type="table" target="#tab_1">Table 1</ref> shows the performance of these models on YT10 for varying levels of Gaussian noise and JPEG compression quality. The proposed framework consistently outperforms both SISR and VSR by 0.36-0.91 dB and 0.18-0.48 dB, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Temporal consistency</head><p>Analyzing the temporal consistency of the results is best done by visual inspection of the video results. However, to compare the results on paper, we follow Caballero et al. <ref type="bibr" target="#b2">[3]</ref> and show temporal profiles, see <ref type="figure">Fig. 6</ref>. A temporal profile is generated by taking the same horizontal row of pixels from a number of frames in the video and stacking them vertically into a new image. Flickering in the video will show up as jitter and jagged lines in the temporal profile. While VSR produces sharper results than SISR, it still has significant flickering artifacts since each output frame is estimated separately. In contrast, FRVSR produces the most consistent results while containing even finer details in each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Range of information flow</head><p>Existing approaches to video super-resolution often use a fixed number (usually 3-7) of input frames to produce a single output frame. Increasing this number increases the SISR VSR FRVSR HR <ref type="figure">Figure 6</ref>: Temporal profiles for Calendar from Vid4. VSR yields finer details than SISR, but it's output still contains temporal inconsistencies (see red boxes). Only FRVSR is able to produce temporally consistent results while reproducing fine details. Best viewed on screen. maximum number of frames over which details can be propagated. While this can result in higher-quality videos, it also substantially increases the computational cost, leading to a tradeoff between efficiency and quality. In contrast, due to its recurrent nature, FRVSR can pass information across a large number of frames without increasing computations. <ref type="figure">Figure 7</ref> shows the performance of FRVSR as a function of the number of frames processed. In the normal mode (blue curve) in which a black frame is used as the first frame's previous HR estimate, the performance steadily improves as more frames are processed and it plateaus at 12 frames. When we replace the first previous HR estimate with the corresponding groundtruth HR frame (red curve), FRVSR carries the high-frequency details across a large number of frames and performs better than the normal mode even after 50 frames.</p><p>To investigate the maximum effective range of information flow, we start the same model at different input frames in the same video and compare the performance. <ref type="figure" target="#fig_4">Figure 8</ref> shows such a comparison for the Foliage video from Vid4. As we can see, the gap between the curves for the models that start at frame 1 and frame 11 only closes towards the end of the clip, showing that FRVSR is propagating information over more than 30 frames. To propagate details over such a large range, previous state-of-the-art methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref> would have to process an inhibiting number of input frames for each output image, which would be computationally infeasible. <ref type="figure">Figure 7</ref>: Performance of FRVSR on YT10 as a function of the number of previous frames processed. In the normal mode (blue), PSNR increases up to 12 frames, after which it remains stable. When the first HR image is given (red), FRVSR propagates high-frequency details across a large number of frames and performs better than the normal mode even after 50 frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Network size and computational efficiency</head><p>To see how the performance of different models varies with the size of the network, we trained and evaluated FRVSR and the baselines with different numbers of residual blocks and convolution filters in SRNet, see <ref type="figure" target="#fig_5">Fig. 9</ref>. It is interesting to note that the video super-resolution models FRVSR and VSR clearly benefit from larger models while the performance of SISR does not change significantly beyond 5 residual blocks. We can also see that FRVSR achieves better results than VSR despite being faster: The FRVSR models with 5 residual blocks outperform the VSR models with 10 residual blocks, and the FRVSR models with 3 residual blocks outperform the VSR models with 5 residual blocks for the same number of convolution filters.</p><p>With our unoptimized TensorFlow implementation on an Nvidia P100, producing a single Full HD frame for 4x up- scaling takes 74ms for FRVSR with 3 residual blocks and 64 filters, and 191ms for FRVSR with 10 blocks and 128 filters. <ref type="table" target="#tab_3">Table 2</ref> compares the proposed FRVSR approach with various state-of-the-art video super-resolution approaches on the standard Vid4 benchmark dataset by PSNR and SSIM. We report results for two FRVSR networks: FRVSR 10-128, which is our best model with 10 residual blocks and 128 convolution filters, and FRVSR 3-64, which is our most efficient model with only 3 residual blocks and 64 convolution filters. For the baselines SISR and VSR, we report their best results which correspond to 10 residual blocks and 128 convolution filters. We also include RAISR <ref type="bibr" target="#b33">[34]</ref> as an off-the-shelf single image super-resolution alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Comparison with prior art</head><p>For all competing methods except <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>, we used the output images provided by the corresponding authors to compute PSNR and SSIM. We did not use the first and last two frames in our evaluation since Liu et al. <ref type="bibr" target="#b27">[28]</ref> do not produce outputs for these frames. Also, for each video, we removed border regions such that the LR input image is a multiple of 8. For <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>, we use the PSNR and SSIM values reported in the respective publications since we could not confirm them independently. For <ref type="bibr" target="#b33">[34]</ref>, we used the models provided by the authors to generate the output images.</p><p>As shown in Tab. 2, FRVSR outperforms the current state of the art by more than 0.5 dB. In fact, even our most efficient model FRVSR 3-64 produces state-of-the-art results by PSNR and beats all previous neural network-based methods by SSIM. It it interesting that our small model, despite being much more efficient, produces results that are very close to the much larger model VSR 10-128 on the Vid4 dataset. <ref type="figure" target="#fig_0">Figure 10</ref> shows a visual comparison of the different approaches. We can see that our models are able to recover fine details and produce visually pleasing results. Even our most efficient network FRVSR 3-64 produces higher-quality results than prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head><p>Caballero et al. <ref type="bibr" target="#b2">[3]</ref> Liu et al. <ref type="bibr">[</ref>   <ref type="bibr" target="#b26">[27]</ref> by PSNR. Using a bigger super-resolution network helps FRVSR 10-128 to add an additional 0.5 dB on top and achieve state-of-the-art results by SSIM as well, showing that the proposed framework can greatly benefit from more powerful networks. Values marked with a star have been copied from the respective publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future work</head><p>Since our framework relies on the HR estimate I est for propagating information, it can reconstruct details and propagate them over a large number of frames (see <ref type="bibr">Sec. 4.6)</ref>. At the same time, any detail can only persist in the system as long as it is contained in I est , as it is the only way through which SRNet can pass information to future iterations. Due to the spatial loss on I est , SRNet has no way to pass on auxiliary information that could potentially be useful for future frames in the video, e.g., for occluded regions. As a result, occlusions irreversibly destroy all previously aggregated details in the affected areas and the best our model can do for the previously occluded areas is to match the performance of single image super-resolution models. In contrast, models that use a fixed number of input frames can still combine information from frames that do not have occlusions to produce better results in these areas. To address this limitation, it is natural to extend the framework with an additional memory channel. However, preliminary experiments in this direction with both static and motion-compensated memory did not improve the overall performance of the architecture, so we leave further investigations in this direction to future work.</p><p>Since the model is conceptually flexible, it can be easily extended to other applications. As an example, one may plug in the original HR frame I HR t−1 in place of the estimated frame I est t−1 for every K-th frame. This could enable an efficient video compression method where only one in K HR-frames needs to be stored while the remaining frames would be reconstructed by the model.</p><p>A further extension of our framework would be the inclusion of more advanced loss terms which have recently been shown to produce more visually pleasing results <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref>. The recurrent architecture in FRVSR naturally encourages the network to produce temporally consistent results, making it an ideal candidate for further research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a flexible end-to-end trainable framework for video super-resolution that is able to generate higher quality results while being more efficient than existing sliding window approaches. In an extensive set of experiments, we show that our model outperforms competing baselines in various different settings. The proposed model also significantly outperforms state-of-the-art video super-resolution approaches both quantitatively and qualitatively on a standard benchmark dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Side-by-side comparison of bicubic interpolation, our FRVSR result, and HR ground truth for 4x upsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the space-to-depth transformation S 2 . Regular LR grids with varying offsets are extracted from an HR image and placed into the channel dimension, see Eq. 5 for a formal definition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Network architectures for SRNet (top) and FNet (bottom) for 4x upsampling. Both networks are fully convolutional and work in LR space. For the inputs, ⊕ denotes the concatenation of images in the channel dimension. All convolutions in both networks use 3×3 kernels with stride 1, except for the transposed convolutions in SRNet which use stride 2 for spatial upsampling. The leaky ReLU units in FNet use a leakage factor of 0.2 and the notation 2x indicates that the corresponding block is duplicated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Performance of FRVSR started at the 1st and 11th frame of Foliage from Vid4. The gap between the curves only closes towards the end of the clip, showing FRVSR's ability to retain details over a large range of video frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Performance on YT10 for different numbers of convolution filters (64 / 128) and residual blocks in SRNet. FRVSR achieves better results than both baselines with significantly smaller super-resolution networks and less computation time. For example, FRVSR with 5 residual blocks is both faster and better than VSR with 10 residual blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.7.</figDesc><table><row><cell>⊕</cell><cell>Conv, 64</cell><cell></cell><cell>ReLU</cell><cell></cell><cell>Conv, 64</cell><cell>ReLU</cell><cell>Conv, 64</cell><cell>+</cell><cell></cell><cell></cell><cell>Conv, 64</cell><cell>ReLU</cell><cell></cell><cell>Conv, 64</cell><cell></cell><cell>Conv Transpose, 64</cell><cell>ReLU</cell><cell>Conv Transpose, 64</cell><cell></cell><cell>ReLU</cell><cell></cell><cell>Conv, 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">10 Residual blocks in total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>⊕</cell><cell>Conv, 32</cell><cell>Leaky ReLU</cell><cell>Max pool 2x</cell><cell>Conv, 64</cell><cell>Leaky ReLU</cell><cell>Max pool 2x</cell><cell>Conv, 128</cell><cell>Leaky ReLU</cell><cell>Max pool 2x</cell><cell>Conv, 256</cell><cell>Leaky ReLU</cell><cell>Bilinear 2x</cell><cell>Conv, 128</cell><cell>Leaky ReLU</cell><cell>Bilinear 2x</cell><cell>Conv, 64</cell><cell>Leaky ReLU</cell><cell>Bilinear 2x</cell><cell>Conv, 32</cell><cell>Leaky ReLU</cell><cell>Conv, 2</cell><cell>Tanh</cell></row><row><cell></cell><cell cols="2">2x</cell><cell></cell><cell cols="2">2x</cell><cell></cell><cell cols="2">2x</cell><cell></cell><cell cols="2">2x</cell><cell></cell><cell cols="2">2x</cell><cell></cell><cell cols="2">2x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>model</cell><cell cols="4">σ = 0.025 σ = 0.075 JPG 40 JPG 70</cell></row><row><cell>SISR</cell><cell>29.93</cell><cell>28.20</cell><cell>27.94</cell><cell>28.88</cell></row><row><cell>VSR FRVSR</cell><cell>30.36 30.84</cell><cell>28.42 28.62</cell><cell>28.12 28.30</cell><cell>29.07 29.29</cell></row></table><note>Average video PSNR of various models under Gaus- sian noise (left) and JPEG artifacts (right) on YT10. In all experiments, FRVSR achieves the highest PSNR.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Visual comparison with previous methods on Foliage from Vid4. Amongst prior art, Liu and Sun<ref type="bibr" target="#b26">[27]</ref> recover the finest details, but their result has blocky artifacts, and their method uses a slow optimization procedure. Between the remaining methods, even the result of our smallest model FRVSR 3-64 is sharper and contains more details than prior art, producing results similar to the much bigger VSR model. Our larger model FRVSR 10-128 recovers the most accurate image.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>28]</cell><cell cols="2">Tao et al. [39]</cell><cell cols="2">Liu and Sun [27]</cell></row><row><cell></cell><cell>SISR</cell><cell>VSR</cell><cell cols="2">FRVSR 3-64</cell><cell cols="2">FRVSR 10-128</cell><cell cols="2">HR ground truth</cell></row><row><cell cols="2">Figure 10: Method Bicubic</cell><cell cols="7">RAISR BRCN VESPCN B 1,2,3 +T DRVSR Bayesian SISR [34] [17] [3] [28] [39] [27] 10-128 10-128 VSR FRVSR FRVSR 3-64 10-128</cell></row><row><cell>PSNR SSIM</cell><cell>23.53 0.628</cell><cell>24.24 24.43* 25.35* 0.665 0.662* 0.756*</cell><cell>25.35 0.738</cell><cell>25.87 0.772</cell><cell>26.16 0.815</cell><cell>24.96 26.25 0.721 0.803</cell><cell>26.17 0.798</cell><cell>26.69 0.822</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of average PSNR and SSIM on the standard Vid4 dataset for scaling factor s = 4. Our smallest model FRVSR 3-64 already produces better results than all prior art including the computationally expensive optimization-based method by Liu and Sun</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NTIRE 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum a posteriori video super-resolution using a new multichannel image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Belekos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Totz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Coherent online video style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Total variation regularization of local-global optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drulea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video-to-video dynamic super-resolution for grayscale and color sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Applied Signal Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Characterizing and improving stability in neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional networks for multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online video deblurring via dynamic temporal blending network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end learning of video super-resolution with motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Super-resolution Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Super-resolution: A comprehensive survey. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PSyCo: Manifold span reduction for super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RAISR: Rapid and accurate image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Isidoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">EnhanceNet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Superresolution without explicit subpixel motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploiting selfsimilarities for single frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
