<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S 4 L: Self-Supervised Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<email>xzhai@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
							<email>avitalo@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
							<email>akolesnikov@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
							<email>lbeyer@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S 4 L: Self-Supervised Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning (S 4 L) and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that S 4 L and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern computer vision systems demonstrate outstanding performance on a variety of challenging computer vision benchmarks, such as image recognition <ref type="bibr" target="#b33">[34]</ref>, object detection <ref type="bibr" target="#b21">[22]</ref>, semantic image segmentation <ref type="bibr" target="#b7">[8]</ref>, etc. Their success relies on the availability of a large amount of annotated data that is time-consuming and expensive to acquire. Moreover, applicability of such systems is typically limited in scope defined by the dataset they were trained on.</p><p>Many real-world computer vision applications are concerned with visual categories that are not present in standard benchmark datasets, or with applications of dynamic nature where visual categories or their appearance may change over time. Unfortunately, building large labeled datasets for all these scenarios is not practically feasible. Therefore, it is an important research challenge to design a learning approach that can successfully learn to recognize new concepts by leveraging only a small amount of labeled examples. The fact that humans quickly understand new concepts after seeing only a few (labeled) examples suggests that this goal is achievable in principle.</p><p>Notably, a large research effort is dedicated towards * equal contribution A schematic illustration of one of the proposed self-supervised semi-supervised techniques: S 4 L-Rotation. Our model makes use of both labeled and unlabled images. The first step is to create four input images for any image by rotating it by 0 • , 90 • , 180 • and 270 • (inspired by <ref type="bibr" target="#b9">[10]</ref>). Then, we train a single network that predicts which rotation was applied to all these images and, additionally, predicts semantic labels of annotated images. This conceptually simple technique is competitive with existing semi-supervised learning methods.</p><p>learning from unlabeled data that, in many realistic applications, is much less onerous to acquire than labeled data. Within this effort, the field of self-supervised visual representation learning has recently demonstrated the most promising results <ref type="bibr" target="#b16">[17]</ref>. Self-supervised learning techniques define pretext tasks which can be formulated using only unlabeled data, but do require higher-level semantic understanding in order to be solved. As a result, models trained for solving these pretext tasks learn representations that can be used for solving other downstream tasks of interest, such as image recognition.</p><p>Despite demonstrating encouraging results <ref type="bibr" target="#b16">[17]</ref>, purely self-supervised techniques learn visual representations that are significantly inferior to those delivered by fullysupervised techniques. Thus, their practical applicability is limited and as of yet, self-supervision alone is insufficient.</p><p>We hypothesize that self-supervised learning techniques could dramatically benefit from a small amount of labeled examples. By investigating various ways of doing so, we bridge self-supervised and semi-supervised learning, and propose a framework of semi-supervised losses arising from self-supervised learning targets. We call this framework self-supervised semi-supervised learning or, in short, S 4 L. The techniques derived in that way can be seen as new semisupervised learning techniques for natural images. <ref type="figure">Figure 1</ref> illustrates the idea of the proposed S 4 L techniques. We thus evaluate our models both in the semi-supervised setup, as well as in the transfer setup commonly used to evaluate self-supervised representations. Moreover, we design strong baselines for benchmarking methods which learn using only 10 % or 1 % of the labels in ILSVRC-2012.</p><p>We further experimentally investigate whether our S 4 L methods could further benefit from regularizations proposed by the semi-supervised literature, and discover that they are complementary, i.e. combining them leads to improved results.</p><p>Our main contributions can be summarized as follows:</p><p>• We propose a new family of techniques for semisupervised learning with natural images that leverage recent advances in self-supervised representation learning.</p><p>• We demonstrate that the proposed self-supervised semi-supervised (S 4 L) techniques outperform carefully tuned baselines that are trained with no unlabeled data, and achieve performance competitive with previously proposed semi-supervised learning techniques.</p><p>• We further demonstrate that by combining our best S 4 L methods with existing semi-supervised techniques, we achieve new state-of-the-art performance on the semi-supervised ILSVRC-2012 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this work we build on top of the current state-of-theart in both fields of semi-supervised and self-supervised learning. Therefore, in this section we review the most relevant developments in these fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semi-supervised Learning</head><p>Semi-supervised learning describes a class of algorithms that seek to learn from both unlabeled and labeled samples, typically assumed to be sampled from the same or similar distributions. Approaches differ on what information to gain from the structure of the unlabeled data.</p><p>Given the wide variety of semi-supervised learning techniques proposed in the literature, we refer to <ref type="bibr" target="#b3">[4]</ref> for an ex-tensive survey. For more context, we focus on recent developments based on deep neural networks.</p><p>The standard protocol for evaluating semi-supervised learning algorithms works as such: (1) Start with a standard labeled dataset; (2) Keep only a portion of the labels (say, 10 %) on that dataset; (3) Treat the rest as unlabeled data. While this approach may not reflect realistic settings for semi-supervised learning <ref type="bibr" target="#b29">[30]</ref>, it remains the standard evaluation protocol, which we follow it in this work.</p><p>Many of the initial results on semi-supervised learning with deep neural networks were based on generative models such as denoising autoencoders <ref type="bibr" target="#b32">[33]</ref>, variational autoencoders <ref type="bibr" target="#b15">[16]</ref> and generative adversarial networks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>. More recently, a line of research showed improved results on standard baselines by adding consistency regularization losses computed on unlabeled data. These consistency regularization losses measure discrepancy between predictions made on perturbed unlabeled data points. Additional improvements have been shown by smoothing predictions before measuring these perturbations. Approaches of these kind include Π-Model <ref type="bibr" target="#b18">[19]</ref>, Temporal Ensembling <ref type="bibr" target="#b18">[19]</ref>, Mean Teacher <ref type="bibr" target="#b40">[41]</ref> and Virtual Adversarial Training <ref type="bibr" target="#b23">[24]</ref>. Recently, fast-SWA <ref type="bibr" target="#b0">[1]</ref> showed improved results by training with cyclic learning rates and measuring discrepancy with an ensemble of predictions from multiple checkpoints. By minimizing consistency losses, these models implicitly push the decision boundary away from high-density parts of the unlabeled data. This may explain their success on typical image classification datasets, where points in each clusters typically share the same class.</p><p>Two additional important approaches for semisupervised learning, which have shown success both in the context of deep neural networks and other types of models are Pseudo-Labeling <ref type="bibr" target="#b19">[20]</ref>, where one imputes approximate classes on unlabeled data by making predictions from a model trained only on labeled data, and conditional entropy minimization <ref type="bibr" target="#b10">[11]</ref>, where all unlabeled examples are encouraged to make confident predictions on some class.</p><p>Semi-supervised learning algorithms are typically <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23</ref>] evaluated on small-scale datasets such as CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> and SVHN <ref type="bibr" target="#b24">[25]</ref>. We are aware of very few examples in the literature where semi-supervised learning algorithms are evaluated on larger, more challenging datasets such as ILSVRC-2012 <ref type="bibr" target="#b33">[34]</ref>. To our knowledge, Mean Teacher <ref type="bibr" target="#b40">[41]</ref> currently holds the state-of-the-art result on ILSVRC-2012 when using only 10 % of the labels. Recent concurrent work <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b12">13]</ref> presents competitive results on ILSVRC-2012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-supervised Learning</head><p>Self-supervised learning is a general learning framework that relies on surrogate (pretext) tasks that can be formulated using only unsupervised data. A pretext task is de-signed in a way that solving it requires learning of a useful image representation. Self-supervised techniques have a variety of applications in a broad range of computer vision topics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In this paper we employ self-supervised learning techniques that are designed to learn useful visual representations from image databases. These techniques achieve stateof-the-art performance among approaches that learn visual representations from unsupervised images only. Below we provided a non-comprehensive summary of the most important developments in this direction. <ref type="bibr">Doersch et al. propose</ref> to train a CNN model that predicts relative location of two randomly sampled nonoverlapping image patches <ref type="bibr" target="#b4">[5]</ref>. Follow-up papers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> generalize this idea for predicting a permutation of multiple randomly sampled and permuted patches.</p><p>Beside the above patch-based methods, there are selfsupervised techniques that employ image-level losses. Among those, in <ref type="bibr" target="#b43">[44]</ref> the authors propose to use grayscale image colorization as a pretext task. Another example is a pretext task <ref type="bibr" target="#b9">[10]</ref> that predicts an angle of the rotation transformation that was applied to an input image.</p><p>Some techniques go beyond solving surrogate classification tasks and enforce constraints on the representation space. A prominent example is the exemplar loss from <ref type="bibr" target="#b5">[6]</ref> that encourages the model to learn a representation that is invariant to heavy image augmentations. Another example is <ref type="bibr" target="#b26">[27]</ref>, that enforces additivity constraint on visual representation: the sum of representations of all image patches should be close to representation of the whole image. Finally, <ref type="bibr" target="#b2">[3]</ref> proposes a learning procedure that alternates between clustering images in the representation space and learning a model that assigns images to their clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section we present our self-supervised semisupervised learning (S 4 L) techniques. We first provide a general description of our approach. Afterwards, we introduce specific instantiations of our approach.</p><p>We focus on the semi-supervised image classification problem. Formally, we assume an (unknown) data generating joint distribution p(X, Y ) over images and labels. The learning algorithm has access to a labeled training set D l , which is sampled i.i.d. from p(X, Y ) and an unlabeled training set D u , which is sampled i.i.d. from the marginal distribution p(X).</p><p>The semi-supervised methods we consider in this paper have a learning objective of the following form:</p><formula xml:id="formula_0">min θ L l (D l , θ) + wL u (D u , θ),<label>(1)</label></formula><p>where L l is a standard cross-entropy classification loss of all labeled images in the dataset, L u is a loss defined on unsupervised images (we discuss its particular instances later in this section), w is a non-negative scalar weight and θ is the parameters for model f θ (·). Note that the learning objective can be extended to multiple unsupervised losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-supervised Semi-supervised Learning</head><p>We now describe our self-supervised semi-supervised learning techniques. For simplicity, we present our approach in the context of multiclass image recognition, even though it can be easily generalized to other scenarios, such as dense image segmentation.</p><p>It is important to note that in practice the objective 1 is optimized using a stochastic gradient descent (or a variant) that uses mini-batches of data to update the parameters θ. In this case the size of a supervised mini-batch x l , y l ⊂ D l and an unsupervised mini-batch x u ⊂ D u can be arbitrary chosen. In our experiments we always default to simplest possible option of using minibatches of equal sizes.</p><p>We also note that we can choose whether to include the minibatch x l into the self-supervised loss, i.e. apply L self to the union of x u and x l . We experimentally study the effect of this choice in our experiments Section 4.4.</p><p>We demonstrate our framework on two prominent selfsupervised techniques: predicting image rotation <ref type="bibr" target="#b9">[10]</ref> and exemplar <ref type="bibr" target="#b5">[6]</ref>. Note, that with our framework, more selfsupervised losses can be explored in the future.</p><formula xml:id="formula_1">S 4 L-Rotation.</formula><p>The key idea of rotation self-supervision is to rotate an input image then predict which rotation degree was applied to these rotated images. The loss is defined as:</p><formula xml:id="formula_2">L rot = 1 |R| r∈R x∈Du L(f θ (x r ), r)<label>(2)</label></formula><p>where R is the set of the 4 rotation degrees</p><formula xml:id="formula_3">{0 • , 90 • , 180 • , 270 • }, x r is the image x rotated by r, f θ (·)</formula><p>is the model with parameters θ,L is the cross-entropy loss. This results in a 4-class classification problem. We follow a recommendation from <ref type="bibr" target="#b9">[10]</ref> and in a single optimization step we always apply and predict all four rotations for every image in a minibatch. We also apply the self-supervised loss to the labeled images in each minibatch. Since we process rotated supervised images in this case, we suggest to also apply a classification loss to these images. This can be seen as an additional way to regularize a model in a regime when a small amount of labeled image are available. We measure the effect of this choice later in Section 4.4.</p><p>S 4 L-Exemplar. The idea of exemplar self-supervision <ref type="bibr" target="#b5">[6]</ref> is to learn a visual representation that is invariant to a wide range of image transformations. Specifically, we use "Inception" cropping <ref type="bibr" target="#b39">[40]</ref>, random horizontal mirroring, and HSV-space color randomization as in <ref type="bibr" target="#b5">[6]</ref> to produce 8 different instances of each image in a minibatch. Following <ref type="bibr" target="#b16">[17]</ref>, we implement L u as the batch hard triplet loss <ref type="bibr" target="#b13">[14]</ref> with a soft margin. This encourages transformation of the same image to have similar representations and, conversely, encourages transformations of different images to have diverse representations.</p><p>Similarly to the rotation self-supervision case, L u is applied to all eight instances of each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semi-supervised Baselines</head><p>In the following section, we compare S 4 L to several leading semi-supervised learning algorithms that are not based on self-supervised objectives. We now describe the approaches that we compare to.</p><p>Our proposed objective 1 is applicable for semi supervised learning methods as well, where the loss L u is the standard semi supervised loss as described below.</p><p>Virtual Adversarial Training (VAT) <ref type="bibr" target="#b23">[24]</ref>: The idea is making the predicted labels robust around input data point against local perturbation. It approximates the maximal change in predictions within an vat vicinity of unlabeled data points, where vat is a hyperparameter. Concretely, the VAT loss for a model f θ is:</p><formula xml:id="formula_4">L vat = 1 |D u | x∈Du KL(f θ (x) f θ (x + ∆x)),<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">∆x = arg max δ s.t. |δ|2= KL(f θ (x) f θ (x + δ)).<label>(4)</label></formula><p>While computing ∆x directly is not tractable, it can be efficiently approximated at the cost of an extra forward and backwards pass for every optimization step. <ref type="bibr" target="#b23">[24]</ref>.</p><p>Conditional Entropy Minimization (EntMin) <ref type="bibr" target="#b10">[11]</ref>: This works under the assumption that unlabeled data indeed has one of the classes that we are training on, even when the particular class is not known during training. It adds a loss for unlabeled data that, when minimized, encourages the model to make confident predictions on unlabeled data. Specifically, the conditional entropy minimization loss for a model f θ (treating f θ as a conditional distribution of labels over images) is:</p><formula xml:id="formula_6">L entmin = 1 |D u | x∈Du y∈Y −f θ (y|x) log f θ (y|x) (5)</formula><p>Alone, the EntMin loss is not useful in the context of deep neural networks because the model can easily become extremely confident by increasing the weights of the last layer. One way to resolve this is to encourage the model predictions to be locally-Lipschitz, which VAT does <ref type="bibr" target="#b37">[38]</ref>. Therefore, we only consider VAT and EntMin combined, not just EntMin alone, i.e. L u = w vat L vat + w entmin L entmin . <ref type="bibr" target="#b19">[20]</ref> is a simple approach: Train a model only on labeled data, then make predictions on unlabeled data. Then enlarge your training set with the predicted classes of the unlabeled data points whose predictions are confident past some threshold of confidence. Re-train your model with this enlarged labeled dataset. While <ref type="bibr" target="#b29">[30]</ref> shows that in a simple "two moons" dataset, psuedo-label fails to learn a good model, in many real datasets this approach does show meaningful gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-Label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ILSVRC-2012 Experiments and Results</head><p>In this section, we present the results of our main experiments. We used the ILSVRC-2012 dataset due to its widespread use in self-supervised learning methods, and to see how well semi-supervised methods scale.</p><p>Since the test set of ILSVRC-2012 is not available, and numbers from the validation set are usually reported in the literature, we performed all hyperparameter selection for all models that we trained on a custom train/validation split of the public training set. This custom split contains 1 231 121 training and 50 046 validation images. We then retrain the model using the best hyperparameters on the full training set (1 281 167 images), possibly with fewer labels, and report final results obtained on the public validation set (50 000 images).</p><p>We follow standard practice <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b31">32]</ref> and perform experiments where class-balanced labels are available for only 10 % of the dataset. Note that 10 % of ILSVRC-2012 still corresponds to roughly 128 000 labeled images, and that previous work uses the full (public) validation set for model selection. While we use a custom validation set extracted from the training set, using such a large validation set does not correspond to a realistic scenario, as already discussed by <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30]</ref>. We also want to cover more realistic cases in our evaluation. We thus perform experiments on 1 % of labeled examples (roughly 13 000 labeled images), while also using a validation set of only 5000 images. We analyze the impact of validation set size in Section 7.</p><p>We always define epochs in terms of the available labeled data, i.e. one epoch corresponds to one full pass through the labeled data, regardless of how many unlabeled examples have been seen. We optimize our models using stochastic gradient descent with momentum on minibatches of size 256 unless specified otherwise. While we do tune the learning rate, we keep the momentum fixed at 0.9 across all experiments. <ref type="table" target="#tab_1">Table 1</ref> summarizes our main results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Plain Supervised Learning</head><p>Whenever new methods are introduced, it is crucial to compare them against a solid baseline of existing methods. The simplest baseline to which any semi-supervised learning method should be compared to, is training a plain supervised model on the available labeled data.</p><p>Oliver et al. <ref type="bibr" target="#b29">[30]</ref> discovered that reported baselines trained on labeled examples alone are unfairly weak, perhaps given that there is not a strong community behind tuning those baselines. They provide strong supervised-only baselines for SVHN and CIFAR-10, and show that the gap shown by the use of unlabeled data is smaller than reported.</p><p>We observed the same in the case of ILSVRC-2012. Thus, we aim to provide a strong baseline for future research by performing a relatively large search over training hyperparameters for training a model on only 10 % of ILSVRC-2012. Specifically, we try weight-decay values in {1, 3} · 10 {−2,−3,−4} , learning rates in {0.3, 0.1, 0.03}, four different learning rate schedules spanning 30 to 500 epochs, and finally we explore various model architectures: ResNet50, ResNet34, ResNet18, in both "regular" (v1) and "pre-activation" (v2) variants, as well as half-, double-, triple-, and quadruple-width variants of these, testing the assumption that smaller or shallower models overfit less.</p><p>In total, we trained several thousand models on our custom training/validation split of the public training set of ILSVRC-2012. In summary, it is crucial to tune both weight decay and training duration while, perhaps surprisingly, model architecture, depth, and width only have a small influence on the final results. We thus use a standard, unmodified ResNet50v2 as model, trained with weight decay of 10 −3 for 200 epochs, using a standard learning rate of 0.1, ramped up from 0 for the first five epochs, and decayed by a factor of 10 at epochs 140, 160, and 180. We train in total for 200 epochs. The standard augmentation procedure of random cropping and horizontal flipping is used during training, and predictions are made using a single central crop keeping aspect ratio.</p><p>We perform a similar search when training our baseline on 1 % of ILSVRC-2012, but additionally include two choices of data augmentation (whether or not to apply random color augmentation) and two minibatch sizes (256 and 1024) in the hyperparameter search. Perhaps somewhat surprisingly, the results here are similar, in that tuning the weight decay and training duration is crucial, but model architecture does not matter much. Additionally, performing color augmentation becomes important. Here too, we use a standard, unmodified ResNet50v2 as model, trained with weight decay of 10 −2 for 1000 epochs, using a learning rate of 0.01 1 , ramped up from 0.0 for the first ten epochs 2 , and decayed by a factor of 10 at epochs 700, 800, and 900. We train in total for 1000 epochs. A more detailed presentation of the results is provided in the supplementary material.</p><p>Using this only slightly altered training procedure, our baseline models achieve 80.43 % top5 accuracy (56.35 % top1) on the public ILSVRC-2012 validation set when For all further experiments, we reuse the best hyperparameters discovered here, except that we try two additional learning rates: {0.3, 0.1, 0.03} for 10 % and {0.03, 0.01, 0.003} for 1 %, and two additional weight decays: {10 −4 , 3·10 −4 , 10 −3 } for 10 % and {3·10 −3 , 10 −2 , 3· 10 −2 } for 1 %. We also try two different weights w u for the additionally introduced loss L u : w u ∈ {0.5, 1.0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semi-supervised Baselines</head><p>We train semi-supervised baseline models using (1) Pseudo-Label, (2) VAT, and (3) VAT+EntMin. To the best of our knowledge, we present the first evaluation of these techniques on ILSVRC-2012.</p><p>Pseudo-Label Using the plain supervised learning models from Section 4.1, we assign pseudo-labels to the full dataset. Then, in a second step, we train a ResNet50v2 from scratch following standard practice, i.e. with learning rate 0.1, weight decay 10 −4 , and 100 epochs on the full (pseudo-labeled) dataset.</p><p>We try both using all predictions as pseudo-labels, as well as using only those predictions with a confidence above 0.5. Both perform closely on our validation set, and we choose no filtering for the final model for simplicity. <ref type="table" target="#tab_1">Table 1</ref> shows that a second step training with pseudolabels consistently improves the results on both 10 % and the 1 % labels case. This motivates us to apply the idea to our best semi supervised model, which is discussed in Section 5.</p><p>VAT We first verify our VAT implementation on CIFAR-10. With 4000 labels, we are able to achieve 86.41 % top-1 accuracy, which is in line with the 86.13 % reported in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Besides the previously mentioned hyperparameters common to all methods, VAT needs tuning vat . Since it corresponds to a distance in pixel space, we use a simple heuristic for defining a range of values to try for vat : values should be lower than half the distance between neighbouring images in the dataset. Based on this heuristic, we try values of vat ∈ {50, 50 · 2 −1/3 , 50 · 2 −2/3 , 25} and found vat ≈ 40 to work best.</p><p>VAT+EntMin VAT is intended to be used together with an additional entropy minimization (EntMin) loss. Ent-Min adds a single hyperparameter to our best VAT model: the weight of the EntMin loss, for which we try w entmin ∈ {0, 0.03, 0.1, 0.3, 1}, without re-tuning vat .</p><p>The results of our best VAT and VAT+EntMin model are shown in <ref type="table" target="#tab_1">Table 1</ref>. As can be seen, VAT performs well in the 10 % case, and adding adding entropy minimization consistently improves its performance. In Section 5, we further extend the co-training idea to include the self-supervised rotation loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Self-supervised Baselines</head><p>Previous work has evaluated features learned via selfsupervision on the unlabeled data in a "semi-supervised" way by either freezing the features and learning a linear classifier on top, or by using the self-supervised model as an initialization and fine-tuning, using a subset of the labels in both cases. In order to compare our proposed way to do self-supervised semi-supervised learning to these common evaluations, we train a rotation and an exemplar model following the best practice from <ref type="bibr" target="#b16">[17]</ref> but with standard width ("4×" in <ref type="bibr" target="#b16">[17]</ref>).</p><p>Following our established protocol, we tune the weight decay and learning rate for the logistic regression, although interestingly the standard values from <ref type="bibr" target="#b11">[12]</ref> of 10 −4 weight decay and 0.1 learning rate worked best.</p><p>The results of evaluating these models with both 10 % and 1 % are presented in <ref type="table" target="#tab_1">Table 1</ref> as "Self-sup. + Linear" and "Self-sup. + Fine-tune". Note that while our results for the linear experiment are similar to those reported in <ref type="bibr" target="#b16">[17]</ref>, they are not directly. This is due to 1) ours being evaluated on the public validation set, while they evaluated on a custom validation set, and 2) they used L-BFGS while we use SGD with standard augmentations. Furthermore, fine-tuning approaches or slightly surpasses the supervised baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Self-supervised Semi-supervised Learning (S 4 L)</head><p>For training our full self-supervised semi-supervised models (S 4 L), we follow the same protocol as for our semisupervised baselines, i.e. we use the best settings of the plain supervised baseline and only tune the learning rate, weight decay, and weight of the newly introduced loss. We found that for both S 4 L-Rotation and S 4 L-Exemplar, the self-supervised loss weight w = 1 worked best (though not by much) and the optimal weight decay and learning rate were the same as for the supervised baseline.</p><p>As described in Section 3.1, we apply the self-supervised loss on both labeled and unlabeled images. Furthermore, both Rotation and Exemplar self-supervision generate augmented copies of each image, and we do apply the supervision loss on all copies of the labeled images. We performed one case study on S 4 L-Rotation in order to investigate this choice, and found that whether or not the self-supervision loss L self is also applied on the labeled images does not have significant impact. On the other hand, applying the supervision loss L sup on the augmented images generated by self-supervision does indeed improve performance by almost 1 %. Furthermore, this allows to use multiple transformed copies of an image at inference-time (e.g. four rotations) and take the average of their predictions. While this 4-rot prediction is 1 % to 2 % more accurate, the results we report do not make use of this in order to keep comparison fair.</p><p>The results shown in <ref type="table" target="#tab_1">Table 1</ref> show that our proposed way of doing self-supervised semi-supervised learning is indeed effective for the two self-supervision methods we tried. We hypothesize that such approaches can be designed for other self-supervision objectives.</p><p>We additionally verified that our proposed method is not sensitive to the random seed, nor the split of the dataset, see Appendix B for details.</p><p>Finally, in order to explore the limits of our proposed models and match capacity of the architectures used in concurrent papers (e.g. <ref type="bibr" target="#b12">[13]</ref>), we train the S 4 L-Rotation model with a more powerful architecture, such as ResNet152v2 2×wider, and also use large computational budget to tune hyperparameters. In this case our model achieves even better results: 86.41 % top-5 accuracy with 10 % labels and 57.50 % with 1 % labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Semi-supervised Learning is Complementary to S 4 L</head><p>Since we found that different types of models perform similarly well, the natural next question is whether they are complementary, in which case a combination would lead to an even better model, or whether they all reach a common "intrinsic" performance plateau.</p><p>In this section, we thus describe our Mix Of All Models (MOAM). In short: in a first step, we combine S 4 L-Rotation and VAT+EntMin to learn a 4× wider <ref type="bibr" target="#b16">[17]</ref> model. We then use this model in order to generate pseudo labels for a second training step, followed by a final fine-tuning step. Results of the final model, as well as the models ob- tained in the two intermediate steps, are reported in <ref type="table" target="#tab_2">Table 2</ref> along with previous results reported in the literature.</p><p>Step 1: Rotation+VAT+EntMin In the first step, our model jointly optimizes the S 4 L-Rotation loss and the VAT and EntMin losses. We iterated on hyperparameters for this setup in a less structured way than in our controlled experiments above (always on our custom validation set) and only mention the final values here. Our model was trained with batch size 128, learning rate 0.1, weight decay 2 · 10 −4 , training for 200 epochs with linear learning rate rampup up to epoch 10, then 10-fold decays at 100, 150, and 190 epochs. We use inception crop augmentation as well as horizontal mirroring. We used the following relative loss weights: w sup = 0.3, w rot = 0.7, w vat = 0.3, w entmin = 0.3. We tried a few heuristics for setting those weights automatically, but found that manually tuning them led to better performance. We also applied Polyak averaging to the model parameters, choosing the decay factor such that parameters decay by 50 % over each epoch. Joint training of these losses consistently improve over the models with a single objective. The model obtained after this first step achieves 88.80% top-5 accuracy on the ILSVRC-2012 dataset.</p><p>Step 2: Retraining on Pseudo Labels Using the above model, we assign pseudo labels to the full dataset by averaging predictions across five crops and four rotations of each image <ref type="bibr" target="#b2">3</ref> . We then train the same network again in the exact same way (i.e. with all the losses) except for the fol- lowing three differences: (1) the network is initialized using the weights obtained in the first step (2) every example has a label: the pseudo label (3) due to this, an epoch now corresponds to the full dataset; we thus train for 18 epochs, decaying the learning rate after 6 and 12 epochs.</p><p>Step 3: Fine-tuning the model Finally, we fine-tune the model obtained in the second step on the original 10 % labels only. This step is trained with weight decay 3·10 −3 and learning rate 5 · 10 −4 for 20 epochs, decaying the learning rate 10-fold every 5 epochs.</p><p>Remember that all hyper-parameters described here were selected on our custom validation set which is taken from the training set. The final model "MOAM (full)" achieves 91.23 % top-5 accuracy, which sets the new state-of-the-art.</p><p>We conduct additional experiments and report performance of MOAM (i.e. only Step 1) with 100 % labels in <ref type="table" target="#tab_2">Table 2</ref>. Interestingly, MOAM achieves promising results even in the high-data regime with 100 % labels, outperforming the fully supervised baseline: +0.87 % for top-5 accuracy and +1.6 % for top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Transfer of Learned Representations</head><p>Self-supervision methods are typically evaluated in terms of how generally useful their learned representation is. This is done by treating the learned model as a fixed feature extractor, and training a linear logistic regression model on top the features it extracts on a different dataset, usually Places205 <ref type="bibr" target="#b44">[45]</ref>. We perform such an evaluation on our S 4 L models in order to gain some insight into the generality of the learned features, and how they compare to those obtained by pure self-supervision. As can be seen, evaluating our models even with only a single validation image per class is robust, and in particular selecting an optimal model with this validation set works as well as with the full validation set.</p><p>We closely follow the protocol defined by <ref type="bibr" target="#b16">[17]</ref>. The representation is extracted from the pre-logits layer. We use stochastic gradient descent (SGD) with momentum for training these linear evaluation models with a minibatch size of 2048 and an initial learning rate of 0.1, warmed up in the first epoch.</p><p>While Kolesnikov et al. <ref type="bibr" target="#b16">[17]</ref> show that a very long training schedule (520 epochs) is required for the linear model to converge using self-supervised representations, we observe dramatically different behaviour when evaluating our self-supervised semi-supervised representations. <ref type="figure" target="#fig_0">Figure 2</ref> shows the accuracy curve of the plain self-supervised rotation method <ref type="bibr" target="#b16">[17]</ref> and our proposed S 4 L-Rotation method trained on 10 % of ILSVRC-2012. As can be seen, the logistic regression is able to find a good separating hyperplane in very few epochs and then plateaus, whereas in the self-supervised case it struggles for a very long number of epochs. This indicates that the addition of labeled data leads to much more separable representations, even across datasets.</p><p>We further investigate the gap between the representation learned by a good S 4 L model (MOAM) and a corresponding baseline trained on 100 % of the labels (the baseline from <ref type="table" target="#tab_2">Table 2)</ref>. Surprisingly, we found that the representation learned by "MOAM (full)" transfers slightly better than the baseline, which used ten times more labelled data: 83.3 % accuracy vs. 83.1 % accuracy, respectively. We provide full details of this experiment in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Is a Tiny Validation Set Enough?</head><p>Current standard practice in semi-supervised learning is to use a subset of the labels for training on a large dataset, but still perform model selection using scores obtained on the full validation set. <ref type="bibr" target="#b3">4</ref> But having a large labeled validation set at hand is at odds with the promised practicality of semi-supervised learning, which is all about having only few labeled examples. This fact has been acknowledged by <ref type="bibr" target="#b32">[33]</ref>, but has been mostly ignored in the semi-supervised literature. Oliver et al. <ref type="bibr" target="#b29">[30]</ref> questions the viability of tuning with small validation sets by comparing the estimated model accuracy on small validation sets. They find that the variance of the estimated accuracy gap between two models can be larger than the actual gap between those models, hinting that model selection with small validation sets may not be viable. That said, they did not empirically evaluate whether it's possible to find the best model with a small validation set, especially when choosing hyperparameters for a particular semi-supervised method.</p><p>We now describe our analysis of this important question. We look at the many models we trained for the plain supervised baseline on 1 % of ILSVRC-2012. For each model, we compute a validation score on a validation set of 1000 labeled images (i.e. one labeled image per class), 5000 labeled images (i.e. five labeled images per class), and compare these scores to those obtained on a "full-size" validation set of 50 046 labeled images. The result is shown in <ref type="figure" target="#fig_1">Figure 3</ref> and it is striking: there is a very strong correlation between performance on the tiny and the full validation set. Especially, while in parts there is high variability, those hyperparameters which work best do so in either case. Most notably, the best model tuned on a small validation set is also the best model tuned on a large validation set. We thus conclude that for selecting hyperparameters of a model, a tiny validation set is enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion and Future Work</head><p>In this paper, we have bridged the gap between selfsupervision methods and semi-supervised learning by suggesting a framework (S 4 L) which can be used to turn any self-supervision method into a semi-supervised learning algorithm.</p><p>We instantiated two such methods: S 4 L-Rotation and S 4 L-Exemplar and have shown that they perform competitively to methods from the semi-supervised literature on the challenging ILSVRC-2012 dataset. We further showed that S 4 L methods are complementary to existing semisupervision techniques, and MOAM, our proposed combination of those, leads to state-of-the-art performance.</p><p>While all of the methods we investigated show promising results for learning with 10 % of the labels on ILSVRC-2012, the picture is much less clear when using only 1 %. It is possible that in this low data regime, when only 13 labeled examples per class are available, the setting fades into the few-shot scenario, and a very different set of methods would be required for reaching much better performance.</p><p>Nevertheless, we hope that this work inspires other researchers in the field of self-supervision to consider extending their methods into semi-supervised methods using our S 4 L framework, as well as researchers in the field of semisupervised learning to take inspiration from the vast amount of recently proposed self-supervision methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Results of the Supervised Baselines</head><p>Since we performed quite extensive hyperparameter search and trained many models in order to find a solid fully-supervised baseline on 10 % and 1 % of ILSVRC-2012, we believe that it is valuable to report the full results to the community, instead of just providing the final best model.</p><p>We present the results in the form of what we call "hypersweep curves" in <ref type="figure" target="#fig_2">Figures 4 and 5</ref>.</p><p>Each plot shows a large collection of models -each point on each plot is a fully trained model. The curves are sorted by accuracy, allowing testing sensitivity to different hyperparameters, not only comparing the best model.</p><p>For each curve, we plot the accuracy of models where one of the hyperparameters is fixed.</p><p>Thus, by comparing curves, one can see:</p><p>1. Which value of a hyperparameter performs best by looking at which curve's rightmost point is highest.</p><p>2. How sensitive the model is to a hyperparameter in the best case by looking at how far apart the curves are from eachother at their rightmost point.</p><p>3. How robust a hyperparameter is on average by looking at how similar the curves are overall.</p><p>4. How independent a specific hyperparameter value is from all others by looking at the curve's shape, and whether curves cross-over (strong interplay) or not (strong independence).</p><p>While the results shown in <ref type="figure" target="#fig_2">Figure 4</ref> use the full (custom) validation set, those in <ref type="figure" target="#fig_3">Figure 5</ref> were computed using the validation set of size 1000, i.e. with only one image per class. As we have shown in Section 7, this is sufficient to determine the best hyperparameters, and we encourage the community to follow this more realistic protocol.</p><p>As can be seen, weight decay and number of training epochs are the two things which matter most when training using only a fraction of ILSVRC-2012.</p><p>Perhaps the most surprising finding is that, contrary to current folklore, reducing model capacity is detrimental to performance on the smaller dataset. Neither reducing depth, nor reducing width improve performance. In fact, the deeper and wider models still outperform their shallower and thinner counterparts, even when using only 1 % of the training data. Even more so, the wider models are more robust to other hyperparameter's values as evidenced by their curves being significantly higher on the left end. This is in line with recent findings suggesting wider models ease optimization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Furthermore, when reducing the dataset size to 1 %, we found that adding the same color augmentation as introduced by Exemplar is helpful. We thereafter tried adding it to our best few models on 10 %, but it did not help there.</p><p>Finally, while in the 1 % case, learning-rate of 0.1 and 0.01 seem to perform equally well in the good cases (right hand side of curves), we manually inspected training curves and found that 0.1 is significantly less robust, typically not learning anything before the first decay, and only catching up later on.</p><p>While we trained thousands of models in order to rigorously test multiple hypotheses (such as that of reducing model capacity), almost all boost in performance could have been achieved in just a few dozen trials with intuitively important hyperparameters (weight decay and epochs), which would take about a week on a modern four-GPU machine.</p><p>Overall, we hope that this thorough baseline investigation inspires the semi-supervised learning community to be more careful with baselines, as those that we found perform almost 20 % absolute better than those previously reported in the literature. There are two factors of randomness of a semi supervised model: (1) labeled subset sampling, (2) run with different seeds. In order to estimate the randomness in the performance we train 9 models with random data subsets and random seeds for our proposed S 4 L method. <ref type="table" target="#tab_4">Table 3</ref> presents the detailed results. Overall, we observe that standard deviation is fairly small across both subsets and different runs and, therefore, our empirical evaluation provides robust comparison of various techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Randomness of S 4 L</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Results in the Transfer Setup</head><p>In this section we present more results from the transfer evaluation task on Places205 <ref type="bibr" target="#b44">[45]</ref>. <ref type="table">Table 4</ref> shows the re- <ref type="table">Table 4</ref>. Accuracy (in percent) obtained by various individual methods when transferring their representation to the Places205 dataset using linear models on frozen representations. All methods use the same plain ResNet50v2 base model, except for the ones marked by * , which use a 4× wider network. When it was necessary, a + marks longer transfer training of 520 epochs. The "%-labels" column shows the percentage of ILSVRC-2012 labels that was used for training the model. sults for the models mentioned in our main paper. For each method, we select the best model and evaluate its transfer to Places205. We follow the same setup as <ref type="bibr" target="#b16">[17]</ref> to train a linear models with SGD on top of frozen representations. The only difference is the training epochs, we train for 30 epochs in total with learning rate decayed at 10 and 20 epochs respectively. The learning rate is linearly ramped up for the first epoch. Kolesnikov et.al. <ref type="bibr" target="#b16">[17]</ref> train for 520 epochs with learning rate decays at 480 and 500 epochs. The schedule used in our paper is much shorter because of our finding that representation learned with labels are more separable and converges significantly faster. (See in Section 6 of the main paper for details.) To make fair comparison with the self-supervised models, results in <ref type="table">Table 4</ref> with 0% labels are trained for 520 epochs to ensure their convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>From the plain supervised baselines, we observe that either more labels or wider networks lead to more transferable representations. Surprisingly, we found that pseudo labels outperforms the other two semi-supervised baselines in the transfer setup. On the 1% labels evaluation setup, pseudo labels achieves the best result comparing to the other methods. With 10% labels, S 4 L is comparable to the semisupervised baselines, and our MOAM clearly outperforms all other models trained on 10% of labels. More interestingly, the MOAM (full) model on 10% is slightly better than the 100% supervised baseline with the same 4× wider network. This indicates that learning a model with multiple losses may lead to representations that generalize better to unseen tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Places205 learning curves of logistic regression on top of the features learned by pre-training a self-supervised versus S 4 L-Rotation model on ILSVRC-2012. The significantly faster convergence ("long" training schedule vs. "short" one) suggests that more easily separable features are learned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Correlation between validation score on a (custom) validation set of 1000, 5000, and 50 046 images on ILSVRC-2012. Each point corresponds to a trained model during a sweep for plain supervised baseline for the 1 % labeled case. The best model according to the validation set of 1 000 is marked in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The "hypersweep curves" for the supervised baseline trained on 10 % of ILSVRC-2012. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The "hypersweep curves" for the supervised baseline trained on 1 % of ILSVRC-2012. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Top-5 accuracy [%] obtained by individual methods when training them on ILSVRC-2012 with a subset of labels. All methods use the same standard width ResNet50v2 architecture.</figDesc><table><row><cell>ILSVRC-2012 labels:</cell><cell>10 %</cell><cell>1 %</cell></row><row><cell>(i.e. images per class)</cell><cell>(128)</cell><cell>(13)</cell></row><row><cell>Supervised Baseline (Section 4.1)</cell><cell>80.43</cell><cell>48.43</cell></row><row><cell>Pseudolabels [20]</cell><cell>82.41</cell><cell>51.56</cell></row><row><cell>VAT [24]</cell><cell>82.78</cell><cell>44.05</cell></row><row><cell>VAT + Entropy Minimization [11]</cell><cell>83.39</cell><cell>46.96</cell></row><row><cell>Self-sup. Rotation [17] + Linear</cell><cell>39.75</cell><cell>25.98</cell></row><row><cell>Self-sup. Exemplar [17] + Linear</cell><cell>32.32</cell><cell>21.33</cell></row><row><cell>Self-sup. Rotation [17] + Fine-tune</cell><cell>78.53</cell><cell>45.11</cell></row><row><cell>Self-sup. Exemplar [17] + Fine-tune</cell><cell>81.01</cell><cell>44.90</cell></row><row><cell>S 4 L-Rotation</cell><cell>83.82</cell><cell>53.37</cell></row><row><cell>S 4 L-Exemplar</cell><cell>83.72</cell><cell>47.02</cell></row></table><note>trained on only 10 % of the full training set. Our 1 % base- line achieves 48.43 % top5 accuracy (25.39 % top1). These results form a solid baseline to compare to, considering that the same standard ResNet50v2 model achieves 92.82 % top5 accuracy (75.89 % top1) on 100 % of the labels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparing our MOAM to previous methods in the literature on ILSVRC-2012 with 10 % of the labels. Note that different models use different architectures, larger than those inTable 1.</figDesc><table><row><cell></cell><cell cols="2">labels Top-5 Top-1</cell></row><row><cell>MOAM full (proposed)</cell><cell cols="2">10% 91.23 73.21</cell></row><row><cell>MOAM + pseudo label (proposed)</cell><cell cols="2">10% 89.96 71.56</cell></row><row><cell>MOAM (proposed)</cell><cell cols="2">10% 88.80 69.73</cell></row><row><cell>ResNet50v2 (4×wider)</cell><cell cols="2">10% 81.29 58.15</cell></row><row><cell>VAE + Bayesian SVM [32]</cell><cell cols="2">10% 64.76 48.41</cell></row><row><cell>Mean Teacher [41]</cell><cell>10% 90.89</cell><cell>-</cell></row><row><cell>† UDA [43]</cell><cell cols="2">10% 88.52  † 68.66  †</cell></row><row><cell>† CPCv2 [13]</cell><cell cols="2">10% 84.88  † 64.03  †</cell></row><row><cell>Training with all labels:</cell><cell></cell><cell></cell></row><row><cell>ResNet50v2 (4×wider)</cell><cell cols="2">100% 94.10 78.57</cell></row><row><cell>MOAM (proposed)</cell><cell cols="2">100% 94.97 80.17</cell></row><row><cell>† UDA [43]</cell><cell cols="2">100% 94.45  † 79.04  †</cell></row><row><cell>† CPCv2 [13]</cell><cell cols="2">100% 93.35  † -</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>† marks concurrent work.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>S 4 L performance for 9 runs with random image subsets. Top-5 accuracies [%] are reported as mean±standard deviation.</figDesc><table><row><cell>Method</cell><cell>10% ImageNet</cell><cell>1% ImageNet</cell></row><row><cell>S 4 L-Rotation</cell><cell>83.91 ± 0.13</cell><cell>53.47 ± 0.22</cell></row><row><cell>S 4 L-Exemplar</cell><cell>83.76 ± 0.06</cell><cell>46.61 ± 0.25</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While the standard learning rate of 0.1 worked equally well, learning curves seemed significantly less stable.<ref type="bibr" target="#b1">2</ref> This was likely not necessary, but kept for consistency.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Generating pseudo-labels using 20 crops only slightly improved performance by 0.25 %, but is cheap and simple.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To make matters worse, in the case of ILSVRC-2012, this validation set is used both to select hyperparameters as well as to report final performance. Remember that we avoid this by creating a custom validation set from part of the training set for all hyperparameter selections.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank the Google Brain Team in Zürich, and especially Sylvain Gelly for discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">There are many consistent explanations of unlabeled data: Why you should average</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robustness via retrying: Closed-loop robotic manipulation with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Topology and geometry of half-rectified network optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01540</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grasp2Vec: Learning object representations from self-supervised grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with deep generative models. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5298</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno>abs/1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-label : The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2013 Workshop : Challenges in Representation Learning (WREPL)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep metric transfer for label propagation with limited annotated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08781</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03976</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3239" to="3250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1606.03498</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Cross and learn: Cross-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03879</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A DIRT-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Theoretical insights into the optimization landscape of overparameterized shallow neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
