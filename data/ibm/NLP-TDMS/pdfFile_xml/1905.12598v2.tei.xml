<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards better substitution-based word sense induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Amrami</surname></persName>
							<email>asaf.amrami@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<email>yoav.goldberg@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards better substitution-based word sense induction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Word sense induction (WSI) is the task of unsupervised clustering of word usages within a sentence to distinguish senses. Recent work obtain strong results by clustering lexical substitutes derived from pre-trained RNN language models (ELMo). Adapting the method to BERT improves the scores even further. We extend the previous method to support a dynamic rather than a fixed number of clusters as supported by other prominent methods, and propose a method for interpreting the resulting clusters by associating them with their most informative substitutes. We then perform extensive error analysis revealing the remaining sources of errors in the WSI task. Our code is available at https://github. com/asafamr/bertwsi. Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365. Benjamin Snyder and Martha Palmer. 2004. The english all-words task. In Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Linfeng Song. 2016. Word embeddings, sense embeddings and their application to word sense induction. Clement. 2015. A sense-topic model for word sense induction with unsupervised data enrichment. . 2016. Semi-supervised word sense disambiguation with neural models. In COLING 2016.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Induction Word Sense Induction (WSI) is the task of clustering in-context usages of words to groups that represent senses. A WSI system is given multiple sentences containing usages (instances) of target lemma+part-of-speech, and is expected to group together usages in which the target is used in the same sense. E.g., in:</p><p>1. I like warm summer evenings 2. They were greeted by a warm welcome 3. The waters of the lake are warm we would like to group (1) and <ref type="formula">(3)</ref> into one sense and (2) to a different one. <ref type="bibr">1</ref> WSI was explored in several SemEval shared tasks <ref type="bibr" target="#b0">(Agirre and Soroa, 2007;</ref><ref type="bibr" target="#b9">Manandhar et al., 2010;</ref><ref type="bibr" target="#b6">Jurgens and Klapaftis, 2013)</ref>, with the gold-labels following human annotation according to the WordNet <ref type="bibr">(Miller, 1998)</ref> sense inventory. WordNet senses are very fine-grained and often hard to tag even for experts <ref type="bibr">(Snyder and Palmer, 2004)</ref>, leading the latest evaluation (Se-mEval 2013 task 13) to replace the hard-clustering task with a soft-clustering one, in which each instance can simultaneously belong to several clusters, each with a different label. This clustering is then compared to the human-taggers' disagreement data.</p><p>Up until recently, state-of-the-art results for WSI were dominated by a series of increasingly sophisticated graphical models <ref type="bibr" target="#b8">(Lau et al., 2013;</ref><ref type="bibr">Wang et al., 2015;</ref><ref type="bibr" target="#b7">Komninos and Manandhar, 2016;</ref><ref type="bibr" target="#b1">Amplayo et al., 2018)</ref>.</p><p>A competing approach relies on substitute vectors: each target instance is represented by a distribution over possible in-context probable substitutes for the word, and clustering is performed over these distributions. This approach by <ref type="bibr" target="#b3">Baskaya et al. (2013)</ref> was implemented in the AI-KU system using n-gram language models (LM).</p><p>In recent work <ref type="bibr" target="#b2">(Amrami and Goldberg, 2018)</ref>, henceforth referred to as LSDP (Language-model Substitution with Dynamic Patterns), we showed that by replacing the n-gram LM with ELMobased biLM <ref type="bibr">(Peters et al., 2018)</ref> and adding a dynamic patterns technique, the substitute vectors approach can achieve state-of-the-art results (Section 2).</p><p>In this work, we further explore the use of substitute-based approaches for WSI. After verifying that the approach transfers to the recently introduced BERT deep masked LM <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> (with a very significant improvement in WSI scores), we make two additional contributions to the mentioned method: (a) we present a method to move from a fixed number of senses across target words to choosing a dynamic number of senses for each target (as supported by most other WSI methods, e.g., <ref type="bibr">(Teh et al., 2005;</ref><ref type="bibr" target="#b7">Komninos and Manandhar, 2016;</ref><ref type="bibr" target="#b1">Amplayo et al., 2018)</ref>; and (b) we suggest to use the substitutes as a mean of interpreting / analyzing the resulting sense cluster by considering prominent word substitutes. This enables a more in-depth error analysis, showcasing and quantifying the remaining kinds of errors in WSI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The LSDP WSI Method</head><p>We describe the LSDP-algorithm, which we extend. Given k in-sentence instances of a target word which we wish to cluster into senses, each instance is associated with r representatives. Each representative is composed of n words, which are sampled with replacement from the LM. <ref type="bibr">2</ref> The sampled words are lemmatized, and each representative is then represented as a onehot vector of its lemmas (multiple occurrences of the same lemma within a representative are discarded). The resulting set of k * r 1-hot vectors goes through TFIDF transform to discount uninformative words, and the resulting vectors are clustered into a predefined number of clusters using hierarchical clustering with cosine distance and average linkage. This provides a hard clustering over representatives. This clustering is converted to a soft-clustering over instances, by associating each instance to a cluster according to the percentage of its representative that are assigned to that cluster.</p><p>Sampling the representatives from the ELMo biLM does not take into account the word itself when predicting substitutes. The dynamic patterns approach in LSDP overcomes this by querying the LM for a linguistically motivated manipulated context that take the target word into account. By way of example, to get substitutes for brown in "my dogs are brown", the forward LM is presented with "my dogs are brown and 2" rather than with "my dogs are 2". This encourages relevant substitutes such as black and discourages less desirable ones like barking.</p><p>3 Better Substitution Based WSI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">From ELMo to Bert</head><p>Contextualized vector representations from the recently introduced BERT model <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> were shown to outperform ELMo on several NLP tasks. Like ELMo, BERT is trained in a self-supervised manner to predict words in a sentence. BERT differs from ELMo by being based on a Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> instead of an LSTM, being truly bidirectional deep model, and, most importantly for our purposes, being trained in a "noisy masked LM" setup in which the training procedure receives a sentence, replaces some words with a MASK symbol and randomly perturbs some others, and then attempts to predict the original words from the resulting text. Thus, the model learns representations which are predictive of the words in context, and can also take into account the current word when making a prediction. For further details, see <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref>.</p><p>The dynamic patterns in LSDP were motivated by the BiLM not "seeing" the target word otherwise. In contrast, BERT's architecture and training procedure does allow providing the word together with its context when predicting substitutes. Simply replacing the ELMo LM with BERT's, without dynamic patterns, already provides state-ofthe-art results for the SemEval 2013 task, with an AVG score of 37.0 compared to 25.4 with the full LSDP. However, during BERT training words are sometimes randomly replaced, making BERT suspicious and often predicting substitutes by context alone: a probable substitute for "my <ref type="bibr">[dogs]</ref> are brown" according to BERT is eyes, which is clearly not a lexical substitute for dog.</p><p>This suggests BERT could also benefit from dynamic patterns. However, the pattern from Amrami and Goldberg (2018) did not show big improvements as-is. Instead, we use parenthetical patterns. We empirically found that the pattern "target ( or even [MASK] )" (e.g. my dogs (or even [MASK]) are brown) yields good results. <ref type="bibr">3</ref> This pattern resulted in similar scores to using the vanilla sentence and predicting over the target token, while yielding somewhat different results. We combine the two by averaging the logits from both predictions prior to the softmax. <ref type="bibr">4</ref> Results Evaluation on SemEval 2013 and Se-mEval 2010 yield state-of-the-art results on both datasets mainly due to BERT powerful LM <ref type="table">(Tables  Model   FNMI  FBC  AVG  Ours</ref> 21.4 (0.5) 64.0 (0.5) 37.0 (0.  1,2). The addition of dynamic patterns contribute almost 2 additional points to the already high AVG scores. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Number of Clusters</head><p>LSDP uses a fixed number of 7 clusters for all target words, a choice which was shown to work well on the SemEval 2013 task 13 dataset. However, using a fixed number this way is obviously sub-optimal. 6 Our proposal is based on the premise-supported by empirical observationthat the substitution-based representation clearly identifies the "core" senses that explain most of the mass in the data (as evident by the resulting high task scores), but also introduces some 5 To comply with the hard-clustering setup of SemEval 2010, we compute the soft-clustering and take the most probable sense for each instance. The hard clustering also suggests the use of V-measure and F-score instead of FNMI and noise around more niche usages or less clear-cut instances.</p><p>We thus follow a strategy in which we provide a relaxed upper-bound on the number of senses (we use 10), induce this number of clusters, and mark each cluster as being either weak or strong. We then discard the weak clusters, merging each of them into a corresponding strong cluster.</p><p>For each target, we induce a soft clustering of the corresponding word occurrences into a fixed number of c = 10 senses. Each instance (word occurrence) is now probabilisticaly associated with c senses. We say that a sense dominates an instance if it is the most probable sense for that instance. We identify senses that dominate less then m = 2 instances and mark them as weak senses. The remaining senses (those that dominate m or more instances) are marked as strong.</p><p>Recall that each sense is also a hard clustering over representatives. We associate each sense with the average vector of its representatives (centroid). For each of the weak senses w, we find the closest strong sense s to w according to the cosine distance between their centroids, assign ws representatives to s and discard w. We then re-do the soft clustering of instances based on the set of strong senses and the representatives within them.</p><p>Evaluating the dynamic number of senses Unfortunately, this dynamic sense number assignment did not improve AVG WSI scores on the Se-mEval 2013 dataset. However, eye-balling the results indicates that the method produces reasonable sense induction solutions. Digging further, we found out that using the gold (oracle) number of senses for each target also had a very minimal effect on the WSI scores (∼ 0.5 AVG addition). The AVG score in the SemEval 2013 WSI task is the geometric mean of the FNMI metric and the FBC metric, where the first one prefers many small clusters, while the second one prefer fewer and larger clusters. Neither of FNMI, FBC and AVG are sufficient for indicating a good number of clusters. <ref type="bibr">7</ref> The metrics also do not penalize overspecification of small senses: while FBC and F-S should discourage over-specification, their measures are proportional in instance pairs and would not punish small mass perturbations, even if those produce an excessive number of senses. We thus aim for a more direct measure for evaluating the produced number of sense-clusters.</p><p>Previous work, e.g. <ref type="bibr">(Song, 2016;</ref><ref type="bibr" target="#b1">Amplayo et al., 2018)</ref>, compare the absolute number of senses. We instead opt for the somewhat easier task of measuring the correlation between the number of induced senses and the gold number of senses. To motivate measuring correlation and not absolute numbers, recall that the SemEval task's sense-inventory is based on WordNet, whose sense hierarchy is very fine-grained. For example, it differentiates between dark used to describe skin color and dark used to describe objects such as pants. A coarser grained WSI solution may constantly produce fewer senses for each target yet still be valuable. 8 By measuring correlation to the gold number of senses rather than absolute difference we could add invariance to sense granularity.</p><p>Our system results in a spearman rank correlation of 0.43±0.05 (all p-values &lt; 0.03) with the gold number of senses on SemEval 2013. Is that a good number? To put the number in context, we compare it to the correlation of the gold WordNet senses to a human-created coarser sense inventory: senses in the New Oxford American Dictionary -NOAD <ref type="bibr">(McKean, 2005)</ref>. We map from WordNet to NOAD by using the work of <ref type="bibr">Yuan et al. (2016)</ref> who annotated the SemCor corpus with NOAD senses in addition to the existing WordNet ones. <ref type="bibr">9</ref> As expected, comparing the solution obtained by this oracle mapping to the SemEval 2013 gold labels result in very high WSI scores <ref type="bibr">(FNMI 52.1,</ref><ref type="bibr">FBC 84.7,</ref><ref type="bibr">AVG 66.4)</ref>. More interesting is the correlation of sense numbers: 0.47, compared to 0.43 obtained by our method. Our method spearman rank correlation to the number of senses in the NOAD labels is 0.44.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cluster Interpretability</head><p>The substitution-based method lends itself to introspection by considering the substitutes. We highlight the most prominent and informative word substitutes for each sense by computing the pointwise mutual information (PMI) between sub-stitute words and sense clusters. We then annotate each sense with its top 10 most associated substitutes (its signature). These sense signatures can be said to present the essence of what is captured by each sense cluster. As an example, one induced sense for the target meet(VERB) 10 is characterized by the words "convene", "group", "crowd", indicating the sense of a meeting that involves many participants. Interestingly, the Word-Net meet(VERB) entry does not make such a distinction between meeting types by the number of their participants, highlighting a case were the unsupervised algorithm refined the human curated lexicon. Inspecting clusters and their signatures allows us to identify good and bad clusters, and identify failure modes in its process, as we do in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detailed Error Analysis</head><p>Armed with the cluster signatures, we turn to manually inspect all the produced sense clusters and their associated words. We identify the following characteristic failure modes: LM: errors of the underlying BERT LM; 11 SPLIT: an additional cluster for an existing sense, for example the sense encouraging, close, personal, ... for warm(ADJ) when the sense compassionate, favorable, kind, ... already exists; TEMPLATE: substitutes rely excessively on a template-like pattern; TOPIC: substitutes rely excessively on topical words; MERGE: cluster includes several distinct senses; OTHER: cluster includes an incoherent mix of multiple senses with incoherent substitutes.</p><p>We sort the SemEval 2013 targets according to our accuracy on them, and consider the 20% top scoring targets (TOP), 20% middle scoring (MID), and 20% bottom scoring (BOT), each containing 10 targets. For each of these groups, we inspect all induced senses, and manually categorizing to the above failure cases, or to OK in case they are correct. <ref type="figure">Figure 1</ref> summarizes the results. 12 A clear trend is that the majority of issues relate to splitting and merging of clusters and relying on topical substitutions, while language-modeling and <ref type="bibr">10</ref> Additional examples are provided in the appendix. 11 Often resulting on transcribed speech, a domain BERT was not trained on. <ref type="bibr">12</ref> The supplementary material shows examples of the analyzed cases as well as suggestions for handling the identified failure cases in future work. template-following problems are far less severe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We improved a recent WSI method by allowing it to produce a dynamic number of senses, and by showing how the resulting clusters could be inspected and validated through the identification of per-sense characteristic substitutes. These are then used to perform error analysis of the method and its culprits, highlighting the major modes of failure and their prevalence, suggesting promising avenues for future work. Additionally, incorporating BERT as an LM improves the state of the art in two recent SemEval WSI tasks by a large margin, and validates the utility of the dynamic-patterns approach of <ref type="bibr" target="#b2">Amrami and Goldberg (2018)</ref>.</p><p>A Handling of identified failure modes Some of the failure modes mentioned in section 4 can be remedied by various means. LM and TEMPLATE cases are relatively rare and stand out when debugging the final solution. Their distinct distribution usually pushes them into clusters of their own, allowing identification and possibly their removal before rerunning the procedure.</p><p>Using a suitable LM for the target domain is important, and indeed most LM failure we encountered are due to transcribed spoken text. Finetuning BERT on the domain of interest could improve results.</p><p>The MERGE and TOPIC cases deal with the discerning resolution of our method. An interesting direction for future work is finding a way to collect additional target usages to better model the borderline cases. This also seems like a promising direction to take with SPLIT cases.</p><p>The OTHER classified senses are cases where our method completely fails. These include targets such as become(VERB) which are indeed hard to sense-induce without some mental process, specifically with substitutions alone. For example, the WordNet senses differentiate between "become: a change in state" and "become: transform into something else", similarly to Spanish's ser/estar distinction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Quality analysis examples</head><p>The following provides examples of the different error categories we use, as well as demonstrates the senses that are induced by the method for some cases, and their descriptions according to the PMI method.</p><p>Each table shows an induced sense, its highranking PMI words, and sentences associated to this sense. We additionally provide our assessment of that sense (OK, SPLIT, MERGE, TOPIC, TEMPLATE, LM, OTHER), as well as the goldlabel WordNet sense for each sentence.</p><p>We begin with the senses for meet(VERB), a target that our method scores high on ( <ref type="table">Table 3)</ref>. We follow with wait(VERB), a target which our methods scores low on <ref type="table">(Table 4</ref>). Finally, <ref type="table">Table  5</ref> demonstrates the error categories not present in the previous ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>FBC for SemEval 2010. We use = 200, temperature = 1.25, nReps = 15, nSenses = 7, minInstances = 2, nRepSamps = 20. The compared systems are: AutoSense (Amplayo et al., 2018), MCC-S (Komninos and Manandhar, 2016), SenseTopic (Wang et al., 2015), SE-WSI-fix (Song, 2016). AI-KU (Baskaya et al., 2013) , BNP-HC (Teh et al., 2005) , LDA (Blei et al., 2003). Numbers taken from the corresponding publications. 6 Indeed, other works on WSI (Teh et al., 2005; Komninos and Manandhar, 2016; Amplayo et al., 2018) does attempt to infer the number of clusters for each sense. Teh et al. (2005) do it by employing a stick breaking clustering process and Komninos and Manandhar (2016) use a model selection criteria to prevent over specification.Amplayo et al. (2018) preform a cleanup stage similar to what we propose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation Results on the SemEval 2010 Task 14 Dataset. We report our mean (STD) scores over 10 runs. ND: no dynamic patterns.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Things are often more challenging: a warm hand, for example, might belong to either of the senses above, requiring larger context to disambiguate.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Sampling is performed from the softmax distribution over the top-logits, and while ignoring the bias terms.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Among others, our search for other patterns included parenthesized and apposition based lexical substitute promoting patterns such as and [MASK],or [MASK] as well as hyponymy promoting patterns like such as[MASK]   4 We also further divide the inputs logits by a temperature parameter to smooth the distribution and diversify the words. We also lemmatize BERT word pieces to predicted lemmatized words, and pad the sentence with the [CLS] and [SEP] tokens as required by BERT.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The story for the SemEval 2010 WSI task metrics is similar, with V-Measure favoring smaller clusters and F-S favoring larger ones.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For example, in the context of query-based search, a user may be satisfied with the coarse grained distinction of "dark(blackness) times" and "dark(sad) times".9  We take the most probable NOAD sense to each Word-Net one, according to the parallel corpus. This reduces the 399 senses SemEval to 205 (89 of which were not found in SemCor and left intact). Overall, 87% of the tokens were mapped to their coarse grained NOAD senses.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>High PMI words This template of X set her sights on Y, produces a distinct substitutes distribution which pushes the sentences above into a sense of their own. This "over-fits" the target usage, giving weight to substitutes such as "lipstick" in the example above. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 02: Evaluating word sense induction and discrimination systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations, Se-mEval &apos;07</title>
		<meeting>the 4th International Workshop on Semantic Evaluations, Se-mEval &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Autosense model for word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amplayo</forename></persName>
		</author>
		<idno>abs/1811.09242</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Seung-won Hwang, and Min Song</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word sense induction with neural bilm and symmetric patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Amrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4860" to="4867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ai-ku: Using substitute vectors and cooccurrence modeling for word sense induction and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osman</forename><surname>Baskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enis</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="300" to="306" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 13: Word sense induction for graded and non-graded senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Klapaftis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured generative models of continuous features for word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Komninos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COL-ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3577" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">unimelb: Topic modelling-based word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="307" to="311" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 14: Word sense induction &amp; disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sameer S Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on semantic evaluation</title>
		<meeting>the 5th international workshop on semantic evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">,</forename></persName>
		</author>
		<title level="m">Class High PMI words 1 OK hug, pick, thank</title>
		<imprint/>
	</monogr>
	<note>surprise, impressed, marrie, hire, welcome , offer, below, violate, comply, accomplish, supply,complete,fill,accommodate</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">) 3 SPLIT conversation, summit, friendly, discussion, business, spend, touching, partner, dining • it&apos;s gonna make the people they&apos;re meeting with feel very uncomfortable</title>
	</analytic>
	<monogr>
		<title level="m">• They could not meet conditions if their competitors were free to ignore them</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>) • Best wishes until we meet again-perhaps over</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Atta agreed to meet later at a location to be determined</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>He</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>7) 4 OK group, convention, weekly, schedule, parliament, convene, celebrate,crowd,originate</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">• A group called the League of Prizren, named for the Kosovo town where it met</title>
		<imprint/>
	</monogr>
	<note>) • cat and bagpipean society a society which met at their office</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">) 5 OK direct, encounter, dare, oppose, reaction, repulse, cause, underwent, face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">• A</forename><surname>Summer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Antiekmarkt or antique market meets at Nieuwmarkt on Sundays. react</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">• They were greeted as liberators by the peasants and met only desultory resistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>astounded by the funny logic of, say, meeting one&apos;s match</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">this understanding has to meet with such hostility, don&apos;t you think? (4) 6 OK maximum, phase, interval, curve, origin, converge, respectively, cancel, border, dip • we can draw a line of those tangencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>It</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>that meet at the initial apple-pear distributions... (6) 7 TOPIC investigate, cost, fund, ease, budget, recover, shoulder, offset, slash, decrease</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">appealed to the state government to help meet the cost of burying armed robbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>) WordNet senses for meet(VERB) in gold labels: 1. get to know. get acquainted with 2. collect in one place</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">At the end of each sentence in parentheses is its tagged WordNet sense in the gold labels. In our manual inspection, Sense #1 seems semantically close enough to be classified OK. Sense #3 is classified as SPLIT due its similarity to sense #1. Sense #7 is classified as TOPIC due to its substitutes which are not lexical. It&apos;s interesting to see the method induced a sense for group meeting, sense #4, with substitutes such as convene and crowd</title>
	</analytic>
	<monogr>
		<title level="m">Table 3: Senses induced for the target meet(VERB) on which our method perform relatively well</title>
		<imprint>
			<biblScope unit="page">creep</biblScope>
		</imprint>
	</monogr>
	<note>Class High PMI. words 1 OK lodge, cool, bray, rock, glide, bend, hush, groom, camp</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">• The horses wait under the cooling shade for their next customers. (1) 2 ∼OK bench, staff, guest, bounce</title>
		<imprint/>
	</monogr>
	<note>pat, pit, ticket, fare, to, other</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">offer the best sightlines, roomier seats, and wait staff who peddle gourmet fare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>3) 3 OTHER reasonable, slack, qualify, delivery, short, a, temporary, week, hesitation, due</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">without the need to wait until everyone is in town for a meeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">operator will be paid at some average earnings rate during the waiting period</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">) 4 TOPIC literally, cooking, everything, pregnant, family, lot, town, money, forever, food • He had a farm waiting for him right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>. and uh i would agree a a short waiting period would be appropriate to uh</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">was a time bomb waiting to explode, then</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><surname>• If Clinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">as you wouldn&apos;t if you had a wife who looked like that waiting for you</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>1) 5 LM ago, they, fade, along, since, drank, though, afterwards, sometimes. uh</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">i sang in a couple of uh community choirs and then um waited for a while</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4) 6 OTHER write, reach, argue, appear, bother, act, seem, star. wish</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">In our manual inspection, sense #6 is classified as OTHER, this class of found senses usually group up a large portion of unrelated sentence, making their differentiating substitutes an incoherent bag of left-overs. Sense #2&apos;s substitutes aren&apos;t very informative for its sense</title>
	</analytic>
	<monogr>
		<title level="m">Table 4: Senses induced for the target wait(VERB) on which our method perform poorly</title>
		<imprint/>
	</monogr>
	<note>but they do distinguish this sentence from the other sensed sentences</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
