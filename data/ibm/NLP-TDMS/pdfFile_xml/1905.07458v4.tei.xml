<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Metric Learning for Fast End-to-End Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Tran</surname></persName>
							<email>tung.tran@uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
							<email>ramakanth.kavuluru@uky.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Division of Biomedical Informatics Department of Internal Medicine Department of Computer Science</orgName>
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Metric Learning for Fast End-to-End Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relation extraction (RE) is an indispensable information extraction task in several disciplines. RE models typically assume that named entity recognition (NER) is already performed in a previous step by another independent model. Several recent efforts, under the theme of end-to-end RE, seek to exploit inter-task correlations by modeling both NER and RE tasks jointly. Earlier work in this area commonly reduces the task to a table-filling problem wherein an additional expensive decoding step involving beam search is applied to obtain globally consistent cell labels. In efforts that do not employ table-filling, global optimization in the form of CRFs with Viterbi decoding for the NER component is still necessary for competitive performance. We introduce a novel neural architecture utilizing the table structure, based on repeated applications of 2D convolutions for pooling local dependency and metric-based features, that improves on the state-of-the-art without the need for global optimization. We validate our model on the ADE and CoNLL04 datasets for end-to-end RE and demonstrate ≈ 1% gain (in F-score) over prior best results with training and testing times that are seven to ten times faster -the latter highly advantageous for time-sensitive end user applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Information extraction (IE) systems are fundamental to the automatic construction of knowledge bases and ontologies from unstructured text. While important, in and of themselves, these resulting resources can be harnessed to advance other important language understanding applications including knowledge discovery and question answering systems. Among IE tasks are named entity recognition (NER) and binary relation extraction (RE) which involve identifying named entities and relations among them, respectively, where the latter is typically a set of triplets identifying pairs of related entities and their relation types.</p><p>We present <ref type="figure" target="#fig_0">Figure 1</ref> as an example of the NER and RE problem given the input sentence "Mrs. Tsuruyama is from Yatsushiro in Kumamoto Prefecture in southern Japan." First, we extract as entities the spans "Mrs. Tsuruyama", "Yatsushiro", "Kumamoto Prefecture", and "Japan" where "Mrs. Tsuruyama" is of type PERSON and the rest are of type LOCATION. Thus, NER consists of identifying both the bounds and type of entities mentioned in the sentence. Once entities are identified, the next step is to extract relation triplets of the form <ref type="bibr">(subject,predicate,object)</ref>, if any, based on the context; for example, (Mrs. Tsuruyama, LIVE_IN, Yatsushiro) is a relation triple that may be extracted from the example sentence as output of an RE system. Given this, it is clear that E2ERE is a complex problem given the sparse nature of the output space; for a sentence of n length with k possible relation types, the output is a variable-length set of relations each drawn from kn 2 possible relation combinations.</p><p>NER and RE have been traditionally treated as independent problems to be solved separately and later combined in an ad-hoc manner as part of a pipeline system. End-to-end RE (E2ERE) is a relatively new research direction that seeks to model NER and RE jointly in a unified architecture. As these tasks are closely intertwined, joint models that simultaneously extract entities and their relations in a single framework have the capacity to exploit inter-task correlations and dependencies leading to potential performance gains. Moreover, joint approaches, like our method, are better equipped to handle datasets where entity annotations are non-exhaustive (that is, only entities involved in a relation are annotated), since standalone NER systems are not designed to handle incomplete annotations. Recent advancements in deep learning for E2ERE are broadly divided into two categories: (1). The first category involves applying deep learning to the table structure first introduced by <ref type="bibr" target="#b25">Miwa and Sasaki (2014)</ref>, including <ref type="bibr" target="#b10">Gupta, Schütze, and Andrassy (2016)</ref>, <ref type="bibr" target="#b27">Pawar, Bhattacharyya, and Palshikar (2017)</ref>,  where E2ERE is reduced to some variant of the table-filling problem such that the (i, j)-th cell is assigned a label that represents the relation between tokens at positions i and j in the sentence. We further describe the table-filling problem in Section 3.1. Recent approaches based on the table structure operate on the idea that cell labels are dependent on features or predictions derived from preceding or adjacent cells; hence, the table is filled incrementally leading to potential efficiency issues. Also, these methods typically require an additional expensive decoding step, involving beam search, to obtain a globally optimal table-wide label assignment. (2). The second category includes models where NER and RE are modeled jointly with shared components or parameters without the table structure. Even state-of-the-art methods not utilizing the table structure rely on conditional random fields (CRFs) as an integral component of the NER subsystem where Viterbi algorithm is used to decode the best label assignment at test time <ref type="bibr">(Bekoulis et al. 2018b,a)</ref>.</p><p>Our model utilizes the table formulation by embedding features along the third dimension. We overcome efficiency issues by utilizing a more efficient and effective approach for deep feature aggregation such that local metric, dependency, and position based features are simultaneously pooled -in a 3 × 3 cellular window -over many applications of the 2D convolution. Intuitively, preliminary decisions are made at earlier layers and corroborated at later layers. Final label assignments for both NER and RE are made simultaneously via a simple softmax layer. Thus, computationally, our model is expected to improve over earlier efforts without a costly decoding step. We validate our proposed method on the CoNLL04 dataset <ref type="bibr" target="#b34">(Roth and Yih 2004)</ref> and the ADE dataset <ref type="bibr" target="#b11">(Gurulingappa et al. 2012)</ref>, which correspond to the general English and the biomedical domain respectively, and show that our method improves over prior state-of-the-art in E2ERE. We also show that our approach leads to training and testing times that are seven to ten times faster, where the latter can be critical for time-sensitive end-user applications. Lastly, we perform extensive error analyses and show that our network is visually interpretable by examining the activity of hidden pooling layers (corresponding to intermediate decisions). To our knowledge, our study is the first to perform this type of visual analysis of a deep neural architecture for end-to-end relation extraction. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we provide an overview of three main types of relation extraction methods in the literature: studies that are limited to relation classification, early E2ERE methods that assume known entity bounds, and recent efforts on E2ERE that perform full entity recognition and relation extraction in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relation Classification</head><p>The majority of past and current efforts in relation extraction treat the problem as a simpler relation classification problem where pairs of entities are known during test time; the goal is to classify the pair of entities, given the context, as being either positive or negative for a particular type of relation. Many works on relation classification preprocess the input as a dependency parse tree <ref type="bibr" target="#b4">(Bunescu and Mooney 2005;</ref><ref type="bibr" target="#b30">Qian et al. 2008</ref>) and exploit features corresponding to the shortest dependency path between candidate entities; this general approach has also been successfully applied in the biomedical domain <ref type="bibr" target="#b1">(Airola et al. 2008;</ref><ref type="bibr" target="#b9">Fundel, Küffner, and Zimmer 2007;</ref><ref type="bibr" target="#b20">Li et al. 2008;</ref><ref type="bibr" target="#b26">Özgür et al. 2008)</ref>, where they typically involve a graph kernel based Support Vector Machine (SVM) classifier <ref type="bibr" target="#b20">(Li et al. 2008;</ref><ref type="bibr" target="#b33">Rink, Harabagiu, and Roberts 2011)</ref>. The concept of network centrality has also been explored <ref type="bibr">(Özgür et al. 2008)</ref> to extract gene-disease relations. Other studies, such as the effort by <ref type="bibr" target="#b8">Frunza, Inkpen, and Tran (2011)</ref>, apply the more traditional bag-of-words approach focusing on syntactic and lexical features while exploring a wide variety of classification algorithms including decision trees, SVMs, and Naïve Bayes. More recently, innovations in relation extraction have centered around designing meaningful deep learning architectures. <ref type="bibr" target="#b22">Liu et al. (2016)</ref> proposed a dependency-based convolutional neural network (CNN) architecture wherein the convolution is applied over words adjacent according to the shortest path connecting the entities in the dependency tree, rather than words adjacent with respect to the order expressed, to detect drug-drug interactions (DDIs). In <ref type="bibr" target="#b16">Kavuluru, Rios, and Tran (2017)</ref>, ensembling of both character-level and word-level recurrent neural networks (RNNs) is further proposed for improved performance in DDI extraction. <ref type="bibr" target="#b31">Raj, SAHU, and Anand (2017)</ref> proposed a deep learning architecture such that word representations are first processed by a bidirectional RNN followed by a standard CNN, with an optional attention mechanism towards the output layer. <ref type="bibr" target="#b42">Zhang, Qi, and Manning (2018)</ref> showed that relation extraction performance can be improved by applying graph convolutions over a pruned version of the dependency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">End-to-End Relation Extraction with Known Entity Bounds</head><p>Early efforts in E2ERE, as covered in this section, assume that entity bounds are known during test time. Hence, the NER aspect of these methods is limited to classifying entity type (e.g., is "President Kennedy" a person, place, or organization?). In a seminal work, <ref type="bibr" target="#b34">Roth and Yih (2004)</ref> proposed an integer linear programming (LP) approach to tackle the end-to-end problem.</p><p>They discovered that the LP component was effective in enhancing classifier results by reducing semantic inconsistencies in the predictions compared to a traditional pipeline wherein the outputs of an NER component are passed as features into the RE component. Their results indicate that there are mutual inter-dependencies between NER and RE as subtasks which can be exploited. The LP technique has been also been successfully applied in jointly modeling entities and relations with respect to opinion recognition by <ref type="bibr" target="#b6">Choi, Breck, and Cardie (2006)</ref>. <ref type="bibr" target="#b14">Kate and Mooney (2010)</ref> proposed a similar approach but presented a global inference mechanism induced by building a graph resembling a card-pyramid structure. A dynamic programming algorithm, similar to CYK (Jurafsky and Martin 2008) parsing, called card-pyramid parsing is applied along with beam search to identify the most probable joint assignment of entities and their relations based on outputs of local classifiers. Other efforts to this end involve the use of probabilistic graphical models <ref type="bibr" target="#b38">(Yu and Lam 2010;</ref><ref type="bibr" target="#b35">Singh et al. 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">End-to-End Relation Extraction</head><p>Li and Ji (2014) proposed one of the first truly joint models wherein entities, including entity mention bounds, and their relations are predicted. Structured perceptrons <ref type="bibr" target="#b7">(Collins 2002)</ref>, as a learning framework, are used to estimate feature weights while beam search is used to explore partial solutions to incrementally arrive at the most probable structure. <ref type="bibr" target="#b25">Miwa and Sasaki (2014)</ref> proposed the idea of using a table representation which simplifies the task into a table-filling problem such that NER and relation labels are assigned to cells of the table; the aim was to predict the most probable label assignment to the table, out of all possible assignments, using beam search. While the representation is in table form, beam search is performed sequentially, one cellassignment per step. The table-filling problem for E2ERE has since been successfully transferred to the deep neural network setting <ref type="bibr" target="#b10">(Gupta, Schütze, and Andrassy 2016;</ref><ref type="bibr" target="#b27">Pawar, Bhattacharyya, and Palshikar 2017;</ref><ref type="bibr" target="#b41">Zhang, Zhang, and Fu 2017)</ref>.</p><p>Other recent approaches not utilizing a table structure involve modeling the entity and relation extraction task jointly with shared parameters <ref type="bibr" target="#b24">(Miwa and Bansal 2016;</ref><ref type="bibr" target="#b19">Li et al. 2016;</ref><ref type="bibr" target="#b43">Zheng et al. 2017a;</ref><ref type="bibr" target="#b18">Li et al. 2017;</ref><ref type="bibr" target="#b15">Katiyar and Cardie 2017;</ref><ref type="bibr" target="#b3">Bekoulis et al. 2018b;</ref><ref type="bibr" target="#b40">Zeng et al. 2018</ref>). <ref type="bibr" target="#b15">Katiyar and Cardie (2017)</ref> and <ref type="bibr" target="#b3">Bekoulis et al. (2018b)</ref> specifically use attention mechanisms for the RE component without the need for dependency parse features. <ref type="bibr" target="#b44">Zheng et al. (2017b)</ref> operate by reducing the problem to a sequence-labeling task that relies on a novel tagging scheme. <ref type="bibr" target="#b40">Zeng et al. (2018)</ref> use an encoder-decoder network such that the input sentence is encoded as fixedlength vector and decoded to relation triples directly. Most recently, <ref type="bibr" target="#b2">Bekoulis et al. (2018a)</ref> found that adversarial training (AT) is an effective regularization approach for E2ERE performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We present our version of the table-filling problem, a novel neural network architecture to fill the table, and details of the training process. Here, Greek letter symbols are used to distinguish hyper-parameters from variables that are learned during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Table-Filling Problem</head><p>Given a sentence of length n, we use an n × n table to represent a set of semantic relations such that the (i, j)-th cell represents the relationship (or non-relation) between tokens i and j. In practice, we assign a tag for each cell in the table such that entity tags are encoded along the diagonal while relation tags are encoded at non-diagonal cells. For entity recognition, we use the BILOU tagging scheme <ref type="bibr" target="#b32">(Ratinov and Roth 2009</ref>). In the BILOU scheme, B, I, and L tags are used to indicate the beginning, inside, and last token of a multi-token entity respectively. The O tag indicates whether the token outside of an entity span, and U is used for unit-length entities.</p><p>In tabular form, entity and relation tags are drawn from a unified list Z serving as the label space; that is, each cell in the table is assigned exactly one tag from Z. For simplicity, the O tag is also used to indicate a null relation when occurring outside of a diagonal. As each entity type requires a BILOU variant, a problem with n ent entity types and n rel relation types has |Z| = 4n ent + n rel + 1 where the last term accounts for the O tag. Our conception of the table-filling problem differs from <ref type="bibr" target="#b25">Miwa and Sasaki (2014)</ref> in that we utilize the entire table as opposed to only the lower triangle; this allows us to model directed relations without the need for additional inverse-relation tags. Moreover, we assign relation tags to cells where entity spans intersect instead of where head words intersect; thus encoded relations manifest as rectangular blocks in the proposed table representation. We present a visualization of our table representation in <ref type="figure">Figure 2</ref>. At test time, entities are first extracted, and relations are subsequently extracted by averaging the output probability estimates of the blocks where entities intersect. We describe the exact procedure for extracting relations from these blocks at test-time in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Model: Relation-Metric Network</head><p>We propose a novel neural architecture, which we call the relation-metric network, combining the ideas of metric learning and convolutional neural networks (CNNs) for table filling. The schematic of the network is shown in <ref type="figure" target="#fig_2">Figure 3</ref>, whose components will be detailed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Context Embeddings</head><p>Layer. In addition to word embeddings, we employ character-CNN based representations as commonly observed in recent neural NER models <ref type="bibr" target="#b5">(Chiu and Nichols 2016)</ref> and E2ERE models <ref type="bibr" target="#b18">(Li et al. 2017</ref>). Character-based features can capture morphological Overview of the network architecture for λ = 2. For simplicity, we ignore punctuation tokens.</p><p>features and help generalize to out-of-vocabulary words. For the proposed model, such representations are composed by convolving over character embeddings of size π using a window of size 3, producing η feature maps; the feature maps are then max-pooled to produce η-length feature representations. As our approach is standard, we refer readers to <ref type="bibr" target="#b5">Chiu and Nichols (2016)</ref> for full details. This portion of the network is illustrated in step 1 of <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Suppose the input is a sentence of length n represented by a sequence of word indices w 1 , . . . , w n into the vocabulary V Word . Each word is mapped to an embedding vector via embedding matrices E Word ∈ R |V Word |×δ such that δ is a hyperparameter that determines the size of word embeddings. Next, let C[i] be the character-based representation for the i th word. An input sentence is represented by matrix S wherein rows are words mapped to their corresponding embedding vectors; or concretely,</p><formula xml:id="formula_0">S =    E Word [w 1 ] C[1]</formula><p>. . .</p><formula xml:id="formula_1">E Word [w n ] C[n]   </formula><p>where is the vector concatenation operator and E Word [i] is the i th row of E Word . Next, we compose context embedding vectors (CVs), which embed each word of the sentence with additional contextual features. Suppose − −−− → LSTM and ← −−− − LSTM represent a long short term memory (LSTM) network composition in the forward and backward direction, respectively, and let ρ be a hyperparameter that determines context embedding size. We feed S to a Bi-LSTM layer of hidden unit size 1 2 ρ such that</p><formula xml:id="formula_2">− → h i = − −−− → LSTM(S[i]), ← − h i = ← −−− − LSTM(S[i]), h i = − → h i ← − h i , for i = 1, . . . , n,</formula><p>where S[i] is the i th row of S and h i ∈ R ρ represents the context centered at the i th word. The output of the Bi-LSTM can be represented as a matrix H ∈ R n×ρ such that H = h 1 , . . . , h n . This concludes step 2 of <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Relation-Metric Learning.</head><p>Our goal is to design a network such that any two CVs can be compared via some "relatedness" measure; that is, we wish to learn a relatedness measure (as a parameterized function) that is able to capture correlative features indicating semantic relationships. A common approach in metric learning to parameterize a relatedness function is to model it in bilinear form. Here, for input vectors x, z ∈ R m , a similarity function in bilinear form is formally defined as</p><formula xml:id="formula_3">s R (x, z) = x Rz<label>(1)</label></formula><p>where R ∈ R m×m is a parameter of the relatedness function, dubbed a relation-metric embedding matrix, that is learned during the training process.</p><p>In machine learning research, Eq. 1 is also associated with a type of attention mechanism commonly referred to as "multiplicative" attention <ref type="bibr" target="#b23">(Luong, Pham, and Manning 2015)</ref>. However, we apply Eq. 1 with the classical goal of learning a variety of metric-based features. Our aim is to compute s R for all pairs of CVs in the sentence. Concretely, we can compute a "relationalmetric table" G ∈ R n×n over all pairs of CVs in the sentence such that G i,j = h i Rh j . In fact, we can learn a collection of κ similarity functions corresponding to κ relation metric tables; for our purposes, this is analogous to learning a diverse set of convolution filters in the context of CNNs. Thus we have the 3-dimensional tensor</p><formula xml:id="formula_4">G i,j,k = h i R k h j , for k = 1, . . . , κ,<label>(2)</label></formula><p>with G ∈ R n×n×κ where the first and second dimension correspond to word position indices while the third dimension embeds metric-based features. This constitutes step 3 of <ref type="figure" target="#fig_2">Figure 3</ref>. We show how G is consumed by the rest of the network in Section 3.2.6. However, as a prerequisite, we first describe how dependency parse and relative position information is prepared in Section 3.2.3 and Section 3.2.4 respectively and define the 2D convolution in Section 3.2.5. <ref type="table">Table.</ref> Let V dep be the vocabulary of syntactic dependency tags (e.g., nsubj, dobj). For an input sentence, let T = {(a 1 , b 1 , z 1 ), . . . , (ad, bd, zd)} be the set of dependency relations where z i are mappings to tags in V dep that express the dependencybased relations between pairs of words at positions a i , b i ∈ {1, . . . , n}, respectively. We define the dependency embedding matrix as F dep ∈ R |V dep |×β , where each unique dependency tag is a β-dimensional embedding. We compose the dependency representation tensor D for T as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Dependency Embeddings</head><formula xml:id="formula_5">D i,j,k = F dep t,k if (i, j, t) ∈ T or (j, i, t) ∈ T , φ k otherwise,</formula><p>for k = 1, . . . , β, where φ is a trainable embedding vector representing the null dependency relation. As shown in the above equation for D i,j,k , we embed the dependency parse tree simply as an undirected graph. <ref type="table">Table.</ref> First proposed by <ref type="bibr" target="#b39">Zeng et al. (2014)</ref>, so called position vectors have been shown to be effective in neural models for relation classification. Position vectors are designed to encode the relative offset between a word and the two candidate entities (for RE) as fixed-length embeddings. We bring this idea to the tabular setting by proposing a position embeddings table P , which is composed the same way as the dependencies table; however, instead of dependency tags, we simply encode the distance between two candidate CVs as discrete labels mapped to fixed-length embeddings (of size γ, a hyperparameter). It</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Position Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4</head><p>2D convolution on 3D input with padding is straightforward to see there will be 2(n max − 1) + 1 distinct position offset labels where n max is the maximum length of a sentence in the training data. Specifically, given a position vocabulary V dist , associated position embedding matrix</p><formula xml:id="formula_6">F dist ∈ R |V dist |×γ , the position embed- dings tensor is P i,j,k = F dist (i−j),k for k = 1, . . . , γ.</formula><p>As an implementation detail, we set V dist to {−n max , . . . , n max } where n max is the maximum sentence length over all training examples. Both dependency and position embedding tensors are concatenated to the metric tensor (Eq. (2)) along the 3rd dimension prior to every convolution operation. Hence they are shown in steps 4 and 6 of <ref type="figure" target="#fig_2">Figure 3</ref> for the network with two convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">2D Convolution</head><p>Operation. Unlike the standard 2D convolution typically used in NLP tasks, which takes 2D input, our 2D convolution operates on 3D input commonly seen in computer vision tasks where colored image data has height, width, and an additional dimension for color channel. The goal of the 2D convolution is to pool information within a 3 × 3 window along the first two dimensions such that metric features and dependency/positional information of adjacent cells are pooled locally over several layers. However, it is necessary to perform a padded convolution to ensure that dimensions corresponding to word positions are not altered by the convolution. We denote this padding transformation using the hat accent. That is, for some tensor input X ∈ R n×n×m , the padded version is X ∈ R (n+2)×(n+2)×m and the zero-padding exists at the beginning and at the end of the first and second dimensions. Next, we define the 2D convolution operation via the operator which corresponds to an element-wise product of two tensors followed by summation over the products; formally, for two input tensors A and B,</p><formula xml:id="formula_7">A B = i j k A i,j,k B i,j,k . Now our 2D convolution step is a tensor map f v (X) : R n×n×u → R n×n×v with v filters of size 3 × 3 × u, defined as f v (X) i,j,k = W k X [i:i+2][j:j+2][1:u] + b k<label>(3)</label></formula><p>for i = 1, . . . , n, j = 1, . . . , n, k = 1, . . . , v, where W k ∈ R 3×3×u for k = 1, . . . , v, and b ∈ R v are filter and bias variables respectively, and X [i:i+2][j:j+2][1:u] is a 3 × 3 × u window of X from i to i + 2 along the first dimension, j to j + 2 along the second dimension, and 1 to u along the final dimension. We show how f v (X) is used to repeatedly pool contextual information in Section 3.2.6. Instead of a 3 × 3 window, the convolution operation can be over any t × t window for some odd t ≥ 3 where large t values lead to larger parameter spaces and multiplication operations. The 2D convolution is illustrated in <ref type="figure">Figure 4</ref> and manifests in steps 5 and 7 of <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>3.2.6 Pooling Mechanism. Central to our architecture is the iterative pooling mechanism designed so that preliminary decisions are made in early iterations and further corroborated in subsequent iterations. It also facilitates the propagation of local metric and dependency/positional features to neighboring cells. Let Z be the set of tags for the target task. We denote hyperparameters κ and λ as the number of channels and the number of CNN layers respectively, where κ is same hyperparameter previously defined to represent the size of metric-based features. The pooling layers are defined recursively with base case L 1 = relu(f κ (G D P )) and</p><formula xml:id="formula_8">L i = relu(f κ (L i−1 D P )) 1 &lt; i &lt; λ, f |Z| (L i−1 D P ) i = λ,</formula><p>where f is the convolution function from Eq. <ref type="formula" target="#formula_7">(3)</ref>, G is the tensor from Eq. <ref type="formula" target="#formula_4">(2)</ref>, and is the tensor concatenation operator along the third dimension, and relu(x) = max(0, x) is the linear rectifier activation function. Here, κ and λ determine the breadth and depth of the architecture. A higher λ corresponds to a larger receptive field when making final predictions. For example at λ = 2, the decision at some cell is informed by its immediate neighbors with a receptive field of 3 × 3. However, at λ = 3, decisions are informed by all adjacent neighbors in a 5 × 5 window. The last layer, L λ , is the output layer immediately prior to application of the softmax function. Given the architecture in <ref type="figure" target="#fig_2">Figure 3</ref> with two convolutional layers, the convolve-and-pool operation is applied twice, indicated as steps 5 and 7 in the figure.</p><p>3.2.7 Softmax Output Layer. Given L λ , we apply the softmax function along the third dimension to obtain a categorical distribution tensor Q ∈ R n×n×|Z| over output tags Z for each word position pair such that Q i,j,k = exp(L λ i,j,k )/( |Z| l=1 exp(L λ i,j,l )), where Q i,j,k is the probability estimate of the pair of words at position i and j being assigned the kth tag. This constitutes the final step 8 of the network <ref type="figure" target="#fig_2">(Figure 3</ref>). Suppose Y ∈ R n×n×|Z| represents the corresponding onehot encoded ground truth along the third dimension such that Y i,j,k ∈ {0, 1}. Then the examplebased loss is obtained by summing the categorical cross-entropy loss over each cell in the table, normalized by the number of words in the sentence; that is,</p><formula xml:id="formula_9">(Y, Q; θ) = − 1 n n i=1 n j=1 |Z| k=1 Y i,j,k log(Q i,j,k ),</formula><p>where θ is the network parameter set. During training, the loss is computed per example and averaged along the mini-batch dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>While we learn concrete tags during training, the process for extracting predictions is slightly more nuanced. Entity spans are straightforwardly extracted by decoding BILOU tags along the diagonal. However, RE is based on "ensembling" the cellular outputs of the table where entity spans intersect. For entities a and b represented by their starting and ending offsets, (a S , a E ) and (b S , b E ), the relation between them is the label computed as argmax 1≤k≤|Z|</p><formula xml:id="formula_10">a E i=a S b E j=b S Q i,j,k ,</formula><p>which indexes a tag in the label space Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>In this section, we describe the established evaluation method, the datasets used for training and testing, and the configuration of our model. We note that the computing hardware is controlled across experiments given we report training and testing run times. Specifically, we used the Amazon AWS EC2 p2.xlarge instance which supports the NVIDIA Tesla K80 GPU with 12 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>We use the well-known F1 measure (along with precision and recall) to evaluate NER and RE subtasks as in prior work. For NER, a predicted entity is treated as a true positive if it is exactly matched to an entity in the groundtruth based on both character offsets and entity type. For RE, a predicted relation is treated as a true positive if it is exactly matched to a relation in the ground truth based on subject/object entities and relation type. As relation extraction performance directly subsumes NER performance, we focus purely on relation extraction performance as the primary evaluation metric of this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>CoNLL04. We use the dataset originally released by <ref type="bibr" target="#b34">Roth and Yih (2004)</ref>  PubMed abstracts and are divided in two partitions: the first partition of 6821 sentences contain at least one drug/disease pair while the second partition of 16695 sentences contain no drug/disease pairs. As with prior work <ref type="bibr" target="#b19">(Li et al. 2016</ref><ref type="bibr" target="#b18">(Li et al. , 2017</ref><ref type="bibr" target="#b3">Bekoulis et al. 2018b</ref>,a), we only use examples from the first partition from which 120 relations with nested entity annotations (such as "lithium intoxication" where lithium and lithium intoxication are the drug/disease pair) are removed. Since sentences are duplicated for each pair of drug/disease mention in the original dataset, when collapsed on unique sentences, the final dataset used in our experiments constitutes 4271 sentences in total. Given there are no official train-test splits, we report results based on 10-fold cross-validation, where results are based on averaging performance across the ten folds, as in prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Configuration</head><p>We tuned our model on the CoNLL04 development set; the corresponding configuration of our model (including hyperparameter values) used in our main experiments is shown in <ref type="table" target="#tab_1">Table 1</ref>. For the ADE dataset, we used Word2Vec embeddings pretrained on the corpus of PubMed abstracts <ref type="bibr" target="#b29">(Pyysalo et al. 2013)</ref>. For the CoNLL04 dataset, we used GloVe embeddings pretrained on Wikipedia and Gigaword <ref type="bibr" target="#b28">(Pennington, Socher, and Manning 2014)</ref>. All other variables are initialized using values drawn from a normal distribution with a mean of 0 and standard deviation of 0.1 and further tuned during training. Words were tokenized on both spaces and punctuations; punctuation tokens were kept as is common practice for NER systems. For part-of-speech and dependency parsing, we use the well-known tool spaCy 2 . For both datasets, we used projective dependency parses produced from the default pretrained English models. We found that using models pretrained on biomedical text (namely, the GENIA <ref type="bibr" target="#b17">(Kim et al. 2003</ref>) corpus) did not improve performance on the ADE dataset. Early experiments showed that applying exponential decay to the learning rate in conjunction with batch normalization <ref type="bibr" target="#b12">(Ioffe and Szegedy 2015)</ref> is essential for stable/effective learning for this particular architecture. We apply exponential decay to the learning rate such that it is roughly halved every 10 epochs; concretely, r k = r b k 10 where r b is the base learning rate and r k is the rate at the kth epoch. We apply dropout <ref type="bibr" target="#b36">(Srivastava et al. 2014</ref>) on h i for i = 1, . . . , n as regularization at the earlier layers. However, dropout had a detrimental impact when applied to later layers. We instead apply batch normalization as a form of regularization on representations G and L i for i = 1, . . . , λ − 1. We optimize the objective loss using RMSProp (Tieleman and Hinton 2012) with a relatively high initial learning rate of 0.005 given exponential decay is used. <ref type="table">Table 2</ref> Results comparing to other methods on the CoNLL04 dataset. We report 95% confidence intervals around the mean F1 over 30 runs for models in the last two rows. Our model was tuned on the CoNLL04 development set corresponding to the configuration from <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Recognition Relation Extraction</head><p>Avg. Epoch Avg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><formula xml:id="formula_11">P (%) R (%) F (%) P (%) R (%) F (%)</formula><p>Train Time Test Time *  <ref type="table">Table 3</ref> Results comparing to other methods on the ADE dataset. We report the mean performance over 10-fold cross-validation for models in the last two rows. Our model was tuned on the CoNLL04 development set corresponding to the configuration from <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Recognition Relation Extraction</head><p>Avg. Epoch Avg.</p><p>Model We report our main results in <ref type="table">Tables 2 and 3</ref> for the CoNLL04 and ADE datasets respectively. As a baseline, we replicate the prior best models <ref type="bibr" target="#b2">(Bekoulis et al. 2018a</ref>) for both datasets based on publicly available source code 3 . Unlike prior work, which reports performance based on a single run, we report the 95% confidence interval around the mean F1 based on 30 runs with differing seed values for the CoNLL04 dataset. For the ADE dataset, we instead report the mean performance over 10-fold cross-validation so that results are comparable to established work. These experiments were performed using the same splits, pretrained embeddings, and computing hardware; hence, results are directly comparable.</p><formula xml:id="formula_12">P (%) R (%) F (%) P (%) R (%) F (%) Train</formula><p>We make the following observations based on our results from <ref type="table">Table 2</ref>. Both our model and the model from <ref type="bibr" target="#b2">Bekoulis et al. (2018a)</ref> tend to skew heavily towards precision. However, our method improves on both precision and recall, and by over 1% F1 on relation extraction where improvements are statistically significant (p &lt; 0.05) based on the two-tailed Student's t-test. We note that our model performs slightly worse when evaluated purely on NER. We contend this is a worthwhile trade-off given our model is tuned purely on relation extraction and the relation extraction metric, being end-to-end, indirectly accounts for NER performance. Based on <ref type="table">Table 3</ref>, when tested on the ADE dataset, our method improves over prior best results by approximately 1% F1 for RE on average. While the prior best skews toward recall in this case, our method exhibits better balance of precision and recall. Based on run time results, we contend that our method is more computationally efficient given training and testing times are nearly seven times lower on the CoNLL04 and ten times lower on the ADE set when compared to prior efforts. We note that dependency parsing accounts approximately one-half second of our testing time. While training time may not be crucial in most settings, we argue that fast and efficient predictions are important for many end-user applications.</p><p>As an auxiliary experiment, we tested the potential for integrating adversarial training (AT) with our model; however, there were no performance gains even with extensive tuning. On the CoNLL04 dataset, our method with AT performs at 62.26% F1, compared to 62.68% without AT. On the ADE dataset, our method performs at 76.83% F1 with AT, compared to 77.29% without AT. Given this, we have elected not to include AT evaluations as part of our main results.  Comparison with More Prior Efforts. Gupta, Schütze, and Andrassy (2016), <ref type="bibr" target="#b0">Adel and Schütze (2017)</ref>,  also experimented with the CoNLL04 dataset; however, Gupta, Schütze, and Andrassy (2016) evaluate on a more relaxed evaluation metric for matching entity bounds while Adel and Schütze (2017) assume entity bounds are known at test time thus treating the NER aspect as a simpler entity classification problem. Of the three studies, results from <ref type="bibr" target="#b41">Zhang, Zhang, and Fu (2017)</ref> are most comparable given they consider entity bounds in their evaluations; however, their results are based on a random 80%-20% split of the train and test set. As we use established splits based on prior work, the two results are not directly comparable. <ref type="table">Table 4</ref> Ablation studies for relation extraction over the CoNLL04 and ADE dataset; each row after the first indicates removal of a particular feature/component. We report ablation analysis results in <ref type="table">Table 4</ref> using our best model as the baseline. We note that the model hyperparameters were tuned on the CoNLL04 development set. Character and dependency based features all had a notable impact on performance for either dataset. On the hand, while position embeddings had a positive effect on the ADE dataset, performance gains were negligible when testing on CoNLL04. For the CoNLL04 dataset, we find that character based features had little effect on precision while improving recall substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Analysis</head><formula xml:id="formula_13">CoNLL04 (Relation) ADE (Relation) Model P (%) R (%) F (%) P (%) R (%) F (%)</formula><p>Unsurprisingly, pretrained word embeddings had the greatest impact on performance in terms of both precision and recall. Early experiments showed that, unlike models from prior work that used static word embeddings <ref type="bibr" target="#b18">(Li et al. 2017;</ref><ref type="bibr" target="#b2">Bekoulis et al. 2018a)</ref>, our model benefits from trainable word embeddings as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Here, trainable word embeddings with downscaled gradients refer to reducing the gradient of word embeddings by a factor of 10 at each training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Error Analyses</head><p>In this section, we first perform a class based analysis where performance variations for different classes of examples are examined. Then, a more in-depth error analysis is performed for interesting example cases. The class based analyses entail partitioning examples by length, entity distance, and relation type and are covered in Section 5.2.1. The more in-depth example based analysis is discussed in Section 5.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Class based analyses.</head><p>Long sentences are a natural source of difficulty for relation extraction models given the potential for long-term dependencies. In this section, we perform straightforward analysis by conducting experiments to assess model performance with respect to increasing sentence length. For this experiment, we train a single model using 80% of the dataset with 20% held out for testing. For some sentence length limitk, we evaluate on a subset of the overall test set that includes only examples with a sentence length that is less than or equal tok.    Results from these experiments are plotted in <ref type="figure" target="#fig_6">Figures 6 and 7</ref>, for the CoNLL04 and ADE datasets respectively, such thatk is varied along the horizontal x-axis. The top graph displays performance, while the bottom graph plots the number of examples with sentence length less than or equal tok that are used for evaluation. As shown, performances for both NER and RE tend to decline as longer sentences are added to the evaluation set. Unsurprisingly, relation extraction is more susceptible to long sentences compared to entity recognition. While there is a decline in both relation extraction precision and recall, we note that recall drops at a faster rate with respect to maximum sentence length and this phenomenon is apparent for both datasets.</p><p>In addition to length-based analysis, we also conducted experiments to study the variation in relation extraction performance with respect to the distance between subject and object entities as shown in <ref type="table">Table 5</ref>. We measure distance by computing the absolute character offset between <ref type="table">Table 5</ref> Relation extraction performance partitioned based on "Entity Distance", which is defined as the number of characters separating the subject and object entities (i.e., absolute character offset). the last character of the first occurring entity and first character of the second occurring entity, which is henceforth simply referred to as "entity distance." Our results show that, at least on the CoNLL04 dataset, notable performance differences occur at the boundary cases; i.e., very short range relations (0-20 entity distance) tend to be easier and very long range relations (80-100 entity distance) tend to be harder (mostly due to changes in recall). For the ADE dataset, performance is similar across all partitions of entity distances. This is surprising, as sentence length appears to have a more notable impact on relation extraction performance than entity distance for this particular architecture.  <ref type="table" target="#tab_6">Table 6</ref> shows variance in performance when examined by relation type. Here, we see that performance depends heavily on the type of relation being extracted; our model exhibits much higher accuracy on the Kill relation at 80% F1, with Located_In and Work_For being the most difficult with performance below 60% F1. These results further corroborate our analysis based on <ref type="table">Table 5</ref> that entity distance does not correlate with example difficulty given that the Kill relation, being the easiest relation to extract, occurs with the highest average entity distance.</p><formula xml:id="formula_14">CoNLL04 (Relation) ADE (Relation) Entity Distance # of Examples P (%) R (%) F (%) # of Examples P (%) R (%) F (%)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Example based analysis.</head><p>A common source of difficulty that occurs is ambiguity with respect to expression of the Live_In and Work_In relation types. For example, consider the sentence "After buying the shawl for $1,600, Darryl Breniser of Blue Ball, said the approximately 2-by-5 foot shawl was worth the money." The ground truth relation is (Darryl Breniser, Live_In, Blue Ball) which indicates that "Blue Ball" is in fact a location. However, it is difficult to assess whether "Blue Ball" is a location or company based on the context alone and without broader geographical knowledge (even for humans). Our model predicted (Darryl Breniser, Work_For, Blue Ball) in this case. We observe a similar pattern in the following case: "Santa Monica artist Tom Van Sant said Monday after the 23-foot-tall statue was found crushed and broken in pieces."; here, we see the same phenomenon where our model mistakes (Tom Van Sant, Live_In, Santa Monica) for (Tom Van Sant, Work_For, Santa Monica). Finally, we present the most interesting example of this type of ambiguity in the sentence: "'Temperatures didn't get too low, but the wind chill was bad', said Bingham County Sheriff's Lt. Bill Gordon." Here, the ground truth indicates that the only relation to be extracted is (Bill Gordon, Live_In, Bingham County); however, our model extracts (Bill Gordon, Work_For, Bingham County Sheriff), which is also technically a valid relation. Such cases present ambiguities that are also difficult for human annotators; here, imbuing the NER component with external knowledge or learning based on a broader level of context may alleviate these types of errors.</p><p>Inconsistencies in the way entities are annotated can also cause issues when it comes to demarcating names that are accompanied with honorifics or titles. For example, some ground truth annotations will include the title, such as "President Park Chung-hee" or "Sen. Bob Dole", and other cases will leave out the title, such as "Kennedy" instead of "President Kennedy." These truth annotations are inconsistent and present a source of difficulty for the model during training and testing. For example, "Navy spokeswoman Lt. Nettie Johnson was unable to say immediately whether the aircraft had experienced problems from faulty check and drain valves." Here, our model extracted (Lt. Nettie Johnson, Work_For, Navy), while the groundtruth is (Nettie Johnson, Work_For, Navy) -while both are technically correctly, the extremely precise nature of the evaluation metric causes this prediction to be considered a false positive.</p><p>We also see such issues with annotation at the relation extraction stage; for example, consider the sentence "In 1964, a jury in Dallas found Jack Ruby guilty of murdering Lee Harvey Oswald, the accused assassin of President Kennedy." <ref type="figure">Figure 8</ref> shows the internal activity of the network as it attempts to extract entities and relations from this particular example. Here, the ground truth annotation includes (Lee Harvey Oswald, Kills, President Kennedy), which our model fails to recognize; we instead obtain the prediction (Jack Ruby, Kills, Lee Harvey Oswald) which is a valid relation missed by the ground truth. In fact, it can be argued that the latter relation is a stronger manifested of the "Kill" relation based on the linguistic context as evidenced by the trigger phrase "found [..] guilty of murdering". We note that our model is able to detect (Lee Harvey Oswald, Kills, President Kennedy) as shown in the center-bottom heatmap of <ref type="figure">Figure 8</ref>; however, signals were not strong enough to warrant a concrete extraction of the relation.</p><p>In the ADE dataset, we mostly observe issues with entity recognition where boundaries of noun phrases are not properly recognized. Modifier phrases are sometimes not predicted as part of the named entity, for example: "protracted neuromuscular block" instead extracted as "neuromuscular block", and "generalized mite infestation" instead extracted as simply "mite infestation." The nature of the data results in especially long named entities that are often entire noun or verb phrases which can be difficult to delimit. For example, consider the following case: "DISCUSSION: Central nervous system (CNS) toxicity has been described with ifosfamide, with most cases reported in the pediatric population." Here, instead of extracting (Central nervous system (CNS) toxicity, ifosfamide) as the relation pair, our model predicts (Central nervous system, ifosfamide) and (CNS, ifosfamide). Essentially, long entity phrases are often not recognized in their entirety, and broken down into segments where each segment is independently involved in a relation. In this particular case, this error in prediction lead to one false negative and two false positives. This phenomenon occurs frequently with coordinated noun phrases which present a nontrivial challenge. For example, "Growth and adrenal suppression in asthmatic children treated with high-dose fluticasone propionate." is annotated with "Growth and adrenal suppression" as a singular entity, while our model falsely recognizes it as two entities "Growth" and "adrenal suppression." We see similar outcomes for the sentence: "Generalized maculopapular and papular purpuric eruptions are per-  Visualization of activity of pooling layers at various depths (L i for i = 1, . . . , λ), as tabular heatmaps, for a network with a depth of λ = 8 given the following input sentence: "In 1964, a jury in Dallas found Jack Ruby guilty of murdering Lee Harvey Oswald, the accused assassin of President Kennedy." Here, we measure activity by sum-pooling the activations along the channel dimension of each hidden representation. For the prediction activity, we simply max-pool probabilities along the relation dimension thus ignoring the exact type of entity or relation.</p><p>haps the most common thionamide-induced reactions." Such cases occur frequently which we suspect are a major source of hampered precision given the increased number of false positives for each predictive mistake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this study, we introduced a novel neural architecture that combines the ideas of metric learning and convolutional neural networks to tackle the highly challenging problem of endto-end relation extraction. Our method is able to simultaneously and efficiently recognize entity boundaries, the type of each entity, and the relationships among them. It achieves this by learning intermediate table representations by pooling local metric, dependency, and position information via repeated application of the 2D convolution. For end-to-end relation extraction, this approach improves over the state-of-the-art across two datasets from different domains with statistically significant results based on examining average performance of repeated runs. Moreover, the proposed architecture operates at substantially reduced training and testing times with testing times that are seven to ten times faster, the latter important for many user-end applications. We also perform extensive error analysis and show that our network can be visually analyzed by observing the hidden pooling activity leading to preliminary or intermediate decisions. Currently, the architecture is designed for extracting relations involving two entities and occur within sentence bounds; handling n-ary relations and exploring document-level extraction involving cross-sentence relations will be the focus of future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 A</head><label>1</label><figDesc>simple relation extraction example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Table representation forthe example in Figure 1. BILOU-encoded entity tags are assigned along the diagonal and relation tags are assigned where entity spans intersect. Empty cells are implicitly assigned the O tag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Overview of the network architecture for λ = 2. For simplicity, we ignore punctuation tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>Mean F1-score (over 10 runs) on CoNLL04 development set with respect to number of training epochs for various embedding training strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 CoNLL04:</head><label>6</label><figDesc>Entity and relation extraction performance with respective to change in maximum sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 ADE:</head><label>7</label><figDesc>Entity and relation extraction performance with respect to change in maximum sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 8 Visualization of activity of pooling layers at various depths (L i for i = 1, . . . , λ), as tabular heatmaps, for a network with a depth of λ = 8 given the following input sentence: "In 1964, a jury in Dallas found Jack Ruby guilty of murdering Lee Harvey Oswald, the accused assassin of President Kennedy." Here, we measure activity by sum-pooling the activations along the channel dimension of each hidden representation. For the prediction activity, we simply max-pool probabilities along the relation dimension thus ignoring the exact type of entity or relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Model configuration as tuned on the CoNLL04 development set.</figDesc><table><row><cell>Setting</cell><cell>Value</cell><cell>Setting</cell><cell>Value</cell></row><row><cell>Optimization Method</cell><cell>RMSProp</cell><cell>Character Embedding Size (π)</cell><cell>25</cell></row><row><cell>Learning Rate</cell><cell>0.005</cell><cell>Character Representation Size (η)</cell><cell>50</cell></row><row><cell>Dropout Rate</cell><cell>0.5</cell><cell>Position Embedding Size (γ)</cell><cell>25</cell></row><row><cell>Num. Epochs</cell><cell>100</cell><cell>Dependency Embedding Size (β)</cell><cell>10</cell></row><row><cell>Num. Channels (κ)</cell><cell>15</cell><cell>Word Embedding Size (δ)</cell><cell>200</cell></row><row><cell>Num. Layers (λ)</cell><cell>8</cell><cell>Context Embedding Size (ρ)</cell><cell>200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table Representation</head><label>Representation</label><figDesc>These results are directly comparable given the same train-test splits, pretrained word embeddings, and computing hardware. * Average test time is per test set of 288 examples; dependency parsing accounts for approximately 0.5 second of our reported test time.</figDesc><table><row><cell>(Miwa and Sasaki 2014)</cell><cell cols="2">81.20 80.20</cell><cell>80.70</cell><cell cols="2">76.00 50.90</cell><cell>61.00</cell><cell>-</cell><cell>-</cell></row><row><cell>Multihead (Bekoulis et al. 2018b)</cell><cell cols="2">83.75 84.06</cell><cell>83.90</cell><cell cols="2">63.75 60.43</cell><cell>62.04</cell><cell>-</cell><cell>-</cell></row><row><cell>Multihead with AT (Bekoulis et al. 2018a)</cell><cell>-</cell><cell>-</cell><cell>83.61</cell><cell>-</cell><cell>-</cell><cell>61.95</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">Replicating Multihead with AT (Bekoulis et al. 2018a) † 84.36 85.80 85.07 ± 0.26 65.81 57.59 61.38 ± 0.50</cell><cell>614 sec</cell><cell>34 sec</cell></row><row><cell>Relation-Metric (Ours) †</cell><cell cols="6">84.46 84.67 84.57 ± 0.29 67.97 58.18 62.68 ± 0.46</cell><cell>101 sec</cell><cell>4.5 sec</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>These results are directly comparable given the same fixed 10-fold splits, pretrained word embeddings, and computing hardware. * Average test time is per test set of 427 examples; dependency parsing accounts for approximately 0.5 second of our reported test time.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time Test Time  *</cell></row><row><cell>Neural Joint Model (Li et al. 2016)</cell><cell cols="6">79.50 79.60 79.50 64.00 62.90 63.40</cell><cell>-</cell><cell>-</cell></row><row><cell>Neural Joint Model (Li et al. 2017)</cell><cell cols="6">82.70 86.70 84.60 67.50 75.80 71.40</cell><cell>-</cell><cell>-</cell></row><row><cell>Multihead (Bekoulis et al. 2018b)</cell><cell cols="6">84.72 88.16 86.40 72.10 77.24 74.58</cell><cell>-</cell><cell>-</cell></row><row><cell>Multihead with AT (Bekoulis et al. 2018a)</cell><cell>-</cell><cell>-</cell><cell>86.73</cell><cell>-</cell><cell>-</cell><cell>75.52</cell><cell>-</cell><cell>-</cell></row><row><cell cols="8">Replicating Multihead with AT (Bekoulis et al. 2018a) † 85.76 88.17 86.95 74.43 78.45 76.36 1567 sec</cell><cell>40 sec</cell></row><row><cell>Relation-Metric (Ours) †</cell><cell cols="6">86.16 88.08 87.11 77.36 77.25 77.29</cell><cell>134 sec</cell><cell>4.5 sec</cell></row><row><cell>†</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Relation extraction performance on the CoNLL04 dataset partitioned based on relation type.</figDesc><table><row><cell>CoNLL04 (Relation)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is included as supplementary material and will be made publicly available on GitHub.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://spacy.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/bekou/multihead_joint_entity_relation_extraction</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global normalization of convolutional neural networks for joint entity and relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1723" to="1729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">All-paths graph kernel for protein-protein interaction extraction with evaluation of cross-corpus learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jari</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional LSTM-CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Pc</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations for opinion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A machine learning approach for identifying disease-treatment relations in short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Frunza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="814" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RelEx -relation extraction using dependency parse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Fundel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Küffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Mateen</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Speech and language processing (prentice hall series in artificial intelligence)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction using card-pyramid parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="917" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extracting drug-drug interactions with word and character-level recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE International Conference on Healthcare Informatics (ICHI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Genia corpusâȂŤa semantically annotated corpus for bio-textmining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junâȃźichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="180" to="182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural joint model for entity and relation extraction from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">198</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint models for extracting adverse drug events from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI 2015)</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI 2015)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="2838" to="2844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel-based learning for biomedical relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiexun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinchun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="756" to="769" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dependency-based convolutional neural network for drug-drug interaction extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1074" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Identifying gene-disease associations using centrality on a literature mined gene-interaction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzucan</forename><surname>Özgür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuy</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Güneş</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="277" to="285" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using neural networks and markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Palshikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Symposium on Languages in Biology and Medicine</title>
		<meeting>5th International Symposium on Languages in Biology and Medicine</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting constituent dependencies for tree kernel-based semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peide</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning local and global contexts using a convolutional recurrent network model for relation classification in biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desh</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Anand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic extraction of relations between medical concepts in clinical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="594" to="600" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Annual Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint inference of entities, relations, and coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 workshop on Automated knowledge base construction</title>
		<meeting>the 2013 workshop on Automated knowledge base construction</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics: Technical Papers (COLING 2014)</title>
		<meeting>the 25th International Conference on Computational Linguistics: Technical Papers (COLING 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction based on a hybrid neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
