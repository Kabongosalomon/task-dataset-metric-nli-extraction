<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometry Normalization Networks for Accurate Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Yue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Guan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Geometry Normalization Networks for Accurate Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large geometry (e.g., orientation) variances are the key challenges in the scene text detection. In this work, we first conduct experiments to investigate the capacity of networks for learning geometry variances on detecting scene texts, and find that networks can handle only limited text geometry variances. Then, we put forward a novel Geometry Normalization Module (GNM) with multiple branches, each of which is composed of one Scale Normalization Unit and one Orientation Normalization Unit, to normalize each text instance to one desired canonical geometry range through at least one branch. The GNM is general and readily plugged into existing convolutional neural network based text detectors to construct end-to-end Geometry Normalization Networks (GNNets). Moreover, we propose a geometry-aware training scheme to effectively train the GNNets by sampling and augmenting text instances from a uniform geometry variance distribution. Finally, experiments on popular benchmarks of ICDAR 2015 and ICDAR 2017 MLT validate that our method outperforms all the state-of-the-art approaches remarkably by obtaining one-forward test Fscores of 88.52 and 74.54 respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) have been dominating the research of general object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>, as well as scene text detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39]</ref> in recent years. Thanks to the generic nature of CNN-based approaches, scene text detection can usually benefit from the rapid progress of general object detection. Despite the great success of CNNs, detecting scene texts has its own challenges as texts have large geometry (e.g., scale or orientation) variances in real application scenarios. Existing methods tackle the scale variance problem by detecting texts on multi-layers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7]</ref> with one detection header on each or the FPN-like multi-scale * Authors contributed equally, youjiangxu@gmail.com, duanji-aqi@hust. <ref type="bibr">edu</ref>   <ref type="bibr" target="#b19">[20]</ref> is trained with both text bounding boxes and word recognition annotations and tested with multi-scale fusion, while ours is trained with bounding boxes only and tested with one single scale only. Pixel-Anchor <ref type="bibr" target="#b14">[15]</ref> and PSENet <ref type="bibr" target="#b13">[14]</ref> are methods proposed very recently (best viewed in color). fusion layer with one detection header <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref>, and predict arbitrary orientations by the bounding box angle estimation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7]</ref> or the orientation-sensitive convolutions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref>. Each of their individual detection header learns all training samples with enormous geometry variances or only one subset of them, which might lead to suboptimal performance.</p><p>Although standard multi-orientation benchmarks such as ICDAR 2015 <ref type="bibr" target="#b10">[11]</ref> and ICDAR 2017 MLT <ref type="bibr" target="#b25">[26]</ref> had great impact on promoting state-of-the-art text detection approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>, the issue of large geometry variances on text detection is overlooked by the research community. Surprisingly, we find that horizontal texts dominate the multi-orientation benchmark ICDAR 2015. To evaluate text detection with large geometry variances, we augment ICDAR 2015 by randomly rotating images (named by ro- tated ICDAR 2015 1 ), so that the orientations are distributed uniformly for better evaluation as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (a). As a case study, EAST <ref type="bibr" target="#b41">[42]</ref> degrades greatly (from 80.6% to 20.9%) on rotated ICDAR 2015. Even after rotation augmentation training, it still performs worse than on the original data before augmentation (see <ref type="figure" target="#fig_0">Figure 1</ref> (b)). We believe that this is because it cannot capture large geometry variances well.</p><p>To this end, in this paper, we conduct a series of controlled experiments to investigate the impacts of geometry variances on the scene text detection, and find that CNN based detectors can only capture limited text geometry variances but making full use of all training samples with large geometry variances could improve their generalization ability. To solve the above dilemma, we propose a novel Geometry Normalization Module (GNM). It has multiple normalization branches, each of which is composed of one Scale Normalization Unit (SNU) and one Orientation Normalization Unit (ONU), and can learn geometry-specific feature maps. The geometry of each scene text instance can be transformed into one desired canonical geometry range through at least one branch of GNM. In this way, large geometry variances of all training samples are normalized to a limited distribution, so that we can train one shared text detection header on them effectively. The proposed GNM is general, and readily plugged into any CNN-based text detector to construct end-to-end Geometry Normalization Networks (GNNets). The general flowchart of GNNets is illustrated in <ref type="figure">Figure 2</ref>. We demonstrate its superiority by equipping state-of-the-art text detectors EAST <ref type="bibr" target="#b41">[42]</ref> and PSENet <ref type="bibr" target="#b13">[14]</ref> with our proposed GNM.</p><p>Training GNNets is non-trivial. We further propose a geometry-aware training strategy to effectively train GN-Nets by randomly augmenting text instances so that they are sampled from a uniform geometry variance distribution.</p><p>In this way, all branches in the GNM have an equal number of valid text samples in each batch, and thus can be trained uniformly.</p><p>Thanks to the intrinsic geometry normalization capability of the GNM, and the proposed effective training strategy, our GNNets outperform state-of-the-art scene text detection methods by impressive margins. Specifically, on ICDAR 2015 and ICDAR 2017 MLT, GNNets achieve the best oneforward test F-Scores of 88.50 and 74.54 respectively (see <ref type="figure" target="#fig_0">Figure 1</ref> (c)), which is even better than end-to-end word spotters such as FOTS MS <ref type="bibr" target="#b19">[20]</ref> trained with word recognition supervision signals, and time-consuming multi-scale test methods such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>. Our model also outperforms methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> that were proposed very recently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene text detection. Detecting scene text in the wild has received great attentions in recent years. Lots of approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref> have been proposed. Comprehensive reviews can be found in the surveys <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref>. Herein, we will discuss about those papers which are mostly relevant to our method in terms of geometry (scale and orientation) robustness.</p><p>Geometry robust text detection methods target at remedying large scale variances or large orientation variances. To suppress the issue of large scale variances in text detection, inspired by SSD <ref type="bibr" target="#b18">[19]</ref>, earlier work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b6">7]</ref> detected texts independently on multi-layers, each of which detects texts with one specific size range. Their low level features lack high level semantics, which easily leads to missing de-tection or false alarms. To compensate the absence of semantics in low-level features, later work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b2">3]</ref> introduced FPN-like <ref type="bibr" target="#b17">[18]</ref> or FCN-like <ref type="bibr" target="#b21">[22]</ref> architectures, and detected texts on one fused layer. The features of texts of different scales vary drastically, which hinders to learn their detection headers mapping from features to bounding boxes well. There exist lots of multi-orientation text detection approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14]</ref>. Different horizontal text detection ones, they either directly predict the orientation of text boxes or connections between text segments <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>, or construct oriented text boxes from bottom to top <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>. To robustly detect multiorientation texts, RRD <ref type="bibr" target="#b16">[17]</ref> extracted rotation-sensitive features by explicitly rotating the convolutional filters. Different from all the aforementioned geometry robust text detection methods, which focus on either scale or orientation robustness in isolation, our proposed GNNets achieve both scale and orientation robustness of text detection in a unified framework. Recently, ITN <ref type="bibr" target="#b35">[36]</ref> predicted an affine transformation for each location, which guides to deform the convolutional filters accordingly, and achieve geometry (including scale and orientation) robust text detection. However, the affine transformation prediction error might greatly degrade the final text detection performance. In contrast, our GNM can automatically normalize each text instance to one desired canonical geometry range through at least one of its branches without any explicit transformation estimation. Experimental results validate that the proposed method significantly outperforms ITN on benchmarks.</p><p>Scale normalization for object detection. General object detection can also benefit from scale normalization. SNIP <ref type="bibr" target="#b31">[32]</ref> proposed a scale normalization method to train the detector of objects with one desired scale range during multi-scale training. To perform multi-scale training more efficiently, SNIPER <ref type="bibr" target="#b32">[33]</ref> selected context regions around the ground-truth instances only and sampled background regions for each scale during training. They share the similar inspiration with our work by reducing scale variances. However, SNIP and SNIPER achieve scale normalization by image pyramid, and thus need multi-forward test. They suffer from inevitable increasing of inference time. Different from them, our proposed method achieves both scale and orientation normalization by feature transformation, and needs one-forward test only. Moreover, it normalizes scale and orientation in a unified framework while SNIP and SNIPER normalizes scale only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivations</head><p>Although state-of-the-art methods alleviate the problem of the scale and orientation variances by novel designs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>, there is still great room of improvement for large geometry variations as discussed in Section 1 and illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (b). In this section, we will investigate the capability of text detection networks of handling large geometry variances by more comprehensive experiments.</p><p>To evaluate the impacts of the geometry variances, we construct a common evaluation benchmark with text boxes in a specific geometry range. Three different sampling strategies are designed to construct a training dataset to examine the capacity of the detector.</p><p>The most popular dataset of ICDAR 2015 <ref type="bibr" target="#b10">[11]</ref> and a state-of-the-art text detector EAST <ref type="bibr" target="#b41">[42]</ref> are used in our experiments. Particularly, we select text boxes with the short side length in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40]</ref> pixels and the angle in [− π 12 , + π 12 ] from ICDAR 2015 test set to form the evaluation benchmark.</p><p>Geometry Specific Sampling (GSS). This sampling strategy constructs text boxes within the same geometry range as evaluation by transforming text instances into the desired geometry range and makes full use of all text box instances in the original training set.</p><p>For each image, we randomly select one text instance and then transform the image, so that the chosen text instance will lie in the desired geometry range. The target parameters including the short side length and the angle are uniformly generated in the range.</p><p>Geometry Variance Sampling (GVS). This sampling strategy is similar to GSS, except that the transformed text boxes are in a larger geometry range than GSS. The resulting short side length is in [0, 90] pixels and the angle is in</p><formula xml:id="formula_0">[− π 2 , + π 2 ]</formula><p>. Limited Geometry Specific Sampling (LGSS). This sampling strategy only selects a subset of the training set, which are in the same geometry range as evaluation. Compared with GSS, fewer text instances are included for training a detector.</p><p>We summarize our observations as follows (see <ref type="table" target="#tab_1">Table 1</ref> for all experimental results):</p><p>• Existing text detectors have limited learning capacity for handling large geometry variances. Samples with large geometry variances degrade the performance of text detectors significantly. EAST <ref type="bibr" target="#b41">[42]</ref> trained with GVS performs 12% worse than GSS on F-score. This observation is also common for other state-of-the-art detectors and we do not list more results due to the space limit. The above observations motivate us to normalize geometry variances during training, so that a geometry specific detector only needs to handle limited geometry range. Instead of transforming images, which suffers from timeconsuming multi-scale multi-orientation feature extraction in both training and test, we design efficient geometry normalization by feature transformation. In order to fully utilize training samples, we randomly augment all text instances, so that each geometry specific detector can learn from all text instances in all images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Geometry Normalization Networks</head><p>In this section, we will first introduce the proposed Geometry Normalization Module (GNM), and then describe overall architecture of Geometry Normalization Networks (GNNets). Finally, the geometry-aware training strategy will be presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Geometry Normalization Module</head><p>GNM targets at normalizing text geometry distribution with large variances into one desired canonical geometry range, so that text detection header can be learned well. We divide the geometry distribution with large variance into multiple appropriate geometry distributions with small variance, each of which is allocated one independent geometryspecific branch to handle. Formally, GNM is given bỹ</p><formula xml:id="formula_1">x i = F o i (F s i (x)) i ∈ {1, . . . , N },<label>(1)</label></formula><p>where x is the input feature maps of the GNM, andx i is the output feature maps of its i th branch. F s i and F o i are the scale normalization unit and the orientation normalization unit of the i th branch, which are designed to normalize scale variances and orientation variances respectively.</p><p>Scale Normalization Unit. Each scale normalization unit extracts scale-specific features. We design it by explicitly downsampling feature maps and thus expanding the receptive field of the convolutional kernels. Concretely, given the input feature maps x ∈ R C×H×W outputted by feature extractors, the scale normalization unit outputs x s ∈ R C ×H ×W as follows:</p><formula xml:id="formula_2">x s = F s (x), F s ∈ {S, S 1 2 },<label>(2)</label></formula><p>where S is one 1×1 conv operator, and S 1 2 is a stack of 1×1 conv, 2 × 2 max-pooling with stride 2, and 3 × 3 conv operators. S preserves the spatial resolution of the input feature maps (i.e., H = H and W = W ), and S 1 2 halves it (i.e., , π] to those with angles in [0, π 4 ]. H = 1 2 H and W = 1 2 W ). We select two scale normalization units, because adding more units achieve marginal performance improvement as shown in experimental results.</p><p>Orientation Normalization Unit. Each orientation normalization unit normalizes texts within a specific angle range to near-horizontal texts, by explicitly rotating and/or flipping operations. Given the input feature maps x s ∈ R C ×H ×W , it outputsx ∈ RC ×H×W as follows:</p><formula xml:id="formula_3">x = F o (x s ), F o ∈ {O, O r , O f , O r+f },<label>(3)</label></formula><p>where O is a 1×1 conv operator, O r denotes a stack of 1×1 conv, rotation, and 3 × 3 conv operators, O f denotes a stack of 1 × 1 conv, flipping, and 3 × 3 conv operators, and O r+f denotes a sequence of 1 × 1 conv, rotation, flipping and 3×3 conv. Rotation and flipping indicate clockwise rotating by π 2 and horizontal flipping, respectively. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref></p><formula xml:id="formula_4">, O, O r , O f , and O r+f transform texts with angles in [0, π 4 ], [− π 2 , − π 4 ], [− π 4 , 0], and [ π 4</formula><p>, π] to those with angles in [0, π 4 ], respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Architecture of GNNets</head><p>The proposed GNM is general, and readily plugged into existing CNN based text detectors to construct Geometry Normalization Networks (GNNets). <ref type="figure">Figure 2</ref> illustrates the general architecture of GNNets. First, images are fed into a CNN based feature extractor to obtain feature maps, which will be the input of the proposed GNM. Then, GNM produces different scale and orientation specific feature maps, and a shared text detection header predicts text boxes on these feature maps. Finally, the predicted text boxes are transformed back accordingly and merged via NMS. </p><formula xml:id="formula_5">F s F o x y h w θ S O x y h w θ S Or H − y x h w θ − π/2 θ ∈ [0, π/2], θ + π/2 θ ∈ [−π/2, 0). S O f x H − y h w −θ S O r+f H − y W − x h w −θ + π/2 θ ∈ [0, π/2], −θ − π/2 θ ∈ [−π/2, 0). S 1 2 O x/2 y/2 h/2 w/2 θ S 1 2</formula><p>Or</p><formula xml:id="formula_6">H/2 − y/2 x/2 h/2 w/2 θ − π/2 θ ∈ [0, π/2], θ + π/2 θ ∈ [−π/2, 0). S 1 2 O f x/2 H/2 − y/2 h/2 w/2 −θ S 1 2 O r+f H/2 − y/2 W/2 − x/2 h/2 w/2 −θ + π/2 θ ∈ [0, π/2], −θ − π/2 θ ∈ [−π/2, 0).</formula><p>In order to demonstrate the generalization ability of our proposed method, we instantiate our GNNets with two state-of-the-art text detectors, i.e., EAST <ref type="bibr" target="#b41">[42]</ref> and PSENet <ref type="bibr" target="#b13">[14]</ref>. We insert our GNM after the last layer of the feature merged branch in EAST, and the concatenation layer of the multi-scale layers of the FPN in PSENet. We optimize the following loss:</p><formula xml:id="formula_7">L = 1 N N n=1 L n (x,ŷ,ĥ,ŵ,θ; x , y , h , w , θ ),<label>(4)</label></formula><p>where N denotes the number of branches. (x,ŷ),ĥ,ŵ, andθ indicate the center, height, width and angle of the predicted text bounding box, respectively. (x , y ), h , w and θ denote center, height, width and angle of the proxy ground truth, respectively, and its relation to the input bounding box (x, y, h, w, θ) is given in <ref type="table" target="#tab_2">Table 2</ref>. L n (·) denotes the loss of the n-th geometry normalization branch of the proposed GNM, which simply follows the loss functions in EAST <ref type="bibr" target="#b41">[42]</ref> and PSENet <ref type="bibr" target="#b13">[14]</ref>. Both EAST based GNNets and PSENet based GNNets outperform its counterpart significantly as shown in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Geometry-Aware Training and Test Strategy</head><p>Together with geometry normalization module, we codesign three critical strategies so that GNNets perform well.</p><p>Feasible geometry range. We allocate one feasible geometry range including one scale interval and one orientation interval to each branch of GNM. The union of the feasible geometry ranges of all branches equals to the whole text geometry distribution. In this way, any text bounding boxes can fall into at least one feasible geometry range and thus can be normalized to the canonical geometry range through its corresponding branch in GMN. We will discuss the allocation in detail in Section 5.3.</p><p>Training sampling strategy. During training, we randomly samples one text instance, and augment it by rotating and resizing 7 times, so that each branch of the proposed GNM has valid text instances in each batch. In this way, all branches of GNM are trained uniformly. The feature maps of all branches are used to train the text detection header of GNNets. All the text instances in one branch are ignored if their ground truth are not in its feasible geometry range during training.</p><p>Test strategy. During testing, we predict text bounding boxes on the transformed feature maps output by all branches in GNM, which are back-projected to the original scale and orientation accordingly. The output bounding boxes which do not lie in a branch's corresponding feasible geometry range are unreliable and discarded. The remaining bounding boxes are merged via NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first perform ablation studies for the proposed GNNets, and then compare the GNNets with the state-of-the-art methods. The experiments are conducted on four datasets: ICDAR 2013 <ref type="bibr" target="#b11">[12]</ref>, ICDAR 2015 <ref type="bibr" target="#b10">[11]</ref>, IC-DAR 2017 MLT <ref type="bibr" target="#b25">[26]</ref> and the Rotated ICDAR 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Benchmark Datasets</head><p>ICDAR 2015 is a dataset proposed in the Challenge 4 of the 2015 Robust Reading Competition for incidental scene text detection. There are 1,000 images and 500 images for training and test, respectively. The text instances are annotated by word-level quadrangles.</p><p>ICDAR 2017 MLT is a dataset provided for ICDAR 2017 competition on multi-lingual scene text detection. This dataset consists of complete scene images from 9 languages. Some languages are labeled in word-level such as English, Bangla, French and Arabic, while others are la- Rotated ICDAR 2015 is constructed from the standard benchmark of ICDAR 2015. By randomly rotating images from the standard ICDAR 2015, the orientations in Rotated ICDAR 2015 are distributed uniformly (see <ref type="figure" target="#fig_0">Figure 1</ref>). There are also 1,000 images and 500 images for training and testing, respectively. Moreover, the partitions of the Rotated ICDAR 2015 stay the same with the standard one.</p><p>ICDAR 2013 contains 229 training images and 223 testing images. Different above dataset, this dataset only contains horizontal text instances. We utilize the training images in the experiments of ablation studies only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>The overall architecture of GNNets has shown in <ref type="figure">Figure 2</ref>. We use the same data augmentation techniques with previous methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b13">14]</ref>. We further perform 'Rotation Augmentation (RA)', 'Initializing (Init)' and 'Training sampling strategies (TSS)' to train the proposed GNNets. Particularly, 'Rotation Augmentation' means that the input images are randomly rotated from −π/2 to π/2. 'Initializing' means that we initialize the proposed GNNets with the reimplemented baseline models, rather than the models pre-trained on ImageNet dataset. 'Training sampling strategies' has been described in the Section 4.3. We use ADAM <ref type="bibr" target="#b12">[13]</ref> to optimize the parameters of GNNets. For training EAST-based GNNets, the learning rate starts at 1e-4, declines to its 1 10 for every 27,300 iterations, and stops at 1e-6. As for training PSENet-based GNNets, the learning rate decays per 200 epochs and there are total of 600 epochs. As for the ablation studies, we use the ICDAR 2013, the ICDAR 2015 and the Rotated ICDAR 2015 as our training data. When comparing with the state-of-the-art methods, we use the IDCAR 2015 and the ICDAR 2017 MLT as our training data, which is the same as <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>The impacts of SNU with different desired canonical scale ranges. We study the impacts of SNU with different desired canonical scale ranges on the ICDAR 2015. The feasible scale range of single branch SNU (i.e., S) is set to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">200]</ref> pixels, which means that the text boxes will be ignored if the length of their shortest size is not in the range. Note that for single branch SNU, the desired canonical scale range is the same as its feasible scale range. As for the SNU with two branches (i.e., S+S 1 2 ), we set the feasible scale range as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">80]</ref> and [60, 200] for S and S 1 2 , respectively. As S 1 2 halves the feature maps, its desired canonical scale range should be <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">100]</ref>. The final desired canonical scale range can be obtained by choosing the real minimum and real maximum desired canonical scales, and thus it should be <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">100]</ref>. In this way, the scale range of text boxes is normalized from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">200]</ref> (i.e., S) to [10, 100] (i.e., S+S 1 2 ) and all normalized text boxes will be used to train the same box detection header. For SNU with three branches (i.e., S+S 1 2 +S 1 4 ), the feasible scale ranges are set as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">60]</ref>, <ref type="bibr" target="#b39">[40,</ref><ref type="bibr">100]</ref> and [80, 200] for S, S 1 2 and S 1 4 , respectively. As a result, the desired canonical scale range of S+S 1 2 +S 1 4 is further normalized to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">60]</ref>. All results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Compared with three SNU branches, both 'EAST+S+S 1 2 ' and 'PSENet+S+S 1 2 ' achieve a better performance on the ICDAR 2015. Thus, we choose SNU with two branches as our default setting.</p><p>The impacts of ONU with different desired canonical orientation ranges. We set the feasible orientation range for our proposed single branch ONU (i.e., O) as [− π 2 , π 2 ]. Note that for single branch ONU O, the desired canonical orientation range is the same as its feasible orientation range. As O r can transform text boxes with angles in [− π 2 , − π 4 ] and [ π 4 , π 2 ] to those with angles in [0, π 4 ] and [− π 4 , 0], respectively, its feasible orientation range is set as [− π 2 , − π 4 ] and [ π 4 , π 2 ]. Note that the feasible orientation range of O, in this case, is set to [− π 4 , π 4 ]. Therefore, the desired canonical orientation range of O+O r should be [− π 4 , π 4 ], because the boxes with angles are not in these ranges will be transformed by O r . Similarly, desired canonical orientation ranges of O+O f and O+O r +O f +O r+f are [0, π 2 ] and [0, π 4 ], respectively. The models with different desired canonical orientation ranges are trained and tested From <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, we find that both narrowing the desired canonical scale range and narrowing the desired canonical orientation range benefit the model performance, which demonstrates that normalizing the geometry is beneficial to the training of the shared text detection header.</p><p>Analysis of training strategies. Rotation augmentation (RA), Initialization (Init), and Training sampling strategy (TSS) are utilized to promise the good performance of our proposed GNNets. All the results are shown in Table 5 and we can conclude as follows: (1) As shown in <ref type="table" target="#tab_5">Table 5</ref> (a), EAST and PSENet, which is reimplemented by us and achieves similar performance with their original implementation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b13">14]</ref>, obtain limited performances of 21.99% and 63.22% on the Rotated ICDAR 2015, respectively. (2) When training with 'RA' (see <ref type="table" target="#tab_5">Table 5</ref> (b)), EAST and PSENet obtain better performances on the Rotated IC-DAR 2015. However, this harms their performance on the ICDAR 2015 about 8.2% and 1.8%, respectively. (3) Comparing with '(c)+RA+GNM', '(d)+RA+GNM+Init' obtains a performance gain on the Rotated ICDAR 2015 about 8.0% for the EAST-based GNNets and 1.5% for PSENet based model (see <ref type="table" target="#tab_5">Table 5</ref> (c) and (d)). (4) From <ref type="table" target="#tab_5">Table 5</ref> (e) and (f), we find that removing either SNU or ONU will damage the performance on both ICDAR 2015 and Rotated IC-DAR 2015, which demonstrates that our proposed GNNets can detect text instances with large geometry variances. (5) When training with both 'Init' and 'TSS' (see <ref type="table" target="#tab_5">Table   5</ref> (g)), our GNM obtains better performances on the Rotated ICDAR 2015 as well as on the ICDAR 2015. Particularly, compared with EAST baseline, GNNets obtain about 3 times performance gains on the Rotated ICDAR 2015 and about 1.4% improvements on the ICDAR 2015. When comparing with PSENet baseline, our GNNets outperform about 16% on the Rotated ICDAR 2015 and simultaneously keeps nearly the same performance on the ICDAR 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparisons with State-of-the-Art Methods</head><p>In this section, we compare our PSENet-based GNNets with several state-of-the-art methods. The same with <ref type="bibr" target="#b13">[14]</ref>, during the testing, the longer side of input images is resized to 2240 and 3200 on ICDAR 2015 and ICDAR 2017 MLT, respectively. As a large proportion of the text instances in both ICDAR 2015 and ICDAR 2017 MLT is near horizontal, the GNM contains only SNU branches. For fair comparisons, we perform only one-forward testing on these two datasets. All the results are shown in <ref type="table" target="#tab_6">Table 6</ref>.</p><p>From <ref type="table" target="#tab_6">Table 6</ref>, we can find that our reimplemented PSENet has similar performance (the differences less than 0.4% on F-score) to the original PSENet <ref type="bibr" target="#b13">[14]</ref>. As the original PSENet <ref type="bibr" target="#b13">[14]</ref> is a very recently proposed method and its source code is still unavailable, we conduct the following experiments based on our implementation. Compared to the original PSENet <ref type="bibr" target="#b13">[14]</ref>, our GNNets achieve a performance improvement about 1.3% and 2.1% on ICDAR 2015 and IC-DAR 2017 MLT, respectively. Compared with EAST <ref type="bibr" target="#b41">[42]</ref> and ITN <ref type="bibr" target="#b35">[36]</ref> on the ICDAR 2015, our GNNets outperform them by absolute about 8% and 9%, respectively. While comparing with FTSN <ref type="bibr" target="#b2">[3]</ref>, we could obtain a performance gain of 4.5%. Our method outperforms FOTS <ref type="bibr" target="#b19">[20]</ref> by 0.6% on the ICDAR 2015 and by 7.3% on the ICDAR 2017 MLT. Noted that FOTS uses text recognition annotations to ben-  efit the network training, while we utilize the detection annotations only. Comparing with Pixel-Anchor <ref type="bibr" target="#b14">[15]</ref>, we also obtain a performance gain of 0.9% and 6.4% on the ICDAR 2015 and ICDAR 2017 MLT, respectively. Finally, our GN-Nets with one-forward test achieves the state-of-the-art performance on both ICDAR 2015 and ICDAR 2017 MLT. <ref type="figure" target="#fig_3">Figure 4</ref> compares the detection results by the PSENet and our proposed GNNets on ICDAR 2015, ICDAR 2017 MLT and Rotated ICDAR 2015. We observe that our proposed GNNets are able to detect scene text instances with large geometry variances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we put forward a novel Geometry Normalization Module (GNM) to generate several geometry aware feature maps. The proposed GNM is general and can be readily plugged into any CNN based detector to construct end-to-end Geometry Normalization Networks (GNNets). Extensive experiments illustrate that our proposed GNNets achieve an excellent performance on the detection of text instances with large geometry variances (e.g., the Rotated ICDAR 2015), and outperform the baselines with a large margin. Furthermore, our GNNets obtain a significant performance gain over the state-of-the-art methods on two popular benchmarks of ICDAR 2015 and ICDAR 2017 MLT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Statistics of benchmarks and performance comparisons. (a) shows the statistics of text angles in ICDAR 2015 and the rotated ICDAR 2015; our proposed method is compared with EAST, and EAST trained with rotation augmentations on both the ICDAR 2015 and the rotated ICDAR 2015 in (b) while compared with the state-of-the-art methods on ICDAR 2017 MLT in (c). Noted that FOTS MS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 2 )</head><label>22</label><figDesc>The framework of the proposed Geometry Normalization Networks. The feature maps extracted by the backbone are fed into the Geometry Normalization Module (GNM) with multi-branches, each of which is composed of one Scale Normalization Unit (SNU) F s and Orientation Normalization Unit (ONU) F o . There are two different scale normalization units (S, S 1 and four orientation normalization units (O, Or, O f , O r+f ). With different combinations of SNU and ONU, GNM generates different geometry normalized feature maps, which are fed into one shared text detection header.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The changes of the text box orientation by applying the ONU. The 'green' box is the original box, the 'grey' box is the intermediate box during transformation and the 'red' box is the result of the ONU (e.g., O, Or, O f , O r+f ). θ and θ are the angle of the original box and the result box, respectively. (a), (b), (c) and (d) are the procedure of O, Or, O f , O r+f , respectively. And they transform texts with angles in [0, π 4 ], [− π 2 , − π 4 ], [− π 4 , 0], and [ π 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on ICDAR 2015, ICDAR 2017 MLT and Rotated ICDAR 2015. We compare the results generated by PSENet (the left column) and our proposed GNNets (the right column) for each test image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Experimental results of the impacts of geometry variances. 'GSS', 'GVS' and 'LGSS' denote 'Geometry Specific Sampling', 'Geometry Variance Sampling' and 'Limited Geometry Specific Sampling', respectively. More transformed samples are helpful for training a geometry specific detector. Compared with LGSS, GSS provides more training samples by transformation and brings approximately 7% performance gain on F-score.</figDesc><table><row><cell cols="4">Setting Recall Precision F-score</cell></row><row><cell>GSS</cell><cell>52.22</cell><cell>73.97</cell><cell>61.22</cell></row><row><cell>GVS</cell><cell>42.80</cell><cell>57.57</cell><cell>49.10</cell></row><row><cell>LGSS</cell><cell>42.34</cell><cell>75.44</cell><cell>54.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Proxy ground truth. (x , y ), h , w , and θ denote the center, height, width and angle of the transformed text box, respectively. And (x, y), h, w, and θ are the center, height, width and angle of the input text box, respectively. H and W denote height and width of the input image.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The impacts of Scale Normalization Unit with different desired canonical scale ranges on the ICDAR 2015. 'ET' and 'PT' denote 'EAST' and 'PSENet', respectively.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Range</cell><cell cols="3">Recall Precision F-score</cell></row><row><cell>ET+S</cell><cell></cell><cell cols="2">[10, 200] 74.96</cell><cell>87.23</cell><cell>80.63</cell></row><row><cell>ET+S+S 1 2</cell><cell></cell><cell cols="2">[10, 100] 82.42</cell><cell>87.34</cell><cell>84.81</cell></row><row><cell>ET+S+S 1 2</cell><cell>+S 1 4</cell><cell>[10, 60]</cell><cell>81.12</cell><cell>85.31</cell><cell>83.16</cell></row><row><cell>PT+S</cell><cell></cell><cell cols="2">[10, 200] 83.53</cell><cell>86.10</cell><cell>85.04</cell></row><row><cell>PT+S+S 1 2</cell><cell></cell><cell cols="2">[10, 100] 85.02</cell><cell>86.90</cell><cell>85.95</cell></row><row><cell>PT+S+S 1 2</cell><cell>+S 1 4</cell><cell>[10, 60]</cell><cell>83.48</cell><cell>87.62</cell><cell>85.50</cell></row><row><cell cols="6">beled in line-level such as Chinese, Japanese and Korean.</cell></row><row><cell cols="6">This dataset provides 7,200 images for training, 1,800 im-</cell></row><row><cell cols="5">ages for validating, and 9,000 images for testing.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The impacts of Orientation Normalization Unit with different desired canonical orientation ranges on the Rotated ICDAR 2015. 'ET' and 'PT' denote 'EAST' and 'PSENet', respectively.</figDesc><table><row><cell>Model</cell><cell>Range</cell><cell>Recall</cell><cell cols="2">Precision F-score</cell></row><row><cell>ET+O ET+O+Or ET+O+O f ET+O+Or+O f +O r+f</cell><cell>[− π 2 , π 2 ] [− π 4 , π 4 ] [0, π 2 ] [0, π 4 ]</cell><cell>63.64 71.64 70.10 72.89</cell><cell>75.89 81.26 79.78 80.32</cell><cell>69.23 76.15 74.62 76.42</cell></row><row><cell>PT+O PT+O+Or PT+O+O f PT+O+Or+O f +O r+f</cell><cell>[− π 2 , π 2 ] [− π 4 , π 4 ] [0, π 2 ] [0, π 4 ]</cell><cell>73.61 75.20 72.36 74.62</cell><cell>81.63. 82.55 83.55 83.87</cell><cell>77.41 78.71 77.55 78.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results of GNNets under different training strategies on the ICDAR 2015 and the Rotated ICDAR 2015. RA, The proposed GNM, Init and TSS are added into the baselines step-by-step, where 'RA', 'Init' and 'TSS' indicate 'Rotation augmentation', 'Initialization' and 'Training sampling strategy', respectively. EAST and PSENet are improved by about 7.2% and 1.5% on the Rotated IC-DAR 2015, respectively. Therefore, we set ONU consists of four branches (O, O r , O f and O r+f ) for the later experiments.</figDesc><table><row><cell cols="2">Backbone Method</cell><cell cols="6">Rotated ICDAR 2015 Recall Precision F-score Recall Precision F-score ICDAR 2015</cell></row><row><cell></cell><cell>(a)Reimplemented EAST (Baseline)</cell><cell>12.52</cell><cell>65.00</cell><cell>20.99</cell><cell>74.96</cell><cell>87.23</cell><cell>80.63</cell></row><row><cell></cell><cell>(b)+RA</cell><cell>63.64</cell><cell>75.89</cell><cell>69.23</cell><cell>66.28</cell><cell>79.72</cell><cell>72.43</cell></row><row><cell></cell><cell>(c)+RA+GNM</cell><cell>58.01</cell><cell>79.59</cell><cell>67.11</cell><cell>63.79</cell><cell>74.57</cell><cell>68.76</cell></row><row><cell>EAST</cell><cell>(d)+RA+GNM+Init</cell><cell>67.06</cell><cell>85.46</cell><cell>75.15</cell><cell>69.62</cell><cell>81.56</cell><cell>75.12</cell></row><row><cell></cell><cell>(e)+RA+ONU+Init+TSS</cell><cell>72.89</cell><cell>80.32</cell><cell>76.42</cell><cell>73.04</cell><cell>84.32</cell><cell>78.28</cell></row><row><cell></cell><cell>(f)+RA+SNU+Init+TSS</cell><cell>71.52</cell><cell>75.50</cell><cell>73.47</cell><cell>78.96</cell><cell>76.70</cell><cell>77.81</cell></row><row><cell></cell><cell cols="2">(g)+RA+GNM+Init+TSS (ours GNNets) 77.28</cell><cell>84.69</cell><cell>80.82</cell><cell>80.45</cell><cell>83.67</cell><cell>82.03</cell></row><row><cell></cell><cell>(a)Reimplemented PSENet (Baseline)</cell><cell>56.30</cell><cell>72.04</cell><cell>63.22</cell><cell>83.53</cell><cell>86.10</cell><cell>85.04</cell></row><row><cell></cell><cell>(b)+RA</cell><cell>73.61</cell><cell>81.63</cell><cell>77.41</cell><cell>83.28</cell><cell>83.24</cell><cell>83.26</cell></row><row><cell></cell><cell>(c)+RA+GNM</cell><cell>69.47</cell><cell>82.71</cell><cell>75.29</cell><cell>78.52</cell><cell>82.54</cell><cell>80.48</cell></row><row><cell>PSENet</cell><cell>(d)+RA+GNM+Init</cell><cell>72.96</cell><cell>82.10</cell><cell>77.92</cell><cell>81.41</cell><cell>86.89</cell><cell>84.06</cell></row><row><cell></cell><cell>(e)+RA+ONU+Init+TSS</cell><cell>74.62</cell><cell>83.87</cell><cell>78.98</cell><cell>82.95</cell><cell>85.33</cell><cell>84.13</cell></row><row><cell></cell><cell>(f)+RA+SNU+Init+TSS</cell><cell>74.57</cell><cell>82.48</cell><cell>78.33</cell><cell>83.96</cell><cell>86.03</cell><cell>84.99</cell></row><row><cell></cell><cell cols="2">(g)+RA+GNM+Init+TSS (ours GNNets) 75.58</cell><cell>83.24</cell><cell>79.23</cell><cell>82.37</cell><cell>88.01</cell><cell>85.10</cell></row><row><cell cols="2">on the Rotated ICDAR 2015. All the experimental results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">are shown in Table 4. By narrowing the desired canonical</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">orientation range from [− π 2 , π 2 ] to [0, π 4 ],</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison with the state-of-the-art methods on both ICDAR 2015 and ICDAR 2017 MLT. The methods proposed in this paper are tested with only one-forward.</figDesc><table><row><cell>Model</cell><cell cols="3">ICDAR 2015 Recall Precision F-score</cell><cell>FPS</cell><cell cols="3">ICDAR 2017 MLT Recall Precision F-score</cell></row><row><cell>CTPN [38]</cell><cell>51.56</cell><cell>74.22</cell><cell>60.85</cell><cell>7.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SegLink [31]</cell><cell>76.50</cell><cell>74.74</cell><cell>75.61</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSTD [7]</cell><cell>73.86</cell><cell>80.23</cell><cell>76.91</cell><cell>7.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WordSup [8]</cell><cell>77.03</cell><cell>79.33</cell><cell>78.16</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EAST [42]</cell><cell>78.33</cell><cell>83.27</cell><cell>80.72</cell><cell>6.52</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ITN [36]</cell><cell>74.10</cell><cell>85.70</cell><cell>79.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RRD [17]</cell><cell>79.0</cell><cell>85.6</cell><cell>82.2</cell><cell>6.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FTSN [3]</cell><cell>80.07</cell><cell>88.65</cell><cell>84.14</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TDN SJTU2017 [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>47.13</cell><cell>64.27</cell><cell>54.38</cell></row><row><cell>SARI FDU RRPN v1 [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.50</cell><cell>71.17</cell><cell>62.37</cell></row><row><cell>SCUT DLVClab1 [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.54</cell><cell>80.28</cell><cell>64.96</cell></row><row><cell>EAST++ [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.42</cell><cell>66.61</cell><cell>72.86</cell></row><row><cell>FOTS [20]</cell><cell>85.17</cell><cell>91.00</cell><cell>87.99</cell><cell>7.5</cell><cell>57.51</cell><cell>80.95</cell><cell>67.25</cell></row><row><cell>Pixel-Anchor [15]</cell><cell>87.50</cell><cell>88.32</cell><cell>87.68</cell><cell>10</cell><cell>59.54</cell><cell>79.54</cell><cell>68.10</cell></row><row><cell>PSENet [14]</cell><cell>85.22</cell><cell>89.30</cell><cell>87.21</cell><cell>2.33</cell><cell>68.35</cell><cell>76.97</cell><cell>72.40</cell></row><row><cell>PSENet (reimplemented)</cell><cell>86.58</cell><cell>88.02</cell><cell>87.30</cell><cell>2.4</cell><cell>69.75</cell><cell>75.99</cell><cell>72.74</cell></row><row><cell>Ours GNNets</cell><cell>86.71</cell><cell>90.41</cell><cell>88.52</cell><cell>2.1</cell><cell>70.06</cell><cell>79.63</cell><cell>74.54</cell></row><row><cell>(a) ICDAR 2015</cell><cell></cell><cell cols="2">(b) ICDAR 2017 MLT</cell><cell></cell><cell></cell><cell></cell><cell>(c) Rotated ICDAR 2015</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The new benchmark of rotated ICDAR 2015 will be available in: https://github.com/bigvideoresearch/GNNets</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://rrc.cvc.uab.es/?ch=8&amp;com=introduction" />
		<title level="m">ICDAR2017 Competition on Multi-Lingual Scene Text Detection and Script Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fastext: Efficient Unconstrained Scene Text Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fused Text Segmentation Networks for Multi-Oriented Scene Text Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youxuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting Text in Natural Scenes with Stroke Width Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Single Shot Text Detector with Regional Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qile</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">WordSup: Exploiting Word Annotations for Character Based Text Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text Localization in Natural Images using Stroke Feature Transform and Text Covariance Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">R2cnn: Rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vijay Ramaseshan Chandrasekhar, Shijian Lu, and Others. ICDAR 2015 Competition on Robust Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez I Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Sergi Robles Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<meeting><address><addrLine>David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Shape Robust Text Detection with Progressive Scale Expansion Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruo-Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02559</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pixel-anchor: A fast oriented scene text detector with combined networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangkun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meifang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07432</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TextBoxes: a Fast Text Detector with a Single Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rotation-Sensitive Regression for Oriented Scene Text Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSD: Single Shot Multibox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fots: Fast Oriented Text Spotting with a Unified Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dagui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TextSnake : A Flexible Representation for Arbitrary Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangbang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joseph Chazalon, and Others. IC-DAR2017 Robust Reading Challenge on Multi-Lingual Scene Text Detection and Script Identification-RRC-MLT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nibal</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Rigaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Method for Text Localization and Recognition in Real-World Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time Scene Text Localization and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster RCNN: Towards Real-time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting Oriented Text in Natural Images by Linking Segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An Analysis of Scale Invariance in Object Detection SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SNIPER: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting Text in Natural Image with Connectionist Text Proposal Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Text Localization and Recognition in Images and Video. Handbook of Document Image Processing and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Uchida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometry-Aware Scene Text Detection with Instance Transformation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Text Detection and Recognition in Imagery: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multi-orientation Scene Text Detection with Adaptive Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yi</forename><surname>Xu-Cheng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Boosting up Scene Text Detectors with Guided CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Symmetry-based Text Line Detection in Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deeptext: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyong</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07314</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">EAST: An Efficient and Accurate Scene Text Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene Text Detection and Recognition: Recent Advances and Future Trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
