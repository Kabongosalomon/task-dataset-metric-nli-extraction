<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised MultiModal Versatile Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
							<email>jalayrac@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><surname>Recasens</surname></persName>
							<email>arecasens@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Schneider</surname></persName>
							<email>rgschneider@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Science</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">HES-SO</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Geneva School of Business Administration (DMML Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>De Fauw</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Engineering Science</orgName>
								<orgName type="laboratory">VGG</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepmind</forename></persName>
						</author>
						<title level="a" type="main">Self-Supervised MultiModal Versatile Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Videos are a rich source of multi-modal supervision. In this work, we learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams. To this end, we introduce the notion of a multimodal versatile network -a network that can ingest multiple modalities and whose representations enable downstream tasks in multiple modalities. In particular, we explore how best to combine the modalities, such that fine-grained representations of the visual and audio modalities can be maintained, whilst also integrating text into a common embedding. Driven by versatility, we also introduce a novel process of deflation, so that the networks can be effortlessly applied to the visual data in the form of video or a static image. We demonstrate how such networks trained on large collections of unlabelled video data can be applied on video, video-text, image and audio tasks. Equipped with these representations, we obtain state-of-the-art performance on multiple challenging benchmarks including UCF101, HMDB51, Kinetics600, Audioset and ESC-50 when compared to previous self-supervised work. Our models are publicly available [1, 2, 3].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our experience of the world is multimodal. From as far back as the crib, we perceive through multi-sensory systems, for instance we watch the flames dancing in the fireplace, we hear the sound of the crackling wood, as well as feel the heat coming off. Through this multimodal synchronous perception, we learn to draw useful connections between modalities [76] which, in turn, enables us to form good representations of the world. Later, comes language that allows us to communicate this fine-grained multimodal experience using higher-level abstract concepts.</p><p>Our objective is to learn representations from such multimodal experience in a self-supervised manner without resorting to any specific manual annotation. The modalities considered are the three that are easily available from large collections of unlabelled videos: visual, audio and language (obtained from narrations) streams. In this, we seek to learn a multimodal versatile network, defined as a network that has the following four properties: (i) it should be able to take as input any of the three modalities; (ii) it should respect the specificity of modalities, in particular the fact that the audio and visual modalities are much more fine-grained than language; (iii) it should enable the different * Equal contribution. † Work done during an internship at DeepMind. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2006.16228v2 [cs.CV] 30 Oct 2020</p><p>2 Related work Self-supervised learning from single modality. Self-supervised methods design pretext tasks that require no manual annotation but facilitate learning of useful representations of the data. A variety of pretext tasks have been developed for vision (i.e. single modality), such as predicting the relative position of patches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b57">58]</ref>, colorization [95], predicting orientation <ref type="bibr" target="#b25">[26]</ref> or invariance to transformation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref>. In videos, works have also leveraged the temporal dimension <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b90">91]</ref>. Recently, methods that maximise the similarity between multiple views (augmented versions) of the same image via contrastive losses <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b58">59]</ref> stand out due to impressive results on the ImageNet benchmark; we draw inspiration from them (e.g. use a contrastive loss and nonlinear projection heads <ref type="bibr" target="#b14">[15]</ref>). However, details of view generation are crucial and require careful design <ref type="bibr" target="#b80">[81]</ref>. In contrast, we argue that using multiple modalities as different views is simpler and more natural <ref type="bibr" target="#b79">[80]</ref>.</p><p>Vision and language. WSABIE [86] and DeVise [23] introduced the idea of embedding text and image in the same space. This allows semantic similarity to be measured by a dot product in a vector space and enables fast and efficient large scale search across modalities <ref type="bibr" target="#b35">[36]</ref>. This idea is at the core of our versatile networks. With larger datasets <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b95">96]</ref>, many works have profited from learning such a joint visual-textual space <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b92">93]</ref>. Recently, instructional videos became a popular source of video and language data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b93">94]</ref> due to not requiring careful manual annotation, e.g. by using Automatic Speech Recognition (ASR) to generate text from narrations. We build on top of <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79]</ref> who learn good representations from such narrated material, but consider learning representations using audio as well.</p><p>Vision and audio. Cross-modal teacher-student methods [9, 61] exploit the temporal co-occurrence between visual and audio modalities in a video to learn good representations. Taking this idea into the self-supervised domain <ref type="bibr" target="#b6">[7]</ref>, multiple works use a pretext task of predicting whether visual and audio signals come from the same video <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b71">72]</ref>. Recent developments such as XDC [6], who employ cross-modality clustering, or Evolving Losses [67], where m any single-and multi-modal pretext tasks are used, demonstrate an impressive ability to learn good representations in both modalities. We propose a simpler method that achieves better performance, and consider the text modality as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>modalities to be easily compared even when they are never seen together during training; and finally (iv) it should be efficiently applicable to visual data coming in the form of dynamic videos or static images.</p><p>The question is how to design a network that respects these four principles? We choose a design that embeds each modality into a vector space such that similarity between modalities is obtained via simple dot products. Each modality is processed by a backbone network adapted to the nature of the signal, and a modality embedding graph is constructed such that the visual and audio embeddings are fine-grained, whilst the textual embedding is semantically coarse-grained. This strategy is based on the observation that the visual and audio spaces are fine-grained (there are many visual or sounds of guitars that might be really different to each other) while the textual domain is more coarse as its goal is to abstract away details (e.g. a single "guitar" word). The network is then trained from scratch via self-supervised contrastive learning on a large set of unlabelled videos.</p><p>To quantitatively evaluate our learned MultiModal Versatile (MMV) networks, we measure their performance on multiple downstream tasks, and in this way assess various properties of the representation of videos and images: verb learning (action classification on HMBD51, UCF101 and Kinetics600); noun learning (image classification on PASCAL VOC and ImageNet); joint text and visual representation (YouCook2, MSRVTT); and audio representation (sound classification on ESC-50 and AudioSet). The proposed MMV achieves state-of-the-art performance for selfsupervised approaches on these benchmarks, and reduces the gap to the state-of-the-art performance for supervised approaches.</p><p>Contributions. Our contributions are the following: (a) we investigate different modality embedding graphs for MMV, and propose a simple yet effective self-supervised training strategy for multimodal representation of audio, visual and language streams; (b) we introduce a deflation approach so that the MMV video network can efficiently ingest a static image; and (c) we demonstrate the superiority of the learned representations on multiple image, video, audio and video-text downstream tasks.</p><p>Vision, audio and language. Using audio, vision and language to learn representations has also been explored in past work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b82">83]</ref>. In particular, Harwath et al. <ref type="bibr" target="#b29">[30]</ref> use a dataset of images and audio descriptions to associate spoken words and their visual representation. Similarly to us, Aytar et al. <ref type="bibr" target="#b9">[10]</ref> train a cross-modal network with image, audio and text modalities. One major difference is that they rely on curated annotated datasets, while our approach requires no annotation.</p><p>From video to image. We reverse the usual route of going from an image network to a video network by inflation <ref type="bibr" target="#b13">[14]</ref>. Historically, this was the usual route <ref type="bibr" target="#b26">[27]</ref> as labels were more readily available for images, e.g. ImageNet, than for videos. However, our perception of the world is actually dynamic, a time series of images, and learning first from videos is more natural. Similarly to <ref type="bibr" target="#b17">[18]</ref>, we enable our network to ingest both dynamic video and still images. But instead of having two different pathways and requiring to learn from both images and videos, we propose a simple deflation mechanism that enables our network purely trained on videos to be directly adapted to still images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We are given a set of unlabelled videos containing different modalities. For example, a video may contain an RGB stream (e.g. a set of frames depicting a dog), an audio track (e.g. the sound of that same dog barking) and some linguistic narrations (e.g. coming from a person providing verbal instructions). We follow previous work <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53]</ref> and obtain language as text by using off-the-shelf Automatic Speech Recognition (ASR) on the audio, leaving the removal of this dependency to future work. Equipped with this, our goal is to learn a model that has the versatile properties described in Section 1. We do so by introducing a bespoke multimodal architecture and optimize its parameters via self-supervised learning. In details, we use the temporal co-occurrence between the modalities to define the self-supervised proxy task and enforce it with a multi-modal pairwise contrastive loss.</p><p>Formally, a video x ∈ X is defined by an instantiation of different modalities M: x = {x m }, m ∈ M. In this work, we focus on three modalities, namely vision x v ∈ X v , audio x a ∈ X a and text x t ∈ X t but the proposed approach could be easily generalized to more modalities. Specifically, x v , x a and x t correspond to few-second sequence of RGB frames, 1D audio samples, and discrete word tokens, respectively. Given a training set containing n videos {x i } n i=1 ∈ X n , we seek to learn modality specific representations as well as ways to compare streams across modalities. To that end, let f m : X m → R dm be a parametrized modality specific backbone neural network that takes as input an instance x m from modality m and outputs a representation vector of dimension d m . To compare different modalities together via simple dot products, we embed them into a shared space S s ⊂ R ds of dimension d s , where s contains the list of modalities that we embed in the space, e.g. s = va for a joint visual-audio space S va , or s = vat for a joint visual-audio-text space S vat . A modality specific representation f m (x m ) is embedded into a space S s via a projection head g m→s : R dm → R ds . We denote by z m,s = g m→s (f m (x m )) the vector representing the input modality x m in the space S s . Section 3.1 explores various model design choices for the MMV networks, which induce different structures of modality spaces S s . It also presents the self-supervised losses that enforce the different modalities to align in the common spaces. In Section 3.2, we explain how to simply adapt models that have been trained on sequences of RGB frames to operate on single frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MMV: MultiModal Versatile Networks</head><p>Recall our goal is to be able to embed different modalities into a vector space where semantic comparisons can be made by simple dot products. Since there are three modalities, multiple configurations of modality spaces with different inter-relations, which we call modality embedding graphs, can be envisaged. An important note is that since the text modality is directly obtained from the audio track using ASR, we do not construct the audio-text space nor the loss that puts them in alignment explicitly. This is because our goal is not to learn ASR but instead to associate a word, e.g. "car", with the sound associated with that entity, e.g. the sound produced by the engine. However, we hypothesize that the model can learn this desired link implicitly thanks to the common visual ground. We consider three options for the modality embedding graphs, illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> and detailed next.</p><p>Option I: Shared space. This is the simplest model where all modalities are embedded into a single shared vector space S vat ⊂ R ds , and in which direct comparisons can be made between modalities <ref type="figure" target="#fig_0">(Figure 1a</ref>). For example, starting from a visual vector f v (x v ), a single projection head is applied to obtain the embedding z v,vat used to compare to the audio and the text modalities. This strategy has the advantage that it is easy to navigate between modalities since they all live in the same space (property (iii)). However, the model implicitly assumes that all modalities have equal granularity and hence does not respect their specificities (lack of property (ii)).</p><formula xml:id="formula_0">V A T (a) Shared V A T (b) Disjoint T V A (c) FAC V T A MIL-NCE NCE (d) FAC details</formula><p>Option II: Disjoint spaces. Another natural option is to have different visual-audio (S va ) and visualtext (S vt ) spaces, as illustrated in <ref type="figure" target="#fig_0">Figure 1b</ref>. For example, starting from the visual representation f v (x v ), there are two distinct projection heads mapping to the S va and the S vt domains, i.e. z v,va = z v,vt . While the disjoint spaces approach enables the specificity of different modality pairs (property (ii)), it does not allow easy navigation between the embedding spaces (lack of property (iii)), for example, text to audio retrieval (e.g. "car" to "engine sound") is not possible.</p><p>Option III: Fine and coarse spaces (FAC). In the introduction, we argue that the visual and the audio domains are different from the language domain in terms of their granularities. Inspired by this intuition, we propose to learn two embedding spaces: vision and audio are compared in the fine-grained space (S va ), while text is compared with vision and audio in the lower dimensional coarse-grained space (S vat ). Crucially, vectors in S va can be embedded into S vat via a simple fine-to-coarse projection g va→vat , as illustrated in <ref type="figure" target="#fig_0">Figure 1c</ref>. For example, to compare vision to audio, the visual representation is projected into the fine-grained space S va via g v→va . To compare vision to text, vision is embedded into the coarse-grained space S vat via projection g v→vat which decomposes as g va→vat • g v→va ; this can be seen as first projecting the vision into the fine-grained space S va via g v→va , followed by projecting the fine-into the coarse-grained space by g va→vat (see <ref type="figure" target="#fig_0">Figure 1d</ref>). Note that even though we do not align audio and text during training (as mentioned before, this is to not learn ASR), the imposed modality embedding graph enables audio-text comparison because audio can still be projected into the coarse-grained space S vat via g va→vat • g a→va . This strategy covers the three relevant properties of the MMV network -as opposed to the shared space solution, it models the text differently from the vision and the audio (property (ii)), and, in contrast to the disjoint spaces approach, it enables easy navigation across modalities (property (iii)).</p><p>Multimodal contrastive loss. Given the previously described embedding graphs joining the three different modalities, the question remains how to actually train the backbones and the projection heads. We wish to do so without resorting to any form of manual annotations in order to leverage large amounts of readily available videos on the internet. Instead, inspired by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b50">51]</ref>, we construct self-supervised tasks which aim to align pairs of modalities: vision-audio or vision-text, but not audiotext as explained earlier. Concretely, positive training pairs across two modalities are constructed by sampling the two streams from the same location of a video. Conversely, negative training pairs are created by sampling streams from different videos. In practice, a minibatch of N video samples is formed, which induces N positive and N 2 − N negative pairs. Given these positive and negative training pairs, we use a contrastive loss <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59]</ref> to make the positive pairs similar and negative pairs dissimilar in their corresponding joint embedding space. The only difference between losses used with different embedding graph designs is the choice of spaces where the dot products are computed; next we give the loss for FAC and provide the shared and disjoint losses in Appendix B. Formally, given a video x, we minimize the multimodal contrastive loss:</p><formula xml:id="formula_1">L(x) = λ va NCE(x v , x a ) + λ vt MIL-NCE(x v , x t ),<label>(1)</label></formula><p>where λ mm corresponds to the weight for the modality pair m and m . The component corresponding to the visual-audio pair is the following NCE loss (for FAC):</p><formula xml:id="formula_2">NCE(x v , x a ) = − log exp(z v,va z a,va /τ ) exp(z v,va z a,va /τ ) + z ∼N (x) exp(z v,va z a,va /τ ) ,<label>(2)</label></formula><p>where N (x) is a set of negative modality pairs for the video x, and τ is the temperature parameter. For the text, recall that we use narrations automatically obtained from speech. As opposed to the audio that is usually better aligned with its visual source (e.g. the sound of the piano is synchronized with the visual of the instrument being played), the correspondence between narrations and what is actually happening in the video is much weaker <ref type="bibr" target="#b50">[51]</ref>. To address that issue, we use the MIL-NCE variant from <ref type="bibr" target="#b50">[51]</ref> that is tailored to account for this misalignment issue. In short, it considers multiple positive candidate pairs as positives by simply replacing the single term exp(z v,vat z t,vat /τ ) in the standard NCE equation (2) by a sum of scores over positive text candidates: z∈P(x) exp(z v,vat z t,vat /τ ).</p><p>As in <ref type="bibr" target="#b50">[51]</ref>, the set of potential positives P(x) is formed from temporally close narrations.</p><p>Missing modalities. Some videos do not have all modalities, for example not all videos contain narration. In that case, we simply discard the corresponding loss component in <ref type="bibr" target="#b0">(1)</ref> and upweight the remaining examples of the same modality pair in the batch in order for the total loss weight to remain constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Video to image network deflation</head><p>To comply with the property (iv) of the multimodal versatile network, we introduce a network deflation operation to transform a video network into a network that can ingest a single image. The deflated network can be evaluated on image downstream tasks while training on videos, and is more efficient than the standard trick of assembling a static video by repeating the image in time.</p><p>Ideally we would wish for video-image equivalence: that the output of the deflated video network on a single image is identical to that obtained by applying the original video network to the single-image static-video. It might be thought that this can simply be achieved by deflating the network over the temporal dimension. In the two types of video networks considered here, this deflation corresponds to the following operations: for 3D convolutional based networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b89">90]</ref>, summing the 3D spatiotemporal filters over the temporal dimension to obtain 2D filters; for TSM networks <ref type="bibr" target="#b45">[46]</ref>, turning off the channel shifting which results in a standard residual architecture (e.g. ResNet50) for images.</p><p>However, due to zero-padding these operations do not achieve the desired equivalence -since filters whose receptive field overlap the clip boundary receive zeros in the single-image static-video, and this is not taken into account by the simple deflation operation above. Note, the padding used in the spatial domain is not a problem, as the spatial padding applies equivalently for both video frames and single images. To take account of the zero-padding, we learn new parameters γ and β for the batch normalization layers to correct for this boundary effect on the filter outputs, and approximate the equivalence we seek. In detail, the γ and β parameters are trained to minimize the L 1 loss between the output of the original video network when presented with single-image static-videos, and the output of the deflated network for the same images; all parameters are frozen apart from γ's and β's of the deflated network. Note that this process only requires images without the need for annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we evaluate the performance of the networks on a wide range of downstream tasks. We start by describing the experimental protocol and the datasets used for self-supervised pretraining and downstream evaluations (Section 4.1), followed by exploring various design choices (Section 4.2). Based on this study, we train final models at a larger scale to compare them to the state-of-the-art (Section 4.3). Finally, we apply the trained video networks on still image tasks (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup, datasets and downstream tasks</head><p>Network architectures, hyperparameters and optimization. For video we explore using S3D-G <ref type="bibr" target="#b89">[90]</ref> (d v = 1024), and TSM <ref type="bibr" target="#b45">[46]</ref> with a ResNet50 backbone (d v = 2048) or a ResNet50x2 backbone (ResNet50 with all channels doubled <ref type="bibr" target="#b40">[41]</ref>, d v = 4096). We apply temporal and spatial average pooling at the last layer of the backbone (before the usual classification layer) to obtain a single vector f v (x v ). During training, 32 (16 for the exploration design) frames are sampled at 10 fps and 200 × 200 crops are used (frames are resized so that the minimum side is 224). We use the following standard augmentation during training: random crop, horizontal flipping, temporal sampling and scale jittering, and color augmentation (details in Appendix A.1). Audio is represented as log MEL spectrogram with 80 bins and processed with ResNet50 and is sampled in sync with the frames. Spatial pooling is applied to obtain f a (x a ) of dimension d a = 2048. For the final audio evaluation (Section 4.3), the network ingests 2 seconds of audio for fair comparison to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref>, otherwise we use the same duration as the input video clip. Following <ref type="bibr" target="#b50">[51]</ref>, text is processed by removing stop words, retaining a maximum or padding to 16 words, then extracting 300-dimensional Google News pre-trained word2vec <ref type="bibr" target="#b53">[54]</ref> and finally applying a linear layer to independently map the word inputs to 2048 dimension followed by a max pooling layer over the 16 words (d t = 2048). The dimension of the shared subspaces is 512, except for the Fine And Coarse (FAC) design where we use 512 dimensions for S va (fine) and 256 for S vat (coarse). More details about architecture are provided in Appendix B. As done in <ref type="bibr" target="#b14">[15]</ref>, we normalize vectors prior to computing their dot products in the NCE and MIL-NCE losses and use a temperature of τ = 0.07 in the softmax as in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b88">89]</ref>. When training with all three modalities on HowTo100M, we observe that a larger weight on the Vision-Text loss is beneficial since text is more prominent. However, when training on HowTo100M+AudioSet, equal loss weights worked best because the audio from AudioSet is more informative. Therefore, a 10:1 loss weight ratio is used when training on HowTo100M and 1:1 for HowTo100M+AudioSet. Finally, all networks are trained from scratch using Adam <ref type="bibr" target="#b38">[39]</ref> with an initial learning rate of 0.002, 5K steps of warm up and a half-period cosine schedule <ref type="bibr" target="#b47">[48]</ref>.</p><p>Training datasets. We use the HowTo100M <ref type="bibr" target="#b52">[53]</ref> and/or the train split of AudioSet <ref type="bibr" target="#b23">[24]</ref> datasets for self-supervised training. The HowTo100M dataset contains more than 100 millions narrated video clips coming from 1 million unique videos where the audio narration is transcribed into text using ASR. We follow the same processing as described in <ref type="bibr" target="#b50">[51]</ref> for creating positive and negative pairs for our contrastive based loss. AudioSet consists of 10 seconds clips coming from 2 million different internet videos. The dataset contains a variety of audio tracks such as musical instruments, animals or mechanical sounds, since it was built for audio classification, but we discard the labels for self-supervised training. Due to the dataset nature, text is considered a missing modality for AudioSet.</p><p>Downstream tasks. The trained networks are evaluated on various downstream tasks that aim to capture different aspects of the learned representations. For action classification, we evaluate the visual representation on the UCF101 <ref type="bibr" target="#b76">[77]</ref> (13K videos and 101 action classes) and the HMDB51 <ref type="bibr" target="#b43">[44]</ref> (7K videos and 51 classes) benchmarks. Two settings are explored -frozen where we simply learn a linear classifier on top of the pretrained f v (x v ) vector, and a finetune setting where the full visual model f v is finetuned. We also propose an additional large scale downstream task by evaluating the performance on Kinetics600 <ref type="bibr" target="#b11">[12]</ref> (30K evaluation clips with 600 classes) in the frozen setting. To evaluate the quality of the audio representation, we use the ESC-50 <ref type="bibr" target="#b65">[66]</ref> (2K audio clips with 50 classes) and AudioSet <ref type="bibr" target="#b23">[24]</ref> (20K eval audio clips with 527 classes) classification task using the frozen setting on the features produced by the last convolution of the audio backbone network. We report mAP on AudioSet and the top-1 accuracy for ESC-50. Some classification datasets have official splits (3 for UCF101/HMDB51 and 5 for ESC-50). As per standard, split#1 serves as the validation set and is therefore used for ablations (Section 4.2), and the average accuracy over all splits is reported when comparing to the state-of-the-art (Section 4.3). The quality of our text-video representation is evaluated on zero-shot text-to-video retrieval on the MSRVTT <ref type="bibr" target="#b91">[92]</ref> (1K videos) and YouCook2 <ref type="bibr" target="#b95">[96]</ref> (3190 videos at the time of publication) benchmarks, by following the evaluation protocol described in <ref type="bibr" target="#b52">[53]</ref> and reporting the recall at 10 (R@10) (and other retrieval metrics in Appendix A.2). Finally, to evaluate how well our video representation transfers to image tasks we use the PASCAL VOC 2007 <ref type="bibr" target="#b19">[20]</ref> and ImageNet <ref type="bibr" target="#b72">[73]</ref> classification tasks. For the image tasks, the frozen setting on the deflated version of f v is used, and, as per standard, we report the mAP on PASCAL and the top-1 and top-5 accuracies on ImageNet. Full details are given in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Design explorations</head><p>We here summarize the effects of various design choices of our method. To facilitate running a large number of experiments, we use the S3D-G <ref type="bibr" target="#b89">[90]</ref> network as the video backbone, with 16 frames per video clip, a total batch size of 512 and 500K training steps (20 hours training on 16 Cloud TPUs). Unless otherwise stated, linear projection heads are used for all modalities, and the networks are trained on HowTo100M. To minimize the amount of hyper-parameter tuning, for UCF101, HMDB51 and ESC-50 we use only the frozen setting and report top-1 accuracy on the split#1. We also report R@10 for YC2 (YR10) and MSRVTT (MR10) under the zero-shot setting. Full details, including all quantitative results, are given in Appendix C. Pairs of modalities. We here summarize the main findings from experiments that consider learning from two modalities -Vision and Text, or Vision and Audio -as this setup makes it easy to isolate the effects of different components and discover the best building blocks to be used in the threemodality setting. For the video backbones, we observe that TSM ResNet50 always beats S3D-G for downstream tasks that involve vision. For Vision and Audio, contrastive based loss consistently outperforms logistic loss (used in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">43]</ref>) by 2% on vision downstream tasks, and is on par for audio. This is in line with findings of recent single-modality self-supervised approaches as well as work in Vision and Text <ref type="bibr" target="#b50">[51]</ref> that demonstrate the superiority of NCE based loss compared to its binary classification counterpart. Regarding the projection heads, the experiments confirm findings of <ref type="bibr" target="#b14">[15]</ref> that adding a non-linear projection head (two layers MLP with BatchNorm and ReLU activations) on top of the visual representations helps (notably for UCF101 and HMDB51). It was not beneficial to have non-linear projection heads for the language and audio branches. We hence keep linear projection heads for audio and text branches and use a non-linear projection head for vision in the rest of the paper. Regarding data augmentation, we observe that despite training on large datasets, removing visual augmentation such as color augment or scale jittering slightly decreases performance, hence we keep them for the rest of the paper. Concerning the audio, we add Gaussian noise to the raw signal, with mean 0 and variance 0.01 × max amplitude, which seems to slightly improve results.</p><p>Mildly jittering with SpecAugment <ref type="bibr" target="#b62">[63]</ref> was not beneficial, and more aggressive augmentations were detrimental; in contrast with the findings of <ref type="bibr" target="#b63">[64]</ref> where SpecAugment helped, presumably due to training on a relatively small dataset. Temporal jittering by randomly offsetting the audio with respect to the visual stream by up to 0.8s (half of the training clip length) reduces the performance on visual tasks by 4%, showing that synchronization is an important training signal.</p><p>Combining Vision, Audio and Text. On HowTo100M, learning with all three modalities clearly outperforms networks trained with only pairs of modalities <ref type="table" target="#tab_0">(Table 1a)</ref>, obtaining significantly better visual representations (UCF101 and HMDB51) and on-par audio representations (ESC-50). The scores are tied on Vision-Text tasks, with the 3-modality network winning on MSRVTT but losing on YC2. These results demonstrate the ability of our network to learn from the complementary training signals coming from the audio and the text modalities. Next we look at the performance of the different modality merging strategies on the combination of HowTo100M and AudioSet in <ref type="table" target="#tab_0">Table 1b</ref>. First, comparing to <ref type="table" target="#tab_0">Table 1a</ref>, we observe that combining AudioSet with HowTo100M improves performance on HMDB51, UCF101 and ESC-50. This confirms again that our networks can leverage the complementary nature of the modalities to learn better representation as well as showcases the advantage of being able to cope with heterogeneous sources of data (AudioSet does not have text). We note a decrease in performance for the video-text benchmarks (MSRVTT and YC2), which can simply be explained by the fact that only a half of the training samples contain text vs. <ref type="table" target="#tab_0">Table 1a</ref> (the other half comes from AudioSet which does not have text). As shown in the next section, this can simply be recovered by training for longer. Second, we note that all strategies for merging the three modalities obtain good representations, but the fine-and-coarse (FAC) method dominates on UCF101, HMDB51 and MSRVTT, achieves a good result on ESC-50 and is second best on YC2. The result agrees with the intuition that care should be taken to account for the specificity of the different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Large-scale experiments and comparison to the state-of-the-art</head><p>Final experimental setup. We use 32 frames per video clip, 500K training steps, and a total batch size of 4096 (S3D-G and TSM-50) or 2048 (TSM-50x2); training TSM-50 takes 3 days on 32 Cloud TPUs. Based on our ablations, the audio and text networks employ a linear projection head, whereas the video network uses a non-linear head. All models use the FAC design when working with the three modalities. Self-supervised training is performed on the combination of HowTo100M and AudioSet datasets with standard augmentation. The full details are in Appendix A.</p><p>Results. <ref type="table" target="#tab_2">Table 2</ref> shows our visual and audio representations match or outperform the state-ofthe-art on all downstream tasks and evaluation modes (linear or finetuning). Impressively, simple linear classifiers are competitive with some of the best previous work that uses finetuning and set a strong baseline on the large scale Kinetics600 downstream task. We also compare to the best externally reported supervised pretraining transfer as a meaningful and strong baseline that selfsupervised methods should aim to surpass. Under that challenging comparison, MMV performance on HMDB51 and UCF101 is getting close to the best supervised method that leverage both ImageNet and Kinetics <ref type="bibr" target="#b89">[90]</ref>, while on ESC-50 it is even better than the best supervised result [74] by 1.7%.</p><p>Comparison with previous works on equal grounds is difficult due to the wide range of backbone architectures and training data sources used. Using the same visual backbone (R(2+1)D-18 <ref type="bibr" target="#b81">[82]</ref>), training dataset (AudioSet) and modalities (Vision+Audio), we obtain similar performance to XDC <ref type="bibr" target="#b5">[6]</ref> and GDT <ref type="bibr" target="#b63">[64]</ref> on UCF101, and significantly outperform them on HMDB51. Comparing to best reported results across past works -our smaller TSM-50 model (trained on Vision+Audio+Text) achieves similar performance to GDT <ref type="bibr" target="#b63">[64]</ref> while being superior to XDC <ref type="bibr" target="#b5">[6]</ref> and ELo <ref type="bibr" target="#b66">[67]</ref>, despite having significantly fewer parameters and being trained with the same amount or less data; note also that XDC <ref type="bibr" target="#b5">[6]</ref> and GDT <ref type="bibr" target="#b63">[64]</ref> train on Instagram65M <ref type="bibr" target="#b24">[25]</ref> which has been collected specifically to mimic action recognition datasets. The superior performance of the larger TSM-50x2 model demonstrates that large networks can benefit from self-supervised training on vast amounts of data, and that our self-supervised task facilitates this process. This has also been observed in previous work in the image domain <ref type="bibr" target="#b14">[15]</ref> and is also confirmed by the better performance of our R(2+1)D-18 backbone versus S3D-G when finetuning on HMDB51 and UCF101.</p><p>Comparing to the two-modality case -Vision+Text with S3D-G is a similar setup to <ref type="bibr" target="#b50">[51]</ref> and training with three modalities is clearly beneficial. Similarly, FAC also beats training with only Vision+Audio, confirming again the advantage of learning from three modalities instead of two. This is particularly significant on the Kinetics600 downstream task (+9.2%) where the semantic contained in the narrations from HowTo100M about objects or actions may be relevant for the Kinetics classes.</p><p>Regarding zero-shot video to text retrieval our MMV S3D-G, TSM-50 and TSM-50x2 respectively obtain a R@10 of 37. Results. <ref type="table" target="#tab_3">Table 3</ref> shows that the deflated networks perform almost as well as the original video model applied on input-inflated 32-frame static videos (the difference is only around 1% when comparing the 'def' and 'i-inf' results). However, the deflated model is an order of magnitude more efficient due to processing single images instead of the full 32-frame videos. Naive deflation underperforms severely due to the strong padding effects, proving that our deflation training is necessary. The state-of-the-art self-supervised models trained on images (SimCLR <ref type="bibr" target="#b14">[15]</ref>) outperform MMV due to not having to bridge the video-image domain gap and in fact has been trained on ImageNet imagesthe performance difference is much smaller on PASCAL. Finally, our approach is significantly better than pre-training in a fully supervised manner on Kinetics-700 <ref type="bibr" target="#b12">[13]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we have explored how to train versatile networks for vision, audio and language in a self-supervised manner. Our method is simple yet it matches or exceeds the state-of-the-art for action and audio classification on five challenging benchmarks: HMDB51, UCF101, Kinetics600, ESC-50 and AudioSet. We encourage future work to use Kinetics600 and AudioSet that are larger scale downstream tasks and hence can better capture the progress of self-supervised methods. Our network can also be used for zero-shot text-to-video retrieval. Our deflation process shows how to train on videos to obtain representation for still images. Given the sheer number of videos available for self-supervised training on the web, we believe this is a more natural route to transfer which we hope will be pursued in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Broader impact</head><p>Potential benefits. Our method can enable a better user experience when searching for visual or audio content on the web since we can index that type of media based on our learned multimodal embeddings. More broadly, learning video representations without labels in such a self-supervised manner greatly increases the scale at which we can train models, to the extent of leveraging any available collection of web video data. This enables capturing a more representative view of the overall distribution of web content as opposed to smaller scale curated datasets such as Kinetics. We believe this can be an important factor in designing methods that better understand whether or not a given content is safe (e.g. to filter out violent or undesired web content) thanks to the better coverage of the overall distribution.</p><p>Potential risks. Every method that learns from data, self-supervised methods even more deeply so, brings the risk of learning biases and perpetuating them in the form of decisions. We encourage the deployment of our method to be done with careful consideration of the consequences from any potential underlying biases in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix overview</head><p>Appendix A contains additional details about optimization during training (A.1) and about the evaluation setup (A.2). Appendix B precisely describes the architecture of the different backbones and projection heads, as well as all the losses for the different embedding graphs. Appendix C provides the quantitative evaluation of the design exploration for pairs of modalities that were summarized in the main paper. Optimization. We train our networks for 500K steps using the Adam optimizer with parameters β 1 = 0.9, β 2 = 0.999 and = 10 −8 . The initial learning rate is 0.002 and a half period cosine schedule is used with 5K steps of linear warm up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Optimization and evaluation details</head><p>Batch norm. We applied batch norm where we aggregate the mean and variance statistics over all workers. We note that we observed a degradation in performance when not sharing the mean and variance across all workers. Both the bias and scale term are learned. We use a decay rate of 0.9 for the moving averages and = 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Downstream tasks details</head><p>Linear classifier on UCF101/HMDB51. We use Scikit-Learn <ref type="bibr" target="#b64">[65]</ref> SVM to optimize a linear classifier on the frozen features generated by our model. We use 16 or 32 frames per video clip (16 for the design explorations and 32 for large-scale experiments), sampled at 10 FPS (to match the FPS used during training). For training, we collect features corresponding to 10 times the size of the training set by applying the same data augmentation described in Section A.1. We resize the frames such that the minimum side is 224 and take a random crop (of size 200 × 200 for HMDB51 and 224 × 224 for UCF101). Before fitting the SVM, features are scaled so that they are zero mean and unit variance using the training statistics. Then the best value for the regularization parameter C is found by validation on the first split. At test time, we take 10 linearly spaced clips and average their predictions to get the final score. We take the central crops of the frames (of size 200 × 200 for HMDB51 and 224 × 224 for UCF101). We do not apply color augmentation, scale jittering or horizontal flipping during test time.</p><p>FT on UCF101/HMDB51. For fine-tuning, we use the SGD optimizer with momentum = 0.9. A learning rate schedule is used where the learning rate gets multiplied by γ at the given steps (values for each dataset are provided in <ref type="table" target="#tab_6">Table 5</ref>). We also apply weight decay to the variables of the linear classifier. Because in FT, the network can readapt to slight changes in the input, we resize the minimum side to 256 and take random crops of size 256 × 256. At test time, we take 10 linearly spaced clips and average their predictions to get the final score. We take the central crops of the frames of size 256 × 256. We do not apply color augmentation, scale jittering or horizontal flipping during test time.</p><p>Linear classifier on Kinetics600. We describe here the setting used to obtain the numbers in <ref type="table" target="#tab_2">Table 2</ref> for Kinetics600. Since Kinetics600 is too large to fit in memory, we cannot use Scikit-Learn direclty. Instead we train the linear layer for 50 epochs using the Adam optimizer <ref type="bibr" target="#b37">[38]</ref> with parameters β 1 = 0.9, β 2 = 0.999 and = 10 −8 . We use an initial learning rate of 2 * 10 −3 with a linear warmup of 5 epochs followed by a square root decay (i.e. learning rate decays as 1 √ k where k is the number of steps). During training, we sample clips of 32 frames using the same data augmentation described in Section A.1. We resize the frames such that the minimum side is 224 and take a random crop of size 224 × 224. Since the dataset is large, we do not use any regularizer for the linear layer. At test time, we take 10 linearly spaced clips and average their predictions to get the final score. We take the central crops of the frames of size 224 × 224. We do not apply color augmentation, scale jittering or horizontal flipping during test time. We report the top 1 accuracy on the validation set.</p><p>Linear classifier on ESC-50. We use Scikit-Learn <ref type="bibr" target="#b64">[65]</ref> SVM to optimize a linear classifier on the frozen features generated by our model. The features produced by the last convolution of the audio backbone network (before pooling) are used for this experiment. The network ingests 2 seconds of audio as done in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref>. For training, we collect features corresponding to 10 times the size of the training set by applying the same audio data augmentation described in Section A.1. Before fitting the SVM, features are scaled so that they are zero mean and unit variance using the training statistics. Then the best value for the regularization parameter C is found by validation on the first split. At test time, we take 10 linearly spaced audio sample and average their predictions to get the final score.</p><p>Linear classifier on AudioSet. We describe here the setting used to obtain the numbers in <ref type="table" target="#tab_2">Table 2</ref> for AudioSet. The features produced by the last convolution of the audio backbone network (before pooling) are used for this experiment. The network ingests 2 seconds of audio as done in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref>. For AudioSet we train a 2 layer-MLP with hidden size 512 to predict scores for the 527 classes as done in <ref type="bibr" target="#b33">[34]</ref>. Since AudioSet is a multi-class dataset we use per-class binary cross-entropy loss to train the MLP classifier. We train for 200 epochs using the Adam optimizer <ref type="bibr" target="#b37">[38]</ref> with parameters β 1 = 0.9, β 2 = 0.999 and = 10 −8 . We use an initial learning rate of 2 * 10 −4 with a linear warmup of 2 epochs followed by a square root decay (i.e. learning rate decays as 1 √ k where k is the number of steps). During training, we sample audio samples of 2 seconds using the same audio data augmentation described in Section A.1. Since the dataset is large, we do not use any regularizer for the linear layer. At test time, we take 10 linearly spaced audio samples and average their predictions to get the final score. We report the mAP metric on the validation set.</p><p>Zero-shot text-to-video retrieval on YouCook2/MSRVTT. For zero-shot text-to-video retrieval, we simply use our networks to map text queries and videos to the same subspace. In that space, we can find the best video matches for a given query by maximizing the cosine similarity. Again to minimize the discrepancy between pretraining and evaluation, we resize the frames to a minimum height/width of 224 and take a central crop of 200 × 200. Embedding features for the video are obtained by first computing features of 10 linearly spaced clips and then averaging them. In <ref type="table" target="#tab_5">Table 4</ref> we provide additional metrics for retrieval on YouCook2 and MSRVTT for the S3D-G, TSM and TSMx2 of Section 4.3. We provide R@K for K= 1, 5, 10 (higher is better) and median rank (MedR), corresponding to the median rank of the correctly retrieved video (lower is better).</p><p>Linear on PASCAL/ImageNet. We evaluate our deflated networks using a linear classifier on PASCAL VOC 2007 and ImageNet benchmarks. To build the deflated S3D-G network, we collapse the 3D temporal filters w t x,y into 2D filters w x,y by summing along the temporal dimension: w x,y = t w t x,y . For TSM, we run the image through the backbone network without any channel shift. We use both train and validation sets as training data. We resize the images to have a minimum side of 224 and then use random crops of 200 × 200. For ImageNet, we augmented the training set with scale jittering and color augmentation as described in Section A.1. For the PASCAL linear experiments, we train the linear layer for 30 epochs using the Adam optimizer <ref type="bibr" target="#b37">[38]</ref> with parameters β 1 = 0.9, β 2 = 0.999 and = 10 −8 . We use per-class binary cross-entropy loss to train the linear classifier. A square root decay (i.e. learning rate decays as 1 √ k where k is the number of steps) is used for the learning rate. The best initial learning rate is selected independently for each of the models using the 'validation' set. We report mAP in the 'test' set using 11-point mAP metric as described in <ref type="bibr" target="#b19">[20]</ref>. For the ImageNet experiments, we train a linear layer for 200 epochs using the Adam optimizer <ref type="bibr" target="#b37">[38]</ref> Text processing lower, tokenization, rm stop words, PAD to 16 with parameters β 1 = 0.9, β 2 = 0.999 and = 10 −8 . We use standard cross-entropy loss to train the classifier. A square root decay is used for the learning rate. The best initial learning rate is selected using an internal validation set (subset of the official training set). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model architecture and losses details</head><p>Backbones. Starting from raw data, our audio, visual and text backbones extract modality specific embeddings as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Linear and non linear projection heads. The precise architectures for the projection heads are given in <ref type="figure" target="#fig_2">Figure 3d</ref>. The non linear head design follows the non linear head from the SimCLR work <ref type="bibr" target="#b14">[15]</ref>.</p><p>Shared head architecture and losses. We provide an illustration of the detailed architecture for the shared embedding graph in <ref type="figure" target="#fig_2">Figure 3a</ref>. In that case, the NCE loss between video and audio is the following:</p><formula xml:id="formula_3">NCE(x v , x a ) = − log exp(z v,vat z a,vat /τ ) exp(z v,vat z a,vat /τ ) + z ∼N (x) exp(z v,vat z a,vat /τ ) .<label>(3)</label></formula><p>The MIL-NCE loss between video and text is defined as follows:  Disjoint architecture and losses. We provide an illustration of the detailed architecture for the disjoint embedding graph in <ref type="figure" target="#fig_2">Figure 3b</ref>. In that case, the NCE loss between video and audio is the following:</p><formula xml:id="formula_4">MIL-NCE(x v , x t ) = − log z∈P(x) exp(z v,vat z t,vat /τ ) z∈P(x) exp(z v,vat z t,vat /τ ) + z ∼N (x) exp(z v,vat z t,vat /τ ) .<label>(4</label></formula><formula xml:id="formula_5">NCE(x v , x a ) = − log exp(z v,va z a,va /τ ) exp(z v,va z a,va /τ ) + z ∼N (x) exp(z v,va z a,va /τ ) .<label>(5)</label></formula><p>The MIL-NCE loss between video and text is defined as follows:</p><formula xml:id="formula_6">MIL-NCE(x v , x t ) = − log z∈P(x) exp(z v,vt z t,vt /τ ) z∈P(x) exp(z v,vt z t,vt /τ ) + z ∼N (x) exp(z v,vt z t,vt /τ ) .<label>(6)</label></formula><p>Fine and Coarse (FAC) architecture and losses. We provide an illustration of the detailed architecture for the FAC embedding graph in <ref type="figure" target="#fig_2">Figure 3c</ref>. In that case, the NCE loss between video and audio is the following:</p><formula xml:id="formula_7">NCE(x v , x a ) = − log exp(z v,va z a,va /τ ) exp(z v,va z a,va /τ ) + z ∼N (x) exp(z v,va z a,va /τ ) .<label>(7)</label></formula><p>The MIL-NCE loss between video and text is defined as follows:</p><formula xml:id="formula_8">MIL-NCE(x v , x t ) = − log z∈P(x) exp(z v,vat z t,vat /τ ) z∈P(x) exp(z v,vat z t,vat /τ ) + z ∼N (x) exp(z v,vat z t,vat /τ ) .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional design choices exploration for pairs of modalities</head><p>In this section, we explore the effects of various design choices of our method. The full results accompany Section 4.2, paragraph on "pairs of modalities".   tuning, for UCF101, HMDB51 and ESC-50 we use only the frozen setting and report top-1 accuracy on the split#1. We also report R@10 for YC2 (YR10) and MSRVTT (MR10) under the zero-shot setting.</p><p>Here we provide full results from experiments that consider learning from two modalities -Vision and Text, or Vision and Audio -as this setup makes it easy to isolate the effects of different components and discover the best building blocks to be used in the three-modality setting.</p><p>Visual backbone. TSM ResNet50 variants always beat S3D-G for downstream tasks that involve vision, with TSM ResNet50x2 being on par or better than TSM ResNet50 <ref type="table" target="#tab_9">(Table 6</ref>).</p><p>Losses. Previous works use the logistic loss when learning from Vision and Audio <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">43]</ref>. The NCE loss consistently outperforms it by 2% on vision downstream tasks, and is on par on audio ( <ref type="table" target="#tab_10">Table 7)</ref>. This is in line with findings of recent single-modality self-supervised approaches that demonstrate the superiority of NCE based loss compared to its binary classification counterpart. Note that due to the multiple candidate positive for the Vision+Text setting, it is not sensible to compare a logistic loss against MIL-NCE. We refer to <ref type="bibr" target="#b50">[51]</ref> for a relevant comparison that draws the same conclusion.</p><p>Projection heads. <ref type="table" target="#tab_11">Table 8</ref> confirms the findings of <ref type="bibr" target="#b14">[15]</ref> that adding a non-linear projection head (see <ref type="figure" target="#fig_2">Figure 3d</ref> for the architecture details of the linear and non linear heads) on top of the visual representations improves the performance on visual downstream tasks (UCF101 and HMDB51 for the frozen setting). However, it was not beneficial to have non-linear projection heads for the language and audio branches.</p><p>Data augmentation. Despite training on large datasets, performing standard video augmentations usually improves downstream performance <ref type="table" target="#tab_12">(Table 9</ref>). Mildly jittering audio with SpecAugment <ref type="bibr" target="#b62">[63]</ref> is not beneficial, and is detrimental with more aggressive augmentations; this is in contrast with the findings of <ref type="bibr" target="#b63">[64]</ref> where SpecAugment helped, presumably due to training on a relatively small dataset. Temporal jittering by randomly offsetting the audio with respect to the visual stream by up to 0.8s (half of the training clip length) reduces the performance on visual tasks by 4%, showing that synchronization is an important training signal. Small additive Gaussian noise applied onto the raw audio signal (0.01 × max amplitude) seems to make a slight difference, but we decide to use it as it is inaudible while it potentially helps with preventing the network from latching onto encoding artefacts.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a)-(c) Modality Embedding Graphs, (d) Projection heads and losses for the FAC graph. V=Vision, A=Audio, T=Text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Backbone architecture for audio, vision and text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a-c) Architecture details for the embedding graphs (linear projection heads are framed by a solid border while the non linear ones are framed by a dashed border). (d) Details of the linear and non linear heads used in this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Design explorations for multiple modalities (HT=HowTo100M, AS=AudioSet). The video networks use non-linear projection heads. Benefits of multiple modalities on HT</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) VAT: modality merging strategies on HT+AS</cell></row><row><cell cols="5">Modalities UCF HMDB YC2 MSRVTT ESC-50</cell><cell cols="4">Strategy UCF HMDB YC2 MSRVTT ESC-50</cell></row><row><cell>VT</cell><cell cols="2">82.7 55.9 33.6</cell><cell>27.5</cell><cell>/</cell><cell cols="2">Shared 84.7 60.2 20.8</cell><cell>22.4</cell><cell>88.5</cell></row><row><cell>VA</cell><cell>75.5 51.6</cell><cell>/</cell><cell>/</cell><cell>79.0</cell><cell cols="2">Disjoint 85.1 59.3 25.0</cell><cell>22.5</cell><cell>87.0</cell></row><row><cell cols="3">VAT (FAC) 84.7 57.3 32.2</cell><cell>28.6</cell><cell>78.7</cell><cell>FAC</cell><cell>86.2 62.5 23.8</cell><cell>23.5</cell><cell>88.0</cell></row></table><note>(a)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The best MMV networks trained in Section 4.3 are deflated and evaluated on image tasks. The deflation (Section 3.2) is trained on 45981 frames of the HowTo100M<ref type="bibr" target="#b52">[53]</ref> training set, where the static videos (ingested by the original video network to produce the regression targets for the deflated image network) are 32-frame long to match the video length used during self-supervised training; the Adam optimizer<ref type="bibr" target="#b37">[38]</ref> is used with initial learning rate of10 −2 decayed by a factor 0.1 every 30 epochs for a total of 100 epochs. Results are reported for linear classification on top of the frozen image features f v (x v ) on the PASCAL VOC 2007 and ImageNet benchmarks. Implementation details are provided in Appendix A.2.</figDesc><table /><note>2, 41.5 and 45.4 on YouCook2 and 29.3, 31.1 and 31.1 on MSRVTT. As explained in Section 4.2, longer training significantly improves the performance on these two benchmarks when compared to the results reported in Table 1b. We are also not far from the state- of-the-art performance reported in [51] for MSRVTT (32.2) and still below for YouCook2 (51.2). However, Miech et al. [51] train 4 times longer on vision-text pairs (same number of total training steps, but 2× larger batches and half of our samples come from AudioSet which has no text). We believe this gap could be further reduced by longer training but leave that for further investigation.4.4 Transfer to image tasks via network deflation Experimental setup.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of learnt representations versus the state-of-the-art. Results are averaged over all splits. The "Mod." column shows which combinations of modalities are used by the methods, possibilities: Vision, Audio, Text, Flow. Dataset abbreviations: AudioSet, HowTo100M, Instagram65M<ref type="bibr" target="#b24">[25]</ref>, SoundNet<ref type="bibr" target="#b8">[9]</ref>, 2M videos from YouTube8M<ref type="bibr" target="#b3">[4]</ref>, Kinetics600; their length in years is given in the "years" column. †<ref type="bibr" target="#b73">[74]</ref> uses a non-linear classifier. We report top-1 accuracy for UCF101, HMDB51, ESC-50, Kinetics600 and mAP for AudioSet.AS+HT  16 VAT 91.8 95.2 67.1 75.0 88.9 30.9 70.5 Supervised [21, 42, 67, 74, 90] 96.8 71.5 75.9 86.5 † 43.9 81.8</figDesc><table><row><cell>UCF101</cell><cell>HMDB51 ESC-50 AS K600</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Image classification results on PASCAL and ImageNet. "V)I" denotes the image handling strategy for the video networks: naive deflation (no training of γ and β), deflation (proposed), and input-inflation (video net ingesting 32-frame static videos).</figDesc><table><row><cell>Method</cell><cell cols="4">V )I Train data PASCAL (mAP) ImageNet (top1) ImageNet (top5)</cell></row><row><cell>Supervised S3D-G</cell><cell>def Kinetics</cell><cell>67.9</cell><cell>42.8</cell><cell>68.0</cell></row><row><cell>MMV S3D-G</cell><cell>n-def AS+HT</cell><cell>41.8</cell><cell>20.7</cell><cell>40.5</cell></row><row><cell>MMV S3D-G</cell><cell>def AS+HT</cell><cell>71.4</cell><cell>45.2</cell><cell>71.3</cell></row><row><cell>MMV S3D-G</cell><cell>i-inf AS+HT</cell><cell>72.1</cell><cell>46.7</cell><cell>72.5</cell></row><row><cell>Supervised TSM</cell><cell>def Kinetics</cell><cell>66.9</cell><cell>43.4</cell><cell>68.3</cell></row><row><cell>MMV TSM</cell><cell>n-def AS+HT</cell><cell>34.4</cell><cell>10.9</cell><cell>24.6</cell></row><row><cell>MMV TSM</cell><cell>def AS+HT</cell><cell>74.8</cell><cell>50.4</cell><cell>76.0</cell></row><row><cell>MMV TSM</cell><cell>i-inf AS+HT</cell><cell>75.7</cell><cell>51.5</cell><cell>77.3</cell></row><row><cell>Supervised TSMx2</cell><cell>def Kinetics</cell><cell>66.9</cell><cell>47.8</cell><cell>72.7</cell></row><row><cell>MMV TSMx2</cell><cell>n-def AS+HT</cell><cell>45.6</cell><cell>20.3</cell><cell>39.9</cell></row><row><cell>MMV TSMx2</cell><cell>def AS+HT</cell><cell>77.4</cell><cell>56.6</cell><cell>81.4</cell></row><row><cell>MMV TSMx2</cell><cell>i-inf AS+HT</cell><cell>77.4</cell><cell>57.4</cell><cell>81.7</cell></row><row><cell>SimCLR [15] ResNet50</cell><cell>/ ImageNet</cell><cell>80.5</cell><cell>69.3</cell><cell>89.0</cell></row><row><cell cols="2">SimCLR [15] ResNet50x2 / ImageNet</cell><cell>/</cell><cell>74.2</cell><cell>92.0</cell></row><row><cell cols="2">SimCLR [15] ResNet50x4 / ImageNet</cell><cell>84.2</cell><cell>76.5</cell><cell>93.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Additional retrieval metrics for zero shot text to video retrieval.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">YouCook2</cell><cell></cell><cell></cell><cell cols="2">MSRVTT</cell></row><row><cell>Method</cell><cell>fv</cell><cell cols="8">R@1↑ R@5↑ R@10↑ MedR ↓ R@1↑ R@5↑ R@10↑ MedR ↓</cell></row><row><cell>MILNCE [51]</cell><cell>S3D-G</cell><cell cols="2">15.1 38.0</cell><cell>51.2</cell><cell>10</cell><cell>9.9</cell><cell>24.0</cell><cell>32.4</cell><cell>30</cell></row><row><cell cols="2">MMV FAC (ours) S3D-G</cell><cell>9.0</cell><cell>25.7</cell><cell>37.2</cell><cell>20</cell><cell>8.2</cell><cell>21.0</cell><cell>29.3</cell><cell>44</cell></row><row><cell cols="2">MMV FAC (ours) TSM-50</cell><cell cols="2">11.5 30.2</cell><cell>41.5</cell><cell>16</cell><cell>9.2</cell><cell>22.4</cell><cell>31.1</cell><cell>37</cell></row><row><cell cols="4">MMV FAC (ours) TSM-50x2 11.7 33.4</cell><cell>45.4</cell><cell>13</cell><cell>9.3</cell><cell>23.0</cell><cell>31.1</cell><cell>38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Parameters for FT on downstream classification tasks.</figDesc><table><row><cell>Parameter</cell><cell>HMDB51</cell><cell>UCF101</cell></row><row><cell>LR base</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>LR decay γ</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>LR schedule</cell><cell cols="2">2.5K/5K/7.5K 6.25K/12.5K/18.75K</cell></row><row><cell>Weight decay</cell><cell>5  *  10 −3</cell><cell>10 −7</cell></row><row><cell>Batch size</cell><cell>256</cell><cell>256</cell></row><row><cell>Training steps</cell><cell>10K</cell><cell>25K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Effects of varying the visual backbone. All experiments use linear projection heads.Training is performed on HowTo100M with 16 frames per video clip. Evaluation is done in the frozen setting, also with 16 frames per video clip.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">train: Vision+Text</cell><cell></cell><cell cols="2">train: Vision+Audio</cell><cell></cell></row><row><cell cols="8">Visual backbone UCF101 HMDB51 YC2 MSRVTT UCF101 HMDB51 ESC-50</cell></row><row><cell>S3D-G</cell><cell>81.0</cell><cell>52.0</cell><cell>35.4</cell><cell>29.0</cell><cell>71.1</cell><cell>49.1</cell><cell>80.0</cell></row><row><cell>TSM Res50</cell><cell>82.9</cell><cell>56.0</cell><cell>37.7</cell><cell>33.3</cell><cell>75.8</cell><cell>52.5</cell><cell>78.0</cell></row><row><cell>TSM Res50x2</cell><cell>86.8</cell><cell>55.1</cell><cell>43.4</cell><cell>32.9</cell><cell>77.1</cell><cell>53.6</cell><cell>79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>NCE vs logistic loss for Vision+Audio. All experiments use linear projection heads and the S3D-G network as the video backbone. Training is performed on HowTo100M with 16 frames per video clip. Evaluation is done in the frozen setting, also with 16 frames per video clip.</figDesc><table><row><cell></cell><cell cols="3">UCF101 HMDB51 ESC-50</cell></row><row><cell>NCE loss</cell><cell>71.1</cell><cell>49.1</cell><cell>80.0</cell></row><row><cell>Logistic loss</cell><cell>69.9</cell><cell>47.5</cell><cell>80.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Effects of varying the projection heads. All experiments use the S3D-G network as the video backbone. Training is performed on HowTo100M with 16 frames per video clip. Evaluation is done in the frozen setting, also with 16 frames per video clip. Best number is in bold.</figDesc><table><row><cell>Second best is</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Effects of data augmentation for Vision+Audio. All experiments use linear projection heads and the S3D-G network as the video backbone. Training is performed on HowTo100M with 16 frames per video clip. Evaluation is done in the frozen setting, also with 16 frames per video clip.</figDesc><table><row><cell cols="2">Video augmentation Audio augmentation</cell><cell cols="3">UCF101 HMDB51 ESC-50</cell></row><row><cell>None</cell><cell>None</cell><cell>70.6</cell><cell>47.9</cell><cell>77.0</cell></row><row><cell>Standard</cell><cell>None</cell><cell>71.1</cell><cell>49.1</cell><cell>80.0</cell></row><row><cell>Standard</cell><cell>Temporal</cell><cell>67.6</cell><cell>45.8</cell><cell>79.0</cell></row><row><cell>Standard</cell><cell>SpecAugment [63] weak [64]</cell><cell>71.3</cell><cell>49.2</cell><cell>79.0</cell></row><row><cell>Standard</cell><cell>SpecAugment [63] strong [64]</cell><cell>70.8</cell><cell>48.4</cell><cell>76.2</cell></row><row><cell>Standard</cell><cell>Gaussian noise</cell><cell>72.8</cell><cell>48.4</cell><cell>78.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank Antoine Miech, Yusuf Aytar and Karen Simonyan for fruitful discussions as well as Luyu Wang and Elena Buchatskaya for help on the evaluation benchmarks. We also want to thank our NeurIPS reviewers and metareviewer for great feedback on the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fac S3d-G Model</surname></persName>
		</author>
		<ptr target="https://tfhub.dev/deepmind/mmv/s3d/1.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fac Tsm Resnet50</surname></persName>
		</author>
		<ptr target="https://tfhub.dev/deepmind/mmv/tsm-resnet50/1.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fac Tsm Resnet50x2</surname></persName>
		</author>
		<ptr target="https://tfhub.dev/deepmind/mmv/tsm-resnet50x2/1.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">YouTube-8M: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12667</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SoundNet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>See</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00932</idno>
		<title level="m">hear, and read: Deep aligned representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A Short Note on the Kinetics-700 Human Action Dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Webly supervised joint embedding for cross-modal image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rameswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Papalexakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeViSE: A Deep Visual-Semantic Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distinit: Learning video representations without a single labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving image-sentence embeddings using large weakly annotated photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Jointly discovering visual objects and spoken words from raw sensory input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Surís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coincidence, categorization, and consolidation: Learning to recognize sounds with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-supervised spatiotemporal feature learning by video geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11387</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">One model to learn them all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05137</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using Fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10211</idno>
		<title level="m">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">TSM: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unpaired image-to-speech synthesis with multimodal information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">What&apos;s cookin&apos;? Interpreting cooking videos using text, speech and vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">End-to-End Learning of Visual Representations from Uncurated Instructional Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Howto100M: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<editor>ICMR. ACM</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12943</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ambient sound provides supervision for visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>InterSpeech</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multi-modal self-supervision from generalized data transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Enhancing video summarization via vision-language embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03800</idno>
		<title level="m">Spatiotemporal contrastive video representation learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">Movie description. IJCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Learning Audio-Visual Language Representations from Instructional Videos</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unsupervised filterbank learning using convolutional restricted boltzmann machine for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InterSpeech</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing of video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">The development of embodied cognition: Six lessons from babies. Artificial life</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">VideoBERT: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">WSABIE: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple parts-ofspeech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speedaccuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">MSR-VTT: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Instructional videos for unsupervised harvesting and learning of action examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
