<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEMO-Net: Degree-specific Graph Neural Networks for Node and Graph Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 4-8, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
							<email>junwu6@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
							<email>jingrui.he@asu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiejun</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">HRL Laboratories, LLC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEMO-Net: Degree-specific Graph Neural Networks for Node and Graph Classification</title>
					</analytic>
					<monogr>
						<title level="m">KDD &apos;19</title>
						<meeting> <address><addrLine>Anchorage, AK, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 4-8, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330950</idno>
					<note>ACM Reference Format: Jun Wu, Jingrui He, and Jiejun Xu. 2019. DEMO-Net: Degree-specific Graph Neural Networks for Node and Graph Classification. In The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD&apos;19), June 22-24, 2019, Anchorage, AK, USA. ACM, New York, NY, USA, 10 pages. ACM ISBN 978-1-4503-6201-6/19/08. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Network</term>
					<term>Degree-specific Convolution</term>
					<term>Multi-task Learning</term>
					<term>Graph Isomorphism Test</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph data widely exist in many high-impact applications. Inspired by the success of deep learning in grid-structured data, graph neural network models have been proposed to learn powerful node-level or graph-level representation. However, most of the existing graph neural networks suffer from the following limitations: (1) there is limited analysis regarding the graph convolution properties, such as seed-oriented, degree-aware and order-free; (2) the node's degreespecific graph structure is not explicitly expressed in graph convolution for distinguishing structure-aware node neighborhoods;</p><p>(3) the theoretical explanation regarding the graph-level pooling schemes is unclear.</p><p>To address these problems, we propose a generic degree-specific graph neural network named DEMO-Net motivated by Weisfeiler-Lehman graph isomorphism test that recursively identifies 1-hop neighborhood structures. In order to explicitly capture the graph topology integrated with node attributes, we argue that graph convolution should have three properties: seed-oriented, degree-aware, order-free. To this end, we propose multi-task graph convolution where each task represents node representation learning for nodes with a specific degree value, thus leading to preserving the degreespecific graph structure. In particular, we design two multi-task learning methods: degree-specific weight and hashing functions for graph convolution. In addition, we propose a novel graph-level pooling/readout scheme for learning graph representation provably lying in a degree-specific Hilbert kernel space. The experimental results on several node and graph classification benchmark data sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net over state-of-the-art graph neural network models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Nowadays, graph data is being generated across multiple highimpact application domains, ranging from bioinformatics <ref type="bibr" target="#b3">[4]</ref> to financial fraud detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, from genome-wide association study <ref type="bibr" target="#b20">[21]</ref> to social network analysis <ref type="bibr" target="#b4">[5]</ref>. In order to leverage the rich information in graph-structured data, it is of great importance to learn effective node or graph representation from both node/edge attributes and the graph topological structure. To this end, numerous graph neural network models have been proposed recently inspired by the success of deep learning architectures on grid-structured data (e.g., images, videos, languages, etc.). One intuition behind this line of approaches is that the topological structure as well as node attributes could be integrated by recursively aggregating and compressing the continuous feature vectors from local neighborhoods in an end-to-end training architecture.</p><p>One key component of graph neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> is the graph convolution (or feature aggregation function) that aggregates and transforms the feature vectors from a node's local neighborhood. By integrating the node attributes with the graph structure information using Laplacian smoothing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> or advanced attention mechanism <ref type="bibr" target="#b17">[18]</ref>, graph neural networks learn the node representation in a low-dimensional feature space where nearby nodes in the graph would share a similar representation. Moreover, in order to learn the representation for the entire graph, researchers have proposed the graph-level pooling schemes <ref type="bibr" target="#b0">[1]</ref> that compress the nodes' representation into a global feature vector. The node or graph representation learned by graph neural networks has achieved state-of-the-art performance in many downstream graph mining tasks, such as node classification <ref type="bibr" target="#b25">[26]</ref>, graph classification <ref type="bibr" target="#b21">[22]</ref>, etc.</p><p>However, most of the existing graph neural networks suffer from the following limitations. (L1) There is limited analysis on graph convolution properties that could guide the design of graph neural networks when learning node representation. (L2) In order to preserve the node proximity, the graph convolution applies a special form of Laplacian smoothing <ref type="bibr" target="#b11">[12]</ref>, which simply mixes the attributes from node's neighborhood. This leads to the loss of degree-specific graph structure information for the learned representation. An illustrative example is shown in <ref type="figure">Figure 1</ref>: although nodes 4 and 5 are structurally different, they would be mapped to similar representation due to first-order node proximity using existing methods. Moreover, the neighborhood sub-sampling methods used to improve model efficiency <ref type="bibr" target="#b4">[5]</ref> significantly degraded the discrimination of degree-specific graph structure. (L3) The theoretical explanation regarding the graph-level pooling schemes is largely missing.</p><p>To address the above problems, in this paper, we propose a generic graph neural network model DEMO-Net that considers the degree-specific graph structure in learning both node and graph representation. Inspired by Weisfeiler-Lehman graph isomorphism test <ref type="bibr" target="#b19">[20]</ref>, the graph convolution of graph neural networks should have three properties: seed-oriented, degree-aware, order-free, in order to map different neighborhoods to different feature representation. As shown in <ref type="figure">Figure 1</ref>, nodes with identical degree value typically share similar subtree (root node followed by its 1-hop neighbors) structures. As a result, the representation of nodes 2 and 8 should be close in the feature space due to the similar subtree structure. On the other hand, nodes 4 and 5 have different subtree structures (i.e., number of subtree leaves), and they indicate different roles in the network, e.g., leader vs. deputy in a covert group. Therefore, they should not be mapped closely in the feature space.</p><p>To the best of our knowledge, very little effort on graph neural networks is devoted to learning the degree-specific representation for each node or the entire graph. To bridge the gap, we present a degree-specific graph convolution by assuming that nodes with the same degree value would share the same graph convolution. It can be formulated as a multi-task feature learning problem where each task represents the node representation learning for nodes with specific degree values.</p><p>In addition, we introduce a degree-specific graph-level pooling scheme to learn the graph representation. We theoretically show that the graph representation learned by our model lies in a Reproducing Kernel Hilbert space (RKHS) induced by a degreespecific Weisfeiler-Lehman graph kernel. The most similar work to us is Graph Isomorphism Network (GIN) <ref type="bibr" target="#b21">[22]</ref> which used the sum-aggregator associated with multi-layer perceptrons as the neighborhood-injective graph convolution that mapped different node neighborhood to different features. However, one issue of GIN is that the degree-aware structures are implicitly expressed in its graph convolution relying on the universal approximation capacity of multi-layer perceptrons.</p><p>The main contributions of this paper are summarized as follows:</p><p>(1) We provide theoretical analysis for graph neural networks from the perspective of Weisfeiler-Lehman graph isomorphism test, which motivates us to design the graph convolution based on the following properties: seed-oriented, degree-aware and order-free. (2) we propose a generic graph neural network framework named DEMO-Net by assuming that nodes with the same degree value would share the same graph convolution. A degree-specific multi-task graph convolution function is presented to learn the node representation. Furthermore, a novel graph-level pooling scheme is introduced for learning the graph representation provably lying in a degree-specific Hilbert kernel space. The rest of the paper is organized as follows. We review the related work in Section 2, followed by the problem definition and background introduction in Section 3. Section 4 presents our proposed DEMO-Net framework for node and graph representation learning. The extensive experiments and discussion are provided in Section 5. Finally, we conclude the paper in Section 6.  <ref type="figure">Figure 1</ref>: Nodes with the same degree value are structurally similar. For example, nodes 1, 3, 5, 7 and 9 in (b), nodes 2 and 8 in (c), nodes 4 and 6 in (d) share similar 1-hop neighborhood structure. Using the proposed model, the learned node representation integrates the degree-specific graph structure and node attributes such that the structurally similar nodes have similar representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review the related work on graph neural networks for node and graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Node Classification</head><p>Most of the existing graph neural networks <ref type="bibr" target="#b5">[6]</ref> learn the node representation by recursively aggregating the continuous feature vectors from local neighborhoods in an end-to-end fashion. They could be fitted into the Message Passing Neural Networks (MPNNs) <ref type="bibr" target="#b3">[4]</ref> which explained the feature aggregation of graph neural networks as message passing in local neighborhoods. Generally, they focus on extracting the spatial topological information by operating the convolutions in the node domain <ref type="bibr" target="#b25">[26]</ref>, which differs from some spectral approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> considering a node representation in the spectral domain. Graph Convolutional Network (GCN) <ref type="bibr" target="#b8">[9]</ref> defined the convolution operation via a neighborhood aggregation function. Following the same intuition, many graph neural network models have been proposed with different aggregation functions, e.g., attention mechanism <ref type="bibr" target="#b17">[18]</ref>, mean and max functions <ref type="bibr" target="#b4">[5]</ref>, etc. However, most of the graph neural network architectures are motivated by the success of deep learning on grid-like data, thus leading to little theoretical analysis for explaining the high performance and guiding the novel methodologies. Up till now, some work have been proposed to explain why graph neural networks work. The convolution of GCN was a special form of Laplacian smoothing on graph <ref type="bibr" target="#b11">[12]</ref>, which explained the over-smoothing phenomena brought by many convolution layers. Lei et al. <ref type="bibr" target="#b9">[10]</ref> showed that the graph representation generated by graph neural networks lies in the Reproducing Kernel Hilbert Space (RKHS) of some popular graph kernels. Moreover, it shows that 1-dimensional aggregation-based graph neural networks are at most as powerful as the Weisfeiler-Lehman (WL) isomorphism test <ref type="bibr" target="#b19">[20]</ref> in distinguishing graphs <ref type="bibr" target="#b21">[22]</ref>. Compared with the existing work on graph neural networks, in this paper, we design a degree-specific graph convolution that captures the node neighborhood structures inspired by WL isomorphism test. This is in sharp contrast to the existing work which focused on preserving the node proximity in the feature space, thus leading to the loss of local graph structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Classification</head><p>The graph-level pooling/readout schemes aim to learn a representation of the entire graph from its node representations for graph-level classification tasks. Mean/max/sum functions are commonly used due to its computational efficiency and effectiveness <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>. One challenge for graph-level pooling is to maintain the invariance to node order. PATCHY-SAN <ref type="bibr" target="#b12">[13]</ref> first adopted the external software to obtain a global node order for the entire graph, which is very time-consuming. More recently, a number of graph neural network models have been proposed <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, which formulated the node representation learning and graph-level pooling into a unified framework. Different from graph kernel approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref> that intuitively extract the graph feature or define the graph similarity using ad-hoc knowledge or random walk properties, graph neural networks would automatically learn the graph representation to integrate node attributes with its topological information via an end-to-end training architecture.</p><p>Nevertheless, very little effort has been devoted to explicitly considering the degree-specific graph structures for graph representation learning. Our proposed degree-specific graph-level pooling method is designed to address this issue by compressing the learned node representation according to degree values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we introduce the notation and problem definition, as well as some background information on graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>Suppose that a graph is represented as G = (V , E), where V = {v 1 , · · · , v n } is the set of n nodes and E ⊆ V × V is the edge set. Let X ∈ R n×D denote the attribute matrix where each row x v is the D-dimensional attribute vector for node v. The graph G can also be represented by an adjacency matrix A ∈ R n×n , where A i j represents the similarity between v i and v j on the graph. For each node v ∈ V , its 1-hop neighborhood is denoted as N (v). Let G = {G 1 , · · · , G t } denote a set of graphs. In this paper, we focus on undirected attributed networks, although our model can be naturally generalized to other types of networks. The main notation used in this paper is summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition</head><p>In this paper, we focus on two problems: node-level and graphlevel representation learning by formulating a novel degree-specific graph neural network model. Furthermore, we analyze the proposed model from various aspects, and empirically demonstrate its superior performance on both node and graph classification.</p><p>Formally, the node-and graph-level representation learning problems can be defined below. </p><formula xml:id="formula_0">nodes {x v , y v } v ∈I G .</formula><p>Output: A vector representation e v ∈ R d for each node v ∈ V on the d-dimensional embedding space where nodes would be well separated if their local neighborhoods are structurally different. </p><formula xml:id="formula_1">Notation Definition G = {G i } t i =1 A set of graphs G = (V , E) A graph G with node set V and edge set E X Attribute matrix A Adjacency matrix n</formula><p>Number of nodes in the graph d Dimensionality of the node or graph representation</p><formula xml:id="formula_2">N (v) 1-hop neighborhood of node v I G</formula><p>Indices of labeled nodes' for node classification I G Indices of labeled graphs' for graph classification</p><formula xml:id="formula_3">y v Label of node v y i Label of graph G i h k v Node v's representation at the k th iteration h N (v ) Feature set within node v's neighborhood T A set of subtrees deдr ee(G) A set of the degree values in graph G Definition 3.2. (Graph-level Representation Learning) Input: (i) A set of attributed graphs G = {G i } t i=1 with adjacency matrix A i ∈ R n i ×n i and node attributes X i ∈ R n i ×D ; (ii) Labeled training graphs {G i ,ŷ i } i ∈I G .</formula><p>Output: A vector representation д i ∈ R d for each graph G i on the d-dimensional embedding space where graphs would be well separated if they have different graph topological structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Neural Networks</head><p>It has been observed that a broad class of graph neural network (GNN) architectures followed the 1-dimensional Weisfeiler-Lehman (WL) graph isomorphism test <ref type="bibr" target="#b19">[20]</ref>. From the perspective of WL isomorphism test, they mainly consist of the following crucial steps at each iteration of feature aggregation:</p><p>• Feature initialization (label 1 initialization): The node features are initialized by original attribute vectors. • Neighborhood detection (multiset-label determination): It decides the local neighborhood in which node gathers the information from neighbors. More specifically, a seed 2 followed by its neighbors generates a subtree pattern. Next, we briefly go over some existing graph neural network models, which follow the aforementioned steps of the 1-dimensional WL algorithms. We would like to point out that graph neural networks would learn the node or graph representation using continuous node attributes, whereas WL algorithms update the node attributes by directly compressing the augmented discrete attributes.</p><p>Taking 1-hop neighborhood N (v) = {u|(v, u) ∈ E} into consideration at each iteration, the following node-level graph neural network variants have the same feature initialization and neighborhood detection on learning node representation. And when element-wise average or max operations are used for feature aggregation, graph neural networks would be invariant to the order of neighbors. We summarize the feature aggregation functions (graph convolution) of those graph neural networks as follows.</p><p>• Graph Convolutional Network (GCN) <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_4">h k v = σ u ∈ {v }∪N (v) a vu W k h k −1 u<label>(1)</label></formula><p>where A = ( a vu ) ∈ R n×n is the re-normalization of the adjacency matrix A with added self-loops, and W k is the trainable matrix at k th layer. It is essentially a weighted feature aggregation from node neighborhood. • Graph Attention Network (GAT) <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_5">h k v = σ u ∈ {v }∪N (v) α vu W k h k −1 u (2)</formula><p>where α vu is a self-attention score indicating the importance of node u to node v on feature aggregation. It is obvious that GCN can be considered as a special case of GAT when the attention score α vu is defined as a vu . • GraphSAGE <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_6">h k N (v) = AGGREGAT E k {h k −1 u |u ∈ N (v)} h k v = σ W k · CONCAT (h k −1 v , h k N (v) )<label>(3)</label></formula><p>where mean-, max-and LSTM-aggregator are presented for feature aggregation. Though LSTM considers node neighbors as an ordered sequence, the LSTM aggregator is adapted on an unordered neighbors with random permutation. There are some observations from these GNN variants: (i) Their feature aggregation schemes are invariant to the order of the neighbors except for GraphSAGE with LSTM-aggregator; (ii) The output feature at k-layer neural network can be seen as the representation of a subtree around the seed; (iii) The node representation become closer and indistinguishable when the neural layers are going deeper, because the subtrees would share more common elements. However, little work theoretically discusses the reasons behind these observations to guide the design of graph neural networks: how is the node representation affected by node degree and order of neighbors? what kind of graph convolution is required to learn the subtree structures? Inspired by WL graph isomorphism test, we present a degree-specific graph neural network model named DEMO-Net in Section 4 to discuss those problems.</p><p>Additionally, the neighborhood aggregation schemes of graph neural networks, such as mean-aggregator in GraphSAGE <ref type="bibr" target="#b4">[5]</ref>, selfattention in GAT <ref type="bibr" target="#b17">[18]</ref>, can be regarded as the relabeling step in WL isomorphism test. <ref type="figure">Figure 2</ref> provides an example to illustrate the essence of feature aggregation on graph neural networks. The node feature is actually a special representation of subtree consisting of the seed followed by its neighbors. For example, node 1's feature  <ref type="figure">Figure 2</ref>: Feature aggregation of the graph neural networks:</p><formula xml:id="formula_7">For node 1, its feature is (a) h k 1 at k th layer; (b) h k +1 1 at (k + 1) th layer compressed from a subtree (h k 1 ; {h k 2 , h k 3 , h k 4 }); (c) h k +2 1 at (k +2) th layer learned from a subtree (h k +1 1 ; {h k +1 2 , h k +1 3 , h k +1 4 }). h k +1 1 represents the subtree (h k 1 ; {h k 2 , h k 3 , h k 4 })</formula><p>collected from previous layer. As a result, graph neural networks with k layers learn the representation of subtree with depth k rooted at the seed. That provides us an intuition to design a graph convolution for explicitly preserving the degree-specific subtree structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED MODEL: DEMO-NET</head><p>In this section, we propose a generic degree-specific graph neural network named DEMO-Net. Key to our algorithm is the degreespecific graph convolution for feature aggregation which can map different subtrees to different feature vectors. <ref type="figure" target="#fig_13">Figure 4</ref> provides an overview of the proposed DEMO-Net framework on learning node and graph representation, which will be described in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Node Representation Learning</head><p>Let h N (v) denote the feature set {h u |u ∈ N (v)} within node v's neighborhood. Let T = h v , h N (v) be the set of subtrees consisting of the features of seed v and its 1-hop neighbors N (v). To formalize our analysis, we first give the definition of structurally identical subtree below. The following lemma shows that graph neural networks could distinguish the local graph structures as well as the WL graph isomorphism test when graph convolution is an injective function that maps two subtrees in T to different features if they are not structurally identical. The feature aggregation of graph neural networks can be simply summarized as follows.</p><p>h Obviously, most of the existing graph neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> did not consider the injective aggregation function when learning node representation. From the perspective of WL isomorphism test, an injective graph convolution has the following properties. If it is an injective function that maps any different subtrees in T to different feature vectors, then it has the following properties:   <ref type="figure" target="#fig_0">Figure 3</ref>(a) to different features due to the distinctive seeds' attributes. Here, we hold that the subtree's structure properties are guided by seed node. Thus they are not structurally identical though both subtrees share the same leaf elements. Seeds' degree values also decide the subtree structure (shown in <ref type="figure" target="#fig_0">Figure 3</ref>(b)) because it is obvious that nodes with identical degree value share the similar structure. <ref type="figure" target="#fig_0">Figure 3</ref>(c) shows that neighbors' order will not change the subtree structure.</p><formula xml:id="formula_8">k v = f ({h k −1 v , h k −1 u |u ∈ N (v)})<label>(4)</label></formula><formula xml:id="formula_9">(i) Seed-oriented: f h i , {h u |u ∈ N (i)} f h j , {h w |w ∈ N (j)} if</formula><p>These properties will guide us to build a structure-specific graph neural network model. Based on properties (i) and (ii), the feature aggregation function in Eq. (4) can be expressed as follow.</p><formula xml:id="formula_10">h k v = f s (h k −1 v ) • f deд(v) {h k −1 u |u ∈ N (v)}<label>(5)</label></formula><p>where f s and f deд(v) are seed-related and degree-specific mapping functions, respectively, and deд(v) denotes the degree value of node v. All the nodes share one seed-oriented mapping function f s , but have a degree-specific function for compressing node neighborhoods. Here, • denotes the vector concatenation which combines the mapped features to form a single vector. If f s and f deд(v) are injective, it will have the first two properties in Lemma 4.2 that subtrees with different seeds' features or degree values would be mapped differently. Additionally, the degree-specific mapping function f deд(v) should be symmetric 3 that is invariant to the order of neighbors. And we have the following theorem (proven in Appendix) to show the existence of mapping functions f s and f deд . Next, we present our graph neural network model where the injective aggregation function could be approximated by multilayer neural network due to its exceptional expression power. For seed-related mapping function f s (·) in Eq. (5), we use a simple one-layer fully-connected neural network as follows.</p><formula xml:id="formula_11">f s (h k−1 v ) = σ (W k 0 h k−1 v )<label>(6)</label></formula><p>where the trainable matrix W k 0 is shared by all the seeds at k th hidden layer. Here σ (·) is a nonlinear activation function.</p><p>For degree-specific neighborhood aggregation on h N (v) , it can be formulated as a multi-task feature learning problem (shown in <ref type="figure" target="#fig_13">Figure 4</ref>(b)(c)) in which each task represents node representation learning for nodes with a specific degree value, thus leading to preserving the degree-specific graph structure. Here, we present two schemes for this multi-task learning problem. Degree-specific weight function: The degree-specific aggregation function can be expressed as follow.</p><formula xml:id="formula_12">f deд(v) (h k −1 N (v) ) = σ u ∈N (v) (W k д + W k deд(v) )h k −1 u<label>(7)</label></formula><p>where W k deд(v) is a degree-specific trainable matrix at k th layer and W k д is a global trainable matrix shared by all the seeds. Hashing function: Since the number of degree values on graphs could be very large, a critical challenge is how to perform multi-task learning efficiently. To address this challenge, hash kernel <ref type="bibr" target="#b18">[19]</ref> (also called feature hashing or hash trick) is applied for our multi-task neighborhood learning problem. Given two vectors x and x ′ , the hash map ϕ and the corresponding kernel K ϕ (·, ·) are defined:</p><formula xml:id="formula_13">ϕ ξ 1 , ξ 2 i (x) = j:ξ 1 (j)=i ξ 2 (j)x j (8) K ϕ (x, x ′ ) = x, x ′ ϕ = ϕ ξ 1 , ξ 2 (x), ϕ ξ 1 , ξ 2 (x ′ )<label>(9)</label></formula><p>where ξ 1 and ξ 2 denote two hash functions such that ξ 1 : N → {1, · · · , m} and ξ 2 : N → {1, −1}. Notice that hash kernel is unbiased, i.e., E ϕ [⟨x, x ′ ⟩ ϕ ] = ⟨x, x ′ ⟩ for any pair of input feature vectors. Let w k deд(v) denote one of the row vectors in W k deд(v) , then we have</p><formula xml:id="formula_14">E ϕ w k deд(v) , h k−1 u ϕ = w k deд(v) , h k−1 u</formula><p>. In this way, the multitask feature aggregation function f deд(v) can be expressed as:</p><formula xml:id="formula_15">f deд(v) (h k −1 N (v) ) = σ u ∈N (v) W k ϕ д (h k −1 u ) + ϕ deд (h k −1 u )<label>(10)</label></formula><p>where</p><formula xml:id="formula_16">W k = ϕ д (W k д ) + deд ϕ deд (W k deд )</formula><p>is the trainable matrix shared by all the nodes, and ϕ д (·) and ϕ deд (·) are global and degreespecific hash maps, respectively.</p><p>One common assumption in multi-task learning is that all the tasks are related with some shared knowledge, and meanwhile have their own task-specific knowledge. As shown in <ref type="figure" target="#fig_0">Figure 3(b)</ref>, two subtrees are structurally different, but they share some common leaves for neighborhood aggregation. By adopting both common (global) and task-specific (local) weight/hash functions, it allows learning the shared sub-structures and degree-specific neighborhood structures simultaneously.</p><p>There might be many different node degrees in real networks. One intuitive idea is that we could partition the degree values into several buckets to reduce the number of tasks. This heuristic solution might improve our model robustness to noisy graph structure or labeled nodes on source networks brought by human annotations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. We leave this as our future work because hashing kernel <ref type="bibr" target="#b18">[19]</ref> used in DEMO-Net is efficient to tackle large-scale multi-task learning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Representation Learning</head><p>The goal of graph representation learning is to use a compact feature vector to represent the entire graph. To this end, we provide a degree-specific graph-level pooling scheme.</p><p>When graph neural networks are going deeper, node representation actually captures the higher-order topological information within its local neighborhood. By mapping the original graph to a sequence of graphs {G 0 , G 1 , · · · , G K } where G 0 denotes the original graph and G k (1 ≤ k ≤ K) represents the graph after the k th layer of feature aggregation (as shown in <ref type="figure" target="#fig_13">Figure 4</ref>(e)-(g)), the k th graph representation can be expressed as follow.</p><formula xml:id="formula_17">h G k = CONCAT v ∈V h k v ·δ (deд(v), d) d ∈ deдree(G) (11)</formula><p>where deдree(G) denotes the set of degree values in graph G, and δ (·, ·) is 1 when its two arguments are equal and 0 otherwise.</p><p>As discussed before, the node representation in G k captures the topological information within k-hop neighborhood. In order to consider all the subtrees' information, we concatenate the representation h G k from all graphs {G 0 , G 1 , · · · , G K }:</p><formula xml:id="formula_18">h G = CONCAT h G k |k = 0, 1, · · · , K<label>(12)</label></formula><p>Next, we compare the degree-specific pooling scheme with existing graph-level pooling methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref> and Weisfeiler-Lehman (WL) subtree kernel <ref type="bibr" target="#b15">[16]</ref>. We define a degree-specific WL kernel:</p><formula xml:id="formula_19">K DW L (G k , G ′ k ) = ϕ DW L (G k ), ϕ DW L (G ′ k ) = v ∈V v ′ ∈V ′ δ (deд(v), deд(v ′ )) · h k v , h k v ′<label>(13)</label></formula><p>The corresponding mapping function is defined as:</p><formula xml:id="formula_20">ϕ DW L (G k ) = c(G k , d 1 ) • · · · • c(G k , d |deдr ee(G k ) | )<label>(14)</label></formula><p>where d i is the degree of G k , and</p><formula xml:id="formula_21">c(G k , d i ) = v ∈V h k v · δ (d i , deд(v))<label>(15)</label></formula><p>As shown in <ref type="bibr" target="#b9">[10]</ref>, the non-linear activation function σ (·) has a mapping function ϕ σ (·) such that σ (w T x) = ϕ σ (x) ,ψ (w) for Properties seed-oriented degree-aware order-free GCN <ref type="bibr" target="#b8">[9]</ref> × × ✓ GAT <ref type="bibr" target="#b17">[18]</ref> ✓</p><formula xml:id="formula_22">× ✓ GraphSAGE [5] ✓ × - DCNN [1] × × ✓ DEMO-Net ✓ ✓ ✓</formula><p>some mapping ψ (w) constructed from w. By the following theorem (proven in Appendix), we show that our graph representation lies in a degree-specific Hilbert kernel space. </p><formula xml:id="formula_23">K σ, DW L (G k , G ′ k ) = ϕ σ (ϕ DW L (G k )), ϕ σ (ϕ DW L (G ′ k ))<label>(16)</label></formula><p>The sum/mean based graph-level pooling approaches make the learned graph representation lie in the kernel as follow.</p><formula xml:id="formula_24">K MW L (G k , G ′ k ) = v ∈V v ′ ∈V ′ h k v , h k v ′<label>(17)</label></formula><p>And WL subtree Kernel <ref type="bibr" target="#b15">[16]</ref> can be expressed as:</p><formula xml:id="formula_25">K W Lsubt r ee (G k , G ′ k ) = v ∈V v ′ ∈V ′ δ (h k v , h k v ′ )<label>(18)</label></formula><p>It is easy to see that: (1) WL subtree kernel cannot be applied to measure the graph similarity when nodes have the continuous attribute vectors. (2) Our graph-level representation lies in a degreespecific kernel space comparing Eq. (13) with <ref type="bibr" target="#b16">(17)</ref>, thus leading to explicitly preserving the degree-specific graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>We compare the proposed DEMO-Net with some existing graph neural networks regarding the properties of graph convolution. Lemma 4.3 shows that an injective aggregation function has three properties: seed-oriented, degree-aware, order-free. We summarize the properties of graph convolution of GCN <ref type="bibr" target="#b8">[9]</ref>, GAT <ref type="bibr" target="#b17">[18]</ref>, GraphSAGE <ref type="bibr" target="#b4">[5]</ref>, and DCNN <ref type="bibr" target="#b0">[1]</ref> in <ref type="table" target="#tab_2">Table 2</ref>. It can be seen that: <ref type="bibr" target="#b0">(1)</ref> The existing graph neural networks do not have all the three properties. More importantly, none of them capture the degree-specific graph structures. (2) For graphSAGE, it is order-free when using mean or max aggregator. But graphSAGE with LTSM-aggregator is not order-free because it considers the node neighborhood as an ordered sequence. (3) Our proposed DEMO-Net considers all the properties, and the degree-aware property in particular allows our model to explicitly preserve the neighborhood structures for node and graph representation learning. In addition, the time complexity of graph convolution of DEMO-Net is linear with respect to the number of nodes and edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this section, we present the experimental results on real networks. In particular, we focus on answering the following questions: Q1: Is the proposed DEMO-Net algorithm effective on node classification compared to the state-of-the-art graph neural networks? Q2: How does the proposed DEMO-Net perform on identifying graph structure compared to structure-aware embedding approaches?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Data Sets: We use seven node classification data sets, including four social networks and three air-traffic networks. Facebook, Wiki-Vote <ref type="bibr" target="#b10">[11]</ref>, BlogCatalog and Flickr 4 are social networks. The posted keywords or tags in BlogCatalog and Flickr networks are used as node attribute information. There are three air-traffic networks <ref type="bibr" target="#b13">[14]</ref>: Brazil, Europe and USA, where each node corresponds to an airport and edge indicates the existence of commercial flights between the airports. Their class labels are assigned based on the level of activity measured by flights or people that passed the airports. Data statistics are summarized in <ref type="table" target="#tab_3">Table 3</ref>. For those networks without node attributes, we use the one-hot encoding of node degrees. In BlogCatalog, Flickr and other air-traffic networks, node class labels are available. In Facebook and Wiki-Vote, we use the degree-induced class labels by labeling the node according to its degree value. In addition, we use four bioinformatics networks to evaluate the model performance on graph classification, including MUTAG, PTC, PROTEINS and ENZYMES <ref type="bibr" target="#b4">5</ref> where the nodes are associated with categorical input features. The detailed statistics for these bioinformatics networks are summarized in <ref type="table" target="#tab_4">Table 4</ref>. Model Configuration: We adopt two hidden layers followed by the softmax activation layer in DEMO-Net, where the proposed multi-task feature learning schemes in Eq. <ref type="bibr" target="#b6">(7)</ref> and (10) are applied to each hidden layer for neighborhood aggregation (termed as DEMO-Net(weight) and DEMO-Net(hash), respectively). In addition, we 4 http://people.tamu.edu/~xhuang/Code.html 5 https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets apply Adam optimizer <ref type="bibr" target="#b7">[8]</ref> with the learning rate 0.005 on the crossentropy loss to train our models. To prevent our models from overfitting, we adopt the dropout <ref type="bibr" target="#b16">[17]</ref> with p = 0.6 and L 2 regularization with λ = 0.0005. The hidden layer size of neural units is set as 64. An early stopping strategy with a patience of 100 epochs on validation set is applied in our experiments. Baseline Methods: The baseline methods used in our experiments are given below: (1) node-level graph neural networks: GCN <ref type="bibr" target="#b8">[9]</ref>, GCN_cheby <ref type="bibr" target="#b8">[9]</ref>, GraphSAGE (mean aggregator) <ref type="bibr" target="#b4">[5]</ref>, Union <ref type="bibr" target="#b11">[12]</ref>, Intersection <ref type="bibr" target="#b11">[12]</ref> and GAT <ref type="bibr" target="#b17">[18]</ref>; (2) node-level structure-aware embedding approaches: RolX <ref type="bibr" target="#b6">[7]</ref>, struc2vec <ref type="bibr" target="#b13">[14]</ref> and GraphWAVE <ref type="bibr" target="#b2">[3]</ref>;</p><p>(3) graph-level graph neural networks: DCNN <ref type="bibr" target="#b0">[1]</ref>, PATCHY-SAN <ref type="bibr" target="#b12">[13]</ref> and DIFFPOOL <ref type="bibr" target="#b23">[24]</ref>; (4) deep graph kernel: DeepWL <ref type="bibr" target="#b22">[23]</ref>. In our experiments, all the baseline models used the default hyperparameters suggested in the original papers.</p><p>All our experiments are performed on a Windows machine with four 3.60GHz Intel Cores and 32GB RAM. The source code will be available at https://github.com/jwu4sml/DEMO-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Node Classification</head><p>For a fair comparison of different architectures <ref type="bibr" target="#b14">[15]</ref>, we use different train/validation/test splits of the networks on node classification. For social networks, we randomly choose 10% and 20% of the graph  <ref type="figure">Figure 5</ref>: Node-level classification accuracy on the Brazil and USA air-traffic networks using different train/test splits nodes as the training and validation set, respectively, and the rest as the test set. For air-traffic networks, the training, validation and test sets are randomly assigned with equal number of nodes. We run 10 times and report the mean accuracy with the standard variance for performance comparison. As shown in <ref type="table" target="#tab_6">Table 5</ref>, we report the classification results on the real networks where the best results are indicated in bold. It can be observed that the proposed DEMO-Netmodels significantly outperform other graph neural networks (answering Q1). In particular, our DEMO-Net models are at least 10% higher on mean accuracy over baseline methods. One explanation is that baseline methods focus on preserving the node proximity by roughly mixing a node with its neighbors, whereas our proposed DEMO-Net models capture the degree-specific structure to distinguish the structural roles of nodes in the networks. We also evaluate the performance of our models against three structure-aware embedding approaches: RolX, struc2vec and Graph-WAVE. All of them are unsupervised embedding approaches identifying the structural roles of nodes in the networks. Following <ref type="bibr" target="#b13">[14]</ref>, we use the one-vs-rest logistic regression with L2 regularization to train a classifier for node representations learned by baseline methods. Here we consider using different train-test splits where the percentage of training nodes ranges from 10% to 90% and the rest is used for testing. The experimental results on the Brazil and USA air-traffic networks are provided in <ref type="figure">Figure 5</ref>. We observe that our proposed DEMO-Net models outperform the comparison methods across all the data sets (answering Q2). Besides, the structure roles identified by those baselines only represent the local graph structure without considering node attributes. Instead, both topological information and node attributes are captured in our DEMO-Net models when learning node representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Graph Classification</head><p>We use four public graph classification benchmarks to evaluate the proposed DEMO-Net models with the degree-specific graph-level pooling scheme. DCNN <ref type="bibr" target="#b0">[1]</ref>, PATCHY-SAN <ref type="bibr" target="#b12">[13]</ref> and DIFFPOOL <ref type="bibr" target="#b23">[24]</ref> adopted the end-to-end training architectures for supervised graph classification. For unsupervised graph kernel method DeepWL <ref type="bibr" target="#b22">[23]</ref>, we use the one-vs-rest logistic regression with L2 regularization  to train a supervised classifier for graph classification. We also consider our model variants (denoted as DEMO-Net_m(hash) and DEMO-Net_m(weight) respectively) which replace the proposed degree-specific graph-level pooling with mean-pooling scheme <ref type="bibr" target="#b0">[1]</ref>. The input graphs are randomly assigned to the training, validation, or test set where each set has the same number of nodes. The graph classification results are shown in <ref type="table" target="#tab_7">Table 6</ref> where the best results are indicated in bold. It is observed that (1) compared to the existing mean-pooling method, the proposed degree-specific pooling method improves the model performance in most cases, which is consistent with our analysis in Section 4.2; (2) the classification results of our DEMO-Net models are comparable to other graph neural networks and graph kernel method (answering Q3). Moreover, on MUTAG and ENZYMES data sets, our proposed DEMO-Net(weight) outperforms the baseline methods. One explanation might be that the graph representation generated by DEMO-Net explicitly preserves the degree-specific graph structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Efficiency Analysis</head><p>It is easy to show that the time complexity of each layer in our proposed DEMO-Net(hash) model is O(nF F ′ + T F + nH F ′ + mF ′ ) where n and m are the number of nodes and edges in the graph, respectively, F and F ′ are the dimensionalities of input and output features at each layer, respectively, T is the number of tasks (degree values) in the graph, and H is the hashing dimension. By observing that T ≤ n in the networks, its time complexity would be O(nF F ′ + mF ′ ), which is on par with GCN and GAT models. Similarly, we can show that the time complexity of each layer in DEMO-Net(weight) is O(T (nF F ′ +mF ′ )). When T ≪ n and T ≪ m, it also scales linearly with respect to the number of nodes and edges.</p><p>Following <ref type="bibr" target="#b8">[9]</ref>, we report the running time (measured in seconds wall-clock time) per epoch (including forward pass, cross-entropy calculation, backward pass) on a synthetic network assigning 2n edges uniformly at random. As shown in <ref type="figure" target="#fig_14">Figure 6</ref>, we observe that (answering Q4) (1) the wall-clock time of our proposed DEMO-Net model is linear with respect to the number of nodes; (2) our models are much more efficient than GAT on node classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we focus on building a degree-specific graph neural network for both node and graph classification. We start by  analyzing the limitations of the existing graph neural networks from the perspective of Weisfeiler-Lehman graph isomorphism test. Furthermore, it is observed that the graph convolution should have the following properties: seed-oriented, degree-aware, orderfree. To this end, we propose a generic graph neural network model named DEMO-Net which formulates the feature aggregation into a multi-task learning problem according to nodes' degree values. In addition, we also present a novel graph-level pooling method for learning graph representations provably lying in a degree-specific Hilbert kernel space. The extensive experiments on real networks demonstrate the effectiveness of our DEMO-Net algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 3 )</head><label>3</label><figDesc>The experimental results on several node and graph classification benchmark data sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 3 . 1 .</head><label>31</label><figDesc>(Node-level Representation Learning) Input: (i) An attributed graph G = (V , E) with adjacency matrix A ∈ R n×n and node attributes X ∈ R n×D ; (ii) Labeled training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>Neighbors sorting (multiset-label sorting): The neighbors are sorted in the ascending or descending order of degree values. The subtrees with permutation order of neighbors are recognized as the same one. • Feature aggregation (label compression): The node feature is updated by compressing the feature vectors of the aggregated neighbors including itself. • Graph-level pooling (graph representation): It summarizes all the node features to form a global graph representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Definition 4 . 1 .</head><label>41</label><figDesc>(Structurally Identical Subtree) Any two subtrees in T are structurally identical if the only possible difference between them is the order of neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 4 . 2 .</head><label>42</label><figDesc>Let G = {V , E} be a graph and u, v ∈ V be two nodes in the graph. When the mapping function f : T → R d in graph neural networks is injective, the learned features of v and u will be different if and only if the WL graph isomorphism test determines that they are not structurally identical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Examples of subtree in T with: (a) different seeds' attributes; (b) different seeds' degree values; (c) different neighbors' order. In such cases, two subtrees in (a) and (b) are mapped to different feature vectors, respectively. Two subtrees in (c) will be mapped to the same feature vector. (Best seen in color. Colors denote the node attributes.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Lemma 4 . 3 .</head><label>43</label><figDesc>(Properties) Let f : T → R d be the aggregation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>the seeds' attributes are different, i.e., h i h j . (ii) Degree-aware: f h i , {h u |u ∈ N (i)} f h j , {h w |w ∈ N (j)} if the seeds' degree values are different, i.e., deд(i) deд(j). (iii) Order-free: f h i , {h u |u ∈ N (i)} = f h j , {h w |w ∈ N (j)} if h i = h j and the only possible difference between {h u |u ∈ N (i)} and {h w |w ∈ N (j)} is the order of neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3</head><label>3</label><figDesc>lists some examples to illustrate those properties. The injective function f maps the subtrees in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Theorem 4 . 4 .</head><label>44</label><figDesc>(Existence Theorem) Assume T is countable, there exist mapping functions f s and { f deд |deд ∈ deдree(G)} such that for any two subtrees in T , the function f : T → R d defined in Eq. (5) maps them to different features if they are not structurally identical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Theorem 4 . 5 .</head><label>45</label><figDesc>For a degree-specific Weisfeiler-Lehman kernel, the graph representation h G in Eq. (12) belongs to the Reproducing Kernel Hilbert Space (RKHS) of kernel K σ, DW L (·, ·) where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Task-specific feature aggregation (d) Node representation (e) Degree-based partition (f) Task-specific graph-level pooling (g) Graph representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 :</head><label>4</label><figDesc>Overview of our proposed DEMO-Net framework (best seen in color). (b) and (c) represent the multi-task feature aggregation. The node representation in (d) can be used for node-level classification. For learning graph embedding, (e)-(g) provide the graph-level pooling method based on node degree distribution for learning graph representation.Q3: How does the proposed DEMO-Net with degree-specific graphlevel pooling perform on graph classification task? Q4: Is the proposed degree-specific graph convolution of DEMO-Net efficient on learning node representation?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Running time per epoch (best seen in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notation</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of graph neural networks</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Data sets for node classification</figDesc><table><row><cell>Data sets</cell><cell cols="4"># nodes # edges # classes # attributes</cell></row><row><cell>Facebook</cell><cell>4039</cell><cell>88234</cell><cell>4</cell><cell>-</cell></row><row><cell>Wiki-Vote</cell><cell>7115</cell><cell>103689</cell><cell>4</cell><cell>-</cell></row><row><cell>BlogCatalog</cell><cell>5196</cell><cell>171743</cell><cell>6</cell><cell>8189</cell></row><row><cell>Flickr</cell><cell>7575</cell><cell>239738</cell><cell>9</cell><cell>12047</cell></row><row><cell>Brazil</cell><cell>131</cell><cell>1038</cell><cell>4</cell><cell>-</cell></row><row><cell>Europe</cell><cell>399</cell><cell>5995</cell><cell>4</cell><cell>-</cell></row><row><cell>USA</cell><cell>1190</cell><cell>13599</cell><cell>4</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Data sets for graph classification</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Data sets</cell><cell cols="13"># graphs # classes Avg # nodes # attributes</cell></row><row><cell></cell><cell cols="3">MUTAG</cell><cell></cell><cell></cell><cell>188</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell cols="2">17.9</cell><cell></cell><cell></cell><cell>7</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">PTC</cell><cell></cell><cell></cell><cell></cell><cell>344</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell cols="2">25.5</cell><cell></cell><cell></cell><cell cols="2">19</cell><cell></cell></row><row><cell></cell><cell cols="4">PROTEINS</cell><cell cols="2">1113</cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell cols="2">39.1</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">ENZYMES</cell><cell></cell><cell>600</cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell cols="2">32.6</cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">DEMO-Net (hash)</cell><cell></cell><cell cols="4">DEMO-Net (weight)</cell><cell cols="2">struc2vec</cell><cell></cell><cell></cell><cell cols="2">GraphWAVE</cell><cell></cell><cell></cell><cell>RolX</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean accuracy</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean accuracy</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Percentage of training examples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Percentage of training examples</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Brazil</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) USA</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Node-level classification accuracy (mean ± standard variance) on the social and air-traffic networks ±0.006 0.678 ±0.010 0.614 ±0.069 0.479 ±0.064 0.659 ±0.020 DEMO-Net(weight) 0.919 ±0.003 0.998 ±0.000 0.849 ±0.000 0.656 ±0.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Social networks</cell><cell></cell><cell cols="3">Air-traffic networks</cell></row><row><cell></cell><cell>Facebook</cell><cell>Wiki-Vote</cell><cell>BlogCatalog</cell><cell>Flickr</cell><cell>Brazil</cell><cell>Europe</cell><cell>USA</cell></row><row><cell>GraphSAGE [5]</cell><cell>0.389 ±0.019</cell><cell>0.245 ±0.000</cell><cell>0.828 ±0.007</cell><cell>0.641 ±0.006</cell><cell>0.404 ±0.035</cell><cell>0.272 ±0.022</cell><cell>0.316 ±0.022</cell></row><row><cell>GCN [9]</cell><cell>0.575 ±0.013</cell><cell>0.329 ±0.029</cell><cell>0.720 ±0.013</cell><cell>0.546 ±0.019</cell><cell>0.432 ±0.064</cell><cell>0.371 ±0.046</cell><cell>0.432 ±0.022</cell></row><row><cell>GCN_cheby [9]</cell><cell>0.646 ±0.012</cell><cell>0.495 ±0.016</cell><cell>0.686 ±0.037</cell><cell>0.479 ±0.023</cell><cell>0.516 ±0.070</cell><cell>0.460 ±0.038</cell><cell>0.526 ±0.045</cell></row><row><cell>Union [12]</cell><cell>0.600 ±0.000</cell><cell>0.463 ±0.000</cell><cell>0.730 ±0.000</cell><cell>0.566 ±0.000</cell><cell>0.466 ±0.006</cell><cell>0.418 ±0.002</cell><cell>0.582 ±0.000</cell></row><row><cell>Intersection [12]</cell><cell>0.598 ±0.000</cell><cell>0.462 ±0.000</cell><cell>0.725 ±0.000</cell><cell>0.557 ±0.000</cell><cell>0.459 ±0.003</cell><cell>0.443 ±0.002</cell><cell>0.573 ±0.000</cell></row><row><cell>GAT [18]</cell><cell>0.570 ±0.036</cell><cell>0.594 ±0.070</cell><cell>0.663 ±0.000</cell><cell>0.359 ±0.000</cell><cell>0.382 ±0.126</cell><cell>0.424 ±0.073</cell><cell>0.585 ±0.021</cell></row><row><cell>DEMO-Net(hash)</cell><cell>0.887 ±0.020</cell><cell>0.997 ±0.000</cell><cell cols="2">0.849 000</cell><cell>0.543 ±0.034</cell><cell>0.459 ±0.025</cell><cell>0.647 ±0.021</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Graph-level classification on the real networks</figDesc><table><row><cell></cell><cell>MUTAG</cell><cell>PTC</cell><cell cols="2">PROTEINS ENZYMES</cell></row><row><cell>DeepWL [23]</cell><cell>0.733</cell><cell>0.537</cell><cell>0.680</cell><cell>0.210</cell></row><row><cell>DCNN [1]</cell><cell>0.670</cell><cell>0.572</cell><cell>0.579</cell><cell>0.160</cell></row><row><cell>PATCHY-SAN [13]</cell><cell>0.795</cell><cell>0.568</cell><cell>0.714</cell><cell>0.170</cell></row><row><cell>DIFFPOOL [24]</cell><cell>0.663</cell><cell>0.251</cell><cell>0.733</cell><cell>0.184</cell></row><row><cell>DEMO-Net_m(hash)</cell><cell>0.760</cell><cell>0.586</cell><cell>0.617</cell><cell>0.236</cell></row><row><cell>DEMO-Net_m(weight)</cell><cell>0.798</cell><cell>0.550</cell><cell>0.616</cell><cell>0.251</cell></row><row><cell>DEMO-Net(hash)</cell><cell>0.771</cell><cell>0.563</cell><cell>0.705</cell><cell>0.251</cell></row><row><cell>DEMO-Net(weight)</cell><cell>0.814</cell><cell>0.572</cell><cell>0.708</cell><cell>0.272</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here, label is an identifier of nodes. In order not to be confused with a class label, we will use node attribute to represent it in this paper.<ref type="bibr" target="#b1">2</ref> The seed denotes the root node to be learned in the graph. For example, node v 1 inFigure 2is a seed when updating its feature at each iteration.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A symmetric function of n variables is one whose value given n arguments is the same no matter the order of the arguments. For example, f (x 1 , x 2 ) = f (x 2 , x 1 ) for any pair (x 1 , x 2 ).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX FOR REPRODUCIBILITY</head><p>To better reproduce the experimental results, we provide additional details about the algorithms.</p><p>Proof of Theorem 4.4. Theorem 4.4 says that there exist mapping functions f s and { f deд |deд ∈ deдree(G)} such that for any two subtrees in T , the function f : T → R d defined in Eq. (5) maps them to different feature vectors if they are not structurally identical.</p><p>Proof. Let T s = {h v |v ∈ V } denote the seed set in T and d m = max {deд} + 1 the maximum degree values plus one. Becuase T is countable, there exists an injective function Z : T → N that maps each subtree from T to an unique natural number. It can be observed that N can be divided into d m disjoint sets:</p><p>There exists an injective function Z s : T s → N 0 that maps each seed from T s to an unique natural number in N 0 . Let T i = {h N (v) |v ∈ V and deд(v) = i} denote the neighbor set consisting of the seeds' neighbors when their degree values are equal to i. Because T is countable, all the subsets T i (1 ≤ i ≤ max {deд}) are countable. There exists the injective, symmetric function Z i : T i → N i that maps each element from T i to an unique real number in N i . Moreover, there is a function Z f : T → N 2 that maps each subtree from T to an unique feature vector in N 2 when</p><p>). Please note that the structurally identical subtrees would be considered the same one when the degree-specific function Z i is symmetric.</p><p>It is easy to construct an injective function д :</p><p>Based on the properties of injective function, д(Z f (·)) will be injective function that maps any two subtrees in T to different feature vectors in R d if they are not structurally identical, which completes the proof. </p><p>Our graph convolution (feature aggregation) function can be written as:</p><p>whereŴ k deд(v) represents the degree-specific parameters, and more specifically,Ŵ deд(v) = W k д + W k deд(v) for degree-specific weight matrix in Eq. (7) andŴ deд(v) = W k (ϕ д (·)+ϕ deд (·)). Because we use the concatenation operator • to combine the learned features of seed and its neighborhood, it holds that h</p><p>Let w 0j denote the j th row from W k 0 . To show our results, we construct a k-regular "reference graph" G r k = (V r k , E r k ) which has the same nodes as the input graph G (i.e., V r k = V ). Its degree value k is d i and each node in "reference graph" is associated with the same feature vector w 0j /n. Then when h G k [i][j] lies in the seed's</p><p>The lemma 1 in <ref type="bibr" target="#b9">[10]</ref> holds that for activation functions σ , there exists kernel functions K σ (·, ·) and the underlying mapping ϕ σ (·) such that f (x) = σ (w T x) = ⟨ϕ σ (x),ψ (w)⟩ for some mapping function ψ (w) constructed from w. Therefore, we have:</p><p>where K σ, DW L (·, ·) is the composition of K σ (·, ·) and K DW L (·, ·), and K σ, DW L (x, y) = ϕ σ (ϕ DW L (x)) T ϕ σ (ϕ DW L (y)). And G σ,r k is the "reference graph" constructed from model parameters and activation function. Letŵ d i j denote the j th row fromŴ k deд(v) with deд(v) = d i . Similarly, we construct a k-regular "reference graph"Ĝ r k = (V r k ,Ê r k ) which has the same nodes as the input graph G with degree value d i . Each node in this "reference graph" is associated with the same fea-</p><p>Please notice that in this case, node features are assumed to be the sum of neighborhood features. And moreover, it can be written as: h G [k, i, j] = K σ, DW L (G k −1 ,Ĝ σ,r k )</p><p>whereĜ σ,r k is the "reference graph" constructed from model parameters and activation function. Therefore, the graph representation h G belongs to the RKHS of kernel K σ, DW L (·, ·), which completes the proof. □</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structural node embeddings via diffusion wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RolX: structural role extraction &amp; mining in large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP datasets: Stanford large network dataset Collection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
		<title level="m">struc2vec: Learning node representations from structural identity</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>SIGKDD</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Leveraging multiple gene networks to prioritize GWAS candidate genes via network representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanwen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph convolutional networks: Algorithms, applications and open challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiejun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Social Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SPARC: Self-paced network representation for few-shot rare category characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A local algorithm for structure-preserving graph cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigit</forename><surname>Mehmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanghang</forename><surname>Alcorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>Davulcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Crowdsourcing via Tensor Augmentation and Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2435" to="2441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MultiC 2 : an optimization framework for learning from task and worker dual heterogeneity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="579" to="587" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
