<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
							<email>lukemk@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Oxford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9% top-1 accuracy, compared to 77.9% and 79.9% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Introduced by <ref type="bibr" target="#b3">[4]</ref>, the vision transformer architecture applies a series of transformer blocks to a sequence of image patches. Each block consists of a multi-head attention layer <ref type="bibr" target="#b10">[11]</ref> followed by a feed-forward layer (i.e. a linear layer, or a single-layer MLP) applied along the feature dimension. The general-purpose nature of this architecture, coupled with its strong performance on image classification benchmarks, has prompted significant interest from the vision community. However, it is still not clear exactly why the vision transformer is effective.</p><p>The most-cited reason for the transformer's success on vision tasks is the design of its attention layer, which gives <ref type="bibr" target="#b0">1</ref> Code is available at https://github.com/lukemelas/do-you-even-needattention ✕ N Patch Embedding <ref type="figure">Figure 1</ref>: The architecture explored in this report is extremely simple, consisting of a patch embedding followed by a series of feed-forward layers. These feed-forward layers are alterately applied to the patch and feature dimensions of the image tokens. The architecture is identical to that of ViT <ref type="bibr" target="#b3">[4]</ref> with the attention layer replaced by a feed-forward layer.</p><p>the model a global receptive field. This layer may be seen as a data-dependent linear layer, and when applied on image patches it resembles (but is not exactly equivalent to) a convolution. Indeed, a significant amount of recent work has gone into improving the efficiency and efficacy of the attention layer.</p><p>In this short report, we conduct an experiment that hopes to shed a little light on why the vision transformer works so well in the first place. Specifically, we remove attention from the vision transformer, replacing it with a feedforward layer applied over the patch dimension. After this change, the model is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion ( <ref type="figure">Figure 1)</ref>.</p><p>In experiments on ImageNet <ref type="table">(Table 1)</ref>, we show that quite strong performance is attainable even without attention. Notably, a ViT-base-sized model gives 74.9% top-1 accuracy without any hyperparameter tuning (i.e. using the same hyperparameters as its ViT counterpart). These results suggest that the strong performance of vision transformers may be attributable less to the attention mechanism and more to other factors, such as the inducive bias produced by the patch embedding and the carefully-curated set of training augmentations.</p><p>The primary purpose of this report is to explore the limits of simple architectures. We do not to break the ImageNet benchmarks; on that front, methods such as neural architecture search (e.g. EfficientNet <ref type="bibr" target="#b7">[8]</ref>) will inevitably perform best. Nonetheless, we hope that the community finds these results interesting, and that these results prompt more researchers to investigate why our current models are as effective as they are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>The context for this report is that over the past few months, there has been an explosion of research into variants of the vision transformer architecture: Deit <ref type="bibr" target="#b8">[9]</ref> adds distillation, DeepViT [14] mixes the attention heads, CaiT <ref type="bibr" target="#b9">[10]</ref> separates the attention layers into two stages, Token-to-Token ViT <ref type="bibr" target="#b12">[13]</ref> aggregates neighboring tokens throughout the network, CrossViT <ref type="bibr" target="#b0">[1]</ref> processes patches at two scales, PiT <ref type="bibr" target="#b5">[6]</ref> adds pooling layers, LeViT <ref type="bibr" target="#b4">[5]</ref> uses convolutional embeddings and modified attention/normalization layers, CvT <ref type="bibr" target="#b11">[12]</ref> uses depthwise convolutions in the attention layer, and Swin/Twins <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> combines global and local attention, just to name a few.</p><p>These works improve upon the vision transformer architecture, each showing strong performance on ImageNet. However, it is not clear how the different parts of ViT or its many variants contribute to the final performance of each of these models. This report details an experiment that investigates one aspect of this issue, namely how important the attention layers are to ViT's success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method and Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Do You Even Need Attention?</head><p>We remove the attention layer from the ViT model, replacing it by a simple feed-forward layer over the patch dimension. We use the same structure as the standard feed-forward network over the feature dimension, which is to say that we project the patch dimension into a higherdimensional space, apply a nonlinearity, and project it back  <ref type="table">Table 1</ref>: A comparison of ImageNet top-1 accuracies for different model sizes. In the first column, P refers to the patch size in pixels. Overall, the models with only feedforward layers (FF Only) perform worse than their counterparts with attention, but they perform surprisingly well regardless. Performance deteriorates for the largest models both with and without attention.</p><p>to the original space. <ref type="figure" target="#fig_1">Figure 2</ref> gives PyTorch code for a single block of the feed-forward-only transformer. We should note that, like the vision transformer and its many variants, this feed-forward-only network bares strong resemblance to a convolutional network. In fact, the feedforward layer over the patch dimension can be viewed as an unusual type of convolution with a full receptive field and a single channel. Since the feed-forward layer over the feature dimension can be seen as a 1x1 convolution, it would be technically accurate to say that the entire network is a sort of convolutional network in disguise. That being said, it is structurally more similar to a transformer than to a traditionally-designed convolutional network (e.g. ResNet/VGG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setup</head><p>We train three models, corresponding to the ViT/DeiT tiny, base, and large networks, on ImageNet <ref type="bibr" target="#b2">[3]</ref> using the setup from DeiT <ref type="bibr" target="#b8">[9]</ref>. The tiny and base networks have patch size 16, while the large network has patch size 32 due to computational constraints. Training and evaluation is performed at resolution 224px. Notably, we use exactly same hyperparameters as DeiT for all models, which means that our performance could likely be improved with hyperparameter tuning. <ref type="table">Table 1</ref> shows the performance of our simple feedforward network on ImageNet. Most notable, the feedforward-only version of ViT/Deit-base achieves surprisingly strong performance (74.9% top-1 accuracy), comparable to a number of older convolutional networks (e.g. VGG-16, ResNet-34). Such a comparison is not exactly fair be-cause the feed-forward model uses stronger training augmentations, but it is nevertheless quite a strong result in an absolute sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head><p>Performance deteriorates for the large models for the models both with and without attention, yeilding 71.2% and 71.4% top-1 accuracy respectively. As detailed in <ref type="bibr" target="#b3">[4]</ref>, pretraining with a larger dataset appears necessary for such enormous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Do You Even Need Feed-Forward Layers?</head><p>Naturally, since we tried training a model with only feedforward layers, we also tried training a model with only attention layers. In this model, we simply replaced the feed-forward layer over the feature dimension with an attention layer over the feature dimension. We only experimented with a tiny-sized model ( 4.0M parameters), but it performed spectacularly poorly (28.2% top-1 accuracy at 100 epochs, at which point we ended the run).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion</head><p>The above experiments demonstrate that it is possible to train a reasonably strong transformer-style image classifiers without attention layers. Furthermore, attention layers without feed-forward layers do not appear to yield similarly strong performance. These results indicate that the strong performance of ViT may be attributable more to its patch embeddings and training procedure than to the design of the attention layer. The patch embeddings in particular provide a strong inductive bias that is likely one of, if not the, main drivers of the model's strong performance.</p><p>From a practical perspective, the feed-forward-only model has one notable advantage over the vision transformer, which is that its complexity is linear with respect to the sequence length as opposed to quadratic. This is the case due to the intermediate projection dimension within the feed-forward layer applied over patches, whose size is not necessarily dependent on the sequence length. Usually the intermediate dimension is chosen to be a multiple of the number of input features (i.e. the number of patches), in which case it the model is indeed quadratic, but this does not necessarily need to the case.</p><p>Apart from its worse performance, one major downside of the feed-forward-only model is that it only functions on fixed-length sequences (due to the feed-forward layer over patches). This is not a big issue for image classificaion, where images are cropped to a standard size, but limits the applicability of the architecture to other tasks.</p><p>Feed-forward-only models shed light on vision transformers and attention mechanisms in general. For the future, it would be interesting to investigate the extent to which these conclusions apply outside of the image domain, for example in NLP/audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Conclusion</head><p>This short report demonstrates that transformer-style networks without attention layers make for surprisingly strong image classifiers. Future work in this direction could attempt to better understand the contributions of other pieces of the transformer architecture (e.g. the normalization layer or initialization scheme). More broadly, we hope that this short report encourages further investigation into why our current models perform as well as they do.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Feed</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>PyTorch code for a single transformer block consisting of two feed-forward layers Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokensto-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021. [14] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer, 2021.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>from torch import nn class LinearBlock</head><label></label><figDesc>(nn.Module): def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act=nn.GELU, norm=nn.LayerNorm, n_tokens=197): # 197 = 16 ** 2 + 1 super().__init__() self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()</figDesc><table><row><cell># FF over features</cell></row><row><cell>self.mlp1 = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act=act, drop=drop)</cell></row><row><cell>self.norm1 = norm(dim)</cell></row><row><cell># FF over patches</cell></row><row><cell>self.mlp2 = Mlp(in_features=n_tokens, hidden_features=int(n_tokens * mlp_ratio), act=act, drop=drop)</cell></row><row><cell>self.norm2 = norm(n_tokens)</cell></row><row><cell>def forward(self, x):</cell></row><row><cell>x = x + self.drop_path(self.mlp1(self.norm1(x)))</cell></row><row><cell>x = x.transpose(-2, -1)</cell></row><row><cell>x = x + self.drop_path(self.mlp2(self.norm2(x)))</cell></row><row><cell>x = x.transpose(-2, -1)</cell></row><row><cell>return x</cell></row><row><cell>class Mlp(nn.Module):</cell></row><row><cell>def __init__(self, in_features, hidden_features, act_layer=nn.GELU, drop=0.):</cell></row><row><cell>super().__init__()</cell></row><row><cell>self.fc1 = nn.Linear(in_features, hidden_features)</cell></row><row><cell>self.act = act_layer()</cell></row><row><cell>self.fc2 = nn.Linear(hidden_features, in_features)</cell></row><row><cell>self.drop = nn.Dropout(drop)</cell></row><row><cell>def forward(self, x):</cell></row><row><cell>x = self.fc1(x)</cell></row><row><cell>x = self.act(x)</cell></row><row><cell>x = self.drop(x)</cell></row><row><cell>x = self.fc2(x)</cell></row><row><cell>x = self.drop(x)</cell></row><row><cell>return x</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Twins: Revisiting spatial attention design in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
