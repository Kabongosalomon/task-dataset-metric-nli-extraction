<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Densely Deformable Efficient Salient Object Detection Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanveer</forename><surname>Hussain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
							<email>saeed.anwar@data61.csiro.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data61-CSIRO</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ullah</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
							<email>khan.muhammad@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Baik</surname></persName>
							<email>sbaik@sejong.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Densely Deformable Efficient Salient Object Detection Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Salient Object Detection (SOD) domain using RGB-D data has lately emerged with some current models' adequately precise results. However, they have restrained generalization abilities and intensive computational complexity. In this paper, inspired by the best background/foreground separation abilities of deformable convolutions, we employ them in our Densely Deformable Network (DDNet) to achieve efficient SOD. The salient regions from densely deformable convolutions are further refined using transposed convolutions to optimally generate the saliency maps. Quantitative and qualitative evaluations using the recent SOD dataset against 22 competing techniques show our method's efficiency and effectiveness. We also offer evaluation using our own created crossdataset, surveillance-SOD (S-SOD), to check the trained models' validity in terms of their applicability in diverse scenarios. The results indicate that the current models have limited generalization potentials, demanding further research in this direction. Our code and new dataset will be publicly available at https://github.com/ tanveer-hussain/EfficientSOD</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Salient object detection (SOD) highlights important and distinctive contents from image data naturally observable by the human visual system. SOD is useful in various computer vision applications, including activity recognition <ref type="bibr" target="#b20">[21]</ref>, video summarization <ref type="bibr" target="#b13">[14]</ref>, etc. With the emergence of CNNs, researchers have achieved significantly precise SOD results, surpassing the traditional hand-crafted features-based methods <ref type="bibr" target="#b2">[3]</ref>. Most current methods follow a supervised learning strategy, using saliency maps with their corresponding RGB and depth images for training. We follow the same strategy but consider only RGB images to generate saliency maps.</p><p>The SOD methods can be categorized based on input data and fusion strategy. Firstly, as the RGB input utilizes a single CNN network without any fusion, where the input is only * Corresponding author: Sung Wook Baik RGB; secondly, the RGB and depth is input separately to a two-stream network with late fusion; thirdly, providing RGB and depth combinedly to the network, called as early fusion; and finally, the one that involves fusion at multiple stages in the network. The SOD methods based on RGB <ref type="bibr" target="#b4">[5]</ref> have limited results in challenging scenarios when objects have a similar texture and homogeneous properties with complex scattered backgrounds. Similarly, the RGB-D or two-steam networks are computationally expensive <ref type="bibr" target="#b25">[26]</ref>, inefficient, and have limited adaptability in real-world applications. Moreover, the RGB-D methods for SOD show comparatively poor results when depth images have noise or heavy occlusions. This paper achieves efficient and SOTA SOD without depth data (see <ref type="figure" target="#fig_0">Figure 1</ref>), thereby significantly fewer training parameters. DDNet employs densely deformable convolutions to model geometrical transformations in training data effectively, enabling generalization potentials. The transpose convolutions with bi-linear interpolation for image smoothing and a basic image processing algorithm as post-processing generates refined saliency maps, covering the aforementioned limitations in the existing literature.</p><p>We present a new cross-dataset, S-SOD (samples in <ref type="figure" target="#fig_1">Figure  2</ref>), having complex images from diverse indoor surveillance scenarios to test the potentials of our SOD models, posing a lower generalization capability; hence, limited performance on open datasets. Typically, the deep SOD models learn millions of parameters to identify, discover, and locate patterns in images. The evaluation process indicates how well the SOD models perform on a certain test set. However, traditionally, this set is disjoint from the same training dataset and usually does not have comparably different samples to evaluate the model's robustness in challenging scenarios. The standard evaluation may be suitable for some background-changing applications <ref type="bibr" target="#b5">[6]</ref>, but not in real-world implementation such as surveillance scenarios, where a little noise can alter the saliency maps. In this article, we claim the following contributions:</p><p>• We propose a novel architecture employing densely deformable convolutions to capture and extract the salient objects' regions and pose comparatively better generalization abilities.</p><p>• To validate the generalization abilities of SOD models, we create a small-scale dataset by collecting the most challenging images with varying brightness and contrast, background and foreground colors overlap, among many others.</p><p>• The proposed model results show that it obtains prominent salient regions with better performance and several ablation studies with varied computation and accuracy. Our final model achieves the best SOTA results than existing competing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Recently, many researchers have focused on fused modalities such as RGB-D data and fusion strategies, i.e., input fusion, early fusion, and late fusion strategies. For instance, <ref type="bibr" target="#b9">[10]</ref> introduced a siamese network for RGB-D data with two modules, including joint learning and densely cooperative fusion, where the first module generates robust saliency features and the second one finds the complementary features. Similarly, ICNet <ref type="bibr" target="#b15">[16]</ref> employed an information conversion network that comprises concatenation operations and correlation layers for effective salient object detection. Similarly, <ref type="bibr" target="#b6">[7]</ref> fused RGB and depth features using the bifurcated backbone strategy to extract multi-level features, depth cues from the RGB, and depth-enhanced module, respectively. Recently, a multi-modal fusion network <ref type="bibr" target="#b1">[2]</ref> employed two networks for SOD by utilizing dilated convolutional layers and 1×1 kernels for global contextual reasoning. The first network takes RGB or depth data to predict dense area in the image, and the second network applies a late fusion strategy to combine RGB-D clues for SOD. In RGB-D based methods, one of the major challenges is crossmodal fusion. For this purpose, <ref type="bibr" target="#b16">[17]</ref> introduced adaptive gated fusion with a two-steams Generative Adversarial network that simultaneously takes RGB and depth images utilizing depth-wise separable residual convolutions to capture the side-output feature in the RGB stream, which is later added with the depth stream decoder network to compensate the non-clarity of the depth images. The discriminator of their network fuses the stream and generates the optimal saliency maps. Working with cross-modal fusion, <ref type="bibr" target="#b26">[27]</ref> proposed compensation-aware loss to investigate cross-modal relationships and effective fusion of their features. The techniques mentioned earlier have sophisticated results for the benchmark datasets, where training and testing data belong to the same dataset. Although traditional CNNs have shown promising results for many challenging image and video analytic domains, but some key limitations are associated with them, such as modeling geometric transformations (varied scales, poses, viewpoint, etc.) and using huge data (extensive data augmentation in some cases) for training. The existing SOD methods do not function well to model unknown transformations but show the best performance only when testing data of the same dataset is used for testing. Unlike existing methods, we employ a set of densely deformable convolutions for SOD, which has comparatively better potential to learn geometric transformations and better pattern representation abilities than traditional convolutions. Our model's details, Densely Deformable Network, abbreviated as, DDNet are given in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DDNet: Our Saliency Detection Model</head><p>Here we explain our proposed saliency detection model's working that uses three main blocks to generate optimal saliency. Firstly, two dense convolution blocks represent low- level features of the input RGB images. Then we propose densely connected deformable convolutions to learn effective features of salient regions and their corresponding boundaries. Finally, we employ transpose convolution and upsampling to generate the resultant saliency image. <ref type="figure" target="#fig_2">Figure 3</ref> is a schematic diagram of our proposed DDNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Distinctive Edge and Shape Features</head><p>In order to get the initial low-level features, such as edges, we test various convolution invariants such as dilated and grouped convolutions with pooling inserted between these layers, but their output features are not sufficient for finer details extraction. On the contrary, the densely connected convolutions proved effective in extracting high-level details. Two densely populated sequential blocks are employed in our network containing six convolution layers of kernel size 1×1 and 3×3, having max-pooling and batch normalization between them. We employ a transition layer before the next dense block with single 1×1 convolutional layer and average pooling with a stride of two to reduce the output size. Next is the final layer of the first main block, which has densely connected convolutions with the same size as the previous one but different in numbers, i.e., a total of 12 convolutional layers for each 1×1 and 3×3 kernel. Our network's initial main block is inspired by the DenseNet <ref type="bibr" target="#b12">[13]</ref>, which has better classification results than residual networks.</p><p>The dense blocks in our network have a significant role in representing the image in terms of features. Moreover, to extract representative features, we experimented with standard convolutions and forwarded to deformable convolutions; the results were not encouraging, as documented in <ref type="table" target="#tab_1">Table 2</ref> for various blocks of VGG16. Therefore, in our final architecture, we merely used two dense blocks. Furthermore, in-creasing the number of blocks may generate better results; however, at the cost of the model's efficiency and overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deformable Convolutions</head><p>The building modules of CNNs do not work perfectly well to model geometric transformations inside images, as they have fixed geometric structures. Deformable convolutions and deformable RoI pooling are introduced to replace the traditional convolutions and several pooling invariants. They have better abilities to adapt geometrical variations inside an image containing objects, enabling them to generate fine saliency maps, particularly when they receive a preprocessed image from the earlier module's dense block in DDNet.</p><p>At first, we apply three deformable convolutions without any dense connection between the output of the first and the third layer's input, which results in coarse saliency maps with some roughness at the edges. Next, we employ the densely connected deformable blocks, having three interconnected convolutions in a sequence, which achieves the most significant and dominant results against SOTA, as demonstrated in <ref type="table" target="#tab_0">Table 1</ref>. Thus the final proposed DDNet contains densely connected three deformable convolution layers. Since our target is an efficient SOD, we did not stack more deformable convolutions, though it will be a more generic representation of input images into their corresponding saliency maps. But at the same time, the network's efficiency is compromised; hence such settings can only be adopted for applications having sufficient resources. In fact, this is the primary motivation for DDNet, to not entirely depend on deformable convolutions due to their complexity compared to regular convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Towards Optimal Salient Image Generation</head><p>The final block in DDNet comprises a series of transpose convolutions that pleasantly generates the image sized output with a single channel, i.e., the annotated saliency map of the corresponding input image. Some of the SOTA methods directly apply upsampling and ignore the edge information obtained via image smoothing. We achieve edge smoothing by transposing convolutions or deconvolutions and applying upsampling to resize the densely deformable block's features. Transpose convolutions in DDNet are employed using bi-linear interpolation that applies a weighted average on the distance of four corresponding nearest cells, thereby significantly smoothing the output. In our DDNet, there are three transpose convolutions and two upsampling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we explain the training and testing methods used to evaluate the performance of our DDNet and comparison with rival methods. We then introduce our newly collected dataset, S-SOD, and present various ablation studies to verify the effectiveness and efficiency of the final DDNet compared to SOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We use the PyTorch framework to implement DDNet, taking three channels input (RGB), generating a grayscale saliency map using a single NVIDIA GeForce RTX 2070 SUPER. Then, the model's dense blocks are initialized using pretrained ImageNet weights of the same model <ref type="bibr" target="#b12">[13]</ref>. Deformable convolutions are implemented with the settings mentioned in <ref type="bibr" target="#b29">[30]</ref>, performing better than the standard version of the deformable convolutions. We use an ADAM optimizer with a base learning rate of 1e −4 , where the training and testing images are resized to 224×224 using transformers during preprocessing. We use 16 mini-batch size to train DDNet for 500 epochs with MSE loss that takes around three hours and 0.0352 to 0.0473 seconds to test an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>Salient Person Dataset: Recently, Salient Person (SIP) dataset is created with 929 challenging images, and their corresponding ground truth masks and depths <ref type="bibr" target="#b5">[6]</ref>. We follow the training and testing split 1 by <ref type="bibr" target="#b5">[6]</ref> to compare our DDNet with 22 competing algorithms on this benchmark dataset.</p><p>A Novel Surveillance SOD Dataset: The current salient object detection models generate near-to-human saliency maps but are limited only to the data used for training and lack generalization capability, and perform poorly due to the absence of cross-dataset validation, referred to as open data evaluation in computer vision. We build a new dataset, S-SOD, with high-resolution images and are collected to cover the most challenging scenarios for SOD methods. The challenges in S-SOD include occlusion, changes in occurrences, multiple persons' appearance, highly variable lights, and variation in distance of persons from the surveillance camera, etc. make 1 https://github.com/DengPingFan/D3NetBenchmark it more generalized towards real-world data, ensuring adaptable, trained models. Most of the images are collected from the Chokepoint dataset <ref type="bibr" target="#b22">[23]</ref>, while the rest of the challenges with varied distance and complex background and foreground are covered using our own captured RGB images.</p><p>There are 70 high-resolution images annotated using the MATLAB 2020b image segmentor tool, and the binary annotations are confirmed by two computer vision experts after being created by an unbiased user in our laboratory. The images and ground truth labels are available in 512 × 512 size. The depth of our dataset is synthetically generated using an existing CNN-based method 2 . We aim to eliminate depth images' dependency while generating saliency, which has high affluence in mainstream existing SOD methods. Most realworld applications deal with RGB images only, such as action recognition and persons' interactions depending on humans' saliency i.e., their localization using saliency maps helps significantly in-accurate predictions. Sample images from our S-SOD are shown in <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Metrics</head><p>We use five metrics to compare DDNet with competing SOD algorithms. The training parameters in <ref type="table" target="#tab_0">Table 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-arts</head><p>Quantitative comparison: We analyze the results of the proposed DDNet against the most recent deep learning models as well as hand-crafted feature-based methods to show its dominance/superiority. DDNet achieves the lowest error rates and the highest E-M against recent competing algorithms. We surpass the best results achieved so far on the SIP dataset and determine a new SOTA SOD, as reported in <ref type="table" target="#tab_0">Table 1</ref>. The initial rows in <ref type="table" target="#tab_0">Table 1</ref> are hand-crafted feature-based methods for SOD, that are reported from recent works <ref type="bibr">[24; 25]</ref>. DDNet achieves the lowest MAE with a 0.02 decrease from the recent UCNet <ref type="bibr" target="#b24">[25]</ref>, and 0.02 margin against the baseline method <ref type="bibr" target="#b5">[6]</ref>. DDNet also scores the highest E-M against rivals, but the S-M and F-M for DDNet are not the best when compared to some recent methods <ref type="bibr">[25; 7]</ref>. It is worth mentioning that these methods take multiple inputs (RGB and depth) when generating saliency maps and our DDNet only depends on the RGB input, posing higher efficiency. Further, multiple inputs processing and their fusion significantly increase the computation cost, and a little noise in depth data adversely affects the saliency results.</p><p>Qualitative comparisons: In <ref type="figure" target="#fig_5">Figure 4</ref>, we visually compare DDNet with the most recent RGB-D saliency detection UC-Net <ref type="bibr" target="#b25">[26]</ref> over challenging images from the SIP dataset test set. We use publicly available codes and trained models of  UC-Net 4 to generate these results. <ref type="figure" target="#fig_5">Figure 4</ref> shows that UC-Net results include non-salient regions, particularly for challenging images. These images contain a complex background and occluded foregrounds with the salient object. DDNet predicts more precise saliency maps than others, indicating effectiveness and robustness. The ablation research results improve from left to right in <ref type="figure" target="#fig_5">Figure 4</ref>, where the Dilated DDNet and VGG16 backbone with SSIM loss extract comparatively better salient regions. We will publish the results, codes, and trained models at GitHub.  <ref type="table" target="#tab_1">Table 2</ref> show that densely connected convolutions for initial input image features representation perform better. Further, we also apply image smoothing with several parameter tuning strategies, but the saliency maps are not convincing enough. The results using efficient baselines, such as EfficientNet, are also reported in <ref type="table" target="#tab_1">Table 2</ref>, which has the lowest number of parameters, but the saliency maps have a lower match with the ground truth in testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study and Results on S-SOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different Loss Functions:</head><p>The loss function used in our final experiments is MSE, but we also experimented using binary cross-entropy (BCE) and implemented structural similarity index measurement (SSIM) negation as a loss function. The ablation results in <ref type="table" target="#tab_1">Table 2</ref> indicate that the best results are achieved using VGG16's initial two convolutional blocks, among other options under consideration.</p><p>Finally, the proposed DDNet's results on our own created S-SOD dataset are reported in <ref type="table" target="#tab_3">Table 3</ref>, indicating lower generalization abilities of our model. <ref type="table" target="#tab_3">Table 3</ref> indicates comparatively better results for DDNet and Dilated DDNet against VGG16 and ENet, but the numbers suggest that further potential research is needed for cross dataset SOD domain.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Directions</head><p>In this paper, we proposed a densely connected and deformable convolutions-based network for salient object detection. Distinct from existing SOD methods, we achieve fine SOD using only RGB data, reducing a model's dependency and extending its applicability in real-world scenarios. The proposed DDNet is inspired by the superior features representation potentials of deformable convolutions, where we proved to achieve more accurate detecting salient objects detection results without even using depth data. Initial low-level features in the proposed DDNet are extracted using dense convolutions that are delicately refined using our dense deformable block. Learning geometrical transforma-tion modeling using deformable blocks makes DDNet more generalized towards unseen data from a completely different environment. The final saliency is generated using transpose convolutions with bi-linear interpolation, and the results indicated the superior performance of DDNet against 22 competing SOTA methods on the SIP dataset. Even though DDNet achieved the best performance over the existing SIP dataset against rivals, its geometrical modeling and salient regions' representation is not enough to be generalized towards other domains. This aspect of SOD is alarming and attentionseeking for future research. In this direction, we introduced a very small-scaled benchmark S-SOD dataset that is collected using surveillance cameras. In the future, we aim to extend the challenges in S-SOD and increase the number of samples. Another direction is to propose an end-to-end model, inputting an RGB image, generating its corresponding depth, followed by the saliency maps generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Applying our DDNet over challenging test images from the SIP dataset, the last row shows proposed results. (a) The right hand of the person is half covered, correctly segmented by DDNet; (b) complex pose with half hand overlapping with the body; (c) half of the body is covered with another foreground and correctly differentiated using DDNet; (d) right foot covered behind another foreground object, accurately distinct-ed by DDNet; (e) even the fingers of the person are correctly segmented by DDNet, as given in the GT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sample images from our S-SOD dataset with a diverse set of challenges. (a) objects very far from the camera with likely foreground and background; (b) multiple objects with occlusion and varying distances from the camera; (c) side-wise object and multiple objects far from the camera; (d) far and side-wise objects with varying brightness scenario; (e) multiple objects with varied poses, directions, appearance ratio, and high-level occlusion. It should be noted that our method does not use depth images; we have only provided the depths for illustration purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The proposed DDNet for efficient saliency detection that inputs only RGB image and generates its corresponding saliency map with a single color channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>show the efficiency of our DDNet. The metrics are E-measure (E-M), S-measure (S-M), Weighted-F (W-F), F-measure (F-M), and Mean Absolute Error (MAE). A useful SOD model has larger E-M, S-M, W-F, F-M, and a smaller MAE. For a fair comparison, we use an existing MATLAB implementation 3 to compute these metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparison of saliency maps predicted by UC-Net, the proposed DDNet, and ablation models. Results improves from left to right, where DDNet outperforms others by predicting more consistent results with the ground truth (given as GT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results obtained over SIP dataset against SOTA methods. Best performance is shown as bold.</figDesc><table><row><cell>Models</cell><cell cols="6">Parameters E-Measure ↑ S-Measure ↑ Weighted-F ↑ F-Measure ↑ MAE ↓</cell></row><row><cell>[3]</cell><cell>-</cell><cell>0.770</cell><cell>0.616</cell><cell>-</cell><cell>0.669</cell><cell>0.298</cell></row><row><cell>[15]</cell><cell>-</cell><cell>0.614</cell><cell>0.732</cell><cell>-</cell><cell>0.542</cell><cell>0.172</cell></row><row><cell>[19]</cell><cell>-</cell><cell>0.759</cell><cell>0.653</cell><cell>-</cell><cell>0.657</cell><cell>0.185</cell></row><row><cell>[11]</cell><cell>-</cell><cell>0.592</cell><cell>0.628</cell><cell>-</cell><cell>0.515</cell><cell>0.164</cell></row><row><cell>[4]</cell><cell>-</cell><cell>0.598</cell><cell>0.683</cell><cell>-</cell><cell>0.499</cell><cell>0.186</cell></row><row><cell>[8]</cell><cell>-</cell><cell>0.651</cell><cell>0.727</cell><cell>-</cell><cell>0.571</cell><cell>0.200</cell></row><row><cell>[12]</cell><cell>-</cell><cell>0.705</cell><cell>0.716</cell><cell>-</cell><cell>0.608</cell><cell>0.139</cell></row><row><cell>[18]</cell><cell>-</cell><cell>0.565</cell><cell>0.653</cell><cell>-</cell><cell>0.464</cell><cell>0.185</cell></row><row><cell>[20]</cell><cell>-</cell><cell>0.645</cell><cell>0.717</cell><cell>-</cell><cell>0.568</cell><cell>0.167</cell></row><row><cell>[29]</cell><cell>-</cell><cell>0.893</cell><cell>0.850</cell><cell>-</cell><cell>0.821</cell><cell>0.064</cell></row><row><cell>[1]</cell><cell>-</cell><cell>0.870</cell><cell>0.835</cell><cell>-</cell><cell>0.803</cell><cell>0.075</cell></row><row><cell>[2]</cell><cell>-</cell><cell>0.845</cell><cell>0.833</cell><cell>-</cell><cell>0.833</cell><cell>0.086</cell></row><row><cell>[22]</cell><cell>-</cell><cell>0.793</cell><cell>0.720</cell><cell>-</cell><cell>0.702</cell><cell>0.118</cell></row><row><cell>[6]</cell><cell>-</cell><cell>0.909</cell><cell>0.860</cell><cell>-</cell><cell>0.861</cell><cell>0.063</cell></row><row><cell>[17]</cell><cell>-</cell><cell>0.891</cell><cell>0.840</cell><cell>-</cell><cell>0.829</cell><cell>0.070</cell></row><row><cell>[7]</cell><cell>-</cell><cell>0.922</cell><cell>0.879</cell><cell>-</cell><cell>0.883</cell><cell>0.085</cell></row><row><cell>[26]</cell><cell>-</cell><cell>0.914</cell><cell>0.875</cell><cell>-</cell><cell>0.867</cell><cell>0.051</cell></row><row><cell>[25] (w/o depth)</cell><cell>-</cell><cell>0.927</cell><cell>0.883</cell><cell>-</cell><cell>0.877</cell><cell>0.045</cell></row><row><cell>[25] (w depth)</cell><cell>-</cell><cell>0.927</cell><cell>0.883</cell><cell>-</cell><cell>0.877</cell><cell>0.045</cell></row><row><cell>[24]</cell><cell>-</cell><cell>0.906</cell><cell>0.879</cell><cell>-</cell><cell>0.868</cell><cell>0.055</cell></row><row><cell>[28]</cell><cell>-</cell><cell>0.913</cell><cell>0.883</cell><cell>-</cell><cell>0.873</cell><cell>0.052</cell></row><row><cell>[9]</cell><cell>-</cell><cell>0.923</cell><cell>0.879</cell><cell>-</cell><cell>0.885</cell><cell>0.051</cell></row><row><cell>DDNet (w/o DD)</cell><cell>3,334,829</cell><cell>0.917</cell><cell>0.844</cell><cell>0.762</cell><cell>0.785</cell><cell>0.050</cell></row><row><cell>DDNet (w DD)</cell><cell>3,334,829</cell><cell>0.935</cell><cell>0.863</cell><cell>0.797</cell><cell>0.813</cell><cell>0.043</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study over SIP dataset using various loss functions and baseline architectures.</figDesc><table><row><cell>Models</cell><cell cols="4">Loss E-M ↑ S-M ↑ F-M ↑ MAE ↓</cell></row><row><cell>DDNet (Dilated=5)</cell><cell>MSE</cell><cell>0.576</cell><cell>0.501 0.167</cell><cell>0.085</cell></row><row><cell>DDNet (Dilated=7)</cell><cell>MSE</cell><cell>0.771</cell><cell>0.718 0.604</cell><cell>0.116</cell></row><row><cell>E-Net</cell><cell>BCE</cell><cell>0.672</cell><cell>0.566 0.478</cell><cell>0.234</cell></row><row><cell>VGG16 (w smoothing)</cell><cell>MSE</cell><cell>0.882</cell><cell>0.821 0.816</cell><cell>0.049</cell></row><row><cell>VGG16-2B</cell><cell>MSE</cell><cell>0.916</cell><cell>0.838 0.828</cell><cell>0.046</cell></row><row><cell>VGG16</cell><cell cols="2">SSIM 0.506</cell><cell>0.441 0.223</cell><cell>0.207</cell></row><row><cell>VGG-2B</cell><cell cols="2">SSIM 0.507</cell><cell>0.414 0.190</cell><cell>0.220</cell></row><row><cell>VGG16</cell><cell>BCE</cell><cell>0.688</cell><cell>0.581 0.552</cell><cell>0.189</cell></row><row><cell>VGG-2B</cell><cell>BCE</cell><cell>0.592</cell><cell>0.460 0.376</cell><cell>0.247</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results of DDNet over the proposed S-SOD.</figDesc><table><row><cell>Models</cell><cell cols="3">E-M ↑ S-M ↑ F-M ↑ MAE ↓</cell></row><row><cell>VGG16 (w smoothing)</cell><cell>0.478</cell><cell>0.305 0.112</cell><cell>0.402</cell></row><row><cell>ENet (BCE)</cell><cell>0.254</cell><cell>0.061 0.087</cell><cell>0.900</cell></row><row><cell>DDNet (Dilated)</cell><cell>0.576</cell><cell>0.501 0.167</cell><cell>0.085</cell></row><row><cell>DDNet</cell><cell>0.595</cell><cell>0.488 0.176</cell><cell>0.108</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ialhashim/DenseDepth 3 https://github.com/jiwei0921/Saliency-Evaluation-Toolbox/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by the National Research Foundation of Korea (NRF) Grant funded by the Korea government (MSIT) (2019R1A2B5B01070067).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjian</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIMCS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Qingming Huang, Xiaochun Cao, and Chunping Hou. Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion. SPL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R3net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking rgb-d salient object detection: Models, data sets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TNNLS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bbs-net: Rgb-d salient object detection with a bifurcated backbone strategy network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local background enclosure for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mc-Carthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jl-dcf: Joint learning and densely-cooperative fusion framework for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Siamese network for rgb-d salient object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cnns-based rgb-d saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiview video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanveer</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hugo C De Albuquerque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">107567</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Icnet: Information conversion network for rgb-d based salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A crossmodal adaptive gated fusion generative adversarial network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rgbd salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqiong</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploiting global priors for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangke</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangling</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conflux lstms network: A novel approach for multi-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanveer</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">414</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adaptive fusion for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Access</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Patch-based probabilistic image quality assessment for face selection and improved video-based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Mau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conrad</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bifurcated backbone strategy for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><surname>Saleh</surname></persName>
		</author>
		<title level="m">Sadegh Aliakbarian, and Nick Barnes. Uncertainty inspired rgb-d saliency detection. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Uc-net: uncertainty inspired rgb-d saliency detection via conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh Sadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Select, supplement and focus for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkun</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Ping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<title level="m">Bilateral attention network for rgb-d salient object detection. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
