<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Adversarial Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yirong</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjun</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Dual Adversarial Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation aims at transferring knowledge from the labeled source domain to the unlabeled target domain. Previous adversarial domain adaptation methods mostly adopt the discriminator with binary or K-dimensional output to perform marginal or conditional alignment independently. Recent experiments have shown that when the discriminator is provided with domain information in both domains and label information in the source domain, it is able to preserve the complex multimodal information and high semantic information in both domains. Following this idea, we adopt a discriminator with 2K-dimensional output to perform both domain-level and class-level alignments simultaneously in a single discriminator. However, a single discriminator can not capture all the useful information across domains and the relationships between the examples and the decision boundary are rarely explored before. Inspired by multi-view learning and latest advances in domain adaptation, besides the adversarial process between the discriminator and the feature extractor, we also design a novel mechanism to make two discriminators pit against each other, so that they can provide diverse information for each other and avoid generating target features outside the support of the source domain. To the best of our knowledge, it is the first time to explore a dual adversarial strategy in domain adaptation. Moreover, we also use the semi-supervised learning regularization to make the representations more discriminative. Comprehensive experiments on two real-world datasets verify that our method outperforms several state-of-the-art domain adaptation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural network has achieved remarkable success in many applications <ref type="bibr" target="#b10">[11]</ref>. However, it requires a large amount of labeled data to train the model for a good generalization. Collecting and annotating sufficient data is very expensive and time-consuming. It is a natural idea to utilize annotated data from a similar domain to help improve the performance, which is the goal of transfer learning. Generally, transfer learning aims at leveraging knowledge from a labeled source domain to use in the unlabeled target domain <ref type="bibr" target="#b20">[21]</ref>. Domain adaptation is a sub-filed of transfer learning, in which the feature space and label space in the source domain and target domain are the same, but the data distribution is different <ref type="bibr" target="#b20">[21]</ref>.</p><p>It is crucial for domain adaptation to reduce the distribution discrepancy across domains <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>. Recently, many adversarial domain adaptation methods inspired by generative adversarial network <ref type="bibr" target="#b5">[6]</ref> have been proposed. Generally, there exists a two-player game between the domain discriminator and the feature extractor <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. The domain discriminator is trained to distinguish the source domain from the target domain, while the feature extractor is trained to learn domain-invariant representations to confuse the discriminator. These methods all focus on domain-level(marginal) distribution alignment, while they can not promise the class-level(conditional) distribution alignment. Then, methods for class-level alignment are proposed either by using separate class-wise domain discriminators, where each discriminator is responsible for only one class <ref type="bibr" target="#b21">[22]</ref>, or by using a discriminator with K-dimensional output, where K is the number of class <ref type="bibr" target="#b30">[31]</ref>. Despite the exciting performance, the domainlevel and class-level alignments are performed independently so the semantic information behind them can not be shared.</p><p>Moreover, some methods have explored both domain-level and class-level alignments in a single discriminator. Recent experiments have shown that the informative discriminator that accesses the domain information in both domains and class information in the source domain is able to preserve the complex multimodal information and high semantic information in both domains <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>. Instead of using a traditional discriminator with binary or K-dimensional output, many complex discriminators are designed. For example, a method using a discriminator with 2K-dimensional output is proposed in <ref type="bibr" target="#b2">[3]</ref>, where the first K-dimensional outputs are the known source classes, the last K-dimension outputs are the unknown target classes. This discriminator can distinguish the domain and class information of the training data simultaneously.</p><p>However, most existing methods adopt a single discriminator for distribution alignment, it is impossible to capture all the useful information to explore complex structure in the feature and label spaces. Besides, the previous methods do not consider the relationship between the examples and the decision boundary. As is described in MCD <ref type="bibr" target="#b23">[24]</ref> and shown in <ref type="figure" target="#fig_3">Figure 1</ref>(a), the samples existing far from the support of the source domain do not have discriminative features because they are not clearly categorized into certain classes.</p><p>Inspired by multi-view learning and the latest advances in domain adaptation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>, we design a mechanism to make two discriminators pit against each other to solve the above issues. On the one hand, the generating complementary information can help them to capture complex semantic information. On the other hand, two adversarial discriminators(classifiers) can obtain the features, in which the support of the target is included in that of the source. Then, in order to measure the similarity between the discriminators as well as to detect the ambiguous examples, we propose to utilize the disagreement of the two discriminators on the prediction for both domains. As shown in figure 1(b), we train the two discriminators to maximize the discrepancy which can detect the target samples excluded by the <ref type="figure" target="#fig_3">Figure 1</ref>: An example of two discriminators with an overview of the proposed method. Note that we use a discriminators with 2K-dimensional output, which can not only distinguish the domain label but also can classify the training data into a certain class. Discrepancy between discriminators refers to the disagreement between the prediction of two discriminators. In (a), we can see that the target samples outside the support of the source can be measured by two different discriminators. In (b), maximizing the discrepancy between the discriminators allows the two discriminators to capture different useful information, so that they can better detect the samples excluded by the support of the source. In (c), we train the feature extractor to minimize the discrepancy which can avoid generating target features outside the support of the source domain. Besides, domain-level and class-level alignments are performed to reduce the distribution discrepancy across domains. Best viewed in color. support of the source. As shown in <ref type="figure" target="#fig_3">Figure 1</ref>(c), we train the feature extractor to minimize the discrepancy which can avoid generating target features outside the support of the source domain.</p><p>In this paper, we focus on unsupervised domain adaptation, where the labeled source data and unlabeled target data are available. Following this line of work, we propose a method called Dual Adversarial Domain Adaptation (DADA). As shown in <ref type="figure">Figure 2</ref>, the proposed DADA consists of a feature extractor, a class predictor and two discriminators with 2K-dimensional output. Each discriminator learns a distribution over domain and class variables in an adversarial way, which can perform both domain-level and classlevel adaptation simultaneously in a single discriminator. Besides, inspired by multi-view learning and the latest advances in domain adaptation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>, we design a mechanism to make these two discriminators pit against each other, which can not only help them to learn complex semantic information from each other, but also help them to obtain the features, in which the support of the target is included in that of the source. Note that there is a dual adversarial process in DADA, which is, to the best of our knowledge, the first time to explore a dual adversarial strategy in domain adaptation. Since labels for the target data are not known, the class predictor is adopted to predict the pseudo labels for the target data. Furthermore, we use the semi-supervised learning(SSL) regularization to make the extracted features more discriminative. Comprehensive experiments on two real-world image datasets are conducted and the results verify the effectiveness of our proposed method.</p><p>Briefly, our contributions lie in three folds:</p><p>(1) The discriminator with 2K-dimensional output is adopted to perform both domain-level and class-level alignments simultaneously in a single discriminator. Moreover, the SSL regularization is used to make the representations more discriminative.</p><p>(2) We design a novel mechanism to make two discriminators pit against each other, which can make them provide diverse information for each other and avoid generating the target features outside the support of the source domain. To the best of our knowledge, it is the first time to perform a dual adversarial strategy in domain adaptation.</p><p>(3) We conduct extensive experiments on two real-world datasets and the results validate the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised domain adaptation is a sub-field of transfer learning, where there are abundant labeled data in the source domain and some unlabeled data in the target domain. Early studies focus on shallow (traditional) domain adaptation. Recently, more and more works pay attention to deep domain adaptation and adversarial domain adaptation.</p><p>Shallow domain adaptation The most common strategy in shallow learning is distribution alignment. The distribution discrepancy between domains includes marginal distribution discrepancy and conditional distribution discrepancy. TCA <ref type="bibr" target="#b19">[20]</ref> tries to align marginal distribution between domains, which learns a domain-invariant representation during feature mapping. Based on TCA, JDA <ref type="bibr" target="#b14">[15]</ref> tries to algin marginal distribution and conditional distribution simultaneously. Considering the balance between marginal distribution and conditional distribution discrepancy, BDA <ref type="bibr" target="#b28">[29]</ref> proposes a balance factor to leverage the importance of different distributions. MEDA <ref type="bibr" target="#b29">[30]</ref> is able to dynamically evaluate the balance factor and has achieved promising performance.</p><p>Deep domain adaptation Most deep domain adaptation methods are based on discrepancy measure. DDC <ref type="bibr" target="#b27">[28]</ref> embeds a domain adaptation layer into the network and minimizes Maximum Mean Discrepancy(MMD) between features of this layer. DAN <ref type="bibr" target="#b12">[13]</ref> minimizes the feature discrepancy between the last three layers and the mutil-kernel MMD is used to better approximate the discrepancy. Other measures are also adopted such as Kullback-Leibler (KL) divergence <ref type="bibr" target="#b34">[35]</ref>, Correlation Alignment (CORAL) <ref type="bibr" target="#b25">[26]</ref> and Central Moment Discrepancy (CMD) <ref type="bibr" target="#b32">[33]</ref>. These methods can utilize the deep neural network to extract more transferable features and also have achieved promising performance.</p><p>Adversarial domain adaptation Recently, adversarial learning is widely used in domain adaptation. DANN <ref type="bibr" target="#b4">[5]</ref> use a discriminator to distinguish the source data from the target data, while the the generator learns domain-invariant feature to confuse the discriminator. Based on the theory in <ref type="bibr" target="#b0">[1]</ref>, when maximizing the error of discriminator, it is actually approximating the H-distance, and minimizing the error of discriminator is actually minimizing the discrepancy between domains. ADDA <ref type="bibr" target="#b26">[27]</ref> designs a symmetrical structure where two feature extractors are adopted. Different from DANN, MCD <ref type="bibr" target="#b23">[24]</ref> Figure 2: Structure of DADA algorithm. Each joint discriminator distinguishes the domain and the class of the train data to perform both domain-level and class-level alignments simultaneously in a single discriminator, while the feature extractor learns domain-invariant representations to confuse the discriminator. Two joint discriminators are trained to pit against each other, so that they can provide diverse information for each other. Specially, there are dual adversarial processes in our algorithm. The class predictor is used to classify source examples as well as predict pseudo labels for the target data.</p><p>proposes a method to minimize the H∆H-distance between domains in an adversarial way. <ref type="bibr" target="#b33">[34]</ref> proposes a new theory using margin loss for mutli-class problem in adaptation, and based on this theory, MDD is proposed to minimize the disparity discrepancy between domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Setting</head><p>In unsupervised domain adaptation, we are given a source domain Ds = {(xi, yi)} ns i=1 of ns labeled source examples and a target domain Dt = {(xi)} n t i=1 of nt unlabeled target examples. The source data are drawn from the distribution P (xs, ys) and the target data are drawn from the distribution Q(xt, yt). Note that the i.i.d. assumption is violated, where P (xs, ys) = Q(xt, yt). Both distributions are defined on X × Y , where Y = {1, 2, ..., K}. The samples are drawn from marginal distribution(domain-level), where xs ∼ ps and xt ∼ qt. Our goal is to design a deep network x → f (x) to reduce the distribution discrepancy across domains in order that the generalization error t(f ) in the target domain can be bounded by source risk s(f ) plus the distribution discrepancy across domains <ref type="bibr" target="#b0">[1]</ref>, where</p><formula xml:id="formula_0">t(f ) = E (x t ,y t )∼Q [f (xt) = yt]<label>(1)</label></formula><formula xml:id="formula_1">s(f ) = E (xs,ys)∼P [f (xs) = ys]<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall</head><p>As is shown in <ref type="figure">Figure 2</ref>, DADA consists of a feature extractor G, a class predictor F and two joint discriminators D1, D2. Note that the output of the class predictor is K-dimensional while the output of the joint discriminator is 2K-dimensional. Each joint discriminator pits against the feature extractor with a 2K-way adversarial loss to learn a distribution over domain and classes variables, which can perform both domain-level and class-level alignments simultaneously in a single discriminator. The feature extractor G aims to learn the domain-invariant representations to confuse the two joint discriminators D1, D2 so that the domain discrepancy can be reduced. We also design a mechanism to make the two joint discriminators pit against each other so that they can benefit from complementary information. Two joint discriminators are trained to increase the discrepancy between the discriminators while the feature extractor helps to perform this adversarial process by minimizing the discrepancy between the discriminators. During the adversarial process, ambiguous target samples can be detected be pushed in the support of the source domain(section 3.5). Since the labels for the target data are unknown, it is impossible to perform class-level alignment directly. We use a class predictor F trained in the source domain to predict the pseudo labels for the target domain. To make the representations more discriminative and the pseudo labels more accurate, we introduce semi-supervised learning regularization, which uses the entropy minimization and Virtual Adversarial Training (VAT)(section 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Class Predictor Loss</head><p>The class predictor F is trained to classify the source samples correctly. During the training process, it is also used to predict pseudo labels for the target domain. The output of the class predictor can be written as,</p><formula xml:id="formula_2">f (x) = F (G(x)) ∈ R K<label>(3)</label></formula><p>We train the network to minimize the cross entropy loss. The source classification loss of class the predictor is as follows:</p><formula xml:id="formula_3">sc(F ) = E (xs,ys)∼P lCE(f (x), y) (4) CE (f (x), y) = − y, log f (x) (5)</formula><p>where, the cross-entropy loss is calculated with one-hot ground-truth labels y ∈ {0, 1} K and label estimates f (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Single Discriminator Loss</head><p>As described in <ref type="bibr" target="#b2">[3]</ref>, the joint discriminator is trained by a 2K-way adversarial loss. The first K are the known source classes, and the second K are the unknown target classes. Such a component can learn a distribution over domain and class variables, so it can perform both domain-level and class-level alignments in a single component. The output of the joint discriminator, taking D1 as an example, can be written as,</p><formula xml:id="formula_4">fD 1 (x) = D1(G(x)) ∈ R 2K<label>(6)</label></formula><p>For the labeled source examples, we also train the two joint discriminators with the same classification loss . The source classification loss of the joint discriminator is defined as,</p><formula xml:id="formula_5">dsc (D1) = E (xs,ys)∼P lCE(D1(G(xs)), [ys, 0])<label>(7)</label></formula><p>where 0 is the zero vector of size K, chosen to make the last K joint probabilities zero for the source samples. Similarly, to capture the label information in the target domain, we also train the discriminators using the target examples. Since the labels for the target data are not known, we use pseudo labels instead. For a target example xt, its predicted label according to the class predictor is 7ŷ</p><formula xml:id="formula_6">= arg max k f (xt)[k]<label>(8)</label></formula><p>The target classification loss of the joint discriminator is,</p><formula xml:id="formula_7">dtc (D1) = Ex t ∼q t lCE(D1(xt), [0,ŷt])<label>(9)</label></formula><p>Here, it is assumed that the source-only model can achieve reasonable performance in the target domain. In experiments, where the source-only model has poor performance initially, we use this loss after training the class predictor for a period of time.</p><p>The feature extractor G is designed to confuse the joint discriminators as in DANN <ref type="bibr" target="#b4">[5]</ref>. The basis idea is that the feature extractor can confuse the joint discriminator with the domain information, but keep label information unchanged. For example, for a source example xs with label ys, the correct output of the joint discriminator should be [ys, 0], while the feature extractor fools the joint discriminator to classify it in the target domain but also using the label ys, which is [0, ys] formally.</p><p>Formally, the source alignment loss of the joint discriminator is,</p><formula xml:id="formula_8">dsa1 (G) = E (xs,ys)∼P lCE(D1(G(xs)), [0, ys])<label>(10)</label></formula><p>Similarly, the target alignment loss of joint discriminator is defined by changing the pseudo-label from [0,ŷt] to [ŷt, 0],</p><formula xml:id="formula_9">dta1 (G) = Ex t ∼q t lCE(D1(G(xt)), [ŷt, 0])<label>(11)</label></formula><p>The last two losses are minimized only by the feature extractor G.</p><p>The same adversarial process is also applied in joint discriminator D2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Adversarial Loss Between Discriminators</head><p>Inspired by multi-view learning <ref type="bibr" target="#b31">[32]</ref> and the latest advances in domain adatation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>, we find that two or more similar but different components can produce diverse information so that these components can learn form each other to improve performance. In this paper, we design a mechanism to make these two discriminators pit against each other so that they can benefit from generating complementary information. Moreover, two adversarial discriminators(classifiers) can obtain the features, in which the support of the target is included in that of the source. In order to measure the similarity between the discriminators, we propose to utilize the disagreement of the two discriminators on the prediction for both domains. The discrepancy between the two joint discriminators is defined by utilizing the absolute values of the difference between the probabilistic output as discrepancy loss:</p><formula xml:id="formula_10">d(fD 1 (x), fD 2 (x)) = 1 K K k=1 |fD 1 (x)[k] − fD 2 (x)[k]|<label>(12)</label></formula><p>We firstly train the discriminators to increase their discrepancy. It can not only help different discriminators to capture different information, but also detect the target samples excluded by the support of the source <ref type="bibr" target="#b23">[24]</ref>. The objective is as follows,</p><formula xml:id="formula_11">max D 1 ,D 2 d (13) d =Ex s ∼ps [d(fD 1 (xs), fD 2 (xs))] +Ex t ∼q t [d(fD 1 (xt), fD 2 (xt))]<label>(14)</label></formula><p>Moreover, the feature extractor is trained to minimize the discrepancy for fixed discriminators. On the one hand, minimizing the discrepancy can make these two joint discriminators not too far away from each other, thus making them similar. On the other hand, minimizing the discrepancy can avoid generating target features outside the support of the source domain <ref type="bibr" target="#b23">[24]</ref>. The objective is as follows,</p><formula xml:id="formula_12">min G d<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">SSL Regularization Loss</head><p>After the distribution alignment, the discrepancy across domains can be smaller. In this case, we can approximate the unsupervised domain adaptation as a semi-supervised learning problem. On this bias, many previous works have explored semi-supervised learning(SSL) regularization in domain adaptation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> and made sufficient improvements. Although lacking of labels, a large amount of unlabeled data can be used to bias the classifier boundaries to pass through the regions containing low density data. Thus, the learned representation can become more discriminative. Entropy minimization is a widely used regularization method to achieve this goal. In our method, the class predictor is also trained to minimize the target entropy loss, which is defined as follows,</p><formula xml:id="formula_13">te(F ) = E (x t ,y t )∼q E (f (x))<label>(16)</label></formula><p>where</p><formula xml:id="formula_14">E (f (x)) = − k f (x)[k] · log f (x)[k]</formula><p>. However, minimizing entropy is only applicable to locally-Lipschitz classifiers <ref type="bibr" target="#b18">[19]</ref>. So we propose to explicitly incorporate the locally-Lipschitz condition via virtual adversarial training(VAT) and add the following losses to the objective, </p><formula xml:id="formula_15">tvat(F ) = Ex t ∼q t [ max ||r||≤ CE (fi(xt)||fi(xt + r))]<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Overall Objective</head><p>We combine the objective functions discussed in section3.4-3.6 and divide our training produce into three steps.</p><p>Step 1 We only use the source data to train the feature extractor G, the class predictor F as well as the joint discriminators D1, D2.  There are three steps in total, step 2 and step 3 are shown in this figure. In step 2, the class predictor and two discriminators minimize the classification loss. Besides, the two discriminators pit against each other to increase the discrepancy between discriminators. In step 3, the feature extractor learns to minimize the discrepancy between discriminators as well as to confuse the discriminator in both domain and class level.</p><p>We minimize the source classification loss of the class predictor and joint discriminators. After the classifier and joint discriminators are trained to classify the source samples correctly, we will go on the next step. The objective of this step is shown as follows,</p><formula xml:id="formula_17">min G,F,D 1 ,D 2 sc(F ) + λ dsc1 dsc (D1) + λ dsc2 dsc (D2)<label>(19)</label></formula><p>Step 2 We fix the feature extractor, and update the class predictor and the discriminators. We use both the source and target data to update the model. This process corresponds to Step 2 in <ref type="figure" target="#fig_2">Figure 3</ref>. We have three sub-objectives. The first one is to minimize the source and target classification loss of the joint discriminators. The second one is to minimize the source classification of the class predictor as well as the SSL regularization loss. Without this loss, we experimentally found that the performance dropped. The last one is to increase the discrepancy between the discriminators. The objective of this step is shown as follows,</p><formula xml:id="formula_18">min F,D 1 ,D 2 F + D 1 + D 2 − λ d d (20) D 1 = λ dsc1 dsc (D1) + λ dtc1 dtc (D1)<label>(21)</label></formula><formula xml:id="formula_19">D 2 = λ dsc2 dsc (D2) + λ dtc2 dtc (D2) (22) F = sc(F ) + λsvat svat(F ) + λte te(F ) +λtvat tvat(F )<label>(23)</label></formula><p>Step 3 We fix the class predictor and the discriminators, and update the feature extractor. We train the model by minimizing the source and target alignment loss of joint discriminators as well as the discrepancy between discriminators. The objective of this step is shown as follows,</p><formula xml:id="formula_20">min G λ dsa1 dsa1 (G) + λ dta1 dta1 (G) + λ dsa2 dsa2 (G)+ λ dta2 dta2 (G) + λ d d<label>(24)</label></formula><p>Step2 and Step3 are repeated alternately in our method. We are concerned on that the feature extractor , class predictor and joint discriminators are trained in an adversarial manner so that they can classify the source samples correctly as well as promote the cross-domain discrepancy decreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Theoretical Understanding</head><p>Most existing domain adaptation methods are based on the domain adaptation theory proposed in <ref type="bibr" target="#b0">[1]</ref>. The generalization error in the target domain t(h) is bounded by three terms: (1) the expected error The binary class discriminator is used in DANN to distinguish where the train data come from(source or target), if it is not able to classify the data correctly, the cross-domain discrepancy can be reduced. In fact, it makes the dH∆H (S, T ) become small. However, an example in <ref type="bibr" target="#b8">[9]</ref> shows that even the dH∆H (S, T ) is zero, there also exists discrepancy across domains. Because if the conditional distribution between domains is not matched, the third term λ can be large. In our method, the joint discriminators can align the conditional distribution across domains, so λ will be small. Besides, two adversarial discriminators can provide diverse information including label formation for each other, so it is helpful to match the conditional distribution better. Moreover, predicting pseudo labels for the target data may cause some mistakes, while the SSL regularization can help correct some errors, thus also making it better to match the conditional distribution across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed method with many state-of-the-art domain adaptation methods on two image datasets. Codes will be available at https://github.com/yaoyueduzhen/DADA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Office-31 <ref type="bibr" target="#b22">[23]</ref> is the most widely used dataset for domain adaptation, with 4,652 images and 31 categories collected from three distinct domains: Amazon (A), Webcam (W) and DSLR (D). From this dataset, we build six transfer tasks:</p><formula xml:id="formula_21">A → W, D → W, W → D, A → D, D → A, and W → A.</formula><p>ImageCLEF-DA <ref type="bibr" target="#b15">[16]</ref> is a dataset organized by selecting the 12 common classes shared by three public datasets (domains): Caltech-256 (C), ImageNet ILSVRC 2012 (I), and Pascal VOC 2012 (P). We evaluate all methods on six transfer tasks: I → P, P → I, I → C, C → I, C → P, and P → C.</p><p>We compare Dual Adversarial Domain Adaptation (DADA) with several state-of-the-art domain adaptation methods: Deep Adaptation Network (DAN) <ref type="bibr" target="#b12">[13]</ref>, Domain Adversarial Neural Network (DANN) <ref type="bibr" target="#b4">[5]</ref>, Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b26">[27]</ref>, Multi-Adversarial Domain Adaptation (MADA) <ref type="bibr" target="#b21">[22]</ref>, Virtual Adversarial Domain Adaptation (VADA) <ref type="bibr" target="#b16">[17]</ref>, Generate to Adapt(GTA) <ref type="bibr" target="#b24">[25]</ref>, Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b23">[24]</ref>, Conditional Domain Adversarial Network (CDAN) <ref type="bibr" target="#b13">[14]</ref>, Transferable Adversarial Training (TAT) <ref type="bibr" target="#b11">[12]</ref> and Regularized Conditional Alignment (RCA) <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Following the standard protocols for unsupervised domain adaptation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, all labeled source samples and unlabeled target samples participate in the training stage. We compare the average classification accuracy based on five random experiments. The results of other methods are reported in the corresponding papers except RCA which is reimplemented by ourselves.</p><p>We use PyTorch to implement our method and use ResNet-50[7] pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref> as the feature extractor. The class predictor and two discriminators are both two-layer fully connected net-works with a width of 1024. We train these new layers and feature extractor using back-propagation, where the learning rates of these new layers are 10 times that of the feature extractor. We adopt mini-batch SGD with the momentum of 0.9 and use the same learning rate annealing strategy as <ref type="bibr" target="#b4">[5]</ref>: the learning rate is adjusted by ηp = η0(1 + αp) −β , where p is the training progress changing from 0 to 1, and η0 = 0.04, α = 10, β = 0.75.</p><p>We fix λ d = 1.0 and search the rest hyperparameters over λ dsc1 , λ dsc2 ∈ {0.1, 0.5, 1.0}, λ dtc1 , λ dtc2 ∈ {0.1, 1.0, 10.0}, λsvat, λtvat ∈ {0.0, 0.1, 1.0}, λte ∈ {0.1, 1.0}, λ dsa1 , λ dta1 , λ dsa2 , λ dta2 ∈ {0.1, 1.0}. We also search for the upper bound of the adversarial perturbation in VAT, where ∈ {0.5, 1.0, 2.0}. The optimal hyperparameters on Office-31 dataset are shown in <ref type="table" target="#tab_2">Table 3</ref>.  </p><formula xml:id="formula_22">.0 λte 0.1 0.1 0.1 0.1 1.0 1.0 λ dsa1 0.1 0.1 0.1 0.1 0.1 0.1 λ dta1 0.1 0.1 0.1 0.1 0.1 0.1 λ dsa2 0.1 0.1 0.1 0.1 0.1 0.1 λ dta2 0.1 0.1 0.1 0.1 0.1 0.1 0.5</formula><p>0.5 0.5 0.5 0.5 0.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The results on Office-31 dataset are shown in <ref type="table" target="#tab_0">Table 1</ref>. As we can see, our method outperforms baseline methods in most tasks and achieves the best result with average accuracy. Compared with DANN and ADDA, which only perform domain-level alignments using a binary class discriminator, our method performs not only domain-level but also class-level alignments and outperforms them. MADA considers domain-level and class-level alignment, but it constrains each discriminator to be responsible for only one class. Our method avoids this limitation by adopting 2K-dimensional discriminators where the classes can share information. The 2K-dimensional joint discriminator is also used in RCA, but we train two discriminators in an adversarial manner so that they can provide complementary information for each other. Moreover, we clearly observe that our method can also perform well on D→A and W→A with relatively large domain shift and imbalanced domain scales.</p><p>The results on ImageCLEF-DA are shown in <ref type="table" target="#tab_1">Table 2</ref>. We have several findings based on the results. Firstly, all methods are better than ResNet-50, which is a source-only model trained without exploiting the target data in a standard supervised learning setting. Our method increases the accuracy from 80.7% to 89.3%. Secondly, the above comparisons with baseline methods on Office-31 are also the same on ImageCLEF-DA, which verifies the effectiveness of our method. Thirdly, DADA outperforms the baseline methods on most transfer tasks, but with less room for improvement. This is reasonable since the three domains in ImageCLEF-DA are of equal size and balanced in each category, which makes domain adaptation easy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Ablation Study. We study the effect of SSL regularization by removing entropy minimization and VAT losses from our method(λte = λsvat = λtvat = 0), which is denoted by DADA(without SSL). The results on Office-31 dataset are reported in <ref type="table" target="#tab_0">Table 1</ref>. Results show that without SSL regularization, our method can perform better in two tasks(A→W, W→D) than baseline methods , but the average accuracy of all tasks is decreased by 1.0% compared to the proposed DADA. The results validate the effectiveness of SSL regularization.</p><p>Feature Visualization. In <ref type="figure">Figure 4</ref>, we visualize the feature representations of task A→W(31 classes) by t-SNE <ref type="bibr" target="#b17">[18]</ref> using the sourceonly method and DADA. The source-only method is trained without exploiting the target training data in a standard supervised learning setting using the same learning procedure. As we can see, source and target samples are better aligned for DADA than the source-only method. This shows the advance of our method in discriminative prediction.</p><p>(a) Source Only (b) Adapted(Ours) <ref type="figure">Figure 4</ref>: Visualization of features obtained from the feature extractor of task A→W using t-SNE <ref type="bibr" target="#b17">[18]</ref>.Red and blue points indicate the source and target samples respectively. We can see that applying our method makes the target samples more discriminative.</p><p>Distribution Discrepancy. The A-distance is a measure of distribution discrepancy, defined as distA = 2(1 − 2 ), where is the test error of a classifier trained to distinguish the source from the target. We use A-distance as a measure of the transferability of feature representations. <ref type="table" target="#tab_3">Table 4</ref> shows the cross-domain A-distance for tasks A→W, W→D. We compute the A-distance of our method based on the output of the feature extractor G, which turns out to be the smallest of all methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a method called Dual Adversarial Domain Adaptation(DADA), which is able to performe both domain-level and class-level alignments simultaneously in a single discriminator. Besides, inspired by mutil-view learning, we design a novel mechanism to make two discriminators pit against each other, and encourage them to learn different semantic information to benefit from each other. Moreover, SSL regularization is used to make the representations more discriminative so that the predicted pseudo labels can be more accurate. We conduct comprehensive experiments and the results verify the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>svat(F ) = Ex s ∼ps [ max ||r||≤ CE (fi(xs)||fi(xs + r))]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Adversarial training steps of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 1</head><label>1</label><figDesc>s(h) in the source domain, (2) the H∆H-distance dH∆H (S, T ) between domains, measuring the disagreement of two hypothesis h, h ∈ H∆H and (3) the error λ of the ideal hypothesis h * in both domains. The main Theorem in [1] is shown as below, Let H be the hypothesis space, then for any h in the hypothesis space, t(h) ≤ s(h) + 1 2 dH∆H (Xs, Xt) + λ (25) where dH∆H (Ds, Dt) = 2 sup h,h ∈H |P rx∼X s [h(x) = h (x)] −P rx∼X t [h(x) = h (x)]| h * = arg min h∈H s(h) + t(h), λ = s(h * ) + t(h * )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracy (%) on Office-31 for unsupervised domain adaptation with ResNet-50.</figDesc><table><row><cell>Method</cell><cell>A→W</cell><cell>D→W</cell><cell>W→D</cell><cell>A→D</cell><cell>D→A</cell><cell>W→A</cell><cell>Avg</cell></row><row><cell>ResNet-50[7]</cell><cell>68.4±0.2</cell><cell>96.7±0.1</cell><cell>99.3±0.1</cell><cell>68.9±0.2</cell><cell>62.5±0.3</cell><cell>60.7±0.3</cell><cell>76.1</cell></row><row><cell>DAN[13]</cell><cell>80.5±0.4</cell><cell>97.1±0.2</cell><cell>99.6±0.1</cell><cell>78.6±0.2</cell><cell>63.6±0.3</cell><cell>62.8±0.2</cell><cell>80.4</cell></row><row><cell>DANN[5]</cell><cell>82.6±0.4</cell><cell>96.9±0.2</cell><cell>99.3±0.2</cell><cell>81.5±0.4</cell><cell>68.4±0.5</cell><cell>67.5±0.5</cell><cell>82.7</cell></row><row><cell>ADDA[27]</cell><cell>86.2±0.5</cell><cell>96.2±0.3</cell><cell>98.4±0.3</cell><cell>77.8±0.3</cell><cell>69.5±0.4</cell><cell>68.9±0.5</cell><cell>82.9</cell></row><row><cell>MADA[22]</cell><cell>90.0±0.1</cell><cell>97.4±0.1</cell><cell>99.6±0.1</cell><cell>87.8±0.2</cell><cell>70.3±0.3</cell><cell>66.4±0.3</cell><cell>85.2</cell></row><row><cell>VADA[17]</cell><cell>86.5±0.5</cell><cell>98.2±0.4</cell><cell>99.7±0.2</cell><cell>86.7±0.4</cell><cell>70.1±0.4</cell><cell>70.5±0.4</cell><cell>85.4</cell></row><row><cell>GTA[25]</cell><cell>89.5±0.5</cell><cell>97.9±0.3</cell><cell>99.7±0.2</cell><cell>87.7±0.5</cell><cell>72.8±0.3</cell><cell>71.4±0.4</cell><cell>86.5</cell></row><row><cell>MCD[24]</cell><cell>88.6±0.2</cell><cell>98.5±0.1</cell><cell>100.0±.0</cell><cell>92.2±0.2</cell><cell>69.5±0.1</cell><cell>69.7±0.3</cell><cell>86.5</cell></row><row><cell>RCA[3]</cell><cell>93.8±0.2</cell><cell>98.4±0.1</cell><cell>100.0±.0</cell><cell>91.6±0.2</cell><cell>68.0±0.2</cell><cell>70.2±0.2</cell><cell>87.0</cell></row><row><cell>CDAN[14]</cell><cell>93.1±0.1</cell><cell>98.6±0.1</cell><cell>100.0±.0</cell><cell>92.9±0.2</cell><cell>71.0±0.3</cell><cell>69.3±0.3</cell><cell>87.5</cell></row><row><cell>DADA(without SSL)</cell><cell>94.0±0.2</cell><cell>98.4±0.2</cell><cell>100.0±.0</cell><cell>91.6±0.2</cell><cell>68.3±0.1</cell><cell>69.7±0.2</cell><cell>87.0</cell></row><row><cell>DADA</cell><cell>94.5±0.2</cell><cell>98.7±0.2</cell><cell>100.0±.0</cell><cell>93.6±0.3</cell><cell>69.5±0.3</cell><cell>71.5±0.2</cell><cell>88.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy (%) on ImageCLEF-DA for unsupervised domain adaptation with ResNet-50.</figDesc><table><row><cell>Method</cell><cell>I→P</cell><cell>P→I</cell><cell>I→C</cell><cell>C→I</cell><cell>C→P</cell><cell>P→C</cell><cell>Avg</cell></row><row><cell>ResNet-50[7]</cell><cell>74.8±0.3</cell><cell>83.9±0.1</cell><cell>91.5±0.3</cell><cell>78.0±0.2</cell><cell>65.5±0.3</cell><cell>91.2±0.3</cell><cell>80.7</cell></row><row><cell>DAN[13]</cell><cell>74.5±0.4</cell><cell>82.2±0.2</cell><cell>92.8±0.2</cell><cell>86.3±0.4</cell><cell>69.2±0.4</cell><cell>89.8±0.4</cell><cell>82.5</cell></row><row><cell>DANN[5]</cell><cell>75.0±0.3</cell><cell>86.0±0.3</cell><cell>96.2±0.4</cell><cell>87.0±0.5</cell><cell>74.3±0.5</cell><cell>91.5±0.6</cell><cell>85.0</cell></row><row><cell>MADA[22]</cell><cell>75.0±0.3</cell><cell>87.9±0.2</cell><cell>96.0±0.3</cell><cell>88.8±0.3</cell><cell>75.2±0.2</cell><cell>92.2±0.3</cell><cell>85.8</cell></row><row><cell>CDAN[14]</cell><cell>76.7±0.3</cell><cell>90.6±0.3</cell><cell>97.0±0.4</cell><cell>90.5±0.4</cell><cell>74.5±0.3</cell><cell>93.5±0.4</cell><cell>87.1</cell></row><row><cell>TAT[12]</cell><cell>78.8±0.2</cell><cell>92.0±0.2</cell><cell>97.5±0.3</cell><cell>92.0±0.3</cell><cell>78.2±0.4</cell><cell>94.7±0.4</cell><cell>88.9</cell></row><row><cell>RCA[3]</cell><cell>78.7±0.2</cell><cell>92.8±0.2</cell><cell>97.7±0.3</cell><cell>92.0±0.2</cell><cell>77.0±0.3</cell><cell>95.0±0.3</cell><cell>88.9</cell></row><row><cell>DADA</cell><cell>79.0±0.3</cell><cell>93.2±0.2</cell><cell>98.2±0.2</cell><cell>92.3±0.2</cell><cell>77.8±0.3</cell><cell>95.0±0.3</cell><cell>89.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Optimal hyperparameters on Office-31 dataset.</figDesc><table /><note>Tasks A→W D→W W→D A→D D→A W→A</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Cross-domain A-distance of different approaches.</figDesc><table><row><cell>Method</cell><cell>D→W</cell><cell>W→A</cell></row><row><cell>ResNet-50[7]</cell><cell>1.27</cell><cell>1.86</cell></row><row><cell>DANN[5]</cell><cell>1.23</cell><cell>1.44</cell></row><row><cell>MCD[24]</cell><cell>1.22</cell><cell>1.60</cell></row><row><cell>DADA</cell><cell>1.14</cell><cell>1.18</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Nanjing University, China, duyuntao@smail.nju.edu.cn 2 Nanjing University, China, yaoyueduzhen@outlook.com 3 Nanjing University, China, chenqian@smail.nju.edu.cn 4 Nanjing University, China, zhangxw@smail.nju.edu.cn 5 Nanjing University, China, ytxmailg@gmail.com 6 Nanjing University, China, chjwang@nju.edu.cn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use the notation x[k] for indexing the value at the kth index of the vector x</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation via regularized conditional alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safa</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="59" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Duplex generative adversarial network for unsupervised domain adaptation&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Co-regularized alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kahini</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogério</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">W</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Looking back at labels: A class based domain adaptation technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Kumar Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><forename type="middle">P</forename><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transferable adversarial training: A general approach to adapting deep classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">199</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8503" to="8512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Balanced distribution adaptation for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1129" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adversarial domain adaptation being aware of class relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanqing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A survey on multiview learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1304.5634</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno>abs/1702.08811</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>abs/1904.05801</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Supervised representation learning: Transfer learning with deep autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Sinno Jialin Pan, and Qing He</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
