<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Denoise Distantly-Labeled Data for Entity Typing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
							<email>yasumasa@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
							<email>gdurrett@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Denoise Distantly-Labeled Data for Entity Typing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Distantly-labeled data can be used to scale up training of statistical models, but it is typically noisy and that noise can vary with the distant labeling technique. In this work, we propose a two-stage procedure for handling this type of data: denoise it with a learned model, then train our final model on clean and denoised distant data with standard supervised training. Our denoising approach consists of two parts. First, a filtering function discards examples from the distantly labeled data that are wholly unusable. Second, a relabeling function repairs noisy labels for the retained examples. Each of these components is a model trained on synthetically-noised examples generated from a small manually-labeled set. We investigate this approach on the ultrafine entity typing task of <ref type="bibr" target="#b0">Choi et al. (2018)</ref>. Our baseline model is an extension of their model with pre-trained ELMo representations, which already achieves state-of-the-art performance. Adding distant data that has been denoised with our learned models gives further performance gains over this base model, outperforming models trained on raw distant data or heuristically-denoised distant data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rise of data-hungry neural network models, system designers have turned increasingly to unlabeled and weakly-labeled data in order to scale up model training. For information extraction tasks such as relation extraction and entity typing, distant supervision <ref type="bibr" target="#b21">(Mintz et al., 2009</ref>) is a powerful approach for adding more data, using a knowledge base <ref type="bibr" target="#b2">(Del Corro et al., 2015;</ref><ref type="bibr" target="#b25">Rabinovich and Klein, 2017)</ref> or heuristics <ref type="bibr" target="#b28">(Ratner et al., 2016;</ref><ref type="bibr" target="#b8">Hancock et al., 2018)</ref> to automatically label instances. One can treat this data just like any other supervised data, but it is noisy; more effective approaches employ specialized probabilistic models <ref type="bibr" target="#b31">(Riedel et al., 2010;</ref><ref type="bibr" target="#b26">Ratner et al., 2018a)</ref>, capturing its interaction with other supervision <ref type="bibr" target="#b37">(Wang and Poon, 2018)</ref> or breaking down aspects of a task on which it is reliable <ref type="bibr" target="#b27">(Ratner et al., 2018b)</ref>. However, these approaches often require sophisticated probabilistic inference for training of the final model. Ideally, we want a technique that handles distant data just like supervised data, so we can treat our final model and its training procedure as black boxes.</p><p>This paper tackles the problem of exploiting weakly-labeled data in a structured setting with a two-stage denoising approach. We can view a distant instance's label as a noisy version of a true underlying label. We therefore learn a model to turn a noisy label into a more accurate label, then apply it to each distant example and add the resulting denoised examples to the supervised training set. Critically, the denoising model can condition on both the example and its noisy label, allowing it to fully leverage the noisy labels, the structure of the label space, and easily learnable correspondences between the instance and the label.</p><p>Concretely, we implement our approach for the task of fine-grained entity typing, where a single entity may be assigned many labels. We learn two denoising functions: a relabeling function takes an entity mention with a noisy set of types and returns a cleaner set of types, closer to what manually labeled data has. A filtering function discards examples which are deemed too noisy to be useful. These functions are learned by taking manuallylabeled training data, synthetically adding noise to it, and learning to denoise, similar to a conditional variant of a denoising autoencoder <ref type="bibr" target="#b36">(Vincent et al., 2008)</ref>. Our denoising models embed both entities and labels to make their predictions, mirroring the structure of the final entity typing model itself.</p><p>We evaluate our model following <ref type="bibr" target="#b0">Choi et al. (2018)</ref>. We chiefly focus on their ultra-fine en-  <ref type="bibr" target="#b38">(Xiong et al., 2019)</ref> and matching the performance of a BERT model <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> on this task. Finally, we show that denoising helps even when the label set is projected onto the OntoNotes label set <ref type="bibr" target="#b13">(Hovy et al., 2006;</ref><ref type="bibr" target="#b5">Gillick et al., 2014)</ref>, outperforming the method of <ref type="bibr" target="#b0">Choi et al. (2018)</ref> in that setting as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setup</head><p>We consider the task of predicting a structured target y associated with an input x. Suppose we have high-quality labeled data of n (input, target) pairs D = { x (1) , y (1) , . . . , (x (n) , y (n) )}, and noisily labeled data of n (input, target) pairs</p><formula xml:id="formula_0">D = {(x (1) , y (1) noisy ), . . . , (x (n ) , y<label>(n )</label></formula><p>noisy )}. For our tasks, D is collected through manual annotation and D is collected by distant supervision. We use two models to denoise data from D : a filtering function f disposes of unusable data (e.g., mislabeled examples) and a relabeling function g transforms the noisy target labels y noisy to look more like true labels. This transformation improves the noisy data so that we can use it to D without introducing damaging amounts of noise. In the second stage, a classification model is trained on the augmented data (D combined with denoised D ) and predicts y given x in the inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Case Study: Ultra-Fine Entity Typing</head><p>The primary task we address here is the finegrained entity typing task of <ref type="bibr" target="#b0">Choi et al. (2018)</ref>. Instances in the corpus are assigned types from a vocabulary of more than 10,000 types, which are divided into three classes: 9 general types, 121 finegrained types, and 10, 201 ultra-fine types. This dataset consists of 6K manually annotated examples and approximately 25M distantly-labeled examples. 5M examples are collected using entity linking (EL) to link mentions to Wikipedia and gather types from information on the linked pages. 20M examples (HEAD) are generated by extracting nominal head words from raw text and treating these as singular type labels. <ref type="figure">Figure 1</ref> shows examples from these datasets which illustrate the challenges in automatic annotation using distant supervision. The manuallyannotated example in (a) shows how numerous the gold-standard labeled types are. By contrast, the HEAD example (b) shows that simply treating the head word as the type label, while correct in this case, misses many valid types, including more general types. The EL example (c) is incorrectly annotated as region, whereas the correct coarse type is actually person. This error is characteristic of entity linking-based distant supervision since identifying the correct link is a challenging problem in and of itself <ref type="bibr" target="#b20">(Milne and Witten, 2008)</ref>: in this case, Gascoyne is also the name of a region in Western Australia. The EL example in (d) has reasonable types; however, human annotators could choose more types (grayed out) to describe the mention more precisely. The average number of types annotated by humans is 5.4 per example while the two distant supervision techniques combined yields 1.5 types per example on average.</p><p>In summary, distant supervision can (1) produce completely incorrect types, and (2) systematically miss certain types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Denoising Model</head><p>To handle the noisy data, we propose to learn a denoising model as shown in <ref type="figure">Figure 2</ref>. This denoising model consists of filtering and relabeling functions to discard and relabel examples, respectively; these rely on a shared mention encoder and type encoder, which we describe in the following sections. The filtering function is a binary classifier that takes these encoded representations and predicts whether the example is good or bad. The relabeling function predicts a new set of labels for the given example. We learn these functions in a supervised fashion. Training data for each is created through synthetic noising processes applied to the manuallylabeled data, as described in Sections 3.3 and 3.4. For the entity typing task, each example (x, y) takes the form ((s, m), t), where s is the sentence, m is the mention span, and t is the set of types (either clean or noisy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mention Encoder</head><p>This encoder is a function Φ m (s, m) which maps a sentence s and mention m to a real-valued vector v m . This allows the filtering and relabeling function to recognize inconsistencies between the given example and the provided types. Note that these inputs s and m are the same as the inputs for the supervised version of this task; we can therefore share an encoder architecture between our denoising model and our final typing model. We use an encoder following <ref type="bibr" target="#b0">Choi et al. (2018)</ref> with a few key differences, which are described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Type Encoder</head><p>The second component of our model is a module which produces a vector v t = Φ t (t). This is an encoder of an unordered bag of types. Our basic type encoder uses trainable vectors as embeddings for each type and combines these with summing. That is, the noisy types t 1 , . . . , t m are embedded into type vectors {t 1 , . . . , t m }. The final embedding of the type set t = j t j .</p><p>Type Definition Encoder Using trainable type embeddings exposes the denoising model to potential data sparsity issues, as some types appear only a few or zero times in the training data. Therefore, we also assign each type a vector based on its definition in WordNet <ref type="bibr" target="#b19">(Miller, 1995)</ref>. Even low-frequent types are therefore assigned a plausible embedding. <ref type="bibr">1</ref> Let w j i denote the ith word of the jth type's most common WordNet definition. Each w j i is embedded using GloVe <ref type="bibr" target="#b23">(Pennington et al., 2014)</ref>. The resulting word embedding vectors w j i are fed into a bi-LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Graves and Schmidhuber, 2005)</ref>, and a concatenation of the last hidden states in both directions is used as the definition representation w j . The final representation of the definitions is the sum over these vectors for each type: w = k w k . Our final v t = [t; w], the concatenation of the type and definition embedding vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Filtering Function</head><p>The filtering function f is a binary classifier designed to detect examples that are completely mislabeled. Formally, f is a function mapping a labeled example (s, m, t) to a binary indicator z of whether this example should be discarded or not.</p><p>In the forward computation, the feature vectors v m and v t are computed using the mention and type encoders. The model prediction is defined as</p><formula xml:id="formula_1">P (error) = σ u Highway ([v m ; v t ])</formula><p>, where σ is a sigmoid function, u is a parameter vector, and Highway(·) is a 1-layer highway network <ref type="bibr" target="#b33">(Srivastava et al., 2015)</ref>. We can apply f to each distant pair in our distant dataset D and discard any example predicted to be erroneous (P (error) &gt; 0.5).</p><p>Training data We do not know a priori which examples in the distant data should be discarded, and labeling these is expensive. We therefore construct synthetic training data D error for f based on the manually labeled data D. For 30% of the examples in D, we replace the gold types for that example with non-overlapping types taken from another example. The intuition for this procedure follows <ref type="figure">Figure 1</ref>: we want to learn to detect examples in the distant data like Gascoyne where heuristics like entity resolution have misfired and given a totally wrong label set.</p><p>Formally, for each selected example ((s, m), t), we repeatedly draw another example ((s , m ), t ) from D until we find t error that does not have any common types with t. We then create a positive training example ((s, m, t error ), z = 1). We create a negative training example ((s, m, t), z = 0) using the remaining 70% of examples. f is trained on D error using binary cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relabeling Function</head><p>The relabeling function g is designed to repair examples that make it through the filter but which still have errors in their type sets, such as missing types as shown in <ref type="figure">Figure 1b</ref> and 1d. g is a function from a labeled example (s, m, t) to an improved type sett for the example.</p><p>Our model computes feature vectors v m and v t by the same procedure as the filtering function f . The decoder is a linear layer with parameters D ∈</p><formula xml:id="formula_2">R |V t |×(dm+dt) . We compute e = σ (D [v m ; v t ]),</formula><p>where σ is an element-wise sigmoid operation designed to give binary probabilities for each type.</p><p>Once g is trained, we make a predictiont for each (s, m, t) ∈ D and replace t byt to create the denoised data D denoise = {(s, m,t), . . . }. For the final prediction, we choose all types t where e &gt; 0.5, requiring at least two types to be present or else we discard the example. Training data We train the relabeling function g on another synthetically-noised dataset D drop generated from the manually-labeled data D. To mimic the type distribution of the distantly-labeled examples, we take each example (s, m, t) and randomly drop each type with a fixed rate 0.7 independent of other types to produce a new type set t . We perform this process for all examples in D and create a noised training set D drop , where a single training example is ((s, m, t ), t). g is trained on D drop with a binary classification loss function over types used in <ref type="bibr" target="#b0">Choi et al. (2018)</ref>, described in the next section.</p><p>One can think of g as a type of denoising autoencoder <ref type="bibr" target="#b36">(Vincent et al., 2008</ref>) whose reconstructed typest are conditioned on v as well as t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Typing Model</head><p>In this section, we define the sentence and mention encoder Φ m , which is use both in the denoising model as well as in the final prediction task. We extend previous attention-based models for this task <ref type="bibr" target="#b32">(Shimaoka et al., 2017;</ref><ref type="bibr" target="#b0">Choi et al., 2018)</ref>. At a high level, we have an instance encoder Φ m that returns a vector v m ∈ R d Φ , then multiply the output of this encoding by a matrix and apply a sigmoid to get a binary prediction for each type as a probability of that type applying. <ref type="figure" target="#fig_0">Figure 3</ref> outlines the overall architecture of our typing model. The encoder Φ m consists of four vectors: a sentence representation s, a wordlevel mention representation m word , a characterlevel mention representation m char , and a headword mention vector m head . The first three of these were employed by <ref type="bibr" target="#b0">Choi et al. (2018)</ref>. We have modified the mention encoder with an additional bi-LSTM to better encode long mentions, and additionally used the headword embedding directly in order to focus on the most critical word. These pieces use pretrained contextualized word embeddings (ELMo) <ref type="bibr" target="#b24">(Peters et al., 2018)</ref> as input.  <ref type="formula" target="#formula_5">2018)</ref>, we learn task specific parameters γ task ∈ R and s task ∈ R 3 governing these embeddings. We do not fine-tune the parameters of the ELMo LSTMs themselves.</p><p>Sentence Encoder Following Choi et al. <ref type="formula" target="#formula_5">(2018)</ref>, we concatenate the mth word vector s m in the sentence with a corresponding location embedding m ∈ R d loc . Each word is assigned one of four location tokens, based on whether (1) the word is in the left context, (2) the word is the first word of the mention span, (3) the word is in the mention span (but not first), and (4) the word is in the right context. The input vectors [s ; ] are fed into a bi-LSTM encoder, with hidden dimension is d hid , followed by a span attention layer <ref type="bibr" target="#b15">(Lee et al., 2017;</ref><ref type="bibr" target="#b0">Choi et al., 2018)</ref>:</p><formula xml:id="formula_3">s = Attention(bi-LSTM([s ; l]))</formula><p>, where s is the final representation of the sentence s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention Encoder</head><p>To obtain a mention representation, we use both word and character information. For the word-level representation, the mention's contextualized word vectors m are fed into a bi-LSTM with hidden dimension is d hid .</p><p>The concatenated hidden states of both directions are summed by a span attention layer to form the word-level mention representation: m word = Attention(bi-LSTM(m )). Second, a character-level representation is computed for the mention. Each character is embedded and then a 1-D convolution <ref type="bibr" target="#b1">(Collobert et al., 2011)</ref> is applied over the characters of the mention. This gives a character vector m char .</p><p>Finally, we take the contextualized word vector of the headword m head as a third component of our representation. This can be seen as a residual connection <ref type="bibr" target="#b9">(He et al., 2016)</ref> specific to the mention head word. We find the headwords in the mention spans by parsing those spans in isolation using the spaCy dependency parser <ref type="bibr" target="#b12">(Honnibal and Johnson, 2015)</ref>. Empirically, we found this to be useful on long spans, when the span attention would often focus on incorrect tokens.</p><p>The final representation of the input x is a concatenation of the sentence, the word-&amp; characterlevel mention, and the mention headword repre-</p><formula xml:id="formula_4">sentations, v = s; m word ; m char ; m head ∈ R d Φ .</formula><p>Decoder We treat each label prediction as an independent binary classification problem. Thus, we compute a score for each type in the type vocabulary V t . Similar to the decoder of the relabeling function g, we compute e = σ (Ev), where E ∈ R |V t |×d Φ and e ∈ R |V t | . For the final prediction, we choose all types t where e &gt; 0.5. If none of e is greater than 0.5, we choose t = arg max e (the single most probable type).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>We use the same loss function as <ref type="bibr" target="#b0">Choi et al. (2018)</ref> for training. This loss partitions the labels in general, fine, and ultra-fine classes, and only treats an instance as an example for types of the class in question if it contains a label for that class. More precisely:</p><formula xml:id="formula_5">L =L general 1 general (t) + L fine 1 fine (t) + L ultra-fine 1 ultra-fine (t),<label>(1)</label></formula><p>where L ... is a loss function for a specific type class: general, fine-grained, or ultra-fine, and 1 ... (t) is an indicator function that is active when one of the types t is in the type class. Each L ... is a sum of binary cross-entropy losses over all types in that category. That is, the typing problem is viewed as independent classification for each type.</p><p>Note that this loss function already partially repairs the noise in distant examples from missing labels: for example, it means that examples from HEAD do not count as negative examples for general types when these are not present. However, we show in the next section that this is not sufficient for denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>The settings of hyperparameters in our model largely follows <ref type="bibr" target="#b0">Choi et al. (2018)</ref> and recommendations for using the pre-trained ELMo-Small model. <ref type="bibr">2</ref> The word embedding size d ELMo is 1024. The type embedding size and the type definition embedding size are set to 1024. For most of other model hyperparameters, we use the same settings as <ref type="bibr" target="#b0">Choi et al. (2018)</ref>: d loc = 50, d hid = 100, d char = 100. The number of filters in the 1-d convolutional layer is 50. Dropout is applied with p = 0.2 for the pretrained embeddings, and p = 0.5 for the mention representations. We limit sentences to 50 words and mention spans to 20 words for computational reasons. The character CNN input is limited to 25 characters; most mentions are short, so this still captures subword information in most cases. The batch size is set to 100. For all experiments, we use the Adam optimizer (Kingma and Ba, 2014). The initial learning rate is set to 2e-03. We implement all models 3 using PyTorch. To use ELMo, we consult the AllenNLP source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Ultra-Fine Entity Typing We evaluate our approach on the ultra-fine entity typing dataset from <ref type="bibr" target="#b0">Choi et al. (2018)</ref>. The 6K manually-annotated English examples are equally split into the training, development, and test examples by the authors of the dataset. We generate syntheticallynoised data, D error and D drop , using the 2K training set to train the filtering and relabeling functions, f and h. We randomly select 1M EL and 1M HEAD examples and use them as the noisy data D . Our augmented training data is a combination of the manually-annotated data D and D denoised .</p><p>OntoNotes In addition, we investigate if denoising leads to better performance on another dataset. We use the English OntoNotes dataset <ref type="bibr" target="#b5">(Gillick et al., 2014)</ref>, which is a widely used benchmark for fine-grained entity typing systems. The original training, development, and test splits contain 250K, 2K, and 9K examples respectively. <ref type="bibr" target="#b0">Choi et al. (2018)</ref> created an augmented training set that has 3.4M examples. We also construct our own augmented training sets with/without denoising using our noisy data D , using the same label mapping from ultra-fine types to OntoNotes types described in <ref type="bibr" target="#b0">Choi et al. (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ultra-Fine Typing Results</head><p>We first compare the performance of our approach to several benchmark systems, then break down the improvements in more detail. We use the model architecture described in Section 4 and train it on the different amounts of data: manually labeled only, naive augmentation (adding in the raw distant data), and denoised augmentation. We compare our model to <ref type="bibr" target="#b0">Choi et al. (2018)</ref> as well as to BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, which we finetuned for this task. We adapt our task to BERT by forming an input sequence "[CLS] sentence [SEP] mention [SEP]" and assign the segment embedding A to the sentence and B to the mention span. 4 Then, we take the output vector at the position of the [CLS] token (i.e., the first token) as the feature vector v, analogous to the usage for sentence pair classification tasks. The BERT model is fine-tuned on the 2K manually annotated examples. We use the pretrained BERT-Base, uncased model 5 with a step size of 2e-05 and batch size 32.</p><p>Results <ref type="table">Table 1</ref> compares the performance of these systems on the development set. Our model with no augmentation already matches the system of <ref type="bibr" target="#b0">Choi et al. (2018)</ref> with augmentation, and incorporating ELMo gives further gains on both precision and recall. On top of this model, adding the distantly-annotated data lowers the performance; the loss function-based approach of <ref type="bibr" target="#b0">(Choi et al., 2018)</ref> does not sufficiently mitigate the noise in this data. However, denoising makes the distantlyannotated data useful, improving recall by a substantial margin especially in the general class. A possible reason for this is that the relabeling function tends to add more general types given finer types. BERT performs similarly to ELMo with denoised distant data. As can be seen in the performance breakdown, BERT gains from improvements in recall in the fine class. <ref type="table">Table 2</ref> shows the performance of all settings on the test set, with the same trend as the performance on the development set. Our approach outperforms the concurrently-published <ref type="bibr" target="#b38">Xiong et al. (2019)</ref>; however, that work does not use ELMo. Their improved model could be used for both de-  <ref type="table">Table 1</ref>: Macro-averaged P/R/F1 on the dev set for the entity typing task of <ref type="bibr" target="#b0">Choi et al. (2018)</ref> comparing various systems. ELMo gives a substantial improvement over baselines. Over an ELMo-equipped model, data augmentation using the method of <ref type="bibr" target="#b0">Choi et al. (2018)</ref> gives no benefit. However, our denoising technique allow us to effectively incorporate distant data, matching the results of a BERT model on this task <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>.</p><p>noising as well as prediction in our setting, and we believe this would stack with our approach. Usage of Pretrained Representations Our model with ELMo trained on denoised data matches the performance of the BERT model. We experimented with incorporating distant data (raw and denoised) in BERT, but the fragility of BERT made it hard to incorporate: training for longer generally caused performance to go down after a while, so the model cannot exploit large external data as effectively. <ref type="bibr" target="#b3">Devlin et al. (2018)</ref> prescribe training with a small batch size and very specific step sizes, and we found the model very sensitive to these hyperparameters, with only 2e-05 giving strong results. The ELMo paradigm of incorporating these as features is much more flexible and modular in this setting. Finally, we note that our approach could use BERT for denoising as well, but this did not work better than our current approach. Adapting BERT to leverage distant data effectively is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Comparing Denoising Models</head><p>We now explicitly compare our denoising approach to several baselines. For each denoising method, we create the denoised EL, HEAD, and EL &amp; HEAD dataset and investigate performance on these datasets. Any denoised dataset is combined with the 2K manually-annotated examples and used to train the final model. Heuristic Baselines These heuristics target the same factors as our filtering and relabeling functions in a non-learned way. SYNONYMS AND HYPERNYMS For each type observed in the distant data, we add its synonyms and hypernyms using WordNet <ref type="bibr" target="#b19">(Miller, 1995)</ref>. This is motivated by the data construction process in <ref type="bibr" target="#b0">Choi et al. (2018)</ref>.  <ref type="table">Table 2</ref>: Macro-averaged P/R/F1 on the test set for the entity typing task of <ref type="bibr" target="#b0">Choi et al. (2018)</ref>. Our denoising approach gives substantial gains over naive augmentation and matches the performance of a BERT model. COMMON TYPE PAIRS We use type pair statistics in the manually labeled training data. For each base type that we observe in a distant example, we add any type which is seen more than 90% of the time the base type occurs. For instance, the type art is given at least 90% of the times the film type is present, so we automatically add art whenever film is observed.</p><p>OVERLAP We train a model on the manuallylabeled data only, then run it on the distantlylabeled data. If there is an intersection between the noisy types t and the predicted typet, we combine them and use as the expanded typet. Inspired by tri-training <ref type="bibr" target="#b42">(Zhou and Li, 2005)</ref>, this approach adds "obvious" types but avoids doing so in cases where the model has likely made an error.</p><p>Results  <ref type="table" target="#tab_4">Table 3</ref>: Macro-averaged P/R/F1 on the dev set for the entity typing task of <ref type="bibr" target="#b0">Choi et al. (2018)</ref> with various types of augmentation added. The customized loss from <ref type="bibr" target="#b0">Choi et al. (2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">OntoNotes Results</head><p>We compare our different augmentation schemes for deriving data for the OntoNotes standard as well.  of providing correct general types. In the EL setting, this yields 730k usable examples out of 1M (vs 540K for no denoising), and in HEAD, 640K out of 1M (vs. 73K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of Denoised Labels</head><p>To understand what our denoising approach does to the distant data, we analyze the behavior of our filtering and relabeling functions.  tions, so those labels often do not include ultrafine types. The filtering function discards similar numbers of examples for the EL and HEAD data: 9.4% and 10% respectively. <ref type="figure">Figure 4</ref> shows examples of the original noisy labels and the denoised labels produced by the relabeling function. In example (a), taken from the EL data, the original labels, {location, city}, are correct, but human annotators might choose more types for the mention span, Minneapolis. The relabeling function retains the original types about the geography and adds ultra-fine types about administrative units such as {township, municipality}. In example (b), from the HEAD data, the original label, {dollar}, is not so expressive by itself since it is a name of a currency. The labeling function adds coarse types, {object, currency}, as well as specific types such as {medium of exchange, monetary unit}. In another EL example (c), the relabeling function tries to add coarse and fine types but struggles to assign multiple diverse ultra-fine types to the mention span Michelangelo, possibly because some of these types rarely cooccur (painter and poet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Past work on denoising data for entity typing has used multi-instance multi-label learning <ref type="bibr">Schütze, 2015, 2017;</ref><ref type="bibr" target="#b22">Murty et al., 2018)</ref>. One view of these approaches is that they delete noisily-introduced labels, but they cannot add them, or filter bad examples. Other work focuses on learning type embeddings <ref type="bibr" target="#b41">(Yogatama et al., 2015;</ref><ref type="bibr">Ren et al., 2016a,b)</ref>; our approach goes beyond this in treating the label set in a structured way. The label set of <ref type="bibr" target="#b0">Choi et al. (2018)</ref> is distinct in not being explicitly hierarchical, making past hierarchical approaches difficult to apply.</p><p>Denoising techniques for distant supervision have been applied extensively to relation extraction. Here, multi-instance learning and probabilis-  <ref type="figure">Figure 4</ref>: Examples of the noisy labels (left) and the denoised labels (right) for mentions (bold). The colors correspond to type classes: general (purple), finegrained (green), and ultra-fine (yellow).</p><p>tic graphical modeling approaches have been used <ref type="bibr" target="#b31">(Riedel et al., 2010;</ref><ref type="bibr" target="#b11">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b34">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b35">Takamatsu et al., 2012)</ref> as well as deep models <ref type="bibr" target="#b17">(Lin et al., 2016;</ref><ref type="bibr" target="#b18">Luo et al., 2017;</ref><ref type="bibr" target="#b16">Lei et al., 2018;</ref><ref type="bibr" target="#b7">Han et al., 2018)</ref>, though these often focus on incorporating signals from other sources as opposed to manually labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we investigated the problem of denoising distant data for entity typing tasks. We trained a filtering function that discards examples from the distantly labeled data that are wholly unusable and a relabeling function that repairs noisy labels for the retained examples. When distant data is processed with our best denoising model, our final trained model achieves state-of-the-art performance on an ultra-fine entity typing task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Pretrained Embeddings Tokens in the sentence s are converted into contextualized word vectors Sentence and mention encoder used to predict types. We compute attention over LSTM encodings of the sentence and mention, as well as using characterlevel and head-word representations to capture additional mention properties. These combine to form an encoding which is used to predict types. using ELMo; let s i ∈ R d ELM o denote the embedding of the ith word. As suggested inPeters  et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>According to the review aggregator Rotten Tomatoes , 89 % of critics gave [the film] positive reviews. film, movie, show, art, entertainment, creation [The film] is based on a hit London and New York play , which was based on a best-selling book. film, movie, show, art, entertainment, creation Rafael Nadal] on Monday, in a rain-delayed U.S. Open final.</figDesc><table><row><cell>(a)</cell><cell>(c)</cell><cell>"A pretty good day all round," said [Gascoyne , a</cell></row><row><cell></cell><cell></cell><cell>British veteran of stints with the original Tyrrell</cell></row><row><cell></cell><cell></cell><cell>team] in a roller-coaster F1 career.</cell></row><row><cell></cell><cell></cell><cell>region</cell></row><row><cell></cell><cell></cell><cell>person ✔</cell></row><row><cell>(b)</cell><cell>(d)</cell><cell>Djokovic lost to [player, tennis player, champion, achiever,</cell></row><row><cell></cell><cell></cell><cell>winner, contestant, person, athlete</cell></row><row><cell cols="3">Figure 1: Examples selected from the Ultra-Fine Entity Typing dataset of Choi et al. (2018). (a) A manually-</cell></row><row><cell cols="3">annotated example. (b) The head word heuristic functioning correctly but missing types in (a). (c) Entity linking</cell></row><row><cell cols="3">providing the wrong types. (d) Entity linking providing correct but incomplete types.</cell></row><row><cell>tity typing scenario and use the same two distant</cell><cell></cell><cell></cell></row><row><cell>supervision sources as them, based on entity link-</cell><cell></cell><cell></cell></row><row><cell>ing and head words. On top of an adapted model</cell><cell></cell><cell></cell></row><row><cell>from Choi et al. (2018) incorporating ELMo (Pe-</cell><cell></cell><cell></cell></row><row><cell>ters et al., 2018), naïvely adding distant data ac-</cell><cell></cell><cell></cell></row><row><cell>tually hurts performance. However, when our</cell><cell></cell><cell></cell></row><row><cell>learned denoising model is applied to the data,</cell><cell></cell><cell></cell></row><row><cell>performance improves, and it improves more than</cell><cell></cell><cell></cell></row><row><cell>heuristic denoising approaches tailored to this</cell><cell></cell><cell></cell></row><row><cell>dataset. Our strongest denoising model gives a</cell><cell></cell><cell></cell></row><row><cell>gain of 3 F</cell><cell></cell><cell></cell></row></table><note>1 absolute over the ELMo baseline, and a 4.4 F 1 improvement over naive incorporation of distant data. This establishes a new state-of- the-art on the test set, outperforming concurrently published work</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>+ + Rafael Nadal Good Bad ✕ tennis player winner athlete baseball player person Yes Yes Yes No Yes ✕</head><label></label><figDesc></figDesc><table><row><cell>Filter</cell><cell>Relabel</cell><cell></cell></row><row><cell></cell><cell>player</cell><cell cols="2">A person who participates in or is skilled at some game</cell></row><row><cell>Djokovic lost to [Rafael Nadal] on Monday, ...</cell><cell cols="2">tennis_player</cell><cell>An athlete who plays tennis</cell></row><row><cell cols="4">Figure 2: Denoising models. The Filter model predicts whether the example should be kept at all; if it is kept,</cell></row><row><cell cols="4">the Relabel model attempts to automatically expand the label set. Φ m is a mention encoder, which can be a</cell></row><row><cell cols="4">state-of-the-art entity typing model. Φ t encodes noisy types from distant supervision.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>28.1 37.3 69.3 77.3 73.0 47.9 35.4 40.7 48.9 12.6 20.0 Ours + ELMo w augmentation 55.2 26.4 35.7 69.4 72.0 70.7 46.6 38.5 42.2 48.7 10.3 17.1 Ours + ELMo w augmentation 50.7 33.1 40.1 66.9 80.7 73.2 41.7 46.2 43.8 45.6 17.4 25.2</figDesc><table><row><cell></cell><cell></cell><cell>Total</cell><cell></cell><cell></cell><cell>General</cell><cell></cell><cell></cell><cell>Fine</cell><cell></cell><cell></cell><cell>Ultra-Fine</cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Ours + GloVe w/o augmentation</cell><cell cols="12">46.4 23.3 31.0 57.7 65.5 61.4 41.3 31.3 35.6 42.4 9.2 15.1</cell></row><row><cell cols="2">Ours + ELMo w/o augmentation 55.6 + filter &amp; relabel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-Base, Uncased</cell><cell cols="12">51.6 32.8 40.1 67.4 80.6 73.4 41.6 54.7 47.3 46.3 15.6 23.4</cell></row><row><cell cols="13">Choi et al. (2018) w augmentation 48.1 23.2 31.3 60.3 61.6 61.0 40.4 38.4 39.4 42.8 8.8 14.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>compares the results on the de-</cell></row><row><cell>velopment set. We report the performance on each</cell></row><row><cell>of the EL &amp; HEAD, EL, and HEAD dataset. On</cell></row><row><cell>top of the baseline ORIGINAL, adding synonyms</cell></row><row><cell>and hypernyms by consulting external knowledge</cell></row><row><cell>does not improve the performance. Expanding</cell></row><row><cell>labels with the PAIR technique results in small</cell></row><row><cell>gains over ORIGINAL. OVERLAP is the most ef-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="2">Acc. Ma-F1 Mi-F1</cell></row><row><cell cols="2">Ours + ELMo w/o augmentation 42.7 72.7</cell><cell>66.7</cell></row><row><cell>Ours + ELMo w augmentation</cell><cell>59.3 76.5</cell><cell>70.7</cell></row><row><cell>Ours + ELMo w augmentation</cell><cell>63.9 84.5</cell><cell>78.9</cell></row><row><cell>+ filter &amp; relabel</cell><cell></cell><cell></cell></row><row><cell>Ours + ELMo w augmentation</cell><cell>64.9 84.5</cell><cell>79.2</cell></row><row><cell>by Choi et al. (2018)</cell><cell></cell><cell></cell></row><row><cell>BERT-Base, Uncased</cell><cell>51.8 76.6</cell><cell>69.1</cell></row><row><cell>Shimaoka et al. (2017)</cell><cell>51.7 70.9</cell><cell>64.9</cell></row><row><cell>AFET (Ren et al., 2016a)</cell><cell>55.1 71.1</cell><cell>64.7</cell></row><row><cell>PLE (Ren et al., 2016b)</cell><cell>57.2 71.5</cell><cell>66.1</cell></row><row><cell>Choi et al. (2018)</cell><cell>59.5 76.8</cell><cell>71.8</cell></row><row><cell cols="2">LABELGCN (Xiong et al., 2019) 59.6 77.8</cell><cell>72.2</cell></row><row><cell>lists the results on the OntoNotes test</cell><cell></cell><cell></cell></row><row><cell>set following the adaptation setting of Choi et al.</cell><cell></cell><cell></cell></row><row><cell>(2018). Even on this dataset, denoising signifi-</cell><cell></cell><cell></cell></row><row><cell>cantly improves over naive incorporation of dis-</cell><cell></cell><cell></cell></row><row><cell>tant data, showing that the denoising approach is</cell><cell></cell><cell></cell></row><row><cell>not just learning quirks of the ultra-fine dataset.</cell><cell></cell><cell></cell></row><row><cell>Our augmented set is constructed from 2M seed</cell><cell></cell><cell></cell></row><row><cell>examples while Choi et al. (2018) have a more</cell><cell></cell><cell></cell></row><row><cell>complex procedure for deriving augmented data</cell><cell></cell><cell></cell></row><row><cell>from 25M examples. Ours (total size of 2.1M) is</cell><cell></cell><cell></cell></row><row><cell>on par with their larger data (total size of 3.4M),</cell><cell></cell><cell></cell></row><row><cell>despite having 40% fewer examples. In this set-</cell><cell></cell><cell></cell></row><row><cell>ting, BERT still performs well but not as well as</cell><cell></cell><cell></cell></row><row><cell>our model with augmented training data.</cell><cell></cell><cell></cell></row><row><cell>One source of our improvements from data aug-</cell><cell></cell><cell></cell></row><row><cell>mentation comes from additional data that is able</cell><cell></cell><cell></cell></row><row><cell>to be used because some OntoNotes type can be</cell><cell></cell><cell></cell></row><row><cell>derived. This is due to denoising doing a better job</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Test results on OntoNotes. Denoising helps substantially even in this reduced setting. Using fewer distant examples, we nearly match the performance using the data from Choi et al. (2018) (see text).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>reports the average numbers of types added/deleted by the relabeling function and the ratio of examples discarded by the filtering function.Overall, the relabeling function tends to add more and delete fewer number of types. The HEAD examples have more general types added than the EL examples since the noisy HEAD labels are typically finer. Fine-grained types are added to both EL and HEAD examples less frequently. Ultra-fine examples are frequently added to both datasets, with more added to EL; the noisy EL labels are mostly extracted from Wikipedia defini-</figDesc><table><row><cell></cell><cell>General</cell><cell>Fine</cell><cell>Ultra-Fine</cell></row><row><cell cols="4">Data Add Del Add Del Add Del Filter (%)</cell></row><row><cell>EL</cell><cell cols="3">0.87 0.01 0.36 0.17 2.03 0.12</cell><cell>9.4</cell></row><row><cell cols="4">HEAD 1.18 0.00 0.51 0.01 1.15 0.16</cell><cell>10.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>The average number of types added or deleted by the relabeling function per example. The right-most column shows that the rate of examples discarded by the filtering function.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>... play their home games at Target Center in[Minneapolis]. (a) location, city location, place, city, country, area, region, township, town, municipality ... Vittoria was influenced also by[Michelangelo]  ... The dollar] has been rising , pushing commodities lower ...</figDesc><table><row><cell>(b)</cell><cell>[dollar</cell><cell cols="2">object, currency, money, medium of exchange, dollar, monetary unit</cell></row><row><cell>(c)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">architect, sculptor, painter, poet</cell><cell>person, artist, writer</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We found this technique to be more effective than using pretrained vectors from GloVe or ELMo. It gave small improvements on an intrinsic evaluation over not incorporating it; results are omitted due to space constraints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://allennlp.org/elmo 3 The code for experiments is available at https:// github.com/yasumasaonoe/DenoiseET</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We investigated several approaches, including taking the head word piece from the last layer and using that for classification (more closely analogous to what<ref type="bibr" target="#b3">Devlin et al. (2018)</ref> did for NER), but found this one to work best. 5 https://github.com/google-research/ bert</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">One possible reason for this is identifying stray word senses; film can refer to the physical photosensitive object, among other things.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NSF Grant IIS-1814522, NSF Grant SHF-1762299, a Bloomberg Data Science Grant, and an equipment grant from NVIDIA. The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing HPC resources used to conduct this research. Results presented in this paper were obtained using the Chameleon testbed supported by the National Science Foundation. Thanks as well to the anonymous reviewers for their thoughtful comments, members of the UT TAUR lab and Pengxiang Cheng for helpful discussion, and Eunsol Choi for providing the full datasets and useful resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ultra-Fine Entity Typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">FINET: Context-Aware Fine-Grained Named Entity Typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Del Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalghani</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Effective Deep Memory Networks for Distant Supervised Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjie</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Context-Dependent Fine-Grained Entity Type Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno>abs/1412.1820</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2005.06.042</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Denoising Distant Supervision for Relation Extraction via Instance-Level Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1805.10959</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Training Classifiers with Natural Language Explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Braden</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paroma</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Improved Non-monotonic Transition System for Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">OntoNotes: The 90% Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2006 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end Neural Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cooperative Denoising for Distantly Supervised Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural Relation Extraction with Selective Attention over Instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/219717.219748</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to Link with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Radovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-Grained Entity Typing with High-Multiplicity Assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Snorkel: Rapid Training Data Creation with Weak Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Training Complex Models with Multi-Task Weak Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Braden</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyash</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno>abs/1810.02840</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data Programming: Creating Large Training Sets, Quickly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling Relations and Their Mentions without Labeled Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural Architectures for Fine-grained Entity Type Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Highway Networks. CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-instance Multi-label Learning for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reducing Wrong Labels in Distant Supervision for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shingo</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Corpus-level Fine-grained Entity Typing Using Contextual Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Embedding Methods for Fine Grained Entity Type Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tri-Training: Exploiting Unlabeled Data Using Three Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2005.186</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1529" to="1541" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
