<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-time Monaural Speech Enhancement With Short-time Discrete Cosine Transform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinglong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixin</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichi</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">Real-time Monaural Speech Enhancement With Short-time Discrete Cosine Transform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech enhancement</term>
					<term>deep learning</term>
					<term>discrete cosine transform</term>
					<term>real-valued spectrum</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech enhancement algorithms based on deep learning have been improved in terms of speech intelligibility and perceptual quality greatly. Many methods focus on enhancing the amplitude spectrum while reconstructing speech using the mixture phase. Since the clean phase is very important and difficult to predict, the performance of these methods will be limited. Some researchers attempted to estimate the phase spectrum directly or indirectly, but the effect is not ideal. Recently, some studies proposed the complex-valued model and achieved state-of-the-art performance, such as deep complex convolution recurrent network (DCCRN). However, the computation of the model is huge. To reduce the complexity and further improve the performance, we propose a novel method using discrete cosine transform as the input in this paper, called deep cosine transform convolutional recurrent network (DCTCRN). Experimental results show that DCTCRN achieves state-of-the-art performance both on objective and subjective metrics. Compared with noisy mixtures, the mean opinion score (MOS) increased by 0.46 (2.86 to 3.32) absolute processed by the proposed model with only 2.86M parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PEECH enhancement (SE) is one important topic in the signal processing area. Its goal is to extract target speech from the background noise environments to improve the intelligibility and perceptual quality. However, it is not easy to restore clean target speech perfectly especially with a single microphone. Researchers have proposed many algorithms to deal with this problem in past years. Traditional methods can deal with stationary noise effectively but powerless against non-stationary noise, such as the Wiener Filter method. Deep learning (DL) based methods make up for the above shortcomings, they regard SE as a supervised learning problem in the time-frequency (T-F) domain or in the time domain.</p><p>Typical TF domain speech enhancement methods only enhance the magnitude spectrum and use the noisy phase to reconstruct the clean target speech <ref type="bibr" target="#b0">[1]</ref>. This may because there is no clear structure in the phase spectrogram, which makes it difficult to estimate the clean phase from the noisy phase. These methods can be divided into two categories: mapping and mask-based methods. The mapping methods use a deep neural network (DNN) to learn the mapping function between the noisy magnitude spectrum and its corresponding target speech magnitude spectrum. The mask-based methods estimate a mask that classifies every portion of the signal either The manuscript was submitted on February 8, 2021. This work was supported by the AI Labs of Unisound AI Technology Co. Ltd.</p><p>The authors are all with AI-Labs of Unisound AI Technology Co. Ltd. (e-mail: {liqinglong, gaofei, guanhaixin, makaichi }@unisound.com).</p><p>as speech or noise, and then by weighting, or filtering the noisy speech with this mask, the enhanced clean speech signal can be generated. Common mask functions include ideal binary mask (IBM) <ref type="bibr" target="#b1">[2]</ref>, ideal ratio mask (IRM) <ref type="bibr" target="#b2">[3]</ref>, and spectral magnitude mask (SMM) <ref type="bibr" target="#b3">[4]</ref>, which show better performance than direct spectral mapping.</p><p>Recently, some research has shown the importance of phase for spectrograms to be resynthesized back into time-domain waveforms <ref type="bibr" target="#b4">[5]</ref>. Subsequently, some methods with phase information estimation were proposed. Phase-sensitive mask (PSM) <ref type="bibr" target="#b5">[6]</ref> was the first one that utilizes phase information showing the feasibility of phase estimation. Later, the complex ratio mask (CRM) <ref type="bibr" target="#b6">[7]</ref> and complex spectral mapping (CSM) <ref type="bibr" target="#b7">[8]</ref> were proposed, which can reconstruct clean speech perfectly in theory. Although the input and the mask are complex-valued, they all use a real-valued network to learn. In 2017, Chiheb et al proposed a deep complex network (DCN) <ref type="bibr" target="#b8">[9]</ref>, which has much better performance than real-valued networks. After that, deep complex u-net (DCUNET) <ref type="bibr" target="#b9">[10]</ref> has combined the advantages of DCN and UNET <ref type="bibr" target="#b10">[11]</ref> to deal with the complex-valued spectrogram. Recently, built upon the models DCN and CRN, a deep complex convolution recurrent network (DCCRN) <ref type="bibr" target="#b11">[12]</ref> was proposed, which achieved state-of-the-art performance in the TF domain. DCCRN won the championship on the realtime track of the Interspeech 2020 deep noise suppression (DNS) challenge.</p><p>Another popular method is to form an end-to-end algorithm in the time domain. In 2019, Luo et al proposed Conv-TasNet, which is an encoder-decoder architecture <ref type="bibr" target="#b12">[13]</ref>. It used an encoder layer to extract the input features, and multiple temporal convolutional networks (TCN) as the processing modules followed by a decoder layer to reconstruct the target speech. Although achieving the best performance, so many 1-dimension convolutional layers are adopted to extract context information leads to large time delay and computational complexity, which limits its practical usage in delay-sensitive applications.</p><p>In this paper, we propose a real-valued neural network to estimate the mask which including implicit phase information. It uses short-time discrete cosine transform (STDCT) as the input feature that has been proved helpful to the model <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>. Experimental results show that the proposed DCTCRN model outperforms the DCCRN in terms of objective and subjective metrics while reducing the calculation greatly. high-level features from the input to reduce the complexity, which is helpful for model training. Then, the processor uses these features to learn temporal dependencies and do the enhancement. Eventually, the decoder reconstructs the lowresolution features to the original size of the input and predicts the output targets. It is worth mentioning that, to improve flows of information and gradients throughout the network, skip connections that concatenate the output of each encoder layer to the input of each decoder layer is often utilized. The architecture is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. CRN is one of the classical ED architecture models <ref type="bibr" target="#b16">[16]</ref>. It uses multi convolutional layers as the encoder, LSTM as the processor, and transpose convolutional layers as the decoder. CRN leads to consistently better objective speech intelligibility and quality than the long short-term memory (LSTM) model. Using convolutional layers and transpose convolutional layers makes trainable parameters much fewer. This conclusion is confirmed again in the complex CRN model <ref type="bibr" target="#b7">[8]</ref>. Combine CRN architecture and complex layer, Hu et al. proposed DC-CRN <ref type="bibr" target="#b11">[12]</ref>. While achieving state-of-the-art performance in the real-time application, the computation of complex layers is huge which leads to a very big challenge to the device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE DCTCRN MODEL</head><p>In reality, the real and imaginary parts of STFT might not be necessarily as strongly correlated as we think, and the manually designed complex layer may not suitable for the real operation of the complex-value. To make up for the shortcomings of DCCRN, in this work, we propose the DCTCRN model. It replaces STFT and ISTFT with STDCT and ISTDCT, respectively, and uses a real-valued network layer to predict the mask. Unlike the explicit phase in STFT, STDCT including an implicit phase, so we can train a realvalued network without manually design complex layers. The DCTCRN model architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Input feature</head><p>There are two main categories of input features for the SE model: the time-domain feature, the transform-domain feature, or its variants. The time-domain feature is a low-dimensional spatial feature with all information of the speech. Generally, the information of speech is very complex which is difficult to distinguish every portion if in low-dimensional space. Therefore, it is helpful to improve the enhancement performance when increasing the complexity of the network. However, the calculation amount is also increased fast at the same time, such as Conv-TasNet.</p><p>The commonly used transform-domain input feature is the discrete Fourier transform (DFT). It transforms the timedomain signal into the frequency domain which is more discriminative for every portion. And there is no information lost during the transformation. However, DFT data is complexvalued, the computation is much complexity when operating using real and imaginary parts at the same time. To reduce the computation, some researchers use the variants of STFT as input, such as amplitude/energy spectrum, Log-Mel spectrum <ref type="bibr" target="#b17">[17]</ref>, etc. They all enhance amplitude spectrum without estimating clean phase, this would limit the enhancement performance.</p><p>DCT is a real-valued transformation without information lost and contains implicit phase. This avoids the problem of manually design a complex network to estimate the explicit phase. Meanwhile, DCT as the input feature of the real-valued network can greatly reduce the amount of calculation. The DCT is defined as:</p><formula xml:id="formula_0">F (µ) = c(µ) 2 N N −1 n=0 f (n) cos[ πµ(2n + 1) 2N</formula><p>], µ = 0, 1, ..., N −1</p><formula xml:id="formula_1">(1) and inverse DCT is f (n) = 2 N N −1 n=0 c(µ)F (µ) cos[ πµ(2n + 1) 2N ], n = 0, 1, ..., N −1 (2) where, c(µ) =    1 2 , µ = 0 1, µ = 1, 2, ..., N − 1</formula><p>where f (n) is the time-domain vector, N is the length of the vector. DCT and IDCT all operate in the real domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training target</head><p>During the training stage, DCTCRN uses the signal approximation method to estimate the ideal cosine mask (ICM). ICM can be defined as:</p><formula xml:id="formula_2">ICM t,f = S t,d Y t,d<label>(3)</label></formula><p>where S t,d and Y t,d denote the STDCT spectrogram of clean and noisy speech at the t-th time frame and the f -th subband respectively. S t,d and Y t,d ∈ R , meaning that ICM t,f is in the range (−∞, ∞).</p><p>To observe the impact of mask range on model performance, we use Parametric Rectified Linear Unit (PReLU, the range is -inf to inf), Sigmoid (the range is 0 to 1), and Tanh (the range is -1 to 1) as the activation functions of the final layer respectively. We named them DCTCRN-P, DCTCRN-S, and DCTCRN-T. Specifically, the models can be represented by the following equations.</p><p>• DCTCRN-P:</p><formula xml:id="formula_3">S = P ReLU (G(Y )) * Y<label>(4)</label></formula><p>• DCTCRN-S:</p><formula xml:id="formula_4">S = Sigmoid(G(Y )) * Y<label>(5)</label></formula><p>• DCTCRN-T: Because the range of PReLU is (−∞, +∞), we do some post-processing on the S to ensure the amplitude of reconstructed speech is not clipped. The post-processing is formed as follows:</p><formula xml:id="formula_5">S = T anh(G(Y )) * Y<label>(6)</label></formula><formula xml:id="formula_6">S t,f = S t,f , if | S t,f | ≤ |Y t,f | sign( S t,f ) * |Y t,f |, if | S t,f | &gt; |Y t,f |<label>(7)</label></formula><p>where sign(·) denotes take the symbol of elements, | · | means take the absolute value. We can get the estimated time-domain speech by using ISTDCT and overlap and add operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss function</head><p>It does not match the characteristics of human hearing when using mean squared error (MSE) as the loss function <ref type="bibr" target="#b18">[18]</ref>- <ref type="bibr" target="#b21">[21]</ref>. Recently, scale-invariant signal noise ratio (SI-SNR) has been commonly used as an evaluation metric <ref type="bibr" target="#b13">[22]</ref>, which is used as the loss function to optimize the model in this work. SI-SNR is defined as: </p><formula xml:id="formula_7">             s target = &lt; s,</formula><p>where s and s are the clean and estimated time-domain speech data, respectively. &lt; ·, · &gt; denotes the dot product between two vectors and || · || 2 is Euclidean norm (L2 norm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS A. datasets</head><p>We use the ICASSP2021 DNS Challenge dataset to the model training <ref type="bibr" target="#b22">[23]</ref>, clean speech in training set is total 760.53 hours: read speech (562.72 hours), singing voice (8.80 hours), emotion data(3.6hours), Chinese mandarin data (185.41 hours). The details about the clean and noisy dataset are described in <ref type="bibr" target="#b23">[24]</ref>. To make full use of these data, we generate the noisy clips with dynamic mixing during model training. In detail, during each training epoch, we convolve speech and noise with different room impulse response (RIR) randomly-selected from the DNS RIR-dataset and then simulate noisy audios dynamically by mixing reverb speech and noise at specific SNR. The SNR is randomly selected from -10dB to 20dB. The development test set in DNS is used to select the best-performed model.</p><p>During the test stage, we use the image method <ref type="bibr" target="#b24">[25]</ref> to generate 10000 simulated RIRs as the test RIR set. The room size is set to 5m×4m×3.5m with T60 range is 0.1:0.1:0.5. The locations of the microphone and speaker are randomly in the room with the height range is 1m to 1.5m. We limit the distance of the mic and speaker to 0.2m to 3m. The TIMIT corpus <ref type="bibr" target="#b25">[26]</ref> is selected as the test clean speech, NOISEX-92 <ref type="bibr" target="#b26">[27]</ref>, and the real-life record noise dataset as the test noise. There are nine common noises in the real-life record noise set: cafeteria, crossroad, background music, songs, public places, car inside, office, white, and train inside. We generate two test sets: reverberant and non-reverberant test set. For the reverberant test set, we first convolve every speech utterance with an RIR randomly selected from the test RIR set. And then mix the non-and reverberant speech with one noise respectively at every SNR (-6dB, -3dB, 0dB, 3dB, 6dB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. model setup and baseline</head><p>In this work, all the waveforms are resampled at 16kHz. We use the Hanning window of size 512 with the hop time of 8ms. The optimizer is Adam gradient <ref type="bibr" target="#b27">[28]</ref>. The initial learning rate is set to 0.001 for the first 100 epochs, and it will decay 0.5 when the validation loss goes up. The number of epochs is set to 300, and we use early stopping to select the best models.</p><p>The proposed DCTCRN model is compared with DCCRN-E, which is the champion model of the Interspeech2020 DNS Challenge in the real-time track and ranked second in the nonreal-time track <ref type="bibr" target="#b23">[24]</ref>. The setups of DCCRN-E is the same as <ref type="bibr" target="#b11">[12]</ref>. One frame after STFT is a 512-dimensional conjugate symmetric complex vector, DCCRN-E uses the last 256 points as the input after removing the direct current component. The input feature of DCTCRN is a 512-dimensional nonsymmetrical real-valued vector. The number of encoder layer channels of DCTCRN is {8, 16, 32, 64, 128, 128, 256} and {128, 128, 64, 32, 16, 8, 1} for decoder layer channels. The kernel size and stride are set to <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b1">2)</ref> and (2,1) for all encoder and decoder layers, respectively. The number of two-layer LSTM nodes is set to 256.</p><p>To meet the delay requirements of the ICASSP2021 DNS Challenge, we implement a causal system. Firstly, we pad one zero-frame in front of each convolutional encoder layer at the time dimension and remove the last time frame at each transpose convolutional decoder. We use a frame length of 32ms with a stride of 8ms resulting in an algorithmic 40ms delay to satisfy the latency requirements, the limitation of the DNS Challenge is within 40ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. experimental results</head><p>In this study, we use the perceptual evaluation of speech quality (PESQ) 1 , the short-time objective intelligibility (STOI) 2 , and the SNR as the objective metrics. <ref type="table" target="#tab_1">Table I, Table  II</ref>, <ref type="table" target="#tab_1">Table III show the objective results on the test set without  reverberation, and Table IV, Table V, Table VI</ref> are the results under reverberant conditions, respectively. In each case, the best result is highlighted by a boldface number.</p><p>From the results on the non-reverberation sets, we can find that DCTCRN-P and DCTCRN-T outperform DCCRN-E in all metrics. DCTCRN-P achieves state-of-the-art performance and DCTCRN-T is very close to it. DCTCRN-S and DCCRN-E yield similar PESQ and STOI scores, but DCTCRN-S gains almost 1dB more than DCCRN-E of SNR metric. Obviously, the range of PReLU is the closest to that of ICM, the network can estimate the mask more accurately. The more in line with the scope of ICM value, the better the speech enhancement performance.</p><p>On the reverberant test sets, DCTCRN-T gets the best results of all conditions. It is more difficult to learn ICM in reverberant scenes, Tanh as activation limits the mask range to (-1,1) to reduce the difficulty of learning. DCTCRN-P performance is very close to DCTCRN-T. All DCTCRN models are much better than DCCRN-E on the same training and test conditions. Not only that, the proposed model has fewer parameters and computations. We use flops-counter.pytorch 3 to compute the floating-point operations (FLOPs) and parameters of the models. The results show that the calculation of DCCRN is almost three times that of the proposed DCTCRN (DCCRN: 120.01M FLOPs, DCTCRN: 41.20M FLOPs). Moreover, the comparison of the parameters of DCCRN and DCTCRN is 3.98M and 2.86M.</p><p>To evaluate the subjective metric, we participated in track-1 in the 2nd DNS-challenge and get the results of the final MOS <ref type="bibr" target="#b22">[23]</ref>. Compared with noisy mixtures, the MOS increased by 0.46 (2.86 to 3.32) absolute processed by the proposed model. During the test stage, we also discovered that the DCTCRN-T model is more robust to speech and noise than the others. This is because the mask with a limited range is easier to learn and the range of (-1,1) is close to the characteristics of the ICM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this work, we proposed a robust deep cosine transform convolutional recurrent network for real-time speech enhancement. Experimental results show that the proposed model has fewer parameters, it greatly reduces the calculations while improving performance comparing with DCCRN. In the future, we will try to deploy DCTCRN in low computational devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A. Encoder-decoder architecture The typical encoder-decoder (ED) architecture including an encoder, a processor, and a decoder. The encoder extracts arXiv:2102.04629v1 [eess.AS] 9 Feb 2021 Encoder-decoder architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>s &gt; ·s s 2 2 e</head><label>2</label><figDesc>noise = s − s SI − SN R = 10 * log 10 (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PESQ</head><label>I</label><figDesc>ON THE NON-REVERBERATION TEST SETS test SNR -6dB -3dB 0dB 3dB 6dB Avg. noisy 1.51 1.70 1.89 2.09 2.30 1.90 DCCRN-E 2.25 2.54 2.82 3.06 3.27 2.79 DCTCRN-P 2.31 2.59 2.85 3.08 3.28 2.82 DCTCRN-S 2.25 2.53 2.78 3.03 3.23 2.77 DCTCRN-T 2.30 2.59 2.85 3.08 3.29 2.82 76.91 83.50 88.65 92.13 94.63 87.16 DCTCRN-P 77.88 84.04 88.94 92.20 94.66 87.55 DCTCRN-S 77.28 83.57 88.48 91.97 94.48 87.16 DCTCRN-T 77.84 84.08 88.87 92.18 94.67 87.53 .93 8.51 10.98 13.09 15.30 10.76 DCTCRN-S 5.65 8.28 10.71 12.91 15.10 10.53 DCTCRN-T 5.77 8.43 10.90 13.04 15.26 10.68</figDesc><table><row><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">STOI (IN %) ON THE NON-REVERBERATION TEST SETS</cell><cell></cell></row><row><cell>test SNR</cell><cell>-6dB -3dB 0dB</cell><cell>3dB</cell><cell>6dB</cell><cell>Avg.</cell></row><row><cell>noisy</cell><cell cols="4">63.39 69.48 75.43 81.17 86.16 75.12</cell></row><row><cell cols="2">DCCRN-E TABLE III</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SNR ON THE NON-REVERBERATION TEST SETS</cell><cell></cell></row><row><cell>test SNR</cell><cell>-6dB -3dB 0dB</cell><cell>3dB</cell><cell>6dB</cell><cell>Avg.</cell></row><row><cell>noisy</cell><cell>-5.92 -2.92 0.08</cell><cell>3.08</cell><cell>6.08</cell><cell>0.08</cell></row><row><cell>DCCRN-E</cell><cell cols="4">4.45 7.30 9.92 12.08 14.29 9.61</cell></row><row><cell cols="2">DCTCRN-P 5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV PESQ</head><label>IV</label><figDesc>ON THE REVERBERATION TEST SETS test SNR -6dB -3dB 0dB 3dB 6dB Avg. noisy 1.51 1.73 1.93 2.15 2.33 1.93 DCCRN-E 1.95 2.16 2.36 2.53 2.64 2.33 DCTCRN-P 2.08 2.31 2.54 2.77 2.96 2.53 DCTCRN-S 2.04 2.27 2.50 2.72 2.91 2.49 DCTCRN-T 2.08 2.33 2.56 2.80 2.99 2.55 64.86 71.76 77.23 81.80 84.53 76.04 DCTCRN-P 65.87 73.12 79.21 84.81 88.69 78.34 DCTCRN-S 65.35 72.74 78.89 84.49 88.03 77.90 DCTCRN-T 66.42 73.80 80.01 85.68 89.35 79.05 DCTCRN-P 2.65 5.20 7.44 9.81 11.97 7.41 DCTCRN-S 2.13 4.86 7.13 9.49 11.54 7.03 DCTCRN-T 2.73 5.21 7.44 9.88 12.02 7.46</figDesc><table><row><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">STOI (IN %) ON THE REVERBERATION TEST SETS</cell><cell></cell></row><row><cell>test SNR</cell><cell>-6dB -3dB 0dB</cell><cell>3dB</cell><cell>6dB</cell><cell>Avg.</cell></row><row><cell>noisy</cell><cell cols="4">51.35 58.65 66.33 74.11 80.30 66.15</cell></row><row><cell cols="2">DCCRN-E TABLE VI</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SNR ON THE REVERBERATION TEST SETS</cell><cell></cell></row><row><cell>test SNR</cell><cell cols="4">-6dB -3dB 0dB 3dB 6dB Avg.</cell></row><row><cell>noisy</cell><cell cols="4">-6.00 -2.99 0.00 3.01 6.00 0.00</cell></row><row><cell>DCCRN-E</cell><cell cols="4">1.27 3.68 5.49 6.95 8.05 5.09</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/vBaiCai/python-pesq 2 https://github.com/mpariente/pystoi</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/sovrasov/flops-counter.pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech segregation based on pitch tracking and amplitude modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No. 01TH8575</title>
		<meeting>the 2001 IEEE Workshop on the Applications of Signal Processing to Audio and Acoustics (Cat. No. 01TH8575</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="79" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Binary and ratio time-frequency masks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1486" to="1501" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and language processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The importance of phase in speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wójcicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">speech communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="465" to="494" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Phasesensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Complex spectral mapping with a convolutional recurrent network for monaural speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6865" to="6869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">B Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F S</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Trabelsi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09792</idno>
		<title level="m">Deep complex networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Phaseaware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dccrn: Deep complex convolution recurrent network for phase-aware speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B L M T X S M Z Y H F J W B H Z Y X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2472" to="2476" />
		</imprint>
	</monogr>
	<note>Interspeech, 2020</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discrete cosine transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="93" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end speech enhancement based on discrete cosine transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="379" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convolutional recurrent neural network for real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="3229" to="3233" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Features for masking-based monaural speech separation in reverberant conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delfarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1085" to="1094" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perception optimized deep denoising autoencoders for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3743" to="3747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A perceptually-weighted deep neural network for monaural speech enhancement in various background noise conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 25th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1270" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Martin-Donas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1680" to="1684" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monaural speech enhancement using deep neural networks by maximizing a short-time objective intelligibility measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5059" to="5063" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06122</idno>
		<title level="m">Icassp 2021 deep noise suppression challenge</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image method for efficiently simulating small-room acoustics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Berkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="943" to="950" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NASA STI/Recon technical report n</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">27403</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Assessment for automatic speech recognition: Ii. noisex-92: A database and an experiment to study the effect of additive noise on speech recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Steeneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="251" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
