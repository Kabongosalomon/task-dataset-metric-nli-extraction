<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transferring Rich Deep Features for Facial Beauty Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Informatics</orgName>
								<orgName type="institution">Huazhong Agricultural University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhai</forename><surname>Xiang</surname></persName>
							<email>jimmyxiang@mail.hzau.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Informatics</orgName>
								<orgName type="institution">Huazhong Agricultural University Wuhan</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yuan</surname></persName>
							<email>xiaohui.yuan@unt.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<settlement>Denton</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transferring Rich Deep Features for Facial Beauty Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>transfer learning</term>
					<term>knowledge adap- tation</term>
					<term>facial beauty prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature extraction plays a significant part in computer vision tasks. In this paper, we propose a method which transfers rich deep features from a pretrained model on face verification task and feeds the features into Bayesian ridge regression algorithm for facial beauty prediction. We leverage the deep neural networks that extracts more abstract features from stacked layers. Through simple but effective feature fusion strategy, our method achieves improved or comparable performance on SCUT-FBP dataset and ECCV HotOrNot dataset. Our experiments demonstrate the effectiveness of the proposed method and clarify the inner interpretability of facial beauty perception.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Facial beauty analysis <ref type="bibr" target="#b0">[1]</ref> has been widely used in many fields such as facial image beautification APPs (e.g., MeiTu and Facetune), plastic surgery, and face-based pose analysis <ref type="bibr" target="#b1">[2]</ref>. In the mobile computing era, billions of images per day are acquired and uploaded to social networks and online platform, leading to the demand for better image processing and analyzing technology. Recently, thanks to the big data and high-performance computational hardware, computational and data-driven approaches have been proposed for solving these questions such as face recognition, facial expression recognition, facial beauty analysis and etc.</p><p>The existing methods resort to machine learning and computer vision techniques to analyze facial beauty and achieve promising results <ref type="bibr" target="#b2">[3]</ref>. The methods often include image feature descriptors (such as HOG, SIFT, LBP, etc) and supervised machine learning predictors (such as SVM, KNN, DNN, LR, etc).</p><p>In order to explore the best facial beauty prediction approach that precisely maps high-level features into face beauty ratings, we propose a method that combines transfer learning and Bayesian regression. The method achieves the improved or comparable performance on SCUT-FBP dataset <ref type="bibr" target="#b3">[4]</ref> and ECCV HotOrNot dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>The main contributions of this paper are as follows:</p><p>• We apply transfer learning to our facial beauty prediction problems for feature extraction. Experimental results show that the transferred deep features can attain more impressive performance compared with the traditional image feature descriptors such as HOG, LBP and gray value features. <ref type="bibr">•</ref> We make a detailed analysis about deep features based on knowledge adaptation. Additionally, we perform an effective feature fusion strategy to build more informative facial features in our facial beauty prediction task. • Studies found that the neural networks are lack of satisfactory interpretation. We make ablative studies by visualizing the face feature and reveal the elements that influence facial beauty perception. The rest of this paper is organized as follows. Section II reviews the related works of facial descriptor and learning methods. Section III describes our proposed method in details, which include deep feature extraction and Bayesian ridge regression. Experimental results and comparisons are presented in Section IV and Section V concludes this paper with a summary and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Facial Descriptors and Machine Learning Predictors</head><p>Many researchers focus on developing new machine learning algorithms to achieve better classification or regression performance, while others focus on designing better facial feature descriptors. Zhang et al. <ref type="bibr" target="#b5">[6]</ref> combine several low-level face representations and high-level features to form a feature vector and perform feature selection to optimize the feature set. Eisenthal et al. <ref type="bibr" target="#b6">[7]</ref> use a vector of gray values created by concatenating the rows or columns of an image. Huang et al. <ref type="bibr" target="#b7">[8]</ref> propose a method to learn hierarchical representations of convolutional deep belief networks. Xie et al. <ref type="bibr" target="#b3">[4]</ref> resort to deep learning to train a predictor and achieve state-of-the-art performance. Amit et al. <ref type="bibr" target="#b8">[9]</ref> use numerous facial features that describe facial geometry, color and texture to predict facial attractiveness. Lu et al. <ref type="bibr" target="#b9">[10]</ref> detect face landmarks with ASM and then extract facial features based on Blocked-LBP which achieved the Pearson Correlation at 0.874 on 400 high-quality female face images. Zhang et al. <ref type="bibr" target="#b10">[11]</ref> compute geometric distances between feature points and ratio vectors composed of geometric distances, and then treat them as features for machine learning algorithm. For the lack of abundant labeled images, it always takes lots of time to fine-tune the deep neural networks architecture and parameters to achieve a comparative result and avoid overfitting problems as well.</p><p>In addition, some research works towards developing or improving new machine learning algorithms. Eisenthal et al. <ref type="bibr" target="#b6">[7]</ref> employ KNN and SVM as classifiers to rate faces belongs to different levels. Gan et al. <ref type="bibr" target="#b11">[12]</ref> use deep self-taught learning to obtain hierarchical representations and learn the concept of facial beauty. Xu et al. <ref type="bibr" target="#b12">[13]</ref> propose a method which constructs a convolutional neural network (CNN) for facial beauty prediction using a new deep cascaded fine tuning scheme with various face inputting channels. Wang et al. <ref type="bibr" target="#b13">[14]</ref> use deep auto encoders to extract features and take a low-rank fusion method to integrate scores, and their method achieves promising results. Xu et al. <ref type="bibr" target="#b14">[15]</ref> propose "psychologically inspired CNN (PI-CNN)" for automatically facial beauty prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep CNN and Transfer Learning</head><p>Deep learning allows computational models that are composed of processing layers to learn representations of data with multiple levels of abstraction <ref type="bibr" target="#b15">[16]</ref>. CNN is a type of neural networks which is designed to process data that come in form of multiple arrays. Deep learning has been used as a dramatically powerful tool in computer vision tasks such as image recognition <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. The features are automatically extracted via stacked layers. Neural networks are trained through back-propagation algorithm to minimize the cost function.</p><p>Deep convolutional neural networks show more extraordinary capacity in feature extraction than traditional handcrafted descriptors. However, we may need to design different networks architectures and train the deep neural networks almost from scratch to satisfy our task, which takes much computational burden. Transfer learning allows us to fine-tune the higher layers based on a pretrained model, or even just treat the pretrained model as a feature extractor.</p><p>Yosinski et al. <ref type="bibr" target="#b20">[21]</ref> show that initializing a network with transferred features from almost any number of layers can produce an improvement to the generalization even after finetuning to the target domain dataset. Yoshua Bengio et al. <ref type="bibr" target="#b21">[22]</ref> explore why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario. Donahue et al. <ref type="bibr" target="#b22">[23]</ref> show that the features extracted by deep convolutional neural networks pretrained on ImageNet can achieve much better performance than many algorithms on lots of classification tasks, which illustrates the great generality and transferability of deep convolutional neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. VGG Network</head><p>We include a brief review of VGG, which is employed by our proposed method. VGG <ref type="bibr" target="#b17">[18]</ref> consists of 16-19 weight layers and very small (3 × 3) convolution filters as well. <ref type="figure">Fig. 1</ref> shows the overall architecture of the VGG16 networks. Though VGG networks architecture is simple, it is widely used in many computer vision tasks. In our experiments, we take a VGG face model which is pretrained on a face verification task <ref type="bibr" target="#b23">[24]</ref>. Although the original task is absolutely different from our facial beauty prediction task, it shows dramatically impressive performance. We believe the main reason for this issue can be attributed to the extraordinary feature representation power of deep CNNs. <ref type="figure">Fig. 1</ref>: Networks architecture. we adopt VGG16 in our feature extraction procedure, which is composed of multiple small convolutional filters to extract more informative features compared with bigger filters used in AlexNet <ref type="bibr" target="#b16">[17]</ref> for ImageNet recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Feature Extraction</head><p>Several research works <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b15">[16]</ref> show that the deep convolutional neural networks can learn increasingly powerful representations as the feature hierarchy becomes deeper. However, due to the limited labeled face images, if we train a deep convolutional neural network directly, we may suffer from severe overfitting problems. Recently, transfer learning has aroused much attention <ref type="bibr" target="#b24">[25]</ref>, which enables us to finetune from a pretrained model or just treat the learned neural network as a feature extractor to satisfy our tasks <ref type="bibr" target="#b20">[21]</ref>.</p><p>We extract facial features with VGG face model [24] pretrained on face verification task. Despite their target task is different from our facial beauty prediction task, the feature can achieve remarkable performance, which indicates extraordinary feature representation power of CNNs to some degree. Researches <ref type="bibr" target="#b20">[21]</ref> show that the features in lower layers contain more detailed information while features in higher layers represent more semantic meaning. Our method concatenates on both relatively low layer's features and relatively high layer's features as our facial representation. We also use HOG, grayscale and LBP features in our experiments for comparison to evaluate the feature extraction capacity of deep CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bayesian Ridge Regression</head><p>We feed the concatenated feature vectors into Bayesian ridge regressor. Bayesian ridge regression includes regularization during estimation procedure: the regularization item is not embedded with cost function directly, but tuned to your data distribution. The L 2 regularization used in Bayesian ridge regression is equal to maximizing a posterior estimation of the parameters w with precision λ −1 under a Gaussian prior. The output y is assumed to be Gaussian distributed around Xw in order to form a fully probabilistic model:</p><formula xml:id="formula_0">P (y|X, w, α) = N (y|Xw, α)<label>(1)</label></formula><p>Bayesian ridge regressor evaluates a probabilistic model of the regression problem. The prior for the parameter w is decided by a spherical Gaussian:</p><formula xml:id="formula_1">P (w|λ) = N w 0, λ −1 I p<label>(2)</label></formula><p>The priors over α and λ are chosen to be Gamma distributions, the conjugate prior for the precision of the Gaussian.</p><p>The parameters w, α and λ are estimated jointly during the fit procedure. The remaining hyperparameters are the parameters of the Gamma priors over α and λ. All the parameters are tuned by maximizing the marginal log likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We implement our method with TensorFlow <ref type="bibr" target="#b25">[26]</ref> and Scikit-Learn <ref type="bibr" target="#b26">[27]</ref> on an Ubuntu server with NVIDIA Tesla K80 GPU and Intel Xeon CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SCUT-FBP Dataset</head><p>The SCUT-FBP dataset <ref type="bibr" target="#b3">[4]</ref> contains images of 500 Asian females. Each image is scored by 10 raters, the main task is to build a computational model to predict the average score of the human portrait image.</p><p>Since the images in SCUT-FBP <ref type="bibr" target="#b3">[4]</ref> are not in same size, deep CNNs can only support fixed squared data as input. We conduct three methods named "Crop", "Warp" and "Padding" to get squared images respectively. In "Crop" setting, we detect face provided by <ref type="bibr" target="#b27">[28]</ref> and crop the face region, then we resize it to 224 × 224. In "Warp" setting, we just warp the image forcely to form a 224 × 224 image. In "Padding" setting, we resize the longer side to 224 and zero-pad the shorter side to form a 224 × 224 image (See <ref type="figure" target="#fig_0">Fig. 2</ref>). We also normalize the input image by substracting the mean and dividing the standard variance of the pixels. Furthermore, we manually crop the central region of the image and treat it as the input for our neural networks in case of failed face detection. In SCUT-FBP dataset, we concatenates the conv5 1 and conv4 1 layer's features. The pipeline is shown in <ref type="figure">Fig. 3</ref>: <ref type="figure">Fig. 3</ref>: Pipeline of our proposed method. The face is detected and then fed into CNNs, we concatenate conv4 1 and conv5 1 layers' feature maps, and flatten them into feature vectors for the input of Bayesian ridge regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Evaluation</head><p>In our experiment, we use Pearson Correlation (PC), Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) as the criteria for evaluating our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RM SE(X, h)</head><formula xml:id="formula_2">= 1 m m i=1 h(x (i) ) − y (i) ) 2 (3) M AE = 1 m m i=1 |h(x (i) ) − y (i) | (4) P C = 1 n − 1 n i=1 ( h(x i ) −h(x) s h(x) )( y i −ȳ s y )<label>(5)</label></formula><p>where m denotes the number of images, x (i) denotes the input feature vector of image i, h(•) denotes the learning algorithm, y (i) denotes the groundtruth attractiveness score of image i. MAE and RMSE measure the fit quality of the learning algorithms, the performance is better if the value is closer to zero. PC measures the linear correlation between h(x (i) ) and y (i) . Its value lays between 1 and -1, where 1 means absolutely positive linear correlation, 0 means no linear correlation, and -1 means absolutely negative linear correlation.</p><p>In order to make the prediction more reliable and reproducable, we follow the provision denoted in <ref type="bibr" target="#b3">[4]</ref> for fair comparison. We randomly select 400 images as training set and the rest 100 images as test set. Finally, we average the 5 experimental results as the final performance to remove sample variances. The results are shown <ref type="table" target="#tab_0">in TABLE I.  TABLE II</ref> shows performance comparison with other methods. The best performance is marked with bold font and the second best is highlighted with an underline. Our method ranks the second place on SCUT-FBP <ref type="bibr" target="#b3">[4]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Analysis</head><p>It is almost a common sense in machine learning practice is that "feature matters". To illustrate the feature extraction capability by deep learning, we conduct experiments based  Performance comparison with other methods, our method ranks the second place on PC and first place on RMSE and MAE, respectively. The best and second results are emphasized in bold and underline respectively. Since RMSE and MAE of CNN-based methods proposed in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b14">[15]</ref> are not given and are hence denoted with "-".</p><p>on different features including HOG, LBP, gray image and transferred deep features for performance comparison and visualization:</p><p>• Raw Grayscale: we convert the RGB facial images into their corresponding gray scale ones, and the flattened pixel gray scale value is used as the feature.  • HOG: HOG is an image feature descriptor which is widely used in computer vision and image processing for object detection tasks. Details can be found in <ref type="bibr" target="#b28">[29]</ref>. • LBP: LBP is a type of feature descriptor which especially cares more about texture details, and is widely used in many machine vision tasks. In addition, we compare the feature performance from different layers to find which layer produces the most discriminative features (See <ref type="figure">Fig. 5</ref>).</p><p>Moreover, among three preprocessing methods (Crop, Warp, and Padding), Crop achieves the best performance on SCUT-FBP, which indicates that facial region plays a more significant part in beauty perception, while background may act as noise in our facial beauty prediction task on SCUT-FBP dataset (See TABLE. IV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5:</head><p>Performance comparison between different layers: the performance gets better as layer goes deeper, which means the deep CNN extracts more discriminative features. It decreases sharply after max pooling operation, which may be attributed as heavy spatial information loss. <ref type="figure">Fig. 5</ref> depicts that as layer goes deeper the performance gets better, and reaches the best at conv5 1. While when feature maps are flattened into vectors, we see a sharp drop in performance, which may be attributed as the heavy spatial information loss. Performance of diffrent preprocessing methods ("Crop", "Warp", and "Padding") on SCUT-FBP. "Crop" achieves the best. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ECCV HotOrNot Dataset</head><p>ECCV HotOrNot dataset <ref type="bibr" target="#b4">[5]</ref> contains 2056 faces which are collected from the Internet. Each face is labeled with a score, and the dataset has already been split into 5 training and test datasets. Unlike SCUT-FBP dataset <ref type="bibr" target="#b3">[4]</ref>, the faces in ECCV HotOrNot dataset <ref type="bibr" target="#b4">[5]</ref> are more challenging because of the variant postures, cluttered background, illumination, low resolution and unaligned faces problems, which make the facial beauty prediction more difficult (See <ref type="figure" target="#fig_2">Fig. 6</ref>).</p><p>ECCV HotOrNot dataset uses Pearson Correlation (PC) for performance metric. We also list MAE and RMSE for more detailed comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We concatenate conv5 2 and conv5 3 layers' feature maps and flatten them to form more informative features. The concatenated features are then fed into Bayesian ridge regression algorithm <ref type="bibr" target="#b29">[30]</ref>.</p><p>We implement two means to evaluate the impact of preprocessing techniques. In solution A, we run face detector <ref type="bibr" target="#b27">[28]</ref> to detect 68 facial landmarks and the facial region. For grayscale images, we replicate the gray pixel value twice to form an RGB channels image. Then we calculate the inclination angle to the horizontal line with two eyes coordinates, which is denoted as θ. If |θ| &gt; 0, we rotate the face around the central point by θ degree and crop the facial region. The mean pixel value is subtracted from the cropped image, which is normalized by its standard deviation. Solution B includes mean subtraction and standard error division on the original images. No additional preprocessing is taken.</p><p>We find that solution B achieves much better performance than solution A, the results can be found in TABLE V. We believe the main reason is that the annotators may also take extra information such as haircut, posture, and clothing into consideration while labeling these facial beauty scores, instead of just measuring face region.</p><p>Additionally, we define = |y i −ŷ i |, which describes the error between the predicted facial beauty score (ŷ i ) and the   ground truth beauty score (y i ). If ≥ τ 1 , we believe there is a relatively severe bias among the predicted values and ground truth scores. If ≤ τ 2 , we believe our algorithm fits these samples perfectly. In this part, we set τ 1 = 2.75 and τ 2 = 0.02 for detailed analysis (See <ref type="figure" target="#fig_3">Fig. 7 and Fig. 8</ref>). We believe the performance could be greatly improved through face alignment techniques. Besides, posture and facial expression may also contribute to beauty perception because our algorithm fails to capture these samples with variant postures. <ref type="table" target="#tab_0">Table VI</ref> compares the Pearson Correlation of our proposed method with five state-of-the-art methods. Our method outperforms other methods and achieves the best performance on ECCV HorOrNot dataset without face alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a method which extracts rich deep facial features through knowledge adaptation, and then trains Bayesian ridge regression algorithm for face beauty prediction. Despite that the VGG model is pretrained for a totally  <ref type="bibr" target="#b13">[14]</ref> 0.437 Ours 0.468</p><p>Performance comparison on ECCV HotOrNot dataset <ref type="bibr" target="#b4">[5]</ref>. Pearson Correlation (PC) is used for evaluating performance. Our method achieves the best result on this dataset as mentioned in <ref type="bibr" target="#b4">[5]</ref>. different task, it also captures more descriptive information than conventional hand-crafted features, and even outperforms many deep learning-based methods in our facial beauty prediction task, which shows the great generality of deep features in transfer learning. With our feature fusion strategy, our method outperforms other methods and achieves the state-ofthe-art performance on ECCV HotOrNot dataset <ref type="bibr" target="#b4">[5]</ref> without face alignment and comparable performance on SCUT-FBP dataset <ref type="bibr" target="#b3">[4]</ref>. In our future work, we plan to explore 3D face alignment and novel networks architecture for extracting more descriptive features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Different settings to form a squared image: (a) Original image in SCUT-FBP. (b) Cropped image. (c) Image warp. (d) Image padding. We conduct these experiments to see whether the facial beauty perception is correlated with nonfacial elements such as haircut, wearing, posture and etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Visualization of the original portrait images and their corresponding visualized features described by different feature extractors: (top) Original portrait images. (middle) Visualization of HOG descriptors. (down) Visualization of LBP descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>ECCV HotOrNot face samples: this dataset is more challenging due to low resolution (b), illumination problem (c), gray version (d), occlusion (e), different race (f), cluttered background (g) and unaligned posture (h). Aligned samples are like (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>≥ 2.75, which means these samples are not well predicted by our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>≤ 0.02, which means these samples are well fitted by our algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>AVERAGE PERFORMANCE</figDesc><table><row><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>PC</cell></row><row><cell>1</cell><cell>0.2569</cell><cell>0.3418</cell><cell>0.8735</cell></row><row><cell>2</cell><cell>0.2594</cell><cell>0.3470</cell><cell>0.8299</cell></row><row><cell>3</cell><cell>0.2479</cell><cell>0.3117</cell><cell>0.8929</cell></row><row><cell>4</cell><cell>0.2651</cell><cell>0.3473</cell><cell>0.8562</cell></row><row><cell>5</cell><cell>0.2680</cell><cell>0.3508</cell><cell>0.8323</cell></row><row><cell>AVG</cell><cell>0.2595</cell><cell>0.3397</cell><cell>0.8570</cell></row><row><cell cols="4">Performance on SCUT-FBP [4] of 5 rounds. We average the 5 results to</cell></row><row><cell cols="3">remove examples variances as denoted in [4].</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>PREDICTOR PERFORMANCE COMPARISON</figDesc><table><row><cell>Method</cell><cell>RMSE</cell><cell>MAE</cell><cell>PC</cell></row><row><cell>KeyPointGabor+PCA+SVR</cell><cell>0.5606</cell><cell cols="2">0.5541 0.5490</cell></row><row><cell>KeyPointGabor+PCA+Gaussian Reg</cell><cell>0.6152</cell><cell cols="2">0.4724 0.4591</cell></row><row><cell>UniSampleGabor+PCA+SVR</cell><cell>0.5452</cell><cell cols="2">0.4230 0.5847</cell></row><row><cell>UniSampleGabor+PCA+Gaussian Reg</cell><cell>0.5164</cell><cell cols="2">0.3969 0.6347</cell></row><row><cell>Combined Features+SVR [4]</cell><cell>0.5120</cell><cell cols="2">0.3961 0.6433</cell></row><row><cell>Combined Features+Gaussian Reg [4]</cell><cell>0.5149</cell><cell>0.3931</cell><cell>0.6482</cell></row><row><cell>CNN-based [4]</cell><cell>-</cell><cell>-</cell><cell>0.8187</cell></row><row><cell>PI-CNN [15]</cell><cell>-</cell><cell>-</cell><cell>0.87</cell></row><row><cell>Ours</cell><cell>0.2595</cell><cell>0.3397</cell><cell>0.8570</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>PERFORMANCE COMPARISON BETWEEN DIFFERENT FEATURES</figDesc><table><row><cell>Feature</cell><cell>RMSE</cell><cell>MAE</cell><cell>PC</cell></row><row><cell>TransCNN</cell><cell>0.2595</cell><cell cols="2">0.3397 0.8570</cell></row><row><cell>HOG</cell><cell>0.3308</cell><cell cols="2">0.4394 0.6216</cell></row><row><cell>LBP</cell><cell>0.3987</cell><cell cols="2">0.4800 0.5631</cell></row><row><cell>Gray Scale</cell><cell>0.4008</cell><cell cols="2">0.4889 0.5149</cell></row><row><cell cols="4">Performance comparison with other feature descriptors on Bayesian ridge</cell></row><row><cell cols="4">regression. Our transferred deep features outperforms other descriptors</cell></row><row><cell cols="4">with a large margin. The best results are given in bold font.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>PEARSON CORRELATION OF DIFFERENT PREPROCESSING METHODS</figDesc><table><row><cell>Crop</cell><cell>Warp</cell><cell>Padding</cell></row><row><cell>PC 0.8570</cell><cell>0.7376</cell><cell>0.8255</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc>The ECCV HotOrNot dataset<ref type="bibr" target="#b4">[5]</ref> has been divided into 5 parts which contain training set and test set respectively. We compare the performance of solution A and solution B. Much to our surprise, solution B achieves better results with a large margin. It may be explained by the extra nonfacial information such as hairstyle, wearing, posture, etc.</figDesc><table><row><cell></cell><cell cols="6">: PERFORMANCE ON ECCV HOTORNOT</cell></row><row><cell>DATASET</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>solution A</cell><cell></cell><cell></cell><cell>solution B</cell><cell></cell></row><row><cell>Dataset</cell><cell>RMSE</cell><cell>MAE</cell><cell>PC</cell><cell>RMSE</cell><cell>MAE</cell><cell>PC</cell></row><row><cell>1</cell><cell>0.9417</cell><cell cols="2">1.1948 0.3970</cell><cell>0.9140</cell><cell cols="2">1.1493 0.4656</cell></row><row><cell>2</cell><cell>0.9755</cell><cell cols="2">1.2406 0.4022</cell><cell>0.9210</cell><cell cols="2">1.1562 0.4728</cell></row><row><cell>3</cell><cell>0.9293</cell><cell cols="2">1.1810 0.3840</cell><cell>0.8989</cell><cell cols="2">1.1258 0.4694</cell></row><row><cell>4</cell><cell>0.9469</cell><cell cols="2">1.1788 0.3898</cell><cell>0.8736</cell><cell cols="2">1.1087 0.4775</cell></row><row><cell>5</cell><cell>0.9394</cell><cell cols="2">1.1856 0.3862</cell><cell>0.9104</cell><cell cols="2">1.1316 0.4541</cell></row><row><cell>Average</cell><cell>0.9466</cell><cell cols="2">1.1962 0.3918</cell><cell>0.9036</cell><cell cols="2">1.1343 0.4679</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>PEARSON CORRELATION OF HOTORNOT DATASET.</figDesc><table><row><cell>Method</cell><cell>PC</cell></row><row><cell>Eigenface</cell><cell>0.180</cell></row><row><cell>Single Layer Model</cell><cell>0.417</cell></row><row><cell>Two Layer Model</cell><cell>0.438</cell></row><row><cell cols="2">Multiscale Model [5] 0.458</cell></row><row><cell>Auto Encoder</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Facial shape and judgements of female attractiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoshikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="issue">6468</biblScope>
			<biblScope unit="page" from="239" to="281" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-level structured hybrid forest for joint head detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Computer models for facial beauty analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scut-fbp: A benchmark dataset for facial beauty perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
			<affiliation>
				<orgName type="collaboration">SMC</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1821" to="1826" />
		</imprint>
	</monogr>
	<note>Systems, Man, and Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting facial beauty without landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="434" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data-driven facial beauty analysis: Prediction, retrieval and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial attractiveness: Beauty and the machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eisenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="142" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2518" to="2525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A humanlike predictor of facial attractiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kagian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leyvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new face beauty prediction model based on blocked lbp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quantitative analysis of human facial beauty using geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="940" to="950" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for facial beauty prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="295" to="303" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02465</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attractive or not?: Beauty prediction with attractiveness-aware encoders and robust late fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="805" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facial attractiveness prediction using psychologically inspired convolutional neural network (pi-cnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1657" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on CVPR</title>
		<meeting>the IEEE conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on CVPR</title>
		<meeting>the IEEE conference on CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</title>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic removal of complex shadows from indoor videos using transfer learning and dynamic thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseny</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compeleceng.2017.12.026</idno>
	</analytic>
	<monogr>
		<title level="j">Computers and Electrical Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
		<title level="m">Dlib-ml: A Machine Learning Toolkit. JMLR.org</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
