<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Cancer Metastases on Gigapixel Pathology Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
							<email>liuyun@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Gadepalli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
							<email>mnorouzi@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
							<email>gdahl@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Kohlberger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksey</forename><surname>Boyko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">Q</forename><surname>Nelson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Hipp</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Verily Life Sciences</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lily</forename><surname>Peng</surname></persName>
							<email>lhpeng@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
							<email>mstumpe@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Cancer Metastases on Gigapixel Pathology Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural network</term>
					<term>pathology</term>
					<term>cancer</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Each year, the treatment decisions for more than 230, 000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 × 100 pixels in gigapixel microscopy images sized 100, 000×100, 000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Came-lyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The treatment and management of breast cancer is determined by the disease stage. A central component of breast cancer staging involves the microscopic examination of lymph nodes adjacent to the breast for evidence that the cancer has spread, or metastasized <ref type="bibr" target="#b2">[3]</ref>. This process requires highly skilled pathologists and is fairly time-consuming and error-prone, particularly for lymph nodes with either no or small tumors. Computer assisted detection of lymph node metastasis could increase the sensitivity, speed, and consistency of metastasis detection <ref type="bibr" target="#b15">[16]</ref>.</p><p>In recent years, deep CNNs have significantly improved accuracy on a wide range of computer vision tasks such as image recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19]</ref>, object detection <ref type="bibr" target="#b7">[8]</ref>, and semantic segmentation <ref type="bibr" target="#b16">[17]</ref>. Similarly, deep CNNs have been applied productively to improve healthcare (e.g., <ref type="bibr" target="#b8">[9]</ref>). This paper presents a CNN framework to aid breast cancer metastasis detection in lymph nodes. We build on <ref type="bibr" target="#b22">[23]</ref> by leveraging a more recent Inception architecture <ref type="bibr" target="#b19">[20]</ref>, careful image patch sampling and data augmentations. Despite performing inference with stride 128 (instead of 4), we halve the error rate at 8 false positives (FPs) per slide, setting a new state-of-the-art. We also found that several approaches yielded no benefits: (1) a multi-scale approach that mimics the human cognition of a pathologist's examination of biological tissue, <ref type="bibr" target="#b1">(2)</ref> pretraining the model on ImageNet image recognition, and (3) color normalization. Finally, we dispense with the random forest classifier and feature engineering used in <ref type="bibr" target="#b22">[23]</ref> and find that the maximum function is an effective whole-slide classification procedure.</p><p>Related Work Several promising studies have applied deep learning to histopathology. The Camelyon16 challenge winner <ref type="bibr" target="#b0">[1]</ref> achieved a sensitivity of 75% at 8 FP per slide and a slide-level classification AUC of 92.5% <ref type="bibr" target="#b22">[23]</ref>. The authors trained a Inception (V1, GoogLeNet) <ref type="bibr" target="#b19">[20]</ref> model on a pre-sampled set of image patches, and trained a random forest classifier on 28 hand-engineered features to predict the slide label. A second Inception model was trained on harder examples, and predicted points were generated using the average of the two models' predictions. This team later improved these metrics to 82.7% and 99.4% respectively <ref type="bibr" target="#b0">[1]</ref> using color normalization <ref type="bibr" target="#b3">[4]</ref>, additional data augmentation, and lowering the inference stride from 64 to 4. The Camelyon organizers also trained CNNs on smaller datasets to detect breast cancer in lymph nodes and prostate cancer biopsies <ref type="bibr" target="#b15">[16]</ref>. <ref type="bibr" target="#b11">[12]</ref> applied CNNs to segmenting or detecting nuclei, epithelium, tubules, lymphocytes, mitosis, invasive ductal carcinoma and lymphoma. <ref type="bibr" target="#b6">[7]</ref> demonstrated that CNNs achieved higher F1 score and balanced accuracy in detecting invasive ductal carcinoma. CNNs were also used to detect mitosis, winning the ICPR12 <ref type="bibr" target="#b5">[6]</ref> and AMIDA13 <ref type="bibr" target="#b21">[22]</ref> mitosis detection competitions. Other efforts at leveraging machine learning for predictions in cancer pathology include predicting prognosis in non-small cell lung cancer <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Given a gigapixel pathology image (slide 1 ), the goal is to classify if the image contains tumor and localize the tumors for a pathologist's review. This use case and the difficulty of pixel-accurate annotation <ref type="figure" target="#fig_1">(Fig. 2</ref>) renders detection and localization more important than pixel-level segmentation. Because of the large size of the slide and the limited number of slides (270), we train models using   smaller image patches extracted from the slide ( <ref type="figure" target="#fig_0">Fig. 1)</ref>. Similarly, we perform inference over patches in a sliding window across the slide, generating a tumor probability heatmap. For each slide, we report the maximum value in the heatmap as the slide-level tumor prediction.</p><p>We utilize the Inception (V3) architecture <ref type="bibr" target="#b19">[20]</ref> with inputs sized 299 × 299 (the default) to assess the value of initializing from existing models pre-trained on another domain. For each input patch, we predict the label of the center 128×128 region. A 128 pixel region can span several tumor cells and was also used in <ref type="bibr" target="#b15">[16]</ref>. We label a patch as tumor if at least one pixel in the center region is annotated as tumor. We explored the influence of the number of parameters by reducing the number of filters per layer while keeping the number of layers constant (e.g., depth multiplier = 0.1 in TensorFlow). We denote these models "small". We also experimented with multi-scale approaches that utilize patches at multiple magnifications centered on the same region ( <ref type="figure" target="#fig_2">Fig. 3</ref>). Because preliminary experiments did not show a benefit from using up to four magnifications, we present results only for up to two magnifications.</p><p>Training and evaluating our models was challenging because of the large number of patches and the tumor class imbalance. Each slide contains 10, 000 to 400, 000 patches (median 90, 000). However, each tumor slide contains 20 to 150, 000 tumors patches (median 2, 000), corresponding to tumor patch percentages ranging from 0.01% to 70% (median 2%). Avoiding biases towards slides containing more patches (both normal and tumor) required careful sampling. First, we select "normal" or "tumor" with equal probability. Next, we select a slide that contains that class of patches uniformly at random, and sample patches from that slide. By contrast, some existing methods pre-sample a set of patches from each slide <ref type="bibr" target="#b22">[23]</ref>, which limits the breadth of patches seen during training.</p><p>To combat the rarity of tumor patches, we apply several data augmentations. First, we rotate the input patch by 4 multiples of 90 • , apply a left-right flip and repeat the rotations. All 8 orientations are valid because pathology slides do not have canonical orientations. Next, we use TensorFlow's image library (tensorflow.image.random X ) to perturb color: brightness with a maximum delta of 64/255, saturation with a maximum delta of 0.25, hue with a maximum delta of 0.04, and contrast with a maximum delta of 0.75. Lastly, we add jitter to the patch extraction process such that each patch has a small x,y offset of up to 8 pixels. The magnitudes of the color perturbations and jitter were lightly tuned using our validation set. Pixel values are clipped to [0, 1] and scaled to [−1, 1].</p><p>We run inference across the slide in a sliding window with a stride of 128 to match the center region's size. For each patch, we apply the rotations and left-right flip to obtain predictions for each of the 8 orientations, and average the 8 predictions.</p><p>Implementation Details We trained our networks with stochastic gradient descent in TensorFlow <ref type="bibr" target="#b1">[2]</ref>, with 8 replicas each running on a NVIDIA Pascal GPU with asynchronous gradient updates and batch size of 32 per replica. We used RMSProp <ref type="bibr" target="#b20">[21]</ref> with momentum of 0.9, decay of 0.9 and = 1.0. The initial learning rate was 0.05, with a decay of 0.5 every 2 million examples. For refining a model pretrained on ImageNet, we used an initial learning rate of 0.002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation and Datasets</head><p>We use the two Camelyon16 evaluation metrics <ref type="bibr" target="#b0">[1]</ref>. The first metric, the area under receiver operating characteristic, (Area Under ROC, AUC) <ref type="bibr" target="#b9">[10]</ref> evaluates slide-level classification. This metric is challenging because of the potential for FPs when 10 5 patch-level predictions are obtained per slide. We obtained 95% confidence intervals using a bootstrap approach 2 .</p><p>The second metric, FROC <ref type="bibr" target="#b4">[5]</ref>, evaluates tumor detection and localization. We first generate a list of coordinates and corresponding predictions from each heatmap. Among all coordinates that fall within each annotated tumor region, the highest prediction is retained. Coordinates falling outside tumor regions are FPs. We use these values to compute the ROC. The FROC is defined as the sensitivity at 0.25, 0.5, 1, 2, 4, 8 average FPs per tumor-negative slide <ref type="bibr" target="#b15">[16]</ref>. This metric is challenging because reporting multiple points per FP region can quickly erode the score. We focused on the FROC as opposed to the AUC because there are approximately twice as many tumors as slides, which improves the reliability of the evaluation metric. Similar to the AUC, we report 95% confidence intervals by computing the FROC over 2000 bootstrap samples of the predicted points. In addition, we report the sensitivity at 8 FP per slide ("@8FP") to assess the false negative rate.</p><p>To generate points for FROC computation, the Camelyon winners [23,1] thresholded the heatmap to produce a bit-mask, and reported a single prediction for each connected component in the bit-mask. By contrast, we use a non-maxima suppression method similar to <ref type="bibr" target="#b5">[6]</ref> that repeats two steps until no values in the heatmap remain above a threshold t: (1) report the maximum and corresponding coordinate, and (2) set all values within a radius r of the maximum to 0. Because we apply this procedure to the heatmap, r has units of 128 pixels. t controls the number of points reported and has no effect on the FROC unless the curve plateaus before 8 FP. To avoid erroneously dropping tumor predictions, we used a conservative threshold of t = 0.5.</p><p>Datasets Our work utilizes the Camelyon16 dataset <ref type="bibr" target="#b0">[1]</ref>, which contains 400 slides: 270 slides with pixel-level annotations, and 130 unlabeled slides as a test set. <ref type="bibr" target="#b2">3</ref> We split the 270 slides into train and validation sets (Appendix) for hyperparameter tuning. Typically only a small portion of a slide contains biological tissue of interest, with background and fat comprising the remainder (e.g., <ref type="figure" target="#fig_1">Fig. 2</ref>). To reduce computation, we removed background patches (gray value &gt; 0.8 <ref type="bibr" target="#b11">[12]</ref>), and verified visually that lymph node tissue was not discarded.</p><p>Additional Evaluation: NHO-1 We digitized another set of 110 slides (57 containing tumor) from H&amp;E-stained lymph nodes extracted from 20 patients (86 biological tissue blocks 4 ) as an additional evaluation set. These slides came with patient-or block-level labels. To determine the slide labels, a board-certified pathologist blinded to the predictions adjudicated any differences, and briefly reviewed all 110 slides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments &amp; Results</head><p>To perform slide-level classification, the current state-of-the-art methods apply a random forest to features extracted from a heatmap prediction <ref type="bibr" target="#b0">[1]</ref>. Unfortunately, we were unable to train slide-level classifiers because the 100% validationset AUC <ref type="table" target="#tab_0">(Table 1</ref>) rendered internal evaluation of improvements impossible. Nonetheless, using the maximum value of each slide's heatmap achieved AUCs &gt; 97%, statistically indistinguishable from the current best results.</p><p>For tumor-level classification, we find that the connected component approach <ref type="bibr" target="#b22">[23]</ref> provides a 1−5% gain in FROC when the FROC is modest (&lt; 80%), by masking FP regions. However, this approach is sensitive to the threshold (up to 10 − 20% variance), and can confound evaluation of model improvements by grouping multiple nearby tumors as one. By contrast, our non-maxima suppression approach is relatively insensitive to r between 4 and 6, although less accurate models benefited from tuning r using the validation set (e.g., 8). Finally, we achieve 100% FROC on larger tumors (macrometastasis), indicating that most false negatives are comprised of smaller tumors. Previous work (e.g., <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>) has shown that pre-training on a different domain improves performance. However, we find that although pre-training significantly improved convergence speed, it did not improve the FROC (see <ref type="table" target="#tab_0">Table 1</ref>: 40X vs. 40X-pretrained). This may be due to a large domain difference between pathology images and natural scenes in ImageNet, leading to limited transferability. In addition, our large dataset size (10 7 patches) and data augmentation may have enabled the training of accurate models without pre-training.</p><p>Next, we studied the effect of model size. Although we were originally motivated by improved experiment turn-around time, we surprisingly found that slimmed-down Inception architectures with only 3% of the parameters achieved similar performance to the full version ( <ref type="table" target="#tab_0">Table 1</ref>: 40X vs. 40X-small). Thus, we performed the remaining experiments using this smaller model.</p><p>We also experimented with a multi-scale approach inspired by pathologists' workflow of examining a slide at multiple magnifications to get context. However, we find no performance benefit in combining 40X with an additional input at lower magnification <ref type="figure" target="#fig_2">(Fig. 3)</ref>. However, these combinations output smoother heatmaps <ref type="figure" target="#fig_3">(Fig. 4)</ref>, likely because of translational invariance of the CNN and overlap in adjacent patches. These visual improvements can be deceptive: some of the speckles in the 40X models reveal small non-tumor regions surrounded by tumor. <ref type="figure" target="#fig_0">Figures 1 and 3</ref> highlight the variability in the images. Although the current leading approaches report improvements from color normalization, our experi- ments revealed no benefit (Appendix). This could be explained by our extensive data augmentations causing our models to learn color-invariant features.</p><p>Finally, we experimented with ensembling models in two ways. First, averaging predictions across the 8 rotations/flips yielded a few percent improvement in the metrics. Second, ensembling across independently trained models yield additional but smaller improvements, and gave diminishing returns after 3 models.</p><p>Additional Validation We also tested our models on another 110 slides that were digitized on different scanners, from different patients, and treated with different tissue preparation protocols. Encouragingly, we obtained an AUC of 97.6 (93.6, 100), on-par with our Camelyon16 test set performance.</p><p>Qualitative Evaluation We discovered tumors in two "normal" slides: 086 and 144. Fortunately, the challenge organizers confirmed that both were data processing errors, and the patients were unaffected. Remarkably, both slides were in our training set, suggesting that our model was relatively resilient to label noise. In addition, we discovered an additional 7 tumor slides with incomplete annotations: 5 in train, 2 in validation (Appendix). Samples of our predictions and corresponding patches are shown in the Appendix.</p><p>Limitations Our errors were related to out-of-focus tissues (macrophages, germinal centers, stroma), and tissue preparation artifacts. These errors could be reduced by better scanning quality, tissue preparation, and more comprehensive labels for different tissue types. In addition, we were unable to exhaustively tune our hyperparameters owing to the near-perfect FROC and AUC on our validation set. We plan to further develop our work on larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our method yields state-of-the-art sensitivity on the challenging task of detecting small tumors in gigapixel pathology slides, reducing the false negative rate to a quarter of a pathologist and less than half of the previous best result. We further achieve pathologist-level slide-level AUCs in two independent test sets. Our method could improve accuracy and consistency of evaluating breast cancer cases, and potentially improve patient outcomes. Future work will focus on improvements utilizing larger datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Soft Labels</head><p>Our experiments used binary labels: a patch is positive if at least one pixel in the center 128 x 128 region is annotated as tumor. We also explored an alternative "soft label" approach in preliminary experiments, assigning as the label the fraction of tumor pixels in the center region. However, we found that the thresholded labels yielded substantially better performance. Because the FROC rewards detecting tumors of all size equally, this might reflect the model being trained to assign lower values to smaller tumors (where on average, a smaller portion of each patch contains tumor cells).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Image Color Normalization</head><p>As can be seen in <ref type="figure" target="#fig_0">Fig. 1 &amp; 3</ref>, the (H&amp;E) stained tissue vary significantly in color. These variations arise from differences in the underlying biological tissue, physical and chemical preparation of the slide, and scanner adjustments. Because reducing these variations have improved performances in other automated detection systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, we experimented with a similar color normalizing approach. However, we have not found this normalization to improve performance, and thus we detail our approach for reference only. This lack of improvement likely stems from our extensive color perturbations encouraging our models to learn color-insensitive features, and thus the color normalization was unnecessary.</p><p>First, we separate color and intensity information by mapping the raw RGB values to a Hue-Saturation-Density (HSD) space <ref type="bibr" target="#b14">[15]</ref>, and then normalize each component separately. This maps each color channel (I R , I G , I B ) ∈ [0, 255] 3 to a corresponding optical density value: D ν = − ln((I ν + 1)/257), ν ∈ {R, G, B}, followed by applying a common Hue-Saturation-Intensity color space transformation with D = (D R + D B + D G )/3 being the intensity value, and c x = D R D − 1 and c y = (D G − D B )/( √ 3 · D) denoting the Cartesian coordinates that span the two-dimensional hue-saturation plane. We chose the HSD mapping over a direct HSI mapping of RGB values <ref type="bibr" target="#b14">[15]</ref>, because it is more compatible with the image acquisition physics and yields more compact distributions in general.</p><p>Next, we fit a single Gaussian to the color coordinates (c x , c y ) i of the pixels in all tissue-containing patches, i.e. compute their empirical mean µ = (µ x , µ y ) T and covariance Σ ∈ R 2×2 , and then determine the transformation T ∈ R 2×2 of the covariance Σ to a reference covariance matrix Σ R using the Monge-Kantorovitch approach presented in <ref type="bibr" target="#b17">[18]</ref>: T = Σ −1/2 Σ 1/2 Σ R Σ 1/2 Σ −1/2 . Subsequently, we normalize the color values by applying the mapping:</p><formula xml:id="formula_0">c x c y = T c x c y − µ x µ y + µ R x µ R y .<label>(1)</label></formula><p>Intensity values, D i , are normalized in the same manner, i.e. by applying the one-dimensional version of Equation 1 in order to transform the empirical mean and variance of all patch intensities to a reference intensity mean and variance.</p><p>As reference means and variances for the color and intensity component, respectively (i.e. µ R v , Σ R for color), we chose the component-wise medians over the corresponding statistical moments of all the training slides.</p><p>Finally, we map the normalized (c x , c y , D ) values back to RGB space by first applying the inverse HSI transform <ref type="bibr" target="#b14">[15]</ref>, followed by inverting the nonlinear mapping, i.e. by applying I ν = exp(−D ν ) · 257 − 1 to each component ν ∈ {R, G, B}.</p><p>We applied this normalization in two ways. First, we applied this at inference only, by testing a model ("40X-small" in <ref type="table" target="#tab_0">Table 1</ref>) on color-normalized slides. Unfortunately, this resulted in a few percent drop in FROC. Next we trained two models on color-normalized slides, both with and without the color perturbations. We then tested these models on color-normalized slides. Neither approach improved the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Sample Results</head><p>Tumor slides with incomplete annotations At the outset, 11 tumor slides were known to have non-exhaustive pixel level annotations: 015, 018, 020, 029, 033, 044, 046, 051, 054, 055, 079, 092, and 095. Thus, we did not use non-tumor patches from these slides as training examples of normal patches. Over the course of our experiments, we discovered several more such cases that we verified with a pathologist: 010, 025, 034, 056, 067, 085, 110.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Left: three tumor patches and right: three challenging normal patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Difficulty of pixel-accurate annotations for scattered tumor cells. Ground truth annotation is overlaid with a lighter shade. Note that the tumor annotations include both tumor cells and normal cells e.g.,white space representing adipose tissue (fat).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The three colorful blocks represent Inception (V3) towers up to the second-last layer (PreLogit). Single scale utilizes one tower with input images at 40X magnification; multi-scale utilizes multiple (e.g.,2) input magnifications that are input to separate towers and merged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Left to right: sample image, ground truth (tumor in white), and heatmap outputs (40X-ensemble-of-3, 40X+20X, and 40X+10X). Heatmaps of 40X and 40Xensemble-of-3 look identical. The red circular regions at the bottom left quadrant of the heatmaps are unannotated tumor. Some of the speckles are either out of focus patches on the image or non-tumor patches within a large tumor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Left: a patch from a H&amp;E-stained slide. The darker regions are tumor, but not the lighter pink regions. Right: the corresponding predicted heatmap that accurately identifies the tumor cells while assigning lower probabilities to the non-tumor regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Left: a patch from a H&amp;E-stained slide, "Normal" 086. The larger pink cells near the top are tumor, while the smaller pink cells at the bottom are macrophages, a normal cell. Right: the corresponding predicted heatmap that accurately identifies the tumor cells while ignoring the macrophages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Left: a patch from a H&amp;E-stained slide, "Normal" 144. The cluster of larger, dark purple cells in the bottom right quadrant are tumor, while the smaller dark purple cells are lymphocytes. The pink areas are connective tissue, with interspersed tumor cells. Right: the corresponding predicted heatmap that accurately identifies the tumor cells while ignoring the connective tissue and lymphocytes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on Camelyon16 dataset (95% confidence intervals, CI). Bold indicates results within the CI of the best model.</figDesc><table><row><cell>Input &amp;</cell><cell></cell><cell>Validation</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell>model size</cell><cell cols="3">FROC @8FP AUC</cell><cell>FROC</cell><cell>@8FP</cell><cell>AUC</cell></row><row><cell>40X</cell><cell cols="6">98.1 100 99.0 87.3 (83.2, 91.1) 91.1 (87.2, 94.5) 96.7 (92.6, 99.6)</cell></row><row><cell>40X-pretrained</cell><cell cols="6">99.3 100 100 85.5 (81.0, 89.5) 91.1 (86.8, 94.6) 97.5 (93.8, 99.8)</cell></row><row><cell>40X-small</cell><cell cols="6">99.3 100 100 86.4 (82.2, 90.4) 92.4 (88.8, 95.7) 97.1 (93.2, 99.8)</cell></row><row><cell>ensemble-of-3</cell><cell>-</cell><cell>-</cell><cell cols="4">-88.5 (84.3, 92.2) 92.4 (88.7, 95.6) 97.7 (93.0, 100)</cell></row><row><cell>20X-small</cell><cell cols="6">94.7 100 99.6 85.5 (81.0, 89.7) 91.1 (86.9, 94.8) 98.6 (96.7, 100)</cell></row><row><cell>10X-small</cell><cell cols="6">88.7 97.2 97.7 79.3 (74.2, 84.1) 84.9 (80.0, 89.4) 96.5 (91.9, 99.7)</cell></row><row><cell>40X+20X-small</cell><cell cols="6">94.9 98.6 99.0 85.9 (81.6, 89.9) 92.9 (89.3, 96.1) 97.0 (93.1, 99.9)</cell></row><row><cell>40X+10X-small</cell><cell cols="6">93.8 98.6 100 82.2 (77.0, 86.7) 87.6 (83.2, 91.7) 98.6 (96.2, 99.9)</cell></row><row><cell>Pathologist [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.3*</cell><cell>73.3*</cell><cell>96.6</cell></row><row><cell>Camelyon16 winner [1,23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.7</cell><cell>82.7</cell><cell>99.4</cell></row></table><note>"Small" models contain 300K parameters per Inception tower instead of 20M. -: not reported. *A pathologist achieved this sensitivity (with no FP) using 30 hours.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.1 Dataset Details</figDesc><table><row><cell></cell><cell cols="6">Number of Slides Number of Patches (M) Number of Tumors</cell></row><row><cell>Dataset/split</cell><cell cols="5">Normal Tumor Total Normal Tumor Total Macro</cell><cell>Micro</cell></row><row><cell>Camelyon-Train</cell><cell>127</cell><cell>88</cell><cell>215 13+8.9* 0.87</cell><cell>23</cell><cell>81</cell><cell>345</cell></row><row><cell cols="2">Camelyon-Validation 32</cell><cell>22</cell><cell>54 3.8+2.3* 0.28</cell><cell>6.4</cell><cell>14</cell><cell>58</cell></row><row><cell>Camelyon-Test</cell><cell>80</cell><cell>50</cell><cell>130</cell><cell></cell><cell>40</cell><cell>185</cell></row><row><cell>NHO-1</cell><cell>53</cell><cell>57</cell><cell>110</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Table 2. Number of slides, patches (in millions), and tumors in each dataset/split.</cell></row><row><cell cols="7">We excluded "Normal" slide 144 because preliminary experiments uncovered tumors</cell></row><row><cell cols="7">in this slide. Later experiments also uncovered tumors in "Normal" 086, but this slide</cell></row><row><cell cols="7">was used in training for the results presented in this paper. In addition, Test slide</cell></row><row><cell cols="7">049 was an accidental duplication by the organizers (Tumor 036), and was not used for</cell></row><row><cell cols="7">evaluation. Tumor sizes: macrometastasis (macro, &gt; 2000µm), micrometastasis (micro,</cell></row><row><cell cols="7">&gt; 200 &amp; ≤ 2000µm). *normal patches extracted from the tumor slides. : additional</cell></row><row><cell cols="4">evaluation set with slide-level labels only.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Each slide contains human lymph node tissue stained with hematoxylin and eosin (H&amp;E), and is scanned at the most common high magnification in a microscope, "40X". We also experimented with 2-and 4-times down-sampled patches ("20X" and "10X").</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Sample with replacement n slides from the dataset/split, where n is the number of slides in the dataset/split, and compute the AUC. Repeat for a total of 2000 bootstrap samples, and report the 2.5 and 97.5 percentile values.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The test slides labels were released recently as part of the training dataset for Came-lyon17. We used these labels for evaluation, but not for parameter tuning.<ref type="bibr" target="#b3">4</ref> A tissue block can contain multiple slides that vary considerably at the pixel level.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 8</ref><p>. Left: a patch from a H&amp;E-stained slide in our additional validation set, NHO-1. The tumor cells are a lighter purple than the surrounding cells. A variety of artifacts are visible: the dark continuous region in the top left quadrant is an air bubble, and the white parallel streaks in the tumor and adjacent tissue are cutting artifacts. Furthermore, the tissue is hemorrhagic, necrotic and poorly processed, leading to color alterations to the typical pink and purple of a H&amp;E slide. Right: the corresponding predicted heatmap that accurately identifies the tumor cells while ignoring the various artifacts, including lymphocytes and the cutting artifacts running through the tumor tissue.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://camelyon16.grand-challenge.org/" />
	</analytic>
	<monogr>
		<title level="j">Camelyon</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2018" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TensorFlow</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sentinel lymph node in breast cancer: Review article from a pathologists point of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Apple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Pathol. and Transl. Medicine</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stain specific standardization of whole-slide histopathological images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="404" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A free response approach to the measurement and characterization of radiographic observer performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Bunch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medicine VI pp</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="page" from="124" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. on Medical Image Comput. and Comput. Interv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cruz-Roa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE medical imaging</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. and Pattern Recognit</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of the Am. Medical Soc</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The meaning and use of the area under a receiver operating characteristic (roc) curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janowczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Pathol. Informatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pathology imaging informatics for quantitative analysis of wholeslide images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kothari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of the Am. Medical Informatics Assoc</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1099" to="1108" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. in Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hue-saturation-density model for stain recognition in digital images from transmitted light microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="284" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The linear monge-kantorovitch linear colour mapping for example-based colour transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pitié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kokaram</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. and Pattern Recognit</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Assessment of algorithms for mitosis detection in breast cancer histopathology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05718</idno>
		<title level="m">Deep learning for identifying metastatic breast cancer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<title level="m">How transferable are features in deep neural networks? Adv. in Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
