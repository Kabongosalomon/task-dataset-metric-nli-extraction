<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Google Landmarks Dataset v2 A Large-Scale Benchmark for Instance-Level Recognition and Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
							<email>weyand@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr√©</forename><surname>Araujo</surname></persName>
							<email>andrearaujo@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Cao</surname></persName>
							<email>bingyi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
							<email>jacksim@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Google Landmarks Dataset v2 A Large-Scale Benchmark for Instance-Level Recognition and Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While image retrieval and instance recognition techniques are progressing rapidly, there is a need for challenging datasets to accurately measure their performance -while posing novel challenges that are relevant for practical applications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new benchmark for large-scale, fine-grained instance recognition and image retrieval in the domain of human-made and natural landmarks. GLDv2 is the largest such dataset to date by a large margin, including over 5M images and 200k distinct instance labels. Its test set consists of 118k images with ground truth annotations for both the retrieval and recognition tasks. The ground truth construction involved over 800 hours of human annotator work. Our new dataset has several challenging properties inspired by realworld applications that previous datasets did not consider: An extremely long-tailed class distribution, a large fraction of out-of-domain test photos and large intra-class variability. The dataset is sourced from Wikimedia Commons, the world's largest crowdsourced collection of landmark photos. We provide baseline results for both recognition and retrieval tasks based on state-of-the-art methods as well as competitive results from a public challenge. We further demonstrate the suitability of the dataset for transfer learning by showing that image embeddings trained on it achieve competitive retrieval performance on independent datasets. The dataset images, ground-truth and metric scoring code are available at https://github.com/cvdfoundation/google-landmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image retrieval and instance recognition are fundamental research topics which have been studied for decades. The task of image retrieval <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref> is to rank images in an index set w.r.t. their relevance to a query image. The task of instance recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37]</ref> is to identify which specific instance of an object class (e.g. the instance "Mona Lisa" of the object class "painting") is shown in a query image. * equal contribution <ref type="figure">Figure 1</ref>: The Google Landmarks Dataset v2 contains a variety of natural and human-made landmarks from around the world. Since the class distribution is very long-tailed, the dataset contains a large number of lesser-known local landmarks. <ref type="bibr" target="#b0">1</ref> As techniques for both tasks have evolved, approaches have become more robust and scalable and are starting to "solve" early datasets. Moreover, while increasingly largescale classification datasets like ImageNet <ref type="bibr" target="#b46">[47]</ref>, COCO <ref type="bibr" target="#b35">[36]</ref> and OpenImages <ref type="bibr" target="#b33">[34]</ref> have established themselves as standard benchmarks, image retrieval is still commonly evaluated on very small datasets. For example, the original Oxford5k <ref type="bibr" target="#b40">[41]</ref> and Paris6k <ref type="bibr" target="#b41">[42]</ref> datasets that were released in 2007 and 2008, respectively, have only 55 query images of 11 instances each, but are still widely used today. Because both datasets only contain images from a single city, results may not generalize to larger-scale settings.</p><p>Many existing datasets also do not present real-world challenges. For instance, a landmark recognition system that is applied in a generic visual search app will be queried with a large fraction of non-landmark queries, like animals, plants, or products, which it is not expected to yield any results for. Yet, most instance recognition datasets have only "on-topic" queries and do not measure the false-positive rate on out-ofdomain queries. Therefore, larger, more challenging datasets are necessary to fairly benchmark these techniques while providing enough challenges to motivate further research.</p><p>A possible reason that small-scale datasets have been the dominant benchmarks for a long time is that it is hard to collect instance-level labels at scale. Annotating millions of images with hundreds of thousands of fine-grained instance labels is not easy to achieve when using labeling services like Amazon Mechanical Turk, since taggers need expert knowledge of a very fine-grained domain. We introduce the Google Landmarks Dataset v2 (GLDv2), a new large-scale dataset for instance-level recognition and retrieval. GLDv2 includes over 5M images of over 200k human-made and natural landmarks that were contributed to Wikimedia Commons by local experts. <ref type="figure">Fig. 1</ref> shows a selection of images from the dataset and <ref type="figure" target="#fig_0">Fig. 2</ref> shows its geographical distribution. The dataset includes 4M labeled training images for the instance recognition task and 762k index images for the image retrieval task. The test set consists of 118k query images with ground truth labels for both tasks. To mimic a realistic setting, only 1% of the test images are within the target domain of landmarks, while 99% are outof-domain images. While the Google Landmarks Dataset v2 focuses on the task of recognizing landmarks, approaches that solve the challenges it poses should readily transfer to other instance-level recognition tasks, like logo, product or artwork recognition.</p><p>The Google Landmarks Dataset v2 is designed to simulate real-world conditions and thus poses several hard challenges. It is large scale with millions of images of hundreds of thousands of classes. The distribution of these classes is very long-tailed ( <ref type="figure">Fig. 1</ref>), making it necessary to deal with extreme class imbalance. The test set has a large fraction of out-of-domain images, emphasizing the need for low falsepositive recognition rates. The intra-class variability is very high, since images of the same class can include indoor and outdoor views, as well as images of indirect relevance to a class, such as paintings in a museum. The goal of the Google Landmarks Dataset v2 is to become a new bench-mark for instance-level recognition and retrieval. In addition, the recognition labels can be used for training image descriptors or pre-training approaches for related domains where less data is available. We show that the dataset is suitable for transfer learning by applying learned descriptors on independent datasets where they achieve competitive performance.</p><p>The dataset was used in two public challenges on Kaggle 2 , where researchers and hobbyists competed to develop models for instance recognition and image retrieval. We discuss the results of the challenges in Sec. 5.</p><p>The dataset images, instance labels for training, the ground truth for retrieval and recognition and the metric computation code are publicly available 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image recognition problems range from basic categorization ("cat", "shoe", "building"), through fine-grained tasks involving distinction of species/models/styles ("Persian cat", "running shoes", "Roman Catholic church"), to instance-level recognition ("Oscar the cat", "Adidas Duramo 9", "Notre-Dame cathedral in Paris"). Our new dataset focuses on tasks that are at the end of this continuum: identifying individual human-made and natural landmarks. In the following, we review image recognition and retrieval datasets, focussing mainly on those which are most related to our work.</p><p>Landmark recognition/retrieval datasets. We compare existing datasets for landmark recognition and retrieval against our newly-proposed dataset in Tab. 1. The Oxford <ref type="bibr" target="#b40">[41]</ref> and Paris <ref type="bibr" target="#b41">[42]</ref> datasets contain tens of query images and thousands of index images from landmarks in Oxford and Paris, respectively. They have consistently been used in image retrieval for more than a decade, and were re-annotated recently, with the addition of 1M worldwide distractor index images <ref type="bibr" target="#b42">[43]</ref>. Other datasets also focus on imagery from a single city: Rome 16k <ref type="bibr" target="#b0">[1]</ref>; Geotagged Streetview Images <ref type="bibr" target="#b31">[32]</ref> containing 17k images from Paris; San Francisco Landmarks <ref type="bibr" target="#b13">[14]</ref> containing 1.7M images; 24/7 Tokyo <ref type="bibr" target="#b55">[56]</ref> containing 1k images under different illumination conditions and Paris500k <ref type="bibr" target="#b60">[61]</ref>, containing 501k images.</p><p>More recent datasets contain images from a much larger variety of locations. The European Cities (EC) 50k dataset <ref type="bibr" target="#b4">[5]</ref> contains images from 9 cities, with a total of 20 landmarks; unannotated images from other 5 cities are used as distractors. This dataset also has a version with 1M images from 22 cities where the annotated images come from a single city <ref type="bibr" target="#b3">[4]</ref> Instance-level recognition datasets. Instance-level recognition refers to a very fine-grained identification problem, where the goal is to visually recognize a single (or indistinguishable) occurrence of an entity. This problem is typically characterized by a large number of classes, with high imbalance, and small intra-class variation. Datasets for such problems have been introduced in the community, besides the landmark datasets mentioned previously. For example: logos <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46]</ref>, cars <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b64">65]</ref>, products <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b49">50]</ref>, artwork <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, among others <ref type="bibr" target="#b10">[11]</ref>.</p><p>Other image recognition datasets. There are numerous computer vision datasets for other types of image recognition problems. Basic image categorization is addressed by datasets such as Caltech 101 <ref type="bibr" target="#b19">[20]</ref>, Caltech 256 <ref type="bibr" target="#b22">[23]</ref>, Im-ageNet <ref type="bibr" target="#b46">[47]</ref> and more recently OpenImages <ref type="bibr" target="#b33">[34]</ref>. Popular fine-grained recognition datasets include CUB-200-2011 <ref type="bibr" target="#b56">[57]</ref>, iNaturalist <ref type="bibr" target="#b25">[26]</ref>, Stanford Cars <ref type="bibr" target="#b32">[33]</ref>, Places <ref type="bibr" target="#b65">[66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Goals</head><p>The Google Landmarks Dataset v2 aims to mimic the following challenges of industrial landmark recognition systems: Large scale: To cover the entire world, a corpus of millions of photos is necessary. Intra-class variability: Pho-tos are taken under varying lighting conditions and from different views, including indoor and outdoor views of buildings. There will also be photos related to the landmark, but not showing the landmark itself, e.g. floor plans, portraits of architects, or views from the landmark. Long-tailed class distribution: There are much more photos of famous landmarks than of lesser-known ones. Out-of-domain queries: The query stream that these systems receive may come from various applications such as photo album apps or visual search apps and often contains only a small fraction of landmarks among many photos of other object categories. This poses a significant challenge for the robustness of the recognition algorithm. We designed our dataset to capture these challenges. An additional goal was to use only images whose licenses permit indefinite retention and reproduction in publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-goals.</head><p>In contrast to many other datasets, we explicitly did not design GLDv2 to have clean query and index sets for the reasons mentioned above. Also, the dataset does not aim to measure generalization of embedding models to unseen data -therefore, the index and training sets do not have disjoint class sets. Finally, we do not aim to provide an image-level retrieval ground truth at this point due to very expensive annotation costs. Instead, the retrieval ground truth is on a class-level, i.e. all index images that belong to the same class as a query image will be marked as relevant in the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scale and Splits</head><p>The Google Landmarks Dataset v2 consists of over 5M images and over 200k distinct instance labels, making it the largest instance recognition dataset to date. It is divided into three subsets: (i) 118k query images with ground truth annotations, (ii) 4.1M training images of 203k landmarks with labels that can be used for training, and (iii) 762k index images of 101k landmarks. We also make available a cleaner, reduced training set of 1.6M images and 81k landmarks (see Sec. 5.1). While the index and training set do not share images, their label space is highly overlapping, with 92k common classes. The query set is randomly split into 1/3 validation and 2/3 testing data. The validation data was used for the "Public" leaderboard in the Kaggle competition, which allowed participants to submit solutions and view their scores in real-time. The test set was used for the "Private" leaderboard, which was used for the final ranking and was only revealed at the end of the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Challenges</head><p>Besides its scale, the Google Landmarks Dataset v2 presents practically relevant challenges, as motivated above. Class distribution. The class distribution is extremely longtailed, as illustrated in <ref type="figure">Fig. 1</ref>. 57% of classes have at most 10 images and 38% of classes have at most 5 images. The dataset therefore contains a wide variety of landmarks, from world-famous ones to lesser-known, local ones. Intra-class variation. As is typical for an image dataset collected from the web, the Google Landmarks Dataset v2 has large intra-class variability, including views from different vantage points and of different details of the landmarks, as well as both indoor and outdoor views for buildings. Out-of-domain query images. To simulate a realistic query stream, the query set consists of only 1.1% images of landmarks and 98.9% out-of-domain images, for which no result is expected. This puts a strong emphasis on the importance of robustness in a practical instance recognition system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Metrics</head><p>The Google Landmarks Dataset v2 uses well-established metrics, which we now introduce. Reference implementations are available on the dataset website.</p><p>Recognition is evaluated using micro Average Precision (¬µAP) <ref type="bibr" target="#b39">[40]</ref> with one prediction per query. This is also known as Global Average Precision (GAP). It is calculated by sorting all predictions in descending order of their confidence and computing:</p><formula xml:id="formula_0">¬µAP = 1 M N i=1 P(i)rel(i),<label>(1)</label></formula><p>where N is the total number of predictions across all queries; M is the total number of queries with at least one landmark from the training set visible in it (note that most queries do not depict landmarks); P(i) is the precision at rank i; and rel(i) is a binary indicator function denoting the correctness of prediction i. Note that this metric penalizes a system that predicts a landmark for an out-of-domain query image; overall, it measures both ranking performance as well as the ability to set a common threshold across different queries.</p><p>Retrieval is evaluated using mean Average Precision@100 (mAP@100), which is a variant of the standard mAP metric   that only considers the top-100 ranked images. We chose this limitation since exhaustive retrieval of every matching image is not necessary in most applications, like image search. The metric is computed as follows:</p><formula xml:id="formula_1">mAP@100 = 1 Q Q q=1 AP@100(q),<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">AP@100(q) = 1 min(m q , 100) min(nq,100) k=1 P q (k)rel q (k) (3)</formula><p>where Q is the number of query images that depict landmarks from the index set; m q is the number of index images containing a landmark in common with the query image q (note that this is only for queries which depict landmarks from the index set, so m q = 0); n q is the number of predictions made by the system for query q; P q (k) is the precision at rank k for the q-th query; and rel q (k) is a binary indicator function denoting the relevance of prediction k for the q-th query. Some query images will have no associated index images to retrieve; these queries are ignored in scoring, meaning this metric does not penalize the system if it retrieves landmark images for out-of-domain queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data Distribution</head><p>The Google Landmarks Dataset v2 is a truly worldspanning dataset, containing landmarks from 246 of the 249 countries in the ISO 3166-1 country code list. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the number of images in the top-20 countries and <ref type="figure" target="#fig_3">Fig. 4</ref> shows the number of images by continent. We can see that even though the dataset is world-spanning, it is by no means a representative sample of the world, because the number of images per country depends heavily on the activity of the local Wikimedia Commons community. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the distribution of the dataset images by landmark category, as obtained from the Google Knowledge Graph. By far the most frequent category is churches,  followed by parks and museums. Counting only those categories with over 25k images, roughly 28% are natural landmarks while 72% are human-made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Image Licenses</head><p>All images in GLDv2 are freely licensed, so that the dataset is indefinitely retainable and does not shrink over time, allowing recognition and retrieval approaches to be compared over a long period of time. All images can be freely reproduced in publications, as long as proper attribution is provided. The image licenses are either Creative Commons 4 or Public Domain. We provide a list of attributions for those images that require it so dataset users can easily give attribution when using the images in print or on the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset Construction</head><p>This section details the data collection process and the construction of the ground truth. <ref type="bibr" target="#b3">4</ref> https://creativecommons.org</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Collection</head><p>Data sources. The main data source of the Google Landmarks Dataset v2 is Wikimedia Commons 5 , the media repository behind Wikipedia. Wikimedia Commons hosts millions of photos of landmarks licensed under Creative Commons and Public Domain licenses, contributed by an active community of photographers as well as partner organizations such as libraries, archives and museums. Its large coverage of the world's landmarks is partly thanks to Wiki Loves Monuments 6 , an annual world-wide contest with the goal to upload high-quality, freely licensed photos of landmarks to the site and to label them within a fine-grained taxonomy of the world's cultural heritage sites. In addition to Wikimedia Commons, we collected realistic query images by crowdsourcing. Operators were sent out to take photos of selected landmarks around the world with smartphones.</p><p>Training and index sets. <ref type="figure" target="#fig_5">Fig. 6</ref> (left) shows the process we used to mine landmark images from Wikimedia Commons. Wikimedia Commons is organized into a hierarchy of categories that form its taxonomy. Each category has a unique URL where all its associated images are listed. We found that the Wikimedia Commons hierarchy does not have a suitable set of top-level categories that map to human-made and natural landmarks. Instead, we found the Google Knowledge Graph 7 to be useful to obtain an exhaustive list of the landmarks of the world. To obtain a list of Wikimedia Commons categories for landmarks, we queried the Google Knowledge Graph with terms like "landmarks", "tourist attractions", "points of interest", etc. For each returned knowledge graph entity, we obtained its associated Wikipedia articles. We then followed the link to the Wikimedia Commons Category page in the Wikipedia article. Note that while Wikipedia may have articles about the same landmark in different languages, Wikimedia Commons only has one category per subject. We then downloaded all images contained in the Wikimedia Commons pages we obtained. To avoid ambiguities, we enforced the restriction that each mined image be associated to a single Wikimedia category or Knowledge Graph entity. We use the Wikimedia Commons category URLs as the canonical class labels. The training and index sets are collected in this manner.</p><p>Query set. The query set consists of "positive" query images of landmarks and "negative" query images not showing landmarks. For collecting the "positive" query set, we selected a subset of the landmarks we collected from Wikimedia Commons and asked crowdsourcing operators to take photos of them. For the "negative" query data collection, we used the same process as for the index and training data, but queried the Knowledge Graph only with terms that are unrelated to landmarks. We also removed any negative query images that had near-duplicates in the index or training sets.</p><p>Dataset partitioning. The landmark images from Wikimedia Commons were split into training and index sets based on their licenses. We used CC0 and Public Domain photos for the index set while photos with "Creative Commons By" licenses that did not have a "No Derivatives" clause were used for the training set. As a result, the label spaces of index and training sets have a large, but not complete, overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Test Set Re-Annotation</head><p>Visual inspection of retrieval and recognition results showed that many errors were due to missing ground truth annotations, which was due to the following reasons: (i) Crowdsourced labels from Wikimedia Commons can contain errors and omissions. (ii) Some query images contain multiple landmarks, but only one of them was present in the ground truth. (iii) There are sometimes multiple valid labels for an image on different hierarchical levels. For example, for a picture of a mountain in a park, both the mountain and the park would be appropriate labels. (iv) Some of the "negative" query images do actually depict landmarks.</p><p>We therefore amend the ground truth with human annotations. However, the large number of instance labels makes this a challenging problem: Each query image would need to be annotated with one out of 200k landmark classes, which is infeasible for human raters. We therefore used the model predictions of the top-ranked teams from the challenges to propose potential labels to the raters. To avoid bias in the new annotation towards any particular method, we used the top-10 submissions which represent a wide range of methods (see Sec. 5.4). A similar idea was used to construct the distractor set of the revisited Oxford and Paris datasets <ref type="bibr" target="#b42">[43]</ref>, where hard distractors were mined using a combination of different retrieval methods. <ref type="figure" target="#fig_5">Fig. 6 (right)</ref> shows the user interface of the re-annotation tool. On the left side, we show a sample of index/train images of a given landmark label. On the right, we show the query images that are proposed for this label and ask raters to click on the correct images. This way, we simplified the question of "which landmark is it?" as "is it this landmark?", which is a simple "yes" or "no" question. Grouping the query images associated with the same landmark class together fur-ther improved the re-annotation efficiency, since raters do not need to switch context between landmark classes. To make efficient use of rater time, we only selected the highestconfidence candidates from the top submissions, since those are more likely to be missing annotations rather than model errors. In total, we sent out ‚àº10k recognition query images and ‚àº90k retrieval query images for re-annotation. To ensure the re-annotation quality, we sent each image to 3 human raters and assigned the label based on majority voting. In total, we leveraged ‚àº800 rater hours on the re-annotation process. This re-annotation increased the number of recognition annotations by 72% and the number of retrieval annotations by 30%. If a "negative" query image was verified to contain a landmark, it was moved to the "positive" query set. We will continue to improve the ground truth and will make future versions available on the dataset website. For comparability, past versions will stay available and each ground truth will receive a version number that we ask dataset users to state when publishing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We demonstrate usage of the dataset and present several baselines that can be used as reference results for future research, besides discussing results from the public challenge. All results presented in this section are w.r.t. version 2.1 of the dataset ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Set Pre-Processing</head><p>The Google Landmarks Dataset v2 training set presents a realistic crowdsourced setting with diverse types of images for each landmark: e.g., for a specific museum there may be outdoor images showing the building facade, but also indoor images of paintings and sculptures that are on display. Such diversity within a class may pose challenges to the training process, so we consider the pre-processing steps proposed in <ref type="bibr" target="#b63">[64]</ref> in order to make each class more visually coherent. Within each class, each image is queried against all others by global descriptor similarity, followed by geometric verification of the top-100 most similar images using local features. The global descriptor is a ResNet-101 <ref type="bibr" target="#b24">[25]</ref> embedding and the local features are DELF <ref type="bibr" target="#b38">[39]</ref>, both trained on the first Google Landmarks Dataset version (GLDv1) <ref type="bibr" target="#b38">[39]</ref>. If an image is successfully matched to at least 3 other images, each   with at least 30 inliers, it is selected; otherwise discarded. We refer to the resulting dataset version as GLDv2-trainclean and make it available on the dataset website. Tab. 2 presents the number of selected images and labels: 1.6M training images (38%) and 81k labels (40%). Even if this version only contains less than half of the data from GLDv2train, it is still much larger than the training set of any other landmark recognition dataset. We also experiment with a variant of GLDv2-train-clean, where classes with fewer than 15 images are removed, referred to as GLDv2-train-no-tail; it has approximately the same number of images as GLDv1train, but 2√ó the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparing Training Datasets</head><p>We assess the utility of our dataset's training split for transfer learning, by using it to learn global descriptor models and evaluating them on independent landmark retrieval datasets: Revisited Oxford (ROxf) and Revisited Paris (RPar) <ref type="bibr" target="#b42">[43]</ref>. A ResNet-101 <ref type="bibr" target="#b24">[25]</ref> model is used, with GeM <ref type="bibr" target="#b43">[44]</ref> pooling, trained with ArcFace loss <ref type="bibr" target="#b17">[18]</ref>. Results are presented in Tab. 3, where we compare against models trained on other datasets, as well as recent state-of-the-art results -including methods based on global descriptors <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, local feature aggregation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b52">53]</ref> and unified global+local features <ref type="bibr" target="#b9">[10]</ref>. Note that "SP" denotes methods using local feature-based spatial verification for re-ranking.</p><p>Model training on GLDv2-train-clean provides a substantial boost in performance, compared to training on GLDv1train: mean average precision (mAP) improves by up to 10%. We also compare with models trained on the Landmarks-full DELF-KD-tree <ref type="bibr" target="#b38">[39]</ref> GLDv1-train <ref type="bibr" target="#b38">[39]</ref> 44.84 41.07 DELF-ASMK* <ref type="bibr" target="#b42">[43]</ref> 16.79 -DELF-ASMK*+SP <ref type="bibr" target="#b42">[43]</ref> 60.16 -DELF-R-ASMK* <ref type="bibr" target="#b52">[53]</ref> 47.54 -DELF-R-ASMK*+SP <ref type="bibr" target="#b52">[53]</ref> 54.03 -DELG global-only <ref type="bibr" target="#b9">[10]</ref> 32.03 32.52 DELG global+SP <ref type="bibr" target="#b9">[10]</ref> 58.45 56.39  and Landmarks-clean datasets <ref type="bibr" target="#b21">[22]</ref>: performance is significantly lower, which is likely due to their much smaller scale. Our simple global descriptor baseline even outperforms all methods on the RPar dataset, and comes close to the state-ofthe-art in ROxf. Results in the GLDv2-train-no-tail variant show high performance, although a little lower than GLDv2train-clean in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Benchmarking</head><p>Tab. 4 and Tab. 5 show results of baseline methods for the recognition and retrieval tasks, respectively. The methods shown use deep local and global features extracted with models that were trained using different datasets and loss functions. All global descriptors use GeM <ref type="bibr" target="#b43">[44]</ref> pooling. For recognition with global descriptors, all methods compose landmark predictions by aggregating the sums of cosine similarities of the top-5 retrieved images; the landmark with the highest sum is used as the predicted label and its sum of cosine similarities is used as the confidence score. For methods with SP, we first find the global descriptor nearest neighbors; then spatially verify the top 100 images; sort images based on the number of inliers; and aggregate scores over the top-5 images to compose the final prediction, where the score of each image is given by min(l,70) 70 + g, where l is the number of inliers and g the global descriptor cosine similarity. For DELF-KD-tree, we use the system proposed   <ref type="table">Table 7</ref>: Top 3 results on retrieval challenge (% mAP@100). GF = global feature similarity search; LF = local feature matching re-ranking; DBA = database augmentation; QE = query expansion; C = re-ranking based on classifier predictions; EGT = Explore-Exploit Graph Traversal. The last two columns show the effect of the re-annotation on the retrieval precision on the testing set (% Precision@100).</p><p>in <ref type="bibr" target="#b38">[39]</ref> to obtain the top prediction for each query (if any). In all cases, training on GLDv1 or GLDv2 improves performance substantially when compared to training on Landmarks-full/clean; for the retrieval task, GLDv2 training performs better, while for the recognition task, GLDv1 performs better. In the retrieval task, our global descriptor approach trained on GLDv2 outperforms all others; in this case, we also report results from <ref type="bibr" target="#b63">[64]</ref> comparing different loss functions; CosFace and ArcFace perform similarly, while Triplet and AP losses perform worse. In the recognition case, a system purely based on local feature matching with DELF-KD-tree outperforms global descriptors; better performance is obtained when combining local features with global features (DELG), or using local feature aggregation techniques (DELF-ASMK +SP). Note that even better performance may be obtained by improving the combination of local and global scores, as presented in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Challenge Results</head><p>Tab. 6 and Tab. 7 present the top 3 results from the public challenges, for the recognition and retrieval tracks, respectively. These results are obtained with complex techniques involving ensembling of multiple global and/or local features, usage of trained detectors/classifiers to filter queries, and several query/database expansion techniques.</p><p>The most important building block in these systems is the global feature similarity search, which is the first stage in all successful approaches. These were learned with different backbones such as ResNet <ref type="bibr" target="#b24">[25]</ref>, ResNeXt <ref type="bibr" target="#b61">[62]</ref>, Squeeze-and-Excitation <ref type="bibr" target="#b26">[27]</ref>, FishNet <ref type="bibr" target="#b50">[51]</ref> and Inception-V4 <ref type="bibr" target="#b51">[52]</ref>; pooling methods such as SPoC <ref type="bibr" target="#b5">[6]</ref>, RMAC <ref type="bibr" target="#b53">[54]</ref> or GeM <ref type="bibr" target="#b43">[44]</ref>; loss functions such as ArcFace <ref type="bibr" target="#b17">[18]</ref>, CosFace <ref type="bibr" target="#b57">[58]</ref>, N-pairs <ref type="bibr" target="#b48">[49]</ref> and triplet <ref type="bibr" target="#b47">[48]</ref>. Database-side augmentation <ref type="bibr" target="#b2">[3]</ref> is also often used to improve image representations.</p><p>The second most widely used type of method is local feature matching re-ranking, with DELF <ref type="bibr" target="#b38">[39]</ref>, SURF <ref type="bibr" target="#b8">[9]</ref> or SIFT <ref type="bibr" target="#b37">[38]</ref>. Other re-ranking techniques which are especially important for retrieval tasks, such as query expansion (QE) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref> and graph traversal <ref type="bibr" target="#b12">[13]</ref>, were also employed.</p><p>These challenge results can be useful as references for fu-ture research. Even with such complex methods, there is still substantial room for improvement in both tasks, indicating that landmark recognition and retrieval are far from solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effect of Re-annotation</head><p>The goal of the re-annotation (Sec. 4.2) was to fill gaps in the ground truth where index images showing the same landmark as a query were not marked as relevant, or where relevant class annotations were missing. To show the effect of this on the metrics, Tab. 6 and 7 also list the scores of the top methods from the challenge before re-annotation. There is a clear improvement in ¬µAP for the recognition challenge, which is due to a large number of correctly recognized instances that were previously not counted as correct. However, a similar improvement cannot be observed for the retrieval results. This is because by the design of the the dataset, the retrieval annotations are on the class level rather than the image level. Therefore, if a class is marked as relevant for a query, all of its images are, regardless of whether they have shared content with the query image. So, while the measured precision of retrieval increases, the measured recall decreases, overall resulting in an almost unchanged mAP score. This is illustrated in the last two columns of Tab. 7, which shows that Precision@100 consistently increases as an effect of the re-annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented the Google Landmarks Dataset v2, a new large-scale benchmark for image retrieval and instance recognition. It is the largest such dataset to date and presents several real-world challenges that were not present in previous datasets, such as extreme class imbalance and out-ofdomain test images. We hope that the Google Landmarks Dataset v2 will help advance the state of the art and foster research that deals with these novel challenges for instance recognition and image retrieval.</p><p>Kaggle for their support in organizing the challenges, CVDF for hosting the dataset and the co-organizers of the Landmark Recognition workshops at CVPR'18 and CVPR'19. We also thank all teams participating in the Kaggle challenges, especially those whose solutions we used for re-annotation. Special thanks goes to team smlyaka <ref type="bibr" target="#b63">[64]</ref> for contributing the cleaned-up dataset and several baseline experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Comparison of Retrieval Subset with Oxford and Paris Datasets</head><p>We offer a more detailed comparison of the retrieval subset of the Google Landmarks Dataset v2 (here denoted GLDv2-retrieval) with the ROxford and RParis datasets <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, which are popular for image retrieval research.</p><p>Scale. While the ROxford and RParis datasets cover 11 landmarks each and focus on a single city, GLDv2-retrieval has 101k landmarks from all over the world. While the ROxford and RParis datasets have 70 query images each, GLDv2-retrieval has 1.1k query images. The ROxford and RParis datasets have 5k and 6k index images of landmarks, respectively and additionally have a set of 1M random distractor images, called R1M. Retrieval scores are typically reported both with and without including the distractor set in the index. GLDv2-retrieval has 762k index images of landmarks and has no additional distractors. When including the 1M distractor set, the ROxford/RParis index becomes larger than GLDv2-retrieval's index. However, there is a difference as to how these index images are collected. For GLDv2retrieval, index images are collected from an online database with tagged landmarks. On the other hand, R1M is collected by filtering unconstrained web images with semi-automatic methods, to select those which are the most challenging for recent landmark retrieval techniques; many of them contain actual landmarks, while others may contain images from other domains but which may lead to image representations which "trick" recent landmark retrieval techniques.</p><p>When not using the distractor set, the ROxford and RParis datasets are more accessible when limited resources are available and evaluations on them have much shorter turnaround times. GLDv2-retrieval covers a wider range of landmarks, so we expect results on it to be more representative of practical applications. Recent papers <ref type="bibr" target="#b54">[55]</ref> have also reported results with a small subset of GLDv2, which we believe is a good direction for making evaluations more feasible when only limited resources are available; using the full dataset should be required, though, to draw more robust conclusions.</p><p>Evaluation protocol. The query set of GLDv2-retrieval is split into a validation and a testing subset, allowing for a clean evaluation protocol that avoids overfitting: methods should tune performance using the validation split, and only report the testing score for the best tuned version. On the other hand, the ROxford and RParis datasets do not offer such a split. In practice, frequent testing during development is often performed without using the distractor set, and experiments with the distractors are done less frequently, to assess large-scale performance <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">45]</ref>. Thus, the original ROxford/RParis datasets are effectively used as the "validation" sets and the datasets with distractors are used as the "testing" sets. This setup is not ideal since the "validation" set is a subset of the "testing" set and usually performance on the small scale versions is relatively the same as on the large scale version. This makes it challenging to detect overfitting on these datasets, and in the future we would recommend that a protocol more similar to Tolias et al. <ref type="bibr" target="#b54">[55]</ref> would be adopted for these datasets, where a separate validation set is used for tuning.</p><p>Challenges. The queries of ROxford and RParis are cropped-out regions of images, such as individual windows of a building. These details are often hard to spot in the index images even for humans. The datasets thus pose significant challenges for scale and perspective invariant matching. The queries of GLDv2 are not cropped, so queries can show both the full landmark as well as architectural details. GLDv2 does not explicitly focus on finding small image regions, but provides a natural spectrum of both easy and hard cases for image matching.</p><p>Moreover, index images from ROxford and RParis are categorized as "Easy", "Hard", "Unclear" or "Negative" for each different query -leading to different experimental protocols "Easy", "Medium", "Hard", depending on the types of index images expected to be retrieved. In these datasets, the common experimental setup is to report results only for Medium and Hard protocols (as suggested by the main results table in <ref type="bibr" target="#b42">[43]</ref>). In contrast, the Google Landmarks Dataset v2 index images can only be "Positive" or "Negative", and there is a single protocol. In this way, we believe that our dataset will more accurately capture effects of easy queries, which are very common in real-world systems. As a concrete example of differences we can observe, stateof-the-art methods based on local feature aggregation (e.g., DELF-R-ASMK <ref type="bibr" target="#b52">[53]</ref>), which excel on ROxford, do not fare as well on GLDv2, being worse than simple embeddings.</p><p>Application. Both ROxford/RParis and GLDv2 address instance-level retrieval tasks; however, the ground-truth is constructed differently. In ROxford/RParis, relevant index images must depict the same instance and the same view as the query image. In contrast, for GLDv2, any index image associated to the same landmark is considered relevant, even if its view does not overlap with the query image.</p><p>In conclusion, both our Google Landmarks Dataset v2 (retrieval subset) and ROxford/RParis have pros and cons, capturing different and complementary aspects of the instance-level retrieval problem. For a comprehensive assessment of instance-level retrieval methods, we would suggest future work to include all of these, to offer a detailed performance analysis across different characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Preventing Unintended Methods</head><p>For the Kaggle competition, we had to make certain design choices to prevent "cheating", i.e. to ensure that participants would only use the images themselves and no metadata attached to the images or found on the web. Therefore, we stripped all images of any metadata like geotags or labels. However, this alone was not sufficient since many images have a "Creative Commons By" (CC-BY) license which requires attributing the author and publishing the original image URL, which would reveal other information. We therefore chose to use only images with CC0 or Public Domain licenses for the index set, so we could keep their image URLs secret; the same for the query set, except that in this case we also added images collected by crowdsourcing operators. For the training set however, the landmark labels needed to be released in order to allow training models. So, we used CC-BY images for the training set and include full attribution information with the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Sample Images from the Dataset</head><p>To give a qualitative impression of the dataset, we show a selection of dataset images. We would also like to refer readers to the dataset website, where a web interface is available for exploring images from the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Intra-Class Variation in Training Set</head><p>Figs. 7 and 8 show a sample of the over 200k classes in the training set. The dataset has a broad coverage of each place, including photos taken from widely different viewing angles, under different lighting and weather conditions and in different seasons. Additionally, it contains historical photos from archives that can help make trained models robust to changes in photo quality and appearance changes over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Retrieval Ground Truth</head><p>Figs. 9, 10 and 11 show a selection of query images with associated index images, highlighting some of the challenges of the retrieval task. Note that because the retrieval ground truth was created on a class level rather than on the image level, not all relevant index images have shared content with the query image. The retrieval task challenges approaches to be robust to a wide rage of variations, including viewpoint, occlusions, lighting and weather. Moreover, invariance to image domain is required since the index contains both digital and analog photographs as well as some drawings and paintings depicting the landmark. <ref type="figure" target="#fig_0">Fig. 12</ref> shows images from the test set. The test set consists of 1.1% images of natural and human-made landmarks, as shown in <ref type="figure" target="#fig_0">Fig. 12a</ref>. These images were taken with smartphones by crowdsourcing operators. They therefore represent realistic query images to visual recognition applications. <ref type="figure" target="#fig_0">Fig. 12b</ref> shows a sample of the 98.9% out-of-domain images in the test set that were collected from Wikimedia Commons. Note that a small fraction of test set images showing landmarks do not have ground truth labels since their landmarks do not exist in the training or query sets.          </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Test Set</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Heatmap of the places in the Google Landmarks Dataset v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Histogram of the number of images from the top-20 countries (blue) compared to their populations (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>c a A n t a r c t i c a Number of images (thousands) Population (millions) Histogram of the number of images per continent (blue) compared to their populations (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Histogram of the number of images by landmark category. This includes only categories with more than 25k images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Left: pipeline for mining images from Wikimedia Commons. Right: the user interface of the re-annotation tool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Fortifica√ß√µes da Pra√ßa de Valen√ßa do Minho -Different views of a landmark that covers a large area. (b) Chapel of Saint John of Nepomuk (ƒåernousy) -Inside and outside views as well as details. (c) All Saints church (Sawley) -Images showing the landmark in different seasons and under different lighting conditions. (d) Goryokaku (Hakodate) -Aerial views and details of a park.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Sample classes from the training set (1 of 2).(a) Franjo Tudman bridge (Dubrovnik) -A wide range of views and weather conditions. (b) Phare de la Madonetta (Bonifacio) -A wide range of scales and historical photographs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(c) Azhdahak (Armenia) -A natural landmark from different views and with different levels of snow coverage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Sample classes from the training set (2 of 2). (a) Sacre Coeur -Different viewpoints and significant occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(b) Trevi Fountain -Different viewpoints and lighting conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Retrieval task: Query images with a sample of relevant images from the index set (1 of 3).(a) Place de la Concorde -Significant scale changes, historical photographs and paintings. (b) Palazzo delle Esposizioni -Day and night photos and a monochrome print.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Retrieval task: Query images with a sample of relevant images from the index set (2 of 3). (a) Teatro Espanol -Some relevant images that are difficult to retrieve: Architectural drawing, painting of the inside, historical photograph of audience members. (b) Azkuna Zentroa -Detail views and photos showing the construction of the building.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Retrieval task: Query images with a sample of relevant images from the index set (3 of 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>(a) Sample landmark images from the test set.(b) Sample out-of-domain images from the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 :</head><label>12</label><figDesc>Sample images from the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. The Landmarks dataset by Li et al.<ref type="bibr" target="#b34">[35]</ref> contains 205k images of 1k famous landmarks. Two other recent landmark datasets, by Gordo et al.<ref type="bibr" target="#b21">[22]</ref> and Radenovic et al.<ref type="bibr" target="#b43">[44]</ref>, have become popular for training image retrieval models,</figDesc><table><row><cell>Dataset name</cell><cell cols="2">Year # Landmarks</cell><cell># Test</cell><cell># Train</cell><cell># Index</cell><cell>Annotation collection</cell><cell>Coverage</cell><cell>Stable</cell></row><row><cell></cell><cell></cell><cell></cell><cell>images</cell><cell>images</cell><cell>images</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Oxford [41]</cell><cell>2007</cell><cell>11</cell><cell>55</cell><cell>-</cell><cell>5k</cell><cell>Manual</cell><cell>City</cell><cell>Y</cell></row><row><cell>Paris [42]</cell><cell>2008</cell><cell>11</cell><cell>55</cell><cell>-</cell><cell>6k</cell><cell>Manual</cell><cell>City</cell><cell>Y</cell></row><row><cell>Holidays [28]</cell><cell>2008</cell><cell>500</cell><cell>500</cell><cell>-</cell><cell>1.5k</cell><cell>Manual</cell><cell>Worldwide</cell><cell>Y</cell></row><row><cell>European Cities 50k [5]</cell><cell>2010</cell><cell>20</cell><cell>100</cell><cell>-</cell><cell>50k</cell><cell>Manual</cell><cell>Continent</cell><cell>Y</cell></row><row><cell>Geotagged StreetView [32]</cell><cell>2010</cell><cell>-</cell><cell>200</cell><cell>-</cell><cell>17k</cell><cell>StreetView</cell><cell>City</cell><cell>Y</cell></row><row><cell>Rome 16k [1]</cell><cell>2010</cell><cell>69</cell><cell>1k</cell><cell>-</cell><cell>15k</cell><cell>GeoTag + SfM</cell><cell>City</cell><cell>Y</cell></row><row><cell>San Francisco [14]</cell><cell>2011</cell><cell>-</cell><cell>80</cell><cell>-</cell><cell>1.7M</cell><cell>StreetView</cell><cell>City</cell><cell>Y</cell></row><row><cell>Landmarks-PointCloud [35]</cell><cell>2012</cell><cell>1k</cell><cell>10k</cell><cell>-</cell><cell>205k</cell><cell>Flickr label + SfM</cell><cell>Worldwide</cell><cell>Y</cell></row><row><cell>24/7 Tokyo [56]</cell><cell>2015</cell><cell>125</cell><cell>315</cell><cell>-</cell><cell>1k</cell><cell>Smartphone + Manual</cell><cell>City</cell><cell>Y</cell></row><row><cell>Paris500k [61]</cell><cell>2015</cell><cell>13k</cell><cell>3k</cell><cell>-</cell><cell>501k</cell><cell>Manual</cell><cell>City</cell><cell>N</cell></row><row><cell>Landmark URLs [7, 22]</cell><cell>2016</cell><cell>586</cell><cell>-</cell><cell>140k</cell><cell>-</cell><cell cols="2">Text query + Feature matching Worldwide</cell><cell>N</cell></row><row><cell>Flickr-SfM [44]</cell><cell>2016</cell><cell>713</cell><cell>-</cell><cell>120k</cell><cell>-</cell><cell>Text query + SfM</cell><cell>Worldwide</cell><cell>Y</cell></row><row><cell>Google Landmarks [39]</cell><cell>2017</cell><cell>30k</cell><cell>118k</cell><cell>1.2M</cell><cell>1.1M</cell><cell>GPS + semi-automatic</cell><cell>Worldwide</cell><cell>N</cell></row><row><cell>Revisited Oxford [43]</cell><cell>2018</cell><cell>11</cell><cell>70</cell><cell>-</cell><cell>5k + 1M</cell><cell>Manual + semi-automatic</cell><cell>Worldwide</cell><cell>Y</cell></row><row><cell>Revisited Paris [43]</cell><cell>2018</cell><cell>11</cell><cell>70</cell><cell>-</cell><cell>6k + 1M</cell><cell>Manual + semi-automatic</cell><cell>Worldwide</cell><cell>Y</cell></row><row><cell cols="2">Google Landmarks Dataset v2 2019</cell><cell>200k</cell><cell>118k</cell><cell>4.1M</cell><cell>762k</cell><cell cols="2">Crowsourced + semi-automatic Worldwide</cell><cell>Y</cell></row><row><cell cols="9">Table 1: Comparison of our dataset against existing landmark recognition/retrieval datasets. "Stable" denotes if the dataset can be retained indefinitely. Our</cell></row><row><cell cols="8">Google Landmarks Dataset v2 is larger than all existing datasets in terms of total number of images and landmarks, besides being stable.</cell><cell></cell></row><row><cell cols="5">containing hundreds of landmarks and approximately 100k</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">images each; note that these do not contain test images, but</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">only training data. The original Google Landmarks Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">[39] contains 2.3M images from 30k landmarks, but due to</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">copyright restrictions this dataset is not stable: it shrinks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">over time as images get deleted by the users who uploaded</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">them. The Google Landmarks Dataset v2 dataset surpasses</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">all existing datasets in terms of the number of images and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">landmarks, and uses images only with licenses that allow</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">free reproduction and indefinite retention.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Number of images and labels for the different training datasets used in our experiments.</figDesc><table><row><cell>Technique</cell><cell cols="2">Training Dataset</cell><cell>Medium ROxf RPar ROxf RPar Hard</cell></row><row><cell></cell><cell cols="3">Landmarks-full [22] 50.8 70.4 24.3 47.1</cell></row><row><cell></cell><cell cols="3">Landmarks-clean [22] 54.2 70.7 28.3 46.0</cell></row><row><cell>ResNet101+ArcFace</cell><cell cols="3">GLDv1-train [39] 68.9 83.4 45.3 67.2</cell></row><row><cell></cell><cell cols="3">GLDv2-train-clean 76.2 87.3 55.6 74.2</cell></row><row><cell></cell><cell cols="3">GLDv2-train-no-tail 76.1 86.6 55.1 72.5</cell></row><row><cell>DELF-ASMK*+SP [43]</cell><cell></cell><cell></cell><cell>67.8 76.9 43.1 55.4</cell></row><row><cell>DELF-R-ASMK* [53]</cell><cell></cell><cell></cell><cell>73.3 80.7 47.6 61.3</cell></row><row><cell>DELF-R-ASMK*+SP [53]</cell><cell></cell><cell></cell><cell>76.0 80.2 52.4 58.6</cell></row><row><cell>ResNet152+Triplet [44]</cell><cell cols="2">GLDv1-train [39]</cell><cell>68.7 79.7 44.2 60.3</cell></row><row><cell>ResNet101+AP [45]</cell><cell></cell><cell></cell><cell>66.3 80.2 42.5 60.8</cell></row><row><cell>DELG global-only [10]</cell><cell></cell><cell></cell><cell>73.2 82.4 51.2 64.7</cell></row><row><cell>DELG global+SP [10]</cell><cell></cell><cell></cell><cell>78.5 82.9 59.3 65.5</cell></row><row><cell cols="2">HesAff-rSIFT-ASMK*+SP [43]</cell><cell>-</cell><cell>60.6 61.4 36.7 35.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Retrieval results (% mAP) on ROxf and RPar of baseline models (ResNet-101 + ArcFace loss) learned on different training sets, compared to other techniques. All global descriptors use GeM pooling [44].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Baseline results (% ¬µAP) for the GLDv2 recognition task.</figDesc><table><row><cell>Technique</cell><cell>Training Dataset</cell><cell cols="2">Testing Validation</cell></row><row><cell></cell><cell cols="2">Landmarks-full [22] 13.27</cell><cell>10.75</cell></row><row><cell>ResNet101+ArcFace</cell><cell cols="2">Landmarks-clean [22] 13.55 GLDv1-train [39] 20.67</cell><cell>11.95 18.82</cell></row><row><cell></cell><cell cols="3">GLDv2-train-clean 25.57 23.30</cell></row><row><cell>DELF-ASMK* [43]</cell><cell></cell><cell>14.76</cell><cell>13.07</cell></row><row><cell>DELF-ASMK*+SP [43]</cell><cell></cell><cell>16.92</cell><cell>15.05</cell></row><row><cell>DELF-R-ASMK* [53] DELF-R-ASMK*+SP [53]</cell><cell>GLDv1-train [39]</cell><cell>18.21 18.78</cell><cell>16.32 17.38</cell></row><row><cell>DELG global-only [10]</cell><cell></cell><cell>21.71</cell><cell>20.19</cell></row><row><cell>DELG global+SP [10]</cell><cell></cell><cell>24.54</cell><cell>21.52</cell></row><row><cell>ResNet101+AP [45]</cell><cell></cell><cell>18.71</cell><cell>16.30</cell></row><row><cell>ResNet101+Triplet [60]</cell><cell>GLDv1-train [39]</cell><cell>18.94</cell><cell>17.14</cell></row><row><cell>ResNet101+CosFace [58]</cell><cell></cell><cell>21.35</cell><cell>18.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Baseline results (% mAP@100) for the GLDv2 retrieval task. The bottom three results were reported in [64]. .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Top 3 results on recognition challenge (% ¬µAP). GF = global feature similarity search; LF = local feature matching re-ranking.</figDesc><table><row><cell>Team Name</cell><cell>Technique</cell><cell cols="6">Before re-annotation Testing Validation Testing Validation After Before Precision@100</cell></row><row><cell>smlyaka [64]</cell><cell>GF ensemble ‚Üí DBA/QE ‚Üí C</cell><cell>37.19</cell><cell>35.69</cell><cell>37.25</cell><cell>35.68</cell><cell>6.09</cell><cell>4.73</cell></row><row><cell>GLRunner [15]</cell><cell>GF ensemble ‚Üí LF ‚Üí DBA/QE ‚Üí C</cell><cell>34.38</cell><cell>32.04</cell><cell>34.75</cell><cell>32.09</cell><cell>6.42</cell><cell>4.83</cell></row><row><cell cols="2">Layer 6 AI [12] GF ensemble ‚Üí LF ‚Üí QE ‚Üí EGT</cell><cell>32.10</cell><cell>29.92</cell><cell>32.18</cell><cell>29.64</cell><cell>5.13</cell><cell>3.97</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/landmark-recognition-2019, https://www.kaggle.com/c/landmark-retrieval-2019 3 https://github.com/cvdfoundation/google-landmark</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://commons.wikimedia.org 6 https://www.wikilovesmonuments.org 7 https://developers.google.com/knowledge-graph</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank the Wikimedia Foundation and the Wikimedia Commons contributors for the immensely valuable source of image data they created,</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building Rome in a Day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Smooth object retrieval using a bag of boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three Things Everyone Should Know to Improve Object Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Retrieving Landmark and Non-landmark Images from Community Photo Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spyrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature Map Hashing: Sub-linear Indexing of Appearance and Global Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aggregating Local Deep Features for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Codes for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Group-Sensitive Triplet Embedding for Vehicle Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Speeded-Up Robust Features (SURF)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unifying Deep Local and Global Features for Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020. 7</title>
		<meeting>ECCV, 2020. 7</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Stanford Mobile Visual Search Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Takacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Reznik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Systems Conference</title>
		<meeting>ACM Multimedia Systems Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Gorti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04944</idno>
		<title level="m">Semi-Supervised Exploration in Image Retrieval</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explore-Exploit Graph Traversal for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">City-Scale Landmark Identification on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pylvanainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roimela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">2nd Place and 2nd Place Solution to Kaggle Landmark Recognition and Retrieval Competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03990</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">{NoisyArt}: A Dataset for Webly-supervised Artwork Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chiaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VISAPP</title>
		<meeting>VISAPP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Total Recall II: Query Expansion Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable Logo Recognition using Proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fehervari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Appalaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Image Retrieval: Learning Global Representations for Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Caltech-256 Object Category Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>7694</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Team JL Solution to Google Landmark Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11874</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The iNaturalist Species Classification and Detection Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aggregating Local Image Descriptors into Compact Codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Logo Retrieval with a Contrario Visual Query Expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable Triangulation-based Logo Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Pueyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trevisiol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMR</title>
		<meeting>ICMR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Avoiding Confusing Features in Place Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshops</title>
		<meeting>ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<title level="m">The Open Images Dataset V4: Unified Image Classification, Object Detection, and Visual Relationship Detection at Scale</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Worldwide Pose Estimation using 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
		<title level="m">Distinctive Image Features from Scale-Invariant Keypoints. IJCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-Scale Image Retrieval with Attentive Deep Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object Retrieval with Large Vocabularies and Fast Spatial Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lost in Quantization: Improving Particular Object Retrieval in Large Scale Image Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenoviƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fine-tuning CNN Image Retrieval with No Human Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenoviƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning with Average Precision: Training Image Retrieval with a Listwise Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable Logo Recognition in Real-world Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pueyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMR</title>
		<meeting>ICMR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Unified Embedding for Face Recognition and Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improved Deep Metric Learning with Multiclass N-Pair Loss Objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Lifted Structured Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fishnet: A Versatile Backbone for Image, Region, and Pixel Level Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Detect-to-Retrieve: Efficient Regional Aggregation for Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image Search with Selective Match Kernels: Aggregation Across Single and Multiple Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning and Aggregating Deep Local Descriptors for Instance-Level Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jenicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjeloviƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cosface: Large Margin Cosine Loss for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07249</idno>
		<title level="m">RPC: A Large-Scale Retail Product Checkout Dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Visual landmark recognition from Internet photo collections: A large-scale evaluation. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Exploiting Multi-Grain Ranking Constraints for Precisely Searching Visually-similar Vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Twostage Discriminative Re-ranking for Large-scale Landmark Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yokoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Vehicle Re-Identification for Automatic Video Traffic Surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zapletal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Places: A 10 million Image Database for Scene Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
