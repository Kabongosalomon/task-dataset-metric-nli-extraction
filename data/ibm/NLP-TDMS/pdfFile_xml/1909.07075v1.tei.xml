<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification-Specific Parts for Improving Fine-Grained Visual Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Korsch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bodesheim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Group</orgName>
								<orgName type="institution">Friedrich-Schiller-University Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Michael Stifel Center Jena</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classification-Specific Parts for Improving Fine-Grained Visual Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained visual categorization is a classification task for distinguishing categories with high intra-class and small inter-class variance. While global approaches aim at using the whole image for performing the classification, part-based solutions gather additional local information in terms of attentions or parts. We propose a novel classificationspecific part estimation that uses an initial prediction as well as backpropagation of feature importance via gradient computations in order to estimate relevant image regions. The subsequently detected parts are then not only selected by a-posteriori classification knowledge, but also have an intrinsic spatial extent that is determined automatically. This is in contrast to most part-based approaches and even to available groundtruth part annotations, which only provide point coordinates and no additional scale information. We show in our experiments on various widely-used fine-grained datasets the effectiveness of the mentioned part selection method in conjunction with the extracted part features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained visual categorization (FGVC) is a challenging subdiscipline of computer vision and aims at distinguishing similar classes of objects that belong to a common major class like birds <ref type="bibr" target="#b22">[19,</ref><ref type="bibr" target="#b24">21]</ref>, cars <ref type="bibr" target="#b13">[11]</ref> or flowers <ref type="bibr" target="#b15">[13]</ref>. The latest FGVC challenges (like <ref type="bibr" target="#b23">[20]</ref>) highlight both importance and difficulties of fine-grained categorization. As shown by others before, a careful selection of data <ref type="bibr" target="#b0">[1]</ref> or gathering additional data from the Internet <ref type="bibr" target="#b12">[10]</ref> for the pretraining of a convolutional neural network (CNN) can yield impressive state-of-the-art results.</p><p>In general, the proposed solutions found in the literature can be divided into algorithms working with global image features <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b18">16]</ref> and part-based or attention-based methods <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b25">22,</ref><ref type="bibr" target="#b26">23]</ref>. From the empirical results reported in these works, it is difficult to conclude which basic approach (global or part-based) works best, since both are competitive in terms of recognition performance. Due to the fact that categories of fine-grained recognition tasks often differ only in small details, part-based features that consider local image regions seem to be promising because they are able to explicitly focus on such distinct patterns. For example, if two bird species can only be distinguished by a characteristic spot on the head, a part feature representing the image region covered by the head of the bird would be beneficial for separating these two classes. Furthermore, parts can support the classification in case of only few training samples or  <ref type="figure">Fig. 1</ref>. An example from our experiments as a motivation for our approach: two visually similar classes are confused by a baseline classifier with global features. A part-based classifier using ground-truth annotations for anatomical parts is able to correct some of the predictions due to additional local information. However, our parts are estimated using a-posteriori classification knowledge and therefore focus on distinguishing highly similar classes. This allows for resolving misclassifications with the additional benefits of automatically determining the spatial extent of each part and being independent from manual annotations.</p><p>highly imbalanced class distributions in the training set, e.g., by applying transfer learning techniques <ref type="bibr" target="#b6">[5]</ref>. For analyzing classification results and failure cases, the attribution of classifier decisions to features and relevant image regions is an important step and parts are helpful for detailed investigations in order to gain a better understanding of the specific recognition task and the problem domain.</p><p>If ground-truth (GT) annotations for part locations are provided, they are usually referring to an underlying concept, e.g., the anatomical parts of a bird such as head, beak, belly, wings, and legs. Although being plausible from a human perspective, these parts may not be the best choice for achieving the highest classification accuracy with a machine learning model. In addition, not all annotated parts are equally relevant for every test image and it can be shown that an optimal part selection would lead to superior performance compared to state-of-the-art methods using all available parts <ref type="bibr" target="#b9">[8]</ref>. Especially in case of noise, few characteristic parts can be outweighed by the remaining larger set of irrelevant parts that confuse the classifier and lead to misclassifications. In our experiments, we have observed that quality of parts is more important for an improved classification accuracy than quantity. For example, if we regroup the provided ground-truth parts for the CUB-200-2011 birds dataset <ref type="bibr" target="#b24">[21]</ref> in more coarse but also more distinct parts, namely "head", "body", "legs", and "tail", the recognition performance can be enhanced.</p><p>However, since ground-truth annotations are not available for all applications and manual part annotations are expensive, an efficient and robust part detector is required. Such a detector has to deal with the following two main questions. First, what are the interesting and important locations that enhance the classification performance? Second, given certain part location, how and to which extent should the part features be extracted?</p><p>In this work, we tackle those questions and show that our classificationspecific part estimation is able to improve classification accuracies on various fine-grained datasets. By studying failure cases of a baseline classifier that makes use of either global image features or part-based features extracted from available ground-truth annotations of the CUB-200-2011 birds dataset <ref type="bibr" target="#b24">[21]</ref>, we have observed that many class confusions occur between visually very similar classes (more details in Sect. 3 and <ref type="figure" target="#fig_1">Fig. 2</ref>). Hence, the idea of our approach is to identify relevant parts based on an initial classification with global features. By considering only the most important features for this initial decision, we estimate parts that are likely to be relevant for visually similar classes as well. With these new parts that are specifically estimated for a given test image based on additional knowledge from an initial classification, we aim at resolving misclassifications between very similar classes. This scenario is also visualized in <ref type="figure">Fig. 1</ref>.</p><p>Our proposed approach consists of the following steps. First, we perform a feature selection in order to estimate the most important features for the current classification task using a baseline classifier with global image features. The idea is then to estimate the most important regions in the image with respect to the actual classification task by only taking the most important features for the part localization into account. From these regions, we estimate parts as bounding boxes with automatically determining the spatial extent (scale) of each part. This is an advantage over most part-based approaches and ground-truth annotations. These provide only x and y coordinates of the part locations such that the size of each part has to be selected appropriately (and is usually fixed for all parts and all images). Given the newly estimated parts, we extract features for these parts as a rich representation that can then be used to improve the classification. More details are given in Sect. 3.</p><p>As it is common practice for many computer vision applications nowadays, our classification scheme relies on features computed with a CNN (called CNN features). Such features can easily be computed by applying either a pretrained CNN model as it is or a pretrained model that has been fine-tuned on the training set of the desired application. In our experiments (Sect. 4) we show that our approach: (i) improves the performance of the baseline methods, (ii) is competitive with other part-based classification approaches, and (iii) achieves state-of-the-art accuracies in some applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fine-grained visual categorization is a challenging and non-trivial classification task. Hence, there are diverse ways to tackle the problem. On the one hand, there are approaches that only use the global information of the image. The idea is either to use a clever way of pretraining the classifier or to use different feature pooling strategies. On the other hand, part-based approaches are applied which differ in the various part detection and extraction techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Global Feature Representations</head><p>First, we consider approaches using only the global information of the image. Cui et al. <ref type="bibr" target="#b0">[1]</ref> use a smart strategy in order to pretrain a CNN by taking largescale datasets like ImageNet or iNaturalist into account, which offer a lot of data. Unfortunately, the difference between these datasets and the desired finegrained datasets is too big. Hence, they suggest to preselect certain classes from the large-scale datasets which match best to the fine-grained training images and show that this preselection improves the performance drastically.</p><p>Krause et al. <ref type="bibr" target="#b12">[10]</ref> enrich the training data with images from the Internet. They use Google Image Search in order to gather additional images for every training class. Though, the retrieved samples may not belong to the queried class, they show that even this noisy data improves the recognition performance by a large amount. Nevertheless, they cannot ensure that the collected data does not contain some images from the validation or the test set, since all these images are also publicly available. Hence, although an impressive effectiveness of noisy data is shown, one should look at the reported results with caution.</p><p>Other approaches focus on advanced ways of feature pooling. Here, bilinear pooling by Lin et al. <ref type="bibr" target="#b14">[12]</ref> or more general the alpha-pooling by Simon et al. <ref type="bibr" target="#b18">[16]</ref> are the most common techniques. Their aim is to highlight features that may have a greater impact on the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Part-based Recognition Approaches</head><p>The second main direction for tackling fine-grained recognition tasks consists of methods that rely on part-based representations. A straightforward way of implementing a part-based recognition system is to employ the ground-truth part annotations if they exist (e.g., for the CUB-200-2011 birds dataset <ref type="bibr" target="#b24">[21]</ref>). Since these annotations are expensive and most fine-grained datasets do not provide them, weakly supervised part detectors are a common choice <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b26">23]</ref>. The only supervision that these detectors use are class label annotations.</p><p>Fu et al. <ref type="bibr" target="#b3">[3]</ref> and Zheng et al. <ref type="bibr" target="#b26">[23]</ref> present similar approaches to extend CNNs with attention networks. The first work considers adjusting the attention recurrently and extracting additional information defined by the attention on different scales. On the other hand, the later work extracts multiple attentions in a single step. The extraction is done by localizing interesting areas from feature maps, regrouping them, and using these grouped areas as parts. In both cases, the whole system is trained end-to-end.</p><p>He et al. <ref type="bibr" target="#b8">[7]</ref> propose a sophisticated reinforcement learning method in order to estimate how many and which image regions are helpful to distinguish the categories. They use multi-scale image representations in order to localize the object and then estimate discriminative part regions.</p><p>Simon et al. <ref type="bibr" target="#b17">[15]</ref> identify part proposals with the aid of back-propagation. Afterwards, these proposals are used to fit a constellation model that determines which of the proposals are more likely to identify real parts. The part proposals with the highest match are then used to extract part features on different scales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Classification-Specific Part Estimation</head><p>In this section we describe our classification-specific part estimation approach that makes use of an initial classification based on global image features. The goal is to estimate parts depending on this first (and probably wrong) decision, such that these parts can help to spot the tiny details. These details are important for distinguishing visually similar classes in order to either confirm an initially correct classification based on the specific part features or to correct an initially wrong classification due to an enhanced representation of the important parts only. Here, we assume that many confusions of a classifier based on the global image features occur between visually very similar classes and that in those cases, the small details that are characteristic for distinguishing them are not well represented by the global features (which in general have to work for distinguishing all classes). <ref type="figure" target="#fig_1">Fig. 2</ref> visualizes this confusion and confirms our assumption. Hence, we look for the important image regions that have led to the initial classification. Then we derive new parts from these regions under the assumption that the resulting parts are also more relevant for disentangling visually similar classes. Our estimated parts are therefore classification-specific rather than based on human knowledge, e.g., from an anatomical point of view in case of the birds.</p><p>The pipeline we propose in this paper is visualized in <ref type="figure">Fig. 3</ref> and will be outlined in the following. First, we describe the feature selection method (Sect. 3.1). Next, based on these selected features, we illustrate how relevant pixels and image regions are identified as candidates for the part locations (Sect. 3.2). Third, we explain our algorithm for estimating bounding-box-parts with the advantage of automatically determining the scale of each part (Sect. 3.3). Finally, an overview about the part feature extraction from the classification-specific bounding-box-parts and about the part-based classification is given in Sect. <ref type="bibr" target="#b3">3</ref>  <ref type="figure">Fig. 3</ref>. The pipeline for our classification-specific part estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Selection</head><p>Nowadays, a common approach in computer vision tasks is to use a pretrained neural network like a CNN, fine-tune its parameters on a dataset for the desired application, and extract features by concatenating the outputs of its penultimate layer in order to obtain a high-level descriptions of the image content. Our approach relies on those CNN features, which typically results in high-dimensional feature vectors (D = 2048 in our case). In case of a fine-grained recognition task, the recognition system often has to focus on some specific information within the features in order to spot tiny details that distinguish two similar classes. Therefore, we first perform a feature selection in order to estimate the most important features for the current classification task. This is done by utilizing a sparsity-inducing classifier equipped with L1-regularization, which could be either a corresponding classification layer in a CNN that allows for end-to-end learning or an L1-regularized linear SVM classifier for the CNN features. Optimization with L1-regularization forces the classifier decisions to be performed on only a small subset of the CNN features. In our experiments, we tried both and found empirically that an SVM performs better in terms of recognition accuracy while still being fast during learning due to efficient SVM solvers in standard libraries like liblinear <ref type="bibr" target="#b2">[2]</ref>.</p><p>In the end, our feature selection is classification-based and determines relevant features for the underlying task by optimizing feature weights during learning of SVM classifiers. Since we consider multi-class recognition scenarios, we train a separate classifier for each class using the "one-vs-rest" strategy. As the result, we obtain a subset of relevant features for each class that best distinguishes this class from all the other classes by only selecting features with nonzero weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identifying Relevant Pixels and Image Regions</head><p>The main idea within our part detection approach is to estimate the most important regions in the image with respect to the actual classification task. Hence, the part localization should only take those important features into account, which have been computed with the classification-based feature selection from the previous section. To this end, we use gradient maps <ref type="bibr" target="#b20">[17]</ref> to identify the most relevant pixels in the image (indicated by large gradients), which have the largest influence on the feature extraction from the CNN model. By restricting the gradient map computations to only the previously selected subset of features, regions with large gradients are more adjusted to the classification task compared to propagating back the gradients of all features to the input image. Since our feature selection is based on a multi-class one-vs-rest SVM classifier, only selected features of the class assigned by this classifier are used for the gradient map computations. Thus, we incorporate knowledge of a baseline classifier with global features in our part detection algorithm.</p><p>The gradient maps are treated as saliency maps in our approach in order to guide the part detection and they depend on the used CNN features. In case of many currently used CNN architectures (Inception <ref type="bibr" target="#b21">[18]</ref>, ResNet <ref type="bibr" target="#b7">[6]</ref>, etc.), features are computed by averaging the values within each of the D output channel of the last convolutional layer. Typically, these output channels are called feature maps and the aforementioned average pooling results in a single number for each feature map. Given D feature maps F (1) (I), . . . , F (D) (I) of size s × u for an image I, this pooling step for computing the elements f (d) (I) of the D-dimensional feature vector f (I) can be written as follows:</p><formula xml:id="formula_0">f (d) (I) = 1 s · u s j=1 u j =1 F (d) j,j (I) ∀ d ∈ {1, . . . , D} .<label>(1)</label></formula><p>Consequently, each value in a feature vector corresponds to a single feature map and since the feature selection method mentioned in Sect. 3.1 is applied to the feature vectors, it can also be viewed as applying the feature selection to the feature maps, i.e., the output channels of the last convolutional layer. Like Simonyan et al. <ref type="bibr" target="#b20">[17]</ref> and Simon et al. <ref type="bibr" target="#b17">[15]</ref>, we use back-propagation through the CNN to identify the regions of interest for each selected feature map. Based on the feature map subset D ⊂ {1, . . . , D} chosen by the feature selection from Sect. 3.1, we compute a saliency map M (I) for an image I as follows:</p><formula xml:id="formula_1">M x,y (I) = 1 |D| d∈D ∂ ∂I x,y f (d) (I) = 1 |D| d∈D ∂ ∂I x,y 1 s · u s j=1 u j =1 F (d) j,j (I) . (2)</formula><p>After estimating the saliency maps, we normalize the resulting values to the range [0 . . . 1] and determine a threshold to discard pixels and regions of low saliency at an early stage. We use the mean saliency value as a threshold. We have also tested Otsu's thresholding method <ref type="bibr" target="#b16">[14]</ref> and it achieved similar performance. The resulting sparse saliency map, which now contains only pixels with large saliency values, is used in the next step for estimating location and spatial extent of parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Estimating Bounding-Box-Parts</head><p>Given an image and a sparse saliency map that discards pixels with low saliency, a set of k peaks P = {p 1 , . . . , p k } with largest saliency can be computed using non-maximum suppression. Each peak serves as the initialization for a new part location. We then determine a region of high saliency around each peak, which directly defines the spatial extent of the estimated part. Like Zhang et al. <ref type="bibr" target="#b25">[22]</ref>, we achieve this by k-means clustering of pixel coordinates (x, y) and the saliencies M x,y (Eq. 2). Additionally, we also consider the RGB values at the corresponding positions in the input image. The clusters are initialized with the previously determined peaks p 1 , . . . , p k .</p><p>This has the effect that the number of selected peaks determines the number of clusters and hence the number of parts to detect. Second, since the peaks are sorted by their saliency values, the most important part is identified by the first cluster. Afterwards, it is easy to translate the clusters into bounding boxes for the parts. For each cluster we estimate the upper left and lower right corners in order to maximize the recall of the cluster pixels surrounded by the corners. The motivation behind the recall maximization is to get bounding boxes that contain as few false negatives as possible.</p><p>The resulting bounding boxes serve as parts for the following part-based classification with the advantage that we automatically determine the spatial extent (scale) of the parts by inferring the size of the bounding boxes based on the clustering and the regression. In contrast to this, most approaches estimate only x and y coordinates of the part locations such that the size of each part has to be selected appropriately. The same holds for the ground-truth annotations of many fine-grained datasets <ref type="bibr" target="#b22">[19,</ref><ref type="bibr" target="#b24">21]</ref>. In most cases, the size for all parts of an image is fixed, which is obviously not very suitable since parts often have different extents in the image, e.g., consider bird parts that correspond to an eye and a wing.</p><p>With our part estimation strategy, we are able to automatically determine different sizes for different parts depending on the content of the image. Hence, we want to emphasize again that we treat parts as bounding boxes with estimated position and estimated spatial extent in our framework rather than only considering point locations with fixed extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Part Feature Extraction and Part-based Classification</head><p>After we have estimated the bounding boxes around the maximum peaks of the sparse saliency map for each image, we extract CNN features from these bounding boxes. This is achieved by treating each bounding box as a single image that is then processed by a pretrained CNN to extract meaningful features from the penultimate layer. Note that this could even be the same CNN that was initially used to extract global image features for the part localization and we use the same CNN architecture for both steps. The resulting part features and the global features are then concatenated prior to applying a linear SVM classifier. This classifier has been trained using part features of the training images that have been computed with our part estimation approach described before.</p><p>To summarize, our part descriptors are classification-specific in the sense that we estimate location and spatial extent of parts via bounding boxes based on an initial classifier decision with its involved feature selection, i.e., our estimated parts focus on the important aspects that are relevant for the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation</head><p>Datasets All of the experiments are performed on widely used fine-grained datasets. These datasets belong to a single common domain (birds, cars, flowers, etc.). Though, some of these datasets provide additional part or bounding box annotations besides the class annotations, we use only the class labels in our experiments. A short description of these datasets can be found in the following.</p><p>CUB-200-2011 <ref type="bibr" target="#b24">[21]</ref> consists of 5994 training and 5794 test images from 200 different bird species. Besides the class labels, this dataset provides bounding box and part annotations.</p><p>NA-Birds <ref type="bibr" target="#b22">[19]</ref> is similar to the CUB-200-2011 dataset. It provides besides class annotations also ground-truth part annotations. This dataset is more challenging, since it has 555 classes spread over 23 929 training and 24 633 test images. Although there are more training samples, the training set is not as balanced as the training set of the CUB-200-2011 dataset. Additionally, this dataset provides a hierarchy information about the classes.</p><p>Stanford-Cars <ref type="bibr" target="#b13">[11]</ref> contains 8144 training and 8041 test images for 196 car models. This dataset provides only bounding box annotations.</p><p>Flowers-102 <ref type="bibr" target="#b15">[13]</ref> has 102 different flower species spread over 2040 training and 6149 test images. Class labels are the only provided annotations.</p><p>Implementation As backbone for our method, we use the ResNet-50 <ref type="bibr" target="#b7">[6]</ref> CNN architecture for Stanford-Cars and Inception-V3 CNN architecture <ref type="bibr" target="#b21">[18]</ref> for the other datasets. For different datasets we use CNN weights proposed by Cui et al. <ref type="bibr" target="#b0">[1]</ref>. These weights are pretrained on either the ImageNet or the iNaturalist 2017 dataset. To allow for fair comparisons with the recognition performances mentioned in <ref type="bibr" target="#b0">[1]</ref>, we use ImageNet weights for Stanford-Cars and iNaturalist weights for all other datasets. This separation makes sense, since iNaturalist consists of living things only, which matches the datasets of flowers and birds best. On the other hand, ImageNet classes are more variable and contain also objects and vehicles, which is more suitable for a dataset of car images. For every fine-grained dataset, we fine-tune a CNN on the corresponding training set, perform the feature selection, part localization and part extraction. Finally, the extracted part features and the global feature are concatenated and a linear SVM classifier is trained. In order to match the number of regrouped groundtruth parts for the CUB-200-2011 dataset mentioned in the introduction, we use k = 4 in the part localization step, which results in four parts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Feature Selection Evaluation First, we show that the usage of a classificationspecific feature selection improves the quality of the extracted parts. For this experiment, we first compute the gradients from the entire feature vector with respect to the input image. Based on this gradient, we detect parts and extract features as mentioned before in Sections 3.2, 3.3, and 3.4. The results obtained with these features can then be compared to the results of our approach. Although the features derived from gradients of the entire feature vector improve the recognition performance compared to the linear classification baseline, using feature selection as presented in Sect. 3.1 yields a larger improvement, as shown in <ref type="table" target="#tab_1">Table 1</ref>. We observe that the feature selection is an important ingredient in our approach. The additional information, in form of the part features determined from the gradients of the entire feature vector, improves the classification performance. Nevertheless, the benefits of the feature selection indicate that this additional information should be picked with care. The saliency maps that determine the parts are computed by the sum of the gradients of every single CNN feature with respect to the input image (Eq. 2). Hence, the feature selection reduces the summation to selected gradients only. This means that in the experiment with feature selection, we use less information but this information is more precise which results in better classification performance. These findings hold for all presented datasets except for the flowers dataset. In case of flowers we see that the baseline linear classifier performs best. One possible explanation is overfitting to the training data. Compared to other datasets, there are on average only 20 training samples per class. The other datasets contain an average of 30 to 40 samples per class.</p><p>Furthermore, as <ref type="figure">Fig. 4</ref> shows, the number of selected features by a L1regularized linear classifier is beneath 3 % for used datasets. As a consequence, the number of aggregated gradients (Eq. 2) is also beneath 3 %. This fact and the results from <ref type="table" target="#tab_1">Table 1</ref> confirm the assumption, that quality of selected information is more important than the quantity.</p><p>Part Feature Evaluation Second, we compare the recognition performance of our extracted parts with the one obtained with ground-truth parts. We have chosen the CUB-200-2011 dataset for this experiment, since it is one of the few datasets that provides these annotations. As indicated in the introduction, we also regrouped the provided ground-truth parts in more coarse but also more Since we perform multi-class classification with the "one-vs-rest" strategy, we obtain for each class a vector of sparse weights for the linear SVM due to the L1 regularization. The distribution of the number of nonzero weights over the different classes (different "one-vs-rest" models) is shown for each dataset that is used in our experiments. Note that both relative and absolute quantities are shown (for 2048 features in total), which correspond to the number of selected features. distinct parts, namely "head", "body", "legs", and "tail". Compared to original ground-truth part annotations, our experiments show that these parts yield a better recognition performance ( <ref type="table" target="#tab_2">Table 2</ref>). This indicates again that the quality of parts is more important than the quantity. In the same table, we compare our classification-specific part detection with the part-based approach of Simon et al. <ref type="bibr" target="#b17">[15]</ref>, who have provided their extracted part locations. Additionally, we report in the table the recognition based on the global feature only. Best results are achieved when combining part features and the global image feature. While using ground-truth part annotations is slightly better, we are able to achieve better recognition results than the NAC parts proposed by Simon et al. <ref type="bibr" target="#b17">[15]</ref>. Thus, our approach yields competitive recognition accuracies without relying on ground-truth part annotations, which makes it applicable in a wider range of applications where part annotations are not available.</p><p>Comparison to State-of-the-Art Finally, we compare our proposed method with current state-of-the-art approaches on commonly used fine-grained datasets. <ref type="table">Table 3</ref>. Comparison of our part-based approach for fine-grained recognition with various state-of-the-art methods (bold = best, italic = best part-based). The results are shown in <ref type="table">Table 3</ref> and the mentioned baseline uses only global image features extracted from the whole image. Furthermore, we differentiate between methods that use only the global information and part-based methods. Besides the method of Cui et al., which utilizes clever pretraining of the CNN weights, we compare to other methods that use sophisticated pooling methods: bilinear pooling <ref type="bibr" target="#b14">[12]</ref> and alpha-pooling <ref type="bibr" target="#b18">[16]</ref>. We also report recognition results and the number of used parts for other part-based approaches. Note that none of the approaches used ground-truth part annotations, neither during training nor in the test phase. <ref type="table">Table 3</ref> shows that our approach is competitive in various fine-grained applications and achieves state-of-the-art performance on the NA-Birds dataset. For the CUB-200-2011 dataset, we outperform a lot of part-based methods even if they are using much more parts. This highlights once again that the quality of the parts is important and that our estimated parts contain meaningful information in only four locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a weakly supervised classification-specific part estimation approach for fine-grained visual categorization. Unlike other part-based approaches, we estimate the part extents based on an initial classification of the whole image. We have shown that part features extracted in a classificationspecific manner result in improved categorization performance. Furthermore, each estimated bounding box part has an implicit spatial extent that automatically determines an appropriate scale of the part.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Confusion matrix created from CUB-200-2011 predictions using global features only. The values are absolute number of correct predictions in log-scale. Similar classes have consecutive class indexes and are confused more often. Here you can see the classes 59-66, which are different gull species, e.g. California Gull, Herring Gull, Ivory Gull or Western Gull</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 4. Since we perform multi-class classification with the "one-vs-rest" strategy, we obtain for each class a vector of sparse weights for the linear SVM due to the L1 regularization. The distribution of the number of nonzero weights over the different classes (different "one-vs-rest" models) is shown for each dataset that is used in our experiments. Note that both relative and absolute quantities are shown (for 2048 features in total), which correspond to the number of selected features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.4.</figDesc><table><row><cell></cell><cell>Apply pretrained CNN</cell><cell cols="2">Feature selection (via L1-SVM)</cell><cell>Gradient computation for selected features</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sect. 3.1</cell><cell>Sect. 3.2</cell></row><row><cell>Input image</cell><cell cols="2">CNN features</cell><cell cols="2">Selected CNN features Thresholding</cell><cell>Saliency map</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sect. 3.2</cell></row><row><cell></cell><cell>Clustering and bounding box estimation Sect. 3.3</cell><cell></cell><cell>Apply pretrained CNN Sect. 3.4</cell><cell>Part-based classifier (linear SVM) Sect. 3.4</cell><cell>"Black Footed Albatross"</cell></row><row><cell>Sparse saliency map</cell><cell cols="3">Bounding-box-parts</cell><cell>Part-based CNN features</cell><cell>Classification result</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our part extraction algorithm with and without our proposed feature selection method (bold = best per dataset).</figDesc><table><row><cell></cell><cell cols="4">CUB-200-2011 NA-Birds Flowers-102 Stanford-Cars</cell></row><row><cell>Global features (baseline)</cell><cell>88.5</cell><cell>87.5</cell><cell>97.8</cell><cell>91.5</cell></row><row><cell>Our parts</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>no feature selection</cell><cell>89.1</cell><cell>88.4</cell><cell>97.0</cell><cell>92.2</cell></row><row><cell>with feature selection (Sect. 3.1)</cell><cell>89.5</cell><cell>88.5</cell><cell>96.9</cell><cell>92.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of the extracted parts on the CUB-200-2011 dataset. Note that we have used the same CNN to extract features from the different part locations: ground-truth (GT), regrouped GT, NAC parts<ref type="bibr" target="#b17">[15]</ref>, and our classification-specific parts (bold = best, italic = best without GT annotations).</figDesc><table><row><cell></cell><cell>global</cell><cell>parts</cell><cell>parts +</cell><cell># of</cell></row><row><cell></cell><cell>features</cell><cell>only</cell><cell>global</cell><cell>parts</cell></row><row><cell>Global features (baseline)</cell><cell>88.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GT parts</cell><cell>-</cell><cell>87.9</cell><cell>89.8</cell><cell>15</cell></row><row><cell>Regrouped GT parts</cell><cell>-</cell><cell>86.9</cell><cell>90.2</cell><cell>4</cell></row><row><cell>NAC part locations of [15]</cell><cell>-</cell><cell>87.9</cell><cell>89.0</cell><cell>20</cell></row><row><cell>Our parts</cell><cell>-</cell><cell>87.4</cell><cell>89.5</cell><cell>4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale finegrained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/cvpr.2018.00432</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00432" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/cvpr.2017.476</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2017.476" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Weakly supervised complementary parts models for finegrained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonparametric part transfer for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Göring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Freytag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2489" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Which and how many regions to gaze: Focus discriminative regions for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01176-2</idno>
		<ptr target="https://doi.org/10.1007/s11263-019-01176-2" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">In defense of active part selection for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<idno type="DOI">10.1134/S105466181804020X</idno>
		<ptr target="https://doi.org/10.1134/S105466181804020X" />
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Image Analysis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="658" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/cvpr.2015.7299194</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7299194" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2013.77</idno>
		<ptr target="https://doi.org/10.1109/iccvw.2013.77" />
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.170</idno>
		<ptr target="https://doi.org/10.1109/iccv.2015.170" />
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on systems, man, and cybernetics</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The whole is more than its parts? from explicit to implicit pose normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TPAMI.2018.2885764</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2885764" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298658</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7298658" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00914</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2018.00914" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised part mining for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09941</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.557</idno>
		<ptr target="https://doi.org/10.1109/iccv.2017.557" />
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
