<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Discovery of Object Landmarks as Structural Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
							<email>yutingzh@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
							<email>guoyijie@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Jin</surname></persName>
							<email>jinyixin@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>He</surname></persName>
							<email>zhiyuan@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Discovery of Object Landmarks as Structural Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks can model images with rich latent representations, but they cannot naturally conceptualize structures of object categories in a human-perceptible way. This paper addresses the problem of learning object structures in an image modeling process without supervision. We propose an autoencoding formulation to discover landmarks as explicit structural representations. The encoding module outputs landmark coordinates, whose validity is ensured by constraints that reflect the necessary properties for landmarks. The decoding module takes the landmarks as a part of the learnable input representations in an end-to-end differentiable framework. Our discovered landmarks are semantically meaningful and more predictive of manually annotated landmarks than those discovered by previous methods. The coordinates of our landmarks are also complementary features to pretrained deep-neural-network representations in recognizing visual attributes. In addition, the proposed method naturally creates an unsupervised, perceptible interface to manipulate object shapes and decode images with controllable structures. The project web page: http://ytzhang.net/projects/lmdis-rep arXiv:1804.04412v1 [cs.CV] 12 Apr 2018 3. The discovered landmarks show strong discriminative performance in recognizing visual attributes. 4. Our landmark-based image decoder is useful for controllable image decoding, such as object shape manipulation and structure-conditioned image generation. 2. Related work Discriminative part learning. Parts are commonly used object structures in computer vision. The deformable partbased model [15] learns object part configurations to optimize the object detection accuracy, where similar ideas are rooted in earlier constellation approaches [16, 66, 6]. A recent method [72] based on the deep neural network performs end-to-end learning of deformable mixture of parts for pose estimation. The recurrent architecture [19] and spatial transformer network [23] are also used to discover and refine object parts for fine-grained image classification [27]. In addition, discriminative mid-level patches can be also discovered without explicit supervision [54]. Object-part discovery based on subspace analysis and clustering techniques is also shown to improve neural-network-based image recognition [52]. Unlike the approaches specific to discriminative tasks, our work focuses on learning landmarks for generic image modeling. Learning structural representations. To capture the intrinsic structures of objects, existing studies [44, 45, 37] disentangle visual content into multiple factors of variations, like the camera viewpoint, motion, and identity. The physical parameters of these factors are, however, still embedded in non-perceptible latent representations. Methods based on multi-task learning [78, 21, 65, 81] can take conceptualized structures (e.g., landmarks, masks, depth) as additional outputs. These structures in this setting are designed by humans and require supervision to learn. Learning explicit structures for image correspondence. Object structures create correspondence among object instances. Colocalization [57, 9] realizes the coarsest level of object correspondence. In a finer granularity, Anchor-Net [41] learns object parts and their correspondence across different objects and categories. WarpNet [24] corresponds images in the same class by estimating the parameter of a thin plate spline (TPS) transformation [4], and it can roughly reconstruct 3D point cloud using a single-view image. The 3D interpreter network [67] utilizes 2D landmark</p><p>annotations to discover 3D skeletons as the explicit structures of objects. Our discovered landmarks are denser than object parts and sparser than 3D points. These landmark representations are also more sensitive to precise locations and obtained without supervision. Landmark discovery with equivariance. Object structures like landmarks should be equivariant to image transformation, including object and camera motions. Using this</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computer vision seeks to understand object structures that reflect the physical states of objects and show invariance to individual appearance changes. Such intrinsic structures can serve as intermediate representations for highlevel visual understanding. However, manual annotations or designs of object structures (e.g., skeleton, semantic parts) are costly and barely available for most object categories, making the automatic representation learning of object structure an attractive solution to this challenge.</p><p>Modern neural networks can learn latent representations to effectively solve various vision problems, including image classification <ref type="bibr">[26,</ref><ref type="bibr">53,</ref><ref type="bibr">56,</ref><ref type="bibr">20]</ref>, segmentation <ref type="bibr">[32,</ref><ref type="bibr">40,</ref><ref type="bibr">21]</ref>, object detection <ref type="bibr">[17,</ref><ref type="bibr">80,</ref><ref type="bibr">49]</ref>, human pose estimation <ref type="bibr">[39]</ref>, 3D reconstruction <ref type="bibr">[13,</ref><ref type="bibr">67,</ref><ref type="bibr">14]</ref>, and image generation <ref type="bibr">[25,</ref><ref type="bibr">18,</ref><ref type="bibr">43]</ref>. Several existing studies <ref type="bibr">[17,</ref><ref type="bibr">76,</ref><ref type="bibr" target="#b0">1]</ref> observe that these representations naturally encode massive templates of particular visual patterns. However, little evi-dence suggests that deep neural networks can naturally conceptualize the intrinsic structures of an object category compactly and perceptibly.</p><p>We aim at learning the physical parameters of conceptualized object structures without supervision. As a typical representation of intrinsic structures, landmarks represent the spatial configuration of stable local semantics across different object instances of the same category. Thewlis et al. <ref type="bibr">[59]</ref> proposed an unsupervised method to locate landmarks at the places where a convolutional neural network can detect stable visual patterns with high spatial equivariance to image transformations. However, this method did not explicitly encourage the landmarks to appear at critical locations for image modeling.</p><p>This paper addresses the problem of discovering landmarks in a generic image modeling process. In particular, we take landmark discovery as an intermediate step for image autoencoding. To leverage the training signals from the landmark-based image decoder, gradients need to go through the landmark coordinates, which makes Thewlis et al. <ref type="bibr">[59]</ref>'s non-differentiable formulation infeasible. With a different way to calculate landmark coordinates, the image decoding module can make the landmark configuration informative regarding image reconstruction. We also introduce additional regularization terms to enforce the desirable properties of the detected landmarks and to prevent the landmark coordinates from encoding irrelevant or redundant latent information.</p><p>Our contributions in this paper are as follows. 1. We develop a differentiable autoencoder framework for object landmark discovery, which allows the image decoder to propagate training signals back to the landmark detection module. We introduce several soft constraints to reflect the properties of landmarks, forcing the discovered representations to be valid landmarks. 2. The proposed method discovers visually meaningful landmarks without supervision for a variety of objects. It outperforms the state-of-the-art method regarding the accuracy of predicting manually-annotated landmarks using discovered landmarks, and it performs comparably to fully supervised landmark detectors trained with a significant amount of labeled data.</p><p>property in 2D image domain, Rocco et al. <ref type="bibr">[50]</ref> proposed to discover TPS control points to match pairs of object images densely. Thewlis et al. <ref type="bibr">[58]</ref> tried to densely map different objects to a canonical coordinate that reflects object structures. Instead of learning dense correspondence, Thewlis et al. <ref type="bibr">[59]</ref> took the same equivariance property as the guidance to train deep neural networks for object landmark discovery without manual supervision. A similar idea was formulated differently using hand-crafted features in early work <ref type="bibr">[30]</ref>. In comparison, our method not only takes the equivariance as a constraint to ensure the validity of the landmarks, but also use a differentiable formulation to incorporate the landmark coordinates into a generic image modeling process. Moreover, our discovered landmarks are more predictive of manually annotated landmarks than those obtained by Thewlis et al. <ref type="bibr">[59]</ref>, and our method works on a broader range of object categories. Image modeling with landmarks. Many unsupervised deep learning techniques exist to model visual content, including stacked autoencoders (SAE) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">36]</ref>, variational autoencoders <ref type="bibr">[25]</ref>, generative adversarial networks (GAN) <ref type="bibr">[18,</ref><ref type="bibr">43]</ref>, and auto-regressive networks [63] (e.g., <ref type="bibr">PixelCNN [62]</ref>). The GAN-and PixelCNN-based image generators conditioned on given object landmarks are proposed in <ref type="bibr">[46,</ref><ref type="bibr">47]</ref>. In contrast, our method uses the SAE framework to automatically discover landmarks that are informative for unsupervised image modeling. Landmark detection. A vast amount of supervised landmark detection methods exist in the literature. For human faces, there are active appearance models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">38,</ref><ref type="bibr" target="#b10">11]</ref>, template-based methods <ref type="bibr">[42,</ref><ref type="bibr">83]</ref>, regression-based methods [61, <ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">48]</ref>, and more recent methods based on deep neural networks <ref type="bibr">[55,</ref><ref type="bibr">77,</ref><ref type="bibr">81,</ref><ref type="bibr">82,</ref><ref type="bibr">75,</ref><ref type="bibr">70,</ref><ref type="bibr">68,</ref><ref type="bibr">33,</ref><ref type="bibr">71]</ref>. Landmark detection methods are also available for human bodies <ref type="bibr">[73,</ref><ref type="bibr">60,</ref><ref type="bibr">39]</ref>, and birds <ref type="bibr">[75]</ref>. We use our discovered landmarks to predict manually annotated landmarks and compare our method with some recent supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Autoencoding-based landmark discovery</head><p>We aim at automatically discovering landmarks as an explicit representation of visual content. We propose an autoencoder that encodes landmark coordinates as (a part of) the encoder outputs (Section 3.1). Without supervision from hand-crafted labels, we introduce several constraints to encourage the discovered landmark coordinates to reflect the visual concept that agrees with human perception (Section 3.2). The proposed constraints prevent landmarkbased representations from degenerating to non-perceptible latent representations. Another pathway of the encoder extracts the local latent descriptor for each discovered landmark (Section 3.3). We use both the landmarks and the latent descriptors to reconstruct the input image (Section 3.4). This section presents the fully differentiable neural network architecture <ref type="figure" target="#fig_0">(Figure 1</ref>) and training objectives (Section 3.5) for landmark discovery and unsupervised image modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture of landmark detector</head><p>We formulate landmark localization as the problem of detecting particular keypoints in the image <ref type="bibr">[39]</ref>. Specifically, each landmark has a corresponding detector, which convolutionally outputs a detection score map with the detected landmark located at the maximum. In this framework, we use a deep neural network to transform an image I to a (K + 1)-channel detection confidence map D ∈ [0, 1] W ×H×(K+1) . This map detects K landmarks, and the (K + 1)-th channel represents background. D's resolution W × H can be either equal to or less than that of I, but they should have the same aspect ratio. Inspired by the success of the stacked hourglass network in human pose estimation [39], we propose a light-weighted hourglass-style network to get the raw detection score map</p><formula xml:id="formula_0">R = hourglass (I; θ ) ∈ R W ×H×(K+1) ,<label>(1)</label></formula><p>where θ denotes the parameters. The hourglass-style architecture (Appendix G.2) allows detectors to focus on the critical local patterns at landmark locations while utilizing higher-level context. Then, we transform the unbounded raw scores to probabilities and encourage each channel to detect a different pattern. To this end, we normalize R across the channels (including the background) using softmax and obtain the detection confidence map</p><formula xml:id="formula_1">D k (u, v) = exp(R k (u, v)) K+1 k =1 exp (R k (u, v)) ,<label>(2)</label></formula><p>where the matrix D k is the k-th channel of D, and the scalar D k (u, v) is the value of D k at the pixel (u, v). Later, we also use the vector D(u, v) ∈ [0, 1] K+1 to denote the multichannel values of D at (u, v). The same notation convention applies to other tensors of three axes. Taking D k as a weighting map, we use the weighted mean coordinate as the location of the k-th landmark, i.e.,</p><formula xml:id="formula_2">(x k , y k ) = 1 ζ k H v=1 W u=1 (u, v) · D k (u, v),<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">ζ k = H v=1 W u=1 D k (u, v)</formula><p>is the spatial normalization factor. This formulation enables back-propagating the gradient from the downstream neural network through the landmark coordinates unless D k 's mass is totally concentrated in a single pixel or totally uniformly distributed, which rarely happens in practice. As a shorthand notation, we write the landmarks and landmark detector as</p><formula xml:id="formula_4">= [x 1 , y 1 , . . . , x K , y K ] = landmark(I; θ ). (4)</formula><p>The left half of the blue pathway in <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the landmark detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual concept of landmarks</head><p>The elements in are supposed to be the discovered landmark coordinates, but so far, there is no guarantee to prevent them from being arbitrary latent representations. Therefore, we propose the following soft constraints as regularizers to enforce the desirable properties for landmarks. Concentration constraint As a detection confidence map for a single location, the mass of D k need to be concentrated in a local region. Taking D k /ζ k (spatially normalized as in (3)) as the density of a bivariate distribution on the image coordinate, we compute its variance σ 2 det,u and σ 2 det,v along the two axes. We define the concentration constraint loss as follows to encourage both variances to be small:</p><formula xml:id="formula_5">L conc = 2πe σ 2 det,u + σ 2 det,v 2 .<label>(5)</label></formula><p>This equation makes L conc the exponential of the entropy of the isotropic Gaussian distribution N ((x k , y k ), σ 2 det I), where σ 2 det = (σ 2 det,u + σ 2 det,v )/2, and I is the identity matrix. This Gaussian distribution is an approximation of D k /ζ k , and lower entropy means a more peaked distribution. Note that, formally, this approximation is</p><formula xml:id="formula_6">D k (u, v) = (1/W H)N (u, v); (x k , y k ), σ 2 det I . (6)</formula><p>Separation constraint Ideally, the autoencoder training objective can automatically encourage the K landmarks to be distributed at different local regions so that the whole image can be reconstructed. However, the initial randomness can make the landmarks, defined as the mean coordinates weighted by D as in <ref type="formula" target="#formula_2">(3)</ref>, all around the image center in the beginning of the training. This can lead to local optima from which the gradient descent may not escape (see Appendix F.2). To circumvent this difficulty, we introduce an explicit loss to spatially separate the landmarks:</p><formula xml:id="formula_7">L sep = 1,...,K k =k exp − (x k , y k ) − (x k , y k ) 2 2 2σ 2 sep . (7)</formula><p>Equivariance constraint A landmark should locate a stable local pattern (with definite semantics). This requires landmarks to show equivariance to image transformations.</p><p>More specifically, a landmark should move according to the transformation (e.g., camera and object motion) applied to the image if the corresponding visual semantics still exist in the transformed image. Let g(·, ·) be a coordinate transformation that map image I to I (u, v) = I(g(u, v)), and = [x 1 , y 1 , . . . , x K , y K ] = landmark(I ). We ideally have g(x k , y k ) = (x k , y k ), inducing the soft constraint</p><formula xml:id="formula_8">L eqv = K k=1 g(x k , y k ) − (x k , y k ) 2 2 ,<label>(8)</label></formula><p>This loss function is well-defined when g is known. Inspired by Thewlis et al.</p><p>[59], we simulate g by a thin plate spline (TPS) <ref type="bibr" target="#b3">[4]</ref> with random parameters. We use random translation, rotation, and scaling to determine the global affine component of the TPS; and, we spatially perturb a set of control points to determine the local TPS component. Besides the conventional way of selecting TPS control points at a predefined uniform grid (as used in [59]), we also take the landmarks detected by the current model as the control points to improve simulated transformation's focus on key image patterns. The two sets of control points are alternatively used in each optimization iteration (see Appendix F.3 for details). Moreover, when training sample appear in the form of video, we can also take the dense motion flow as g and the actual next frame as I .</p><p>Cross-object correspondence Our model does not explicitly ensure the semantic correspondence among the landmarks discovered on different object instances. The crossobject semantic stability of the landmarks mainly relies on the fact that visual patterns activating the same convolutional filter are likely to share semantic similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local latent descriptors</head><p>For simple images, like in MNIST [29] (see results for MNIST in Appendix B), multiple landmarks can be enough to describe the object shapes. For most natural images, however, landmarks are insufficient to represent all visual content, so extra latent representations are needed to encode complementary information. Though necessary, the latent representations should not encode too much holistic information that can overwhelm the image structures reflected by the landmarks. Otherwise, the autoencoder would not provide enough driving force to localize landmarks at meaningful locations. To achieve this trade-off, we attach a lowdimensional local descriptor to each landmark.</p><p>An hourglass-style neural network (see Appendix G.2) is introduced to obtain a feature map F, which has the same size as the detection confidence map D:</p><formula xml:id="formula_9">F = hourglass f (I; θ f ) ∈ R W ×H×S .<label>(9)</label></formula><p>Note that F is in a feature space shared among all landmarks and has S channels.</p><p>For each landmark, we use an average pooling weighted by a soft mask centered at the landmark to extract the local feature in the shared space. In particular, we take D k , which is the Gaussian approximation of the detection confidence map defined in <ref type="bibr" target="#b5">(6)</ref>, as the soft mask. Then, a learnable linear operator is introduced for each landmarks to map the feature representation into a lower-dimensional individual space. Thus, the latent descriptor for the k-th landmark is</p><formula xml:id="formula_10">f k = W k H v=1 W u=1 D k (u, v) · F(u, v) ∈ R C ,<label>(10)</label></formula><p>where C &lt; S. The landmark-specific linear operator enables each landmark descriptor to encode a particular pattern in limited bits. We can also use (10) to extract a low-dimensional background descriptor. Since it is unreasonable to approximate the background confidence map with a Gaussian distribution, we exactly set D K+1 = D K+1 /ζ K+1 . Note that f k is differentiable regarding both the feature map and the detection confidence map.</p><p>Putting all latent descriptors together, we have f = vec [f 1 , f 2 , . . . , f K+1 ] ∈ R C×(K+1) . The left half of the red pathway in <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the neural network architecture to extract the landmark descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Landmark-based decoder</head><p>We approximately invert the landmark coordinates to the detection confidence mapD ∈ R W ×H×(K+1) . Concretely, we use the probability density of an isotropic Gaussian distribution centered at each landmark to get raw score maps</p><formula xml:id="formula_11">R k (u, v) = N (u, v); (x k , y k ), σ 2 dec I ,R K+1 = 1. (11)</formula><p>and the background channel is set to 1.R is then normalized across channels to obtain the reconstructed detection confidence map <ref type="figure" target="#fig_0">Figure 1</ref> (right half of the blue pathway) illustrates this. For each landmark (including the background) descriptor f k , we transform it into a shared feature space by the landmark-specific operatorW k and an activation function (e.g., <ref type="bibr">LeakyReLU [34]</ref>). UsingD as the soft switches for global unpooling, we recover the feature map</p><formula xml:id="formula_12">D(u, v) =R k (u, v)/ K+1 k=1R k (u, v).<label>(12)</label></formula><formula xml:id="formula_13">F(u, v) = K+1 k=1D k (u, v) · τ (W k f k ) ∈ R W ×H×S , (13)</formula><p>where τ (·) is the non-linear activation function. This is illustrated by the right half of the red pathway in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Though alternative neural network architectures are available (e.g., in [46, 47]) for landmark-conditioned image decoding, our proposed architecture enables backpropagation through the landmark coordinates. The Gaussian variance σ 2 dec determines how much the neighboring pixels can contribute to the gradients for the landmark coordinates and how sharp the descriptor is localized in the recovered feature map. While it is important to include more pixels for back-propagation in the early stage of training, sharpness becomes more important as training goes on. To balance the two needs, we obtain multiple versions ofD,F under different values of σ dec , say,</p><formula xml:id="formula_14">(D 1 ,F 1 ), (D 2 ,F 2 ), . . . , (D M ,F M ).</formula><p>Let · · · be the channel-wise concatenation. We use another hourglass-style network to reconstruct the imagẽ</p><formula xml:id="formula_15">I = hourglass d ( D 1 ,F 1 , . . . ,D M ,F M ; θ d )<label>(14)</label></formula><p>The gray pathway in <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the image decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Overall training objective</head><p>The image reconstruction loss L recon drives the training of the entire autoencoder. We define L recon as I −Ĩ 2 F , and I is normalized to  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on a variety of datasets, Section 4.1 describes the datasets and shows the qualitative results of landmark discovery. In Section 4.2, we use the discovered landmarks to predict human-annotated landmarks, and we take the landmark detection accuracy as an indicator of the quality of discovered landmark. Section 4.3 demonstrates that our discovered landmarks can serve as effective image representations to predict shape-related facial attributes on CelebA. In Section 4.3, we show that our decoding module and the automatically discovered landmarks can be used to manipulate the object shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Landmark discovery on multiple datasets</head><p>We train and evaluate landmark discovery models on a variety of objects. The detailed architectures of the neural network modules (i.e., hourglass |f |d ) depend on the image sizes on different datasets. Appendix G describes implementation details, including data preprocessing, network architectures, model parameters, and optimization methods. CelebA Following [59], we use all facial images in the CelebA training set excluding those also appearing in the MAFL the test set 1 (then 16,1962 images in total) to train models for landmark discovery. We use the MAFL testing set (1000 images) for all testing cases and reserve the   MAFL training set (19,000 images) to train prediction models for manually-annotated landmarks. By default, we use the cropped and aligned images provided in the dataset.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, our method can automatically discover facial landmarks at semantically meaningful and stable locations, such as the forehead center, eyes, eyebrows, nose, and mouth corners. Compared to Thewlis et al.</p><p>[59]'s method, which results in a few significant errors, our method can locate landmarks more robustly against pose variations and occlusions. Interestingly, our method can work out-of-the-box on head-shoulder portraits without training on exactly the same type of images ( <ref type="figure" target="#fig_3">Figure 3</ref>). <ref type="figure" target="#fig_4">Figure 4</ref> shows that our method can also learn and detect a larger number (e.g., 30) of high-quality landmarks on unaligned facial images. Appendix E.1 shows more results.    in <ref type="figure" target="#fig_5">Figure 5</ref>, our model can discover consistent landmarks (e.g., ears, nose, mouth) across different cat species and interestingly predict landmark locations under significant occlusion (the first image). Appendix E.3 shows more results. Cars We build the profile-view car dataset by cropping the car images from the PASCAL 3D dataset. This dataset has a limited number of samples (567 images for training and 63 images for testing). As shown in <ref type="figure" target="#fig_8">Figure 7</ref>, our method can still learn meaningful landmarks (e.g., the windshield, driver-side door, wheels, rear) using a relatively small training set. Note that we transform the 3D annotations of the cars to 2D landmarks, so this dataset is ready for quantitative evaluation. Appendix E.4 shows more results. Shoes We use the same setting as in [59] (49,525, training images and 500 testing images). As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, landmarks are detected at semantically stable locations for different types of shoes. Appendix E.5 shows more results. Human3.6M Human3.6M contains human activity videos in stable backgrounds. We use all 7 subjects in Human3.6M training set for our evaluation (6 for training and 1 for validation) <ref type="bibr" target="#b1">2</ref> . We consider six activities (direction, discussion, posing, waiting, greeting, walking), in which human bodies are in the upright direction most of the time, resulting in 796,648 image frames for training and 87,975 image frames for testing. We removed the background using the off-the-shelf unsupervised background subtraction method provided in the dataset. The human bodies are cropped and roughly aligned regarding the foot location so that the excessive background regions are removed.</p><p>Compared to previously mentioned object types, human bodies have much more shape variations. As shown in <ref type="figure" target="#fig_9">Figure 8</ref>, our method can discover roughly consistent landmarks across a range of poses. In particular, the landmarks at the head, back, waist, and legs are stable across images. The landmarks at the arms are relatively less consistent across different poses, but they are still at semantically meaningful locations. Since the human body appearances in the frontal and back views are similar, we do not expect our discovered landmarks to distinguish the left and right sides of the human body, which means that a landmark at the left leg in the frontal view can locate the right leg in the back view. Since the training data is in the video format, optical flows are used as a short-term self-supervision for the eqvuivariance constraint in <ref type="bibr" target="#b7">(8)</ref>. Appendix C describes more details and results for Human3.6M experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Prediction of ground truth landmarks</head><p>Unsupervised landmark learning is useful because of its potential to discover object structures that are coherent with the human's perception. We evaluate discovered landmarks' quality by predicting manually-annotated landmarks. Specifically, we use a linear model without a bias term to regress from the discovered landmarks to the human-annotated landmarks. Ground truth landmark annotations are needed to train this linear regressor. Thewlis et al.</p><p>[59] extensively used random TPS to augment both discovered and labeled landmarks for training (on CelebA and ALFW). However, we do not use data augmentation for our method to minimize the complexity of training. Even in this case, our method shows stronger performance. Stronger relevance to human-designed landmarks. In <ref type="table" target="#tab_2">Table 1a</ref>, we regress the landmarks discovered using the models trained on the CelebA training set to the 5 annotated landmarks. The landmark labels in either the CelebA training set or the much smaller MAFL training set are used to train the regressor. Our method is not sensitive to the decreased size of the labeled training set. It outperforms Thewlis et al.</p><p>[59]'s by 55% decrease of the landmark detection error and Thewlis et al.</p><p>[58]'s by 45%. Notably, we achieve this with 30 discovered landmarks while theirs uses 50 landmarks or dense object frames. Additionally, <ref type="table" target="#tab_4">Table 2</ref> 2 Training subject IDs: S1,S5,S6,S7,S8,S9; Validation subject IDs: S11.  demonstrates the consistent superiority of our method on the cat head dataset (7 target landmarks 3 ), the car dataset (6 target landmarks), and Human3.6M 4 (32 target landmarks). <ref type="figure" target="#fig_10">Figure 9</ref> illustrates the landmark regression results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># discovered landmarks</head><p>Competitive performance compared to fully supervised methods. Putting the landmark discovery model together with the linear regressor, we obtain a detector of humandesigned landmarks. Unlike fully supervised methods, our model is trainable with a huge amount of unlabeled data, and the linear regressor can be trained using a relatively small amount of labeled data within a few minutes. <ref type="table" target="#tab_2">Table 1b</ref> demonstrates that our model outperforms previous unsupervised methods and off-the-shelf pretrained fully-supervised models on the MAFL and AFLW testing sets. On AFLW, we take the 5 always-visible landmarks as the regression target. All models reported are either trained on the MAFL training set or publicly available.   Landmark detection with few labeled samples. Taking our model as a detector of manually annotated landmarks, we find that less than 200 samples are enough for our model to achieve less than 4% mean error on the MAFL testing set, which is better than the performance of TCDCN and MTCNN. Learning curves are provided in Appendix F.1.</p><p>Effectiveness of different loss terms. Our method combines several loss terms in the training objective <ref type="bibr">(15)</ref>. <ref type="table" target="#tab_2">Table 1c</ref> shows that the removal of any term can cause performance drop of our model. In particular, the removal of the separation loss can devastate the model, and more detailed discussion about this loss term is in Appendix F.2.</p><p>Our new differentiable formulation of the landmark validity constraints can already lead to a lower landmark detection error than Thewlis et al.</p><p>[59]'s. Adding the reconstruction loss can further improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visual attribute recognition</head><p>Landmarks reflect object shapes. We use our discovered landmarks as a feature representation to recognize the shape-related binary facial attributes (13 labeled attributes are found) on CelebA. We still take the MAFL testing set for the quantitative evaluation. A linear SVM is trained for each attribute on the CelebA training set. We also compare our landmark coordinates with pretrained FaceNet [51] (Incep-tionV1) top-layer (128-dim) and top conv-layer (1792-dim) features for the attribute recognition task. As shown in Table 3, our discovered landmarks (60-dim) outperforms the  FaceNet top-layer features for most attributes. The convlayer features outperform our landmarks slightly but have a much higher dimension. Combining the landmark coordinates and the FaceNet features, higher accuracy is achieved. This suggests that the discovered landmarks are complementary to image features pretrained on classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image manipulation and generation</head><p>Our jointly trained image decoding module conditioned its outputs on the input landmarks and their latent descriptors. If the two conditions are disentangled, we should be able to manipulate the object shape without changing other appearance factors by adjusting only the landmarks; or, vice versa. Note that landmark-based image morphing is not a new topic, and landmark-based hierarchical image decoding has also been explored recently [46, 64, 47]. However, these landmarks are all designed and annotated by humans. So far, little evidence has suggested that the automatically discovered landmarks are accurate and representative enough as a reliable condition for image generation.</p><p>In <ref type="figure" target="#fig_0">Figure 10</ref>, we synthesize flows to adjust the discovered landmarks of an input image. Fixing the landmark latent descriptors, we obtain realistic facial and human-body images whose shapes agree with the new landmarks. Other than the facial and body shape, then appearance factors of the input image are not visually changed. This result suggests that our image decoding module can synthesize realistic image using the landmarks learned without supervision, and it also suggests that our discovered landmarks have become an explicit representation disentangled from other fac-   tors of variations for image modeling. Implementation details and more results about unsupervised landmark-based face manipulation are available in Appendix A.</p><p>In <ref type="figure" target="#fig_0">Figure 11</ref>, instead of adjusting the landmark coordinates, we use the discovered landmarks of a reference image as the control signal to generate new facial images. Following the GAN framework [18], the latent representation of the generated image is randomly drawn from a prior distribution. As in Reed et al. <ref type="bibr">[46]</ref>, the landmark coordinates and latent representation are combined for image generation. We adopt BEGAN <ref type="bibr" target="#b2">[3]</ref> for the discriminator and training objective. In addition, we apply a cyclic loss for the landmark coordinates, which encourages the same landmarks to be detected on the generated images as on the reference image. Our results provide additional evidence on the usefulness of the discovered landmarks for image modeling. Implementation details are in Appendix G.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We address the problem of unsupervised object landmark discovery and take it as an intermediate step of image representation learning. In particular, a fully differentiable neural network architecture is proposed for determining the landmark coordinates, together with soft contraints to enforce the validity of the detected landmarks. The discovered landmarks are visually meaningful and quantitatively more relevant to human-designed landmarks. In our framework, the discovered landmarks are an explicit part of the learned image representations. They are disentangled from the latent representations of the other appearance factors. The landmark-based explicit representations not only provide an interface for manipulating the image generation process but also appear to be complementary to pretrained deep-neuralnetwork features for solving discriminative tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More details and results on face manipulation using unsupervised landmarks</head><p>The discovered landmarks constitute the explicitly structural part of the image representation learned by our model. They provide an interface for humans to manipulate the image representation intuitively. Our decoding module can generate realistic facial images using the landmark descriptors extracted from a given image and different sets of landmarks.</p><p>In addition to the results shown in the main paper, we provide more qualitative results for unsupervised landmark-based face manipulation in this section. We train models of 10, 20, and 30 landmarks. To evaluate our method on many target landmarks, we take the landmarks discovered from other images and the interpolation/extrapolation between the landmarks discovered on two images as the targets.</p><p>In this section, we show results for our 30-landmark model (results for our 10,20-landmark model are available as supplementary videos). <ref type="figure" target="#fig_0">Figure 12 and 13</ref> show results for manipulating all 30 landmarks. <ref type="figure" target="#fig_0">Figure 14 and 15</ref> show results for manipulating the 3 landmarks at the mouth. <ref type="figure" target="#fig_0">Figure 16 and 17</ref> show results for manipulating the 5 landmarks at the mouth and jaw. Videos are available in the following folders for gradually morphing the landmarks from their original coordinates to the target by linear interpolation.</p><p>• 30-landmark models:  Face manipulation by modifying all 30 discovered landmarks on the MAFL testing set. 1st column: input images; 2nd column: discovered landmarks and reconstructed images; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/face.30landmark-model.manipulate-all-landmarks for the morphing process.     <ref type="figure" target="#fig_0">Figure 14</ref>:</p><p>Face manipulation by modifying 3 discovered mouth landmarks on the MAFL testing set. 1st column: input images; 2nd column: discovered landmarks and reconstructed images; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/face.30landmark-model.manipulate-mouth-landmarks for the morphing process.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our model without landmark descriptors on MNIST</head><p>We train our landmark discovery model without the landmark descriptor pathway on MNIST in two settings: one model for each digit (Appendix B.1) and one model for all ten digits (Appendix B.2). Using the model for all digits, we can perform geometrically meaningful morphing between different digits (Appendix B.3), e.g., morphing 2 to 9. Videos for the morphing process are available in the folder videos/mnist-morphing .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Models for individual digits</head><p>In this section, our landmark discovery model is trained for each digit independently. As shown in <ref type="figure" target="#fig_0">Figure 18</ref>, the discovered landmarks are consistent within each digit despite the shape variations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Models for all digits</head><p>In this section, we train a single model for all ten digits together. In <ref type="figure" target="#fig_0">Figure 19</ref>, the corresponding landmarks are shown in the same color. The landmarks discovered on the same digits are consistent across different image. More interesting, the corresponding landmarks across different digits are also semantically consistent. For examples, the orange cross is always in the middle of a digits, the blue cross at the bottom, and the green one at the most left-top part of every digit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Digit morphing using discovered landmarks</head><p>Our model trained on the mix of all ten digits can discover corresponding patterns among different digit categories. Using this model, we can perform geometrically meaningful cross-category morphing. <ref type="figure" target="#fig_2">Figure 20</ref> and 21 illustrate the morphing process. Note that the landmark coordinates constitute the full image representation for MNIST digits. Videos for the morphing process are available in the folder videos/mnist-morphing .  <ref type="figure" target="#fig_2">Figure 20</ref>: Geometric digital morphing using our discovered landmarks and image decoding module. The landmark coordinates are linearly interpolated between the source digit and the target digit. 1st column: input images; 2nd column: discovered landmarks and reconstructed images of the source digit; last column: discovered landmarks and reconstructed images of the target digit; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/mnist-morphing for the morphing process.   <ref type="figure" target="#fig_2">Figure 20</ref>. Geometric digital morphing using our discovered landmarks and image decoding module. The landmark coordinates are linearly interpolated between the source digit and the target digit. 1st column: input images; 2nd column: discovered landmarks and reconstructed images of the source digit; last column: discovered landmarks and reconstructed images of the target digit; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/mnist-morphing for the morphing process.</p><p>C. Details and more results on Human3.6M</p><p>On the Human3.6M dataset, we train our landmark discovery model on six actions: waiting, posing, greeting, directions, discussions, and walking. We report quantitative results on predicting the 32 annotated landmarks 5 (acquired by wearable markers) using our models trained for the mix of all six actions and each independent action. We also show qualitative results of the mixed-action model (trained for all six actions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Optical flow as self-supervision for equivariance</head><p>The Human3.6M training data is in the video format. We can calculate the optical flows between nearby frames and take them as self-supervision for the equivariance constraint defined in <ref type="bibr" target="#b7">(8)</ref>. Following the same notations, the two frames are I and I , and the optical flows define the transformation g(·, ·). In particular, we use the Farneback method in OpenCV to compute the dense optical flows at 5-frame intervals. We then accumulate two optical flow fields to calculate the optical flows at 10-frame intervals. The 10-frame-interval optical flow fields in addition to the random TPS transform are used in the equivariance constraint.</p><p>Note that using optical flow as the self-supervision for the equivariance constraint can push the landmarks to the background region. This phenomenon occurs because locating landmarks at image regions with weaker optical flows can result in low equivariance loss. To prevent trivial landmarks, we encourage the landmarks to be discovered at locations with strong optical flows. Let O x and O y be the x, y components of the optical flow map from the current image to another frame. The</p><formula xml:id="formula_16">flow magnitude map is O n (u, v) = (O x (u, v)) 2 + (O y (u, v)) 2 .</formula><p>Recall thatR is the multi-channel heatmap computed from the landmark locations. We encourage O n andR to have a significant correlation. In particular, loss to encourage</p><formula xml:id="formula_17">L flow-prefer = − H v=1 W u=1 O n (u, v) K k=1R k (u, v) H v=1 W u=1 O n (u, v)<label>(16)</label></formula><p>We use the same loss weight λ eqv for L flow-prefer as L eqv . Using the above formulation, we can reduce the ground landmark prediction error from 4.91 (without optical flows) to 4.14 (with optical flows). For all experiments on Human3.6M, we use the optical flow as self-supervision for equivariance as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Quantitative results</head><p>We compare our model with Thewlis et al.</p><p>[59]'s unsupervised landmark discovery method regarding the annotatedlandmark prediction accuracy. Both models discover 16 landmarks. The whole training set is used to train the linear mapping from the discovered landmarks to the annotated ones. As discussed in the main paper, we do not expect the two unsupervised methods to distinguish the frontal and back views. Thus, in the evaluation, we compute the errors against the original landmark annotations and its left-right-flipped counterpart <ref type="bibr" target="#b5">6</ref> , and then we choose the minimum value as the final error. Note that, when flipping the landmark annotations, the landmarks for the whole body are flipped simultaneously. As to the linear regressor training, we propose the following training strategy.</p><p>1. <ref type="figure">Figure out</ref> the rough orientations of the human body, heuristically. If more than 2/3 of the left-hand side annotated landmarks are to the right of the right-hand side annotated landmarks, the human is in the frontal view. 2. Train the regressor using the images with the landmark annotations in the frontal view. The other images are ignored in this step. 3. Use the aforementioned evaluation protocol to determine if the landmark annotations on other images should be flipped or not. The model is then retrained with all the training images. 4. Repeat the step 3 until the model is converged. As shown in <ref type="table">Table 4</ref>, our method outperforms Thewlis et al.</p><p>[59]'s method significantly. We also report the results obtained by Newell et al.</p><p>[39]'s supervised stacked hourglass network using their off-the-shelf pretrained 16-landmark model. Both unsupervised methods perform worse than the supervised stacked hourglass network. However, our model is unsupervised, and our neural network architectures are also smaller. We believe that our results show the potential of unsupervised methods for discovering complicated object structures.  <ref type="table">Table 4</ref>: Comparison with unsupervised and supervised methods for annotated landmark prediction on the Human 3.6M testing sets. The error is in % regarding the edge length of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Qualitative results</head><p>We train our model and Thewlis et al.</p><p>[59]'s model on all six chosen actions and perform the annotated-landmark prediction. <ref type="figure" target="#fig_2">Figure 22</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on animals of mixed species</head><p>On the animal-with-attributes (AwA) dataset [28], we choose the profile images from five animal categories (antelope, deer, moose, horse, zebra) and try to detect landmarks on these mixed species of animals. As shown in <ref type="figure" target="#fig_2">Figure 24</ref>, even though multiple species of animals with different appearance are mixed, our method can still find several consistent landmarks. For example, the yellow cross is always on the hoof, the orange cross always above the back and the light green cross at the buttock. The landmarks are consistently detected despite the significant variations in species, pose and individual appearance. See next page for the figure.   Taking our model as a detector of manually annotated landmarks, we find that less than 200 samples are enough for our model to achieve less than 4% mean error on the MAFL testing set, which is better than the performance of two popular offthe-shelf models. This result suggests that it is possible to train a high-accurate landmark detector using only a few labeled sample when sufficient unlabeled samples are given to train our unsupervised model. We show its performance versus the number of labeled samples in <ref type="figure" target="#fig_3">Figure 38</ref>. <ref type="figure" target="#fig_3">Figure 39</ref> shows the detection confidence maps of an input image at different training stage of our model. In the beginning, the heatmap shows random values over the whole image. As a result, the landmarks, defined as the mean coordinates weighted by the confidence maps, are all at the center of the image. As the training goes on, the values gradually becomes spatially concentrated. With the separation loss defined in <ref type="bibr" target="#b6">(7)</ref>, the peaked value of each channel of the confidence map can move to a different location. Without the separation loss, every channel can have a peaked value at the center of the images, resulting in degenerate landmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Evolution of detection confidence map during training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. TPS control points</head><p>For the random TPS in our equivariance constraint (defined in <ref type="formula" target="#formula_8">(8)</ref>), we both use the regular-grid control points and take the discovered landmarks in the current iteration as the control points. The two sets of control points are alternatively used in each optimization iteration with 7:3 chance. We do not exhaustively tune the ratio and keep it the same in all experiments. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, the performance of our model is fairly insensitive to this ratio when the other hyper-parameters are fixed. However, introducing the discovered landmarks as the TPS control points does benefit. grid:landmark 10:0 1:9 3:7 5:5 7:3 9:1 GT prediction error / % 4.17 3.86 3.46 3.38 3.46 3.41 <ref type="table">Table 5</ref>: Impact of the ratio between two types of TPS control points (i.e., regular grid and discovered landmarks). Our 10-landmark CelebA model is trained under different ratios of the TPS control points. The evaluation metric is the ground truth (GT) landmark prediction errors with the CelebA training set for linear regressor training.</p><p>Note that the discovered landmarks are clustered at the center of the image (see discussion about the separation constraint in Section 3.2) and cannot serve as good TPS control points. As a result, we use only the regular-grid control points in the beginning and start to apply the previously mentioned ratio after training the model for several thousands of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Implementation details G.1. Data preprocessing</head><p>The main paper and this supplementary materials report results on several datasets. <ref type="table" target="#tab_16">Table 6</ref> summarizes the image size we used for each each dataset. In our landmark discovery formulation, we need to perform random TPS to calculate the equivariance constraint in <ref type="bibr" target="#b7">(8)</ref>. It requires the image to have large enough margins so that the foreground will not be out of image due to the random transformation.  For different datasets, we crop the foreground images and prepare input images as follows. CelebA We started from the cropped-and-aligned images (218×178) in CelebA dataset, scaled them to 100×100 pixels and then cropped the 80×80 center patches. AFLW The dataset provides annotated bounding boxes. We enlarged the bounding boxes on each image with a margin on the top (1/4 height of the original bounding box), margins on the left and right (1/8 width of the original bounding box) so that the cropped facial images look similar to the CelebA data. The cropped images were also scaled to 80×80. Cat Heads The dataset provides ground truth landmarks on the cat head. We figured out the bounding box for cropping each image according to the landmarks and then scaled the cropped images to 80×80. Shoes We scaled the shoe images (102×136) to 64×64, and padded the images to be 80×80 with white margins. We linearly transformed the color value range from [0, 1] to [0.1, 0.9] to avoid a huge amount of saturated responses for the output layer with the sigmoid activation. The color was scaled back for visualization. Cars The PASCAL 3D dataset provides annotations for the orientation and landmarks of several objects. We cropped the profile car images according to the bounding box of the ground truth landmarks. Slight margins are added to the bounding box, and the cropped image is scaled to a square image without preserving the aspect ratio. Animals from AwA We manually annotated bounding boxes and orientation labels (i.e., left, right, frontal, back) for several types of animals. For each rectangular bounding box, we enlarged its shorter edge to make the box square, cropped the patch, and scaled it to 64×64. We ignored the images with frontal and back views, and we flipped the right-facing animals horizontally so that all animals in the image face to the left. Human3.6M The human body was cropped using a square bounding box from the original video frames. We use the 3D landmarks provided in this dataset, acquired by wearable markers, as side information to roughly align the scales and foot locations of the human bodies in different images. The cropped square images are scaled to 128×128 pixels. We use the provided segmentation masks, obtained by an off-the-shelf unsupervised background removal method, to mask out the background image with gray color. For visualization, we show the gray color as white for the printing clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel size</head><p>Output channels <ref type="table" target="#tab_2">Input  n/a  3  80   Convolution  3  16  80  Convolution  3  16  80  Pooling  2  16  40  Convolution  3  32  40  Convolution  3  32  40  Pooling  2  32  20  Convolution  3  64  20  Convolution  3  64  20  Pooling  2  64  10  Convolution  3</ref>   <ref type="table" target="#tab_2">Input  n/a  3  80  Convolution  3  32  80  Convolution  3  64  80  Pooling  2  64  40  Convolution  3  128  40  Pooling  2  128  20  Convolution  3  256  20  Pooling  2  256  10  Convolution  3  512  10  Convolution  3  512  10  Upsampling  2</ref>   <ref type="table" target="#tab_2">Input  n/a  3  96  Convolution  3  32  96  Convolution  3  32  96  Pooling  2  32  48  Convolution  3  64  48  Convolution  3  64  48  Pooling  2  64  24  Convolution  3  64  24  Convolution  3  64  24  Pooling  2  64  12  Convolution  3  64  12  Convolution  3</ref>   <ref type="table" target="#tab_2">Output  size  2  256  10  Convolution  3  512  10  Pooling  2  512  5  Convolution  3  512  5  Convolution  3  512  5  Upsampling  2  512  10  Convolution  3  512  10  Upsampling  2  512  20  Convolution  3  256  20  Upsampling  2  128  40  Convolution  3  128  40  Upsampling  2  64  80  Convolution  3  64  80  Convolution  3</ref> 3 80  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Network architectures</head><p>In Section 3 of the main paper, we describe the key architectures of our model and leave some details unspecified. This section describes the detailed neural network architectures. <ref type="figure" target="#fig_4">Figure 40</ref> summarizes the hourglass-like architectures that we used for images of different sizes. The image padding is explaned and specified in Appendix G.1. In general, an hourglass architecture has mirrored encoding (high-resolution to low-resolution) and decoding (low-resolution and high-resolution) architectures. Skip-links, made up of convolutional layers, create shortcuts from the encoding feature maps and decoding feature maps of the same resolution. The responses of the skip-links are fused with the main stream decoding responses using element-wise addition. We use the max-pooling to reduce the feature map size for encoding, and we upsample feature maps by the nearest interpolation for decoding. For each linear layer, a batch normalization layer is followed, and LeakyReLU [34] is the default activation function.</p><p>Based on the neural network architectures in <ref type="figure" target="#fig_4">Figure 40</ref>, a convolutional layer of 1×1 kernels calculates the raw detection score map of K + 1 channels (recall that K is the number of landmarks). For image output, the last convolutional layer's responses (∈ R) are mapped to (0, 1) with a sigmoid function for the reconstructed color intensity and to (0, +∞) by log(1 + 2 exp(z))/2 for the pixel-wise standard deviation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Training strategy</head><p>We use Adam with an initial learning rate of 0.001 to optimize the neural network parameters. We set the training batch size to 16 or 32 7 for 80×80 and 60×60 images and 8 for 128×128 images. The learning rate starts from 10 −3 and decreases to 10 −4 and 10 −5 later. For color images, we do random brightness and contrast jittering.</p><p>For batch normalization, the global mean and variance are computed using a random subset of the training set when the neural network training is done. Note that using the running average during training for the global mean and variance can hurt the performance.</p><p>To implement the equivariance constraint in (8), we use random TPS transformation to obtain warped input images (paired with the original images) in each training iteration. Taking the normalized image height and width as 1, the random transformation parameters are:</p><p>• Global affine component. Uniform random translation in ±0.15; Gaussian random rotation with the standard deviation of 10 • ; Gaussian random scaling in the base-2 logarithm scale with the standard deviation of 1.25. • Local TPS. Gaussian random translation with the standard deviation of 0.1 (regarding the regular-grid control points) or 0.05 (regarding the landmark control points). For the reconstruction loss weight λ recon in (15), we find it helpful to start with a small value and increase it to its final value at a later stage. At the early training stage, the discovered landmarks change significantly for each iteration, and the latent descriptor of landmarks inputted to the decoder also varies a lot. The model parameters of the decoder and the gradients from it can change too drastically if the reconstruction loss weights is large, which would harm the training of both the landmark detector and the landmark-based image decoder. As a particular training strategy, we increase the value of λ recon by ×10 twice during the training. <ref type="table" target="#tab_21">Table 7</ref> in Appendix G.4 summarizes the detailed hyper-parameters.</p><p>For the L2 reconstruction loss L recon , we scale the image pixel values to [0, 1]. The loss is explicitly defined as the negative logarithm likelihood (NLL) to draw the input image I from the Gaussian distribution centered at the reconstructed imageĨ the with a fix variation σ color = 0.05. More specifically,</p><formula xml:id="formula_18">L recon = 1 σ 2 color I −Ĩ 2 F + ln (2πσ 2 color ).<label>(17)</label></formula><p>The value of the loss weight λ recon is based on this definition. <ref type="bibr" target="#b6">7</ref> There is no significant difference in the performance when using either 16 or 32 as the training batch size.  G.5. Details about face generations using unsupervised landmarks <ref type="figure" target="#fig_0">Figure 11</ref> in the main paper shows results of generating facial images conditioned on our discovered landmarks. In this experiment, we fix our landmark discovery module and use it to detect landmarks on training images. For the decoding module, we take the detected landmarks as a given input condition and map an isotropic Gaussian random variable to the latent part of the image representation. Inspired by [46], we first use deconvolutional layers to get a feature map from the random variable and use convolutional layers to get another map of the same size from the reconstructed detection confidence map. We use element-wise multiplication and channel-wise concatenation to fuse the two into one feature map as the input of the decoding neural network. Thus, the way of calculating the input feature map of the decoding module is not the same as our landmark discovery model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4. Hyper-parameters</head><p>We use the boundary equilibrium GAN (BEGAN) <ref type="bibr" target="#b2">[3]</ref> framework to design the discriminator, which encourages the decoder to generate realistic images. More concretely, an autoencoder is trained as the energy function to distinguish the real and generated images. To make sure the generated images are consistent with the landmark condition, we first use our landmark discovery module to detect landmarks on them. We then take the L1 distance between them and those from the corresponding real images (i.e., input training images for getting the landmarks) as an extra training loss for the decoding module.</p><p>This experiment is mainly to show that our discovered landmark is accurate and meaningful enough for controllable image generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>RFigure 1 :</head><label>1</label><figDesc>: raw detection score map D : normalized detection confidence map F : shared feature map : element-wise multiplication with broadcasting to singular dimensions W k : shared-to-individual feature mapping f : landmark-specific individual featurẽ : the decoding/reconstructed version Landmark coordinates Neural network architectures of our autoencoding framework for unsupervised landmark discovery. See text for the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[0, 1]. The full loss is L AE = λ recon L recon + λ conc L conc + λ sep L sep + λ eqv L eqv . (15)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Discovering 10 landmarks on CelebA images. All figures for Thewlis et al. [59]'s come from their paper. The last row shows unsuccessful cases from [59] with error descriptions below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Discovering 10 landmarks on unaligned head-shoulder images using our model trained on aligned facial images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Discovering 30 landmarks on unaligned CelebA images using our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Discovering landmarks on cat head images using our method. Top row: 10 landmarks; Bottom row: 20 landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>AFLW Face images in AFLW are cropped differently from CelebA. The landmark discovery models (both ours and Thewlis et al. [59]'s) are pretrained on CelebA and finetuned on the AFLW training set (10,122 images) for adaptation. Sampled results on the AFLW testing set (2,991 images) are in Appendix E.2. Cat heads Our model is trained on 7,747 cat head images and tested on 1,257 images. Compared to human faces, cat heads show more holistic appearance variations. As shown</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Discovering 8 landmarks on shoes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Discovering 10 landmarks on the profile images of cars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Discovering 16 landmarks on Human3.6M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Prediction of annotated landmarks. Colorful cross: discovered landmark; Red dot: annotated landmark; Circle: regressed landmark, whose color represent its distance to the annotated landmarks. See the color bar for the distance (i.e., prediction error).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Image manipulation with our discovered landmarks and landmark-based decoder on the MAFL and Human3.6M testing set. 1st column: input images; 2nd column: discovered landmarks and reconstructed images; other columns: the red dots for new landmark locations, the gray lines for the synthetic adjustment of landmarks, and the images for the decoder outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Face generation conditioned on discovered landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Face manipulation by modifying all 30 discovered landmarks on the MAFL testing set. 1st column: input images; 2nd column: discovered landmarks and reconstructed images; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/face.30landmark-model.manipulate-all-landmarks for the morphing process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Continued fromFigure 12. Face manipulation by modifying all 30 discovered landmarks on the MAFL testing set. 1st column: input images; 2nd column: discovered landmarks and reconstructed images; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/face.30landmark-model.manipulate-all-landmarks for the morphing process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>M</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>Continued fromFigure 14. Face manipulation by modifying mouth 3 discovered mouth landmarks on the MAFL testing set. 1st column: input images; 2nd column: discovered landmarks and reconstructed images; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/face-manipulation-mouth-landmarks for the morphing process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 :</head><label>17</label><figDesc>Continued fromFigure 14. Face manipulation by modifying mouth 6 discovered mouth and jaw landmarks on the MAFL testing set. 1st column: input images; 2nd column: discovered landmarks and reconstructed images; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/face-manipulation-mouthext-landmarks for the morphing process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 :</head><label>18</label><figDesc>Discovering 7 landmarks on MNIST. Our model is trained independently for each digit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 19 :</head><label>19</label><figDesc>Discovering 7 landmarks on MNIST for all digits. A single model is trained for all ten digits. The corresponding landmarks for different images are in the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 :</head><label>21</label><figDesc>Continued from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>shows the side-by-side comparison. In general, our method visually outperforms Thewlis et al. [59]'s. Figure 23 shows landmark discovery examples, where our method outperforms Thewlis et al. [59]'s method very significantly. See next page for the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 22 :</head><label>22</label><figDesc>Prediction of 32 annotated landmarks on Human 3.6M. Colorful cross: discovered landmark; Red dot: annotated landmark; Circle: regressed landmark, whose color represent its distance to the annotated land-marks. See the color bar for the distance (i.e., prediction error). Our method shows more deep blue circles (for example, the image in second row second column, the image in last row second column), which means more landmarks with low error compared with Thewlis et al.[59]   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 23 :</head><label>23</label><figDesc>Discovering 16 landmarks on Human3.6M testing set. We illustrate some cases when our method outperforms Thewlis et al. [59]'s very significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 24 :</head><label>24</label><figDesc>Discovering 10 landmarks on mixed animal images of five different species: antelope, deer, moose, horse, zebra E. More qualitative results on human faces, cat heads, cars, and shoes In this section, we show more result of landmark discovery and ground truth landmark prediction compared with Thewlis et al. [59]. All the shown images are randomly sampled from the test set. E.1. CelebA Figure 25: Discovering 10 landmarks on CelebA, the detected landmarks are highly aligned with facial features such as mouth corner, eyes corner and nose</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 26 :Figure 27 :Figure 29 :Figure 31 :</head><label>26272931</label><figDesc>Discovering 30 landmarks on CelebA Prediction of 5 annotated landmarks on CelebA. Colorful cross: discovered landmark; Red dot: annotated landmark; Circle: regressed landmark, whose color represent its distance to the annotated land-marks. See the color bar for the distance (i.e., prediction error). Our method shows more deep blue circles (for example, the image in first row first column, the image in first row fourth column), which means more landmarks with low error compared with Thewlis et al.[59]   E.2. AFLW Discovering 10 landmarks on AFLW E.3. Cat heads Discovering 10 landmarks on Cat Head dataset, our method find some landmarks on the nose, eyes and base of earlobe</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 32 :Figure 33 :Figure 34 :Figure 35 :Figure 36 :Figure 37 :F. 1 .Figure 38 :</head><label>323334353637138</label><figDesc>Discovering 20 landmarks on Cat Head dataset Prediction of 7 annotated landmarks on cat head. Colorful cross: discovered landmark; Red dot: annotated landmark; Circle: regressed landmark, whose color represent its distance to the annotated land-marks. See the color bar for the distance (i.e., prediction error).Our method shows more deep blue circles (for example, the image in fourth row third column, the image in second row first column), which means more landmarks with low error compared with Thewlis et al.[59]   E.4. Cars Discovering 10 landmarks on PASCAL-VOC 3D Car dataset Discovering 24 landmarks on PASCAL-VOC 3D Car dataset Prediction of 6 annotated landmarks on car. Colorful cross: discovered landmark; Red dot: annotated landmark; Circle: regressed landmark, whose color represent its distance to the annotated land-marks. See the color bar for the distance (i.e., prediction error).Our method shows more deep blue circles (for example, the image in last row first column, the image in last row last column), which means more landmarks with low error compared with Thewlis et al.[59]   Landmark discovery results of our model on shoes using 8 landmarks F. Ablative study Number of labeled samples for annotated-landmark prediction Prediction errors of manually-annotated landmarks on the MAFL testing set when using different numbers of labeled samples to train the linear regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 39 :</head><label>39</label><figDesc>The evolution of the detection confidence map D during training. The training of a 10-landmark CelebA model is monitored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>decided by the number of landmarks. ? ** : decided by the number of landmarks and the dimension of the shared feature space. ? *** : this layer is used only for hourglass , and the number of channels is decided by the number of landmarks. ? **** : decided by the number of landmarks for hourglass , = 32 for hourglass f , and = 3 for hourglass d . Each skip-link in (a), (b), (c), (d), and (f) consists of three convolutional layers of 3×3 kernels. Each skip-link in (e), (g), and (h) consists of two convolutional layers of 3×3 kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 40 :</head><label>40</label><figDesc>Architectures of our hourglass-style networks. (a),(b), (c), (d): For 80×80 and 64×64 images, the outputs of hourglass and hourglass f have the same size as the image. (g): For 128×128 images, the outputs of hourglass and hourglass f are 64×64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Mean errors of the annotated landmark prediction on hu- man face datasets. Errors are in % regarding the biocular distance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Mean errors of the annotated landmark prediction on the cat heads, cars, and human bodies. Errors are in % regarding the biocular distance, bi-wheel distance, and image size, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Visual attribute recognition using pretrained FaceNet features and our discovered 30 landmarks on the MAFL test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>[ 13 ]</head><label>13</label><figDesc>David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NIPS, 2014. 1 [14] Haoqiang Fan, Hao Su, and Leonidas Guibas. A point set generation network for 3D object reconstruction from a single image. In CVPR, 2017. 1 [15] Pedro Felzenszwalb, Ross Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010. 2 [16] Robert Fergus, Pietro Perona, and Andrew Zisserman. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1 [21] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 1, 2 [22] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014. 5 [23] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In NIPS, 2015. 2 [24] Angjoo Kanazawa, David W Jacobs, and Manmohan Chandraker. WarpNet: Weakly supervised matching for singleview reconstruction. In CVPR, 2016. 2 [25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014. 1, 2 [26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Im-ageNet classification with deep convolutional neural networks. In NIPS, 2012. 1 [27] Michael Lam, Behrooz Mahasseni, and Sinisa Todorovic. Fine-grained recognition as hsnet search for informative image parts. In CVPR, 2017. 2 [28] Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by betweenclass attribute transfer. In CVPR, 2009. 5, 28 [29] Yann LeCun. The MNIST database of handwritten digits. Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015. 5 [32] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1 [33] Jiangjing Lv, Xiaohu Shao, Junliang Xing, Cheng Cheng, and Xi Zhou. A deep regression architecture with two-stage re-initialization for high performance facial landmark detection. In CVPR, 2017. 2 [34] Andrew Maas, Awni Hannun, and Andrew Ng. Rectifier nonlinearities improve neural network acoustic models. In ICML, 2013. 5, 47 [35] Peter Roth Martin Koestinger, Paul Wohlhart and Horst Bischof. Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization. David Novotny, Diane Larlus, and Andrea Vedaldi. An-chorNet: A weakly supervised network to learn geometrysensitive features for semantic matching. 2017. 2 [42] Marco Pedersoli, Tinne Tuytelaars, and Luc Van Gool. Using a deformation field model for localizing faces and facial points under weak supervision. In CVPR, 2014. 2 [43] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016. 1, 2 [44] Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of variation with manifold interaction. In ICML, 2014. 2 [45] Scott Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In NIPS, December 2015. 2 [46] Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. Learning what and where to draw. In NIPS, 2016. 2, 5, 8, 9, 48 Face alignment at 3000 FPS via regressing local binary features. In CVPR, 2014. 2 [49] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015. 1 [50] Ignacio Rocco, Relja Arandjelović, and Josef Sivic. Convolutional neural network architecture for geometric matching. In CVPR, 2017. 2 [51] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face recognition and clustering. In CVPR, 2015. 8, 9 [52] Ronan Sicre, Yannis Avrithis, Ewa Kijak, and Frédéric Jurie. Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning to generate longterm future via hierarchical prediction. In ICML, 2017. 8 [65] Xiaolong Wang and Abhinav Gupta. Generative image modeling using style and structure adversarial networks. Single image 3D interpreter network. In ECCV, 2016. 1, 2 [68] Yue Wu, Chao Gou, and Qiang Ji. Simultaneous facial landmark detection, pose and deformation estimation under facial occlusion. In CVPR, 2017. 2 [69] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond PASCAL: A benchmark for 3D object detection in the wild. Appendices Unsupervised Discovery of Object Landmarks as Structural Representations Supplementary videos referred to in the appendices can be obtained at http://ytzhang.net/files/lmdis-rep/supp-videos.tar.gz Contents A. More details and results on face manipulation using unsupervised landmarks . . . . . . . . . 13 B. Our model without landmark descriptors on MNIST . . . . . . . . . . . . . . . . . . . . . . 20 B.1. Models for individual digits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2. Models for all digits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3. Digit morphing using discovered landmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C. Details and more results on Human3.6M . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.1. Optical flow as self-supervision for equivariance . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2. Quantitative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.3. Qualitative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 D. Results on animals of mixed species . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 E . More qualitative results on human faces, cat heads, cars, and shoes . . . . . . . . . . . . . . 30 E.1 . CelebA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 E.2 . AFLW . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 E.3 . Cat heads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 E.4 . Cars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 E.5 . Shoes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 F . Ablative study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 F.1 . Number of labeled samples for annotated-landmark prediction . . . . . . . . . . . . . . . . . 43 F.2 . Evolution of detection confidence map during training . . . . . . . . . . . . . . . . . . . . . . 43 F.3 . TPS control points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 G. Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 G.1. Data preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 G.2. Network architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47</figDesc><table><row><cell>2,1</cell></row><row><cell>Ob-ject class recognition by unsupervised scale-invariant learn-ing. In CVPR, 2003. 2 [17] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Region-based convolutional networks for accurate object detection and segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(1):142-158, Jan 2016. 1 [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 1, 2, 9 [19] Alex Graves, Marcus Liwicki, Santiago Fernández, Roman Bertolami, Horst Bunke, and Jürgen Schmidhuber. A novel connectionist system for unconstrained handwriting recog-nition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5):855-868, 2009. 2 [20] . 1, 2, 3, 24, 25 [40] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In ICCV, 2015. 1 [41] Unsupervised part learning for visual recognition. In CVPR, 2017. 2 [53] Karen Simonyan and Andrew Zisserman. Very deep convo-lutional networks for large-scale image recognition. In ICLR, 2015. 1 [54] Saurabh Singh, Abhinav Gupta, and Alexei A Efros. Unsu-2 [63] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In ICML, 2016. 2 [70] Shengtao Xiao, Jiashi Feng, Junliang Xing, Hanjiang Lai, Shuicheng Yan, and Ashraf Kassim. Robust facial land-mark detection via recurrent attentive-refinement networks. In ECCV, 2016. 2, 7 [71] Shengtao Xiao, Jiashi Feng, Luoqi Liu, Xuecheng Nie, Wei 2d dual learning for large-pose facial landmark detection. In ICCV, 2017. 2 [72] Wei Yang, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. End-to-end learning of deformable mixture of parts 2, 7 [78] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE Signal Processing Letters, 2016. 2 [79] Weiwei Zhang, Jian Sun, and Xiaoou Tang. Cat head detec-tion -how to effectively exploit shape and texture features. ECCV, 2008. 5 [80] Yuting Zhang, Kihyuk Sohn, Ruben Villegas, Gang Pan, and Honglak Lee. Improving object detection with deep convo-lutional networks via Bayesian optimization and structured prediction. In CVPR, 2015. 1 [81] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep multi-task learning. In ECCV, 2014. 2, 5, 7 [82] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Learning deep representation for face alignment with auxiliary attributes. IEEE Transactions on Pattern Analysis 1 University of Michigan, Ann Arbor 2 Google Brain {yutingzh, guoyijie, jinyixin, lyjtour, zhiyuan, honglak}@umich.edu honglak@google.com [64] In WACV, 2014. 5 Wang, Shuicheng Yan, and Ashraf Kassim. Recurrent 3d-The project web page (code and results): http://ytzhang.net/projects/lmdis-rep</cell></row><row><cell>and Machine Intelligence, 38(5):918-930, 2016. 2, 7</cell></row><row><cell>[83] Xiangxin Zhu and Deva Ramanan. Face detection, pose es-</cell></row><row><cell>timation, and landmark localization in the wild. In CVPR,</cell></row><row><cell>2012. 2</cell></row></table><note>http://yann.lecun.com/exdb/mnist/, 1998. 4 [30] Karel Lenc and Andrea Vedaldi. Learning covariant fea- ture detectors. In ECCV Workshop on Geometry Meets Deep Learning, 2016. 2 [31] Ziwei Liu,In First IEEE International Workshop on Benchmarking Fa- cial Image Analysis Technologies, 2011. 5 [36] Jonathan Masci, Ueli Meier, Dan Cireşan, and Jürgen Schmidhuber. Stacked convolutional auto-encoders for hi- erarchical feature extraction. In International Conference on Artificial Neural Networks, 2011. 2 [37] Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann LeCun. Disentan- gling factors of variation in deep representation using adver- sarial training. In NIPS, 2016. 2 [38] Iain Matthews and Simon Baker. Active appearance models revisited. International Journal of Computer Vision, 60(2): 135-164, 2004. 2 [39] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour- glass networks for human pose estimation. In ECCV, 2016[47] Scott Reed, Aäron van den Oord, Nal Kalchbrenner, Ser- gio Gómez Colmenarejo, Ziyu Wang, Dan Belov, and Nando de Freitas. Parallel multiscale autoregressive density estima- tion. In ICML, 2017. 2, 5, 8 [48] Shaoqing Ren, Xudong Cao, Yichen Wei, and Jian Sun.pervised discovery of mid-level discriminative patches. In ECCV, 2012. 2 [55] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep convolu- tional network cascade for facial point detection. In CVPR, 2013. 2, 7 [56] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. In CVPR, 2015. 1 [57] Kevin Tang, Armand Joulin, Li-Jia Li, and Li Fei-Fei. Co- localization in real-world images. In CVPR, 2014. 2 [58] James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsuper- vised learning of object frames by dense equivariant image labelling. In NIPS, 2017. 2, 7 [59] James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsu- pervised learning of object landmarks by factorized spatial embeddings. In ICCV, 2017. 1, 2, 4, 5, 6, 7, 8, 24, 25, 26, 27, 30, 32, 38, 40 [60] Alexander Toshev and Christian Szegedy. DeepPose: Hu- man pose estimation via deep neural networks. In CVPR, 2014. 2 [61] Michel Valstar, Brais Martinez, Xavier Binefa, and Maja Pantic. Facial point detection using boosted regression and graph models. In CVPR, 2010. 2 [62] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image gen- eration with pixelcnn decoders. In NIPS, 2016.In ECCV, 2016. 2 [66] Markus Weber, Max Welling, and Pietro Perona. Towards automatic discovery of object categories. In CVPR, 2000. 2 [67] Jiajun Wu, Tianfan Xue, Joseph J Lim, Yuandong Tian, Joshua B Tenenbaum, Antonio Torralba, and William T Freeman.and deep convolutional neural networks for human pose es- timation. In CVPR, pages 3073-3082, 2016. 2 [73] Yi Yang. Articulated Human Pose Estimation with Flexi- ble Mixtures of Parts. PhD thesis, University of California, Irvine, 2013. 2 [74] Aron Yu and Kristen Grauman. Fine-grained visual compar- isons with local learning. In CVPR, 2014. 5 [75] Xiang Yu, Feng Zhou, and Manmohan Chandraker. Deep deformation network for object landmark localization. In ECCV, 2016. 2 [76] Matthew D. Zeiler and Rob Fergus. Visualizing and under- standing convolutional networks. In ECCV, 2014. 1 [77] Jie Zhang, Shiguang Shan, Meina Kan, and Xilin Chen. Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment. In ECCV, 2014.Yuting Zhang 1 , Yijie Guo 1 , Yixin Jin 1 , Yijun Luo 1 , Zhiyuan He 1 , Honglak LeeG.3. Training strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 G.4. Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 G.5. Details about face generations using unsupervised landmarks . . . . . . . . . . . . . . . . . . 48</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Video: videos/face.30landmark-model.manipulate-all-landmarks/01.mp4</figDesc><table><row><cell>Or ig in al im ag es</cell><cell>M a n ip u la te d im a g e s</cell><cell></cell></row><row><cell></cell><cell>(a) La nd m ar ks</cell><cell></cell></row><row><cell></cell><cell>M</cell><cell></cell></row><row><cell></cell><cell>a</cell><cell></cell></row><row><cell>Or ig ag im in al es</cell><cell>n ip u la te d a im g e s</cell><cell></cell></row><row><cell cols="3">videos/face.30landmark-model.manipulate-{all|mouth|mouthext}-landmarks</cell></row><row><cell cols="3">• 10,20-landmark models: (b) Video: videos/face.30landmark-model.manipulate-all-landmarks/02.mp4</cell></row><row><cell cols="3">videos/face.{10|20}landmark-model.manipulate-{all|mouth}-landmarks La nd m ar ks</cell></row><row><cell></cell><cell>M</cell><cell></cell></row><row><cell></cell><cell>a</cell><cell></cell></row><row><cell>Or ig ag im in al es</cell><cell>n ip u la te d a im g e s</cell><cell></cell></row><row><cell></cell><cell>La nd m ar ks</cell><cell></cell></row><row><cell></cell><cell>M</cell><cell></cell></row><row><cell></cell><cell>a</cell><cell></cell></row><row><cell>Or ig ag im in al es</cell><cell>n ip u la te d a im g e s</cell><cell></cell></row><row><cell></cell><cell>La nd m ar ks</cell><cell></cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell><cell>See next page for the figure.</cell></row></table><note>(c) Video: videos/face.30landmark-model.manipulate-all-landmarks/03.mp4(d) Video: videos/face.30landmark-model.manipulate-all-landmarks/04.mp4(e) Video: videos/face.30landmark-model.manipulate-all-landmarks/06.mp4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Video: videos/face.30landmark-model.manipulate-all-landmarks/06.mp4 Video: videos/face.30landmark-model.manipulate-all-landmarks/09.mp4 Video: videos/face.30landmark-model.manipulate-all-landmarks/10.mp4</figDesc><table><row><cell></cell><cell>La nd m ar ks</cell></row><row><cell>Or ig in al im ag es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(a) La nd m ar ks</cell></row><row><cell></cell><cell>M</cell></row><row><cell></cell><cell>a</cell></row><row><cell>Or ig ag im in al es</cell><cell>n ip u la te d a im g e s</cell></row><row><cell></cell><cell>(b) Video: videos/face.30landmark-model.manipulate-all-landmarks/07.mp4</cell></row><row><cell></cell><cell>La nd m ar ks</cell></row><row><cell></cell><cell>M</cell></row><row><cell></cell><cell>a</cell></row><row><cell>Or ig ag im in al es</cell><cell>n ip u la te d a im g e s</cell></row><row><cell></cell><cell>La nd m ar ks</cell></row><row><cell></cell><cell>M</cell></row><row><cell></cell><cell>a</cell></row><row><cell>Or ig ag im in al es</cell><cell>n ip u la te d a im g e s</cell></row><row><cell></cell><cell>(d) La nd m ar ks</cell></row><row><cell></cell><cell>M</cell></row><row><cell></cell><cell>a</cell></row><row><cell>Or ig ag im in al es</cell><cell>n ip u la te d a im g e s</cell></row><row><cell></cell><cell>(e)</cell></row></table><note>(c) Video: videos/face.30landmark-model.manipulate-all-landmarks/08.mp4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Video: videos/face.30landmark-model.manipulate-mouth-landmarks/01.mp4 Video: videos/face.30landmark-model.manipulate-mouth-landmarks/04.mp4</figDesc><table><row><cell>Or ig ag im in al es</cell><cell>a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(a) La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(b) Video: videos/face.30landmark-model.manipulate-mouth-landmarks/02.mp4</cell></row><row><cell></cell><cell>La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(d) La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(e) Video: videos/face.30landmark-model.manipulate-mouth-landmarks/05.mp4</cell></row></table><note>(c) Video: videos/face.30landmark-model.manipulate-mouth-landmarks/03.mp4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Video: videos/face.30landmark-model.manipulate-mouth-landmarks/06.mp4 Video: videos/face.30landmark-model.manipulate-mouth-landmarks/09.mp4 Video: videos/face.30landmark-model.manipulate-mouth-landmarks/10.mp4</figDesc><table><row><cell></cell><cell>La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(a) La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(b) Video: videos/face.30landmark-model.manipulate-mouth-landmarks/07.mp4</cell></row><row><cell></cell><cell>La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(d) La nd m ar ks</cell></row><row><cell>Or ig ag im in al es</cell><cell>M a n ip u la te d im a g e s</cell></row><row><cell></cell><cell>(e)</cell></row></table><note>(c) Video: videos/face.30landmark-model.manipulate-mouth-landmarks/08.mp4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/01.mp4 Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/04.mp4 Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/05.mp4Figure 16: Face manipulation by modifying 6 discovered mouth and jaw landmarks on the MAFL testing set. 1st column: input images; 2nd column: discovered landmarks and reconstructed images; other columns: the red dots denote the target landmark locations (gray dots means not too much offset regarding the original landmarks), the gray lines denote the synthetic adjustment of landmarks, and the facial images are the decoder outputs. Best viewed in zoom mode. Videos are available at videos/face.30landmark-model.manipulate-mouthext-landmarks for the morphing process. Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/06.mp4 Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/09.mp4 Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/10.mp4</figDesc><table><row><cell>Or ig in al ag im es Or ig in al im ag es Or ig in al im ag es Or ig in al im ag es Or ig in al im ag es Or ig in al im ag es Or ig in al im ag es Or ig in al im ag es Or ig in al im ag es Or ig in al im ag es</cell><cell>La nd m ar ks M a n ip u la te d im a g e s (a) La nd m ar ks M a n ip u la te d im a g e s (b) Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/02.mp4 La nd m ar ks M a n ip u la te d im a g e s La nd m ar ks M a n ip u la te d im a g e s (d) La nd m ar ks M a n ip u la te d im a g e s M a n ip u la te d im a g e s (a) La nd m ar ks M a n ip u la te d im a g e s (b) Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/07.mp4 La nd m ar ks M a n ip u la te d im a g e s La nd m ar ks M a n ip u la te d im a g e s (d) La nd m ar ks M a n ip u la te d im a g e s (e) La nd m ar ks (e)</cell></row></table><note>(c) Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/03.mp4(c) Video: videos/face.30landmark-model.manipulate-mouthext-landmarks/08.mp4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 6</head><label>6</label><figDesc>also summarizes the image size after padding with the edge values.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Image size Padded size</cell><cell>Remark</cell></row><row><cell>CelebA, AFLW, Cat Heads, Shoes</cell><cell>80×80</cell><cell>96×96</cell><cell>-</cell></row><row><cell>Profile Cars from PASCAL 3D</cell><cell>64×64</cell><cell>96×96</cell><cell>visualized as W : H = 2 : 1</cell></row><row><cell>Animals from AwA</cell><cell>64×64</cell><cell>80×80</cell><cell>-</cell></row><row><cell>Human3.6M</cell><cell>128×128</cell><cell>192×192</cell><cell>-</cell></row><row><cell>MNIST</cell><cell>28×28</cell><cell>56×56</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 6 :</head><label>6</label><figDesc>Data processing parameters for different dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 7</head><label>7</label><figDesc>summarizes the dataset-specific hyper-parameters. Note that our model is not sensitive to minor changes of the hyper-parameters, but adjusting the hyper-parameters for each dataset can improve the performance slightly. conc σ sep λ sep λ eqv C</figDesc><table><row><cell>Dataset</cell><cell># land-mark</cell><cell>1 st LR decay iter.</cell><cell>2 nd LR decay iter.</cell><cell>Initial λ recon</cell><cell>1 st λ recon increase iter.</cell><cell>2 nd λ recon increase iter.</cell><cell></cell></row><row><cell>CelebA</cell><cell>10</cell><cell>100K</cell><cell>200K</cell><cell>0.01</cell><cell>100K</cell><cell>200K</cell><cell>100 0.06 16 10 4 8</cell></row><row><cell>CelebA</cell><cell>30</cell><cell>100K</cell><cell>200K</cell><cell>0.1</cell><cell>100K</cell><cell>200K</cell><cell>100 0.04 10 10 4 8</cell></row><row><cell>AFLW</cell><cell>10</cell><cell>100K</cell><cell>200K</cell><cell>0.1</cell><cell>100K</cell><cell>200K</cell><cell>100 0.06 16 10 4 8</cell></row><row><cell>AFLW</cell><cell>30</cell><cell>100K</cell><cell>200K</cell><cell>0.0001</cell><cell>100K</cell><cell>200K</cell><cell>100 0.04 10 10 4 8</cell></row><row><cell>Cat</cell><cell>10</cell><cell>100K</cell><cell>200K</cell><cell>0.0001</cell><cell>100K</cell><cell>200K</cell><cell>100 0.08 20 10 4 8</cell></row><row><cell>Cat</cell><cell>20</cell><cell>100K</cell><cell>200K</cell><cell>0.0001</cell><cell>100K</cell><cell>200K</cell><cell>100 0.05 10 10 4 8</cell></row><row><cell>Car</cell><cell>10</cell><cell>40K</cell><cell>80K</cell><cell>0.001</cell><cell>40K</cell><cell>50K</cell><cell>100 0.08 200 10 4 8</cell></row><row><cell>Car</cell><cell>24</cell><cell>40K</cell><cell>80K</cell><cell>0.001</cell><cell>40K</cell><cell>50K</cell><cell>100 0.05 200 10 4 8</cell></row><row><cell>Animal</cell><cell>10</cell><cell>20K</cell><cell>50K</cell><cell>0.001</cell><cell>40K</cell><cell>50K</cell><cell>100 0.08 20 10 4 2</cell></row><row><cell>Shoes</cell><cell>8</cell><cell>100K</cell><cell>20K</cell><cell>0.01</cell><cell>100K</cell><cell>200K</cell><cell>100 0.05 20 10 4 8</cell></row><row><cell>Human</cell><cell>16</cell><cell>100K</cell><cell>200K</cell><cell>0.1</cell><cell>100K</cell><cell>200K</cell><cell>100 0.06 20 10 4 8</cell></row></table><note>λ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 7 :</head><label>7</label><figDesc>The hyper-parameters for our models on different datasets. When computing loss Lconc, Lsep, Leqv, the coordinates is first normalized with respect to the image edge length (i.e., the square root of the image area). All hyper-parameters (including, λconc, σseq, λseq, λeqv) are set according the normalized landmark coordinates.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This paper is published at CVPR 2018.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The MAFL dataset [81] is a subset of CelebA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">9 annotated landmarks in total. We do not use the 2 at the ears.<ref type="bibr" target="#b3">4</ref> See Appendix C for details</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Some markers are close to each other (e.g., on each foot, there are two markers), so the effective locations annotated by the markers are less than 32 (around 16).<ref type="bibr" target="#b5">6</ref> For examples, we swap the coordinates of the left-shoulder landmark and the right-shoulder landmark.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 28: Discovering 30 landmarks on unaligned CelebA images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 30: Discovering 30 landmarks on AFLW</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported in part by ONR N00014-13-1-0762, NSF CAREER IIS-1453651, and Sloan Research Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Input  n/a  3  96  Convolution  3  32  96  Convolution  3  64  96  Pooling  2  64  48  Convolution  3  128  48  Pooling  2  128  24  Convolution  3  256  24  Pooling  2  256  12  Convolution  3  512  12  Convolution  3  512  12  Upsampling  2  512  24  Convolution  3  256  24  Upsampling  2  128  48  Convolution  3</ref>  <ref type="table">Convolution  3  32  192  Pooling  2  32  96  Convolution  3  64  96  Pooling  2  64  48  Convolution  3  128  48  Pooling  2  128  24  Convolution  3  256  24  Pooling  2  256  12  Convolution  3  512  12  Pooling  2  512  6  Convolution  3  512  6  Convolution  3  512  6  Upsampling  2  512  12  Convolution  3  512  12  Upsampling  2  512  24  Convolution  3  256  24  Upsampling  2  256  48  Convolution  3  128  48  Upsampling  2  128  96  Convolution  3</ref>  <ref type="table">Convolution  3  64  64  Convolution  3  64  64  Pooling  2  64  32  Convolution  3  128  32  Pooling  2  128  16  Convolution  3  256  16  Pooling  2  256  8  Convolution  3  512  8  Pooling  2  512  4  Convolution  3  512  4  Convolution  3  512  4  Upsampling  2  512  8  Convolution  3  512  8  Upsampling  2  512  16  Convolution  3  256  16  Upsampling  2  128  32  Convolution  3  128  32  Upsampling  2  64  64  Convolution  3  64  64  Convolution  3  3</ref>  <ref type="table">80  Convolution  3  64  80  Convolution  3  64  80  Pooling  2  64  40  Convolution  3  128  40  Pooling  2  128  20  Convolution  3  256  20  Pooling</ref> </div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Be-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<title level="m">Boundary equilibrium generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Principal warps: Thin-plate splines and the decomposition of deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A probabilistic approach to object recognition using local photometry and global geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Burl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Face alignment by explicit shape regression. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu Catalin Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic feature localisation with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3054" to="3067" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
