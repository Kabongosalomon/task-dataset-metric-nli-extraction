<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ODE 2 VAE: Deep generative second order ODEs with Bayesian neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Agatay Yıldız</surname></persName>
							<email>cagatay.yildiz@aalto.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<postCode>FI-00076</postCode>
									<settlement>Finland</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Heinonen</surname></persName>
							<email>markus.o.heinonen@aalto.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<postCode>FI-00076</postCode>
									<settlement>Finland</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Lähdesmäki</surname></persName>
							<email>harri.lahdesmaki@aalto.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Aalto University</orgName>
								<address>
									<postCode>FI-00076</postCode>
									<settlement>Finland</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ODE 2 VAE: Deep generative second order ODEs with Bayesian neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Ordinary Differential Equation Variational Auto-Encoder (ODE 2 VAE), a latent second order ODE model for high-dimensional sequential data. Leveraging the advances in deep generative models, ODE 2 VAE can simultaneously learn the embedding of high dimensional trajectories and infer arbitrarily complex continuous-time latent dynamics. Our model explicitly decomposes the latent space into momentum and position components and solves a second order ODE system, which is in contrast to recurrent neural network (RNN) based time series models and recently proposed black-box ODE techniques. In order to account for uncertainty, we propose probabilistic latent ODE dynamics parameterized by deep Bayesian neural networks. We demonstrate our approach on motion capture, image rotation and bouncing balls datasets. We achieve state-of-the-art performance in long term motion prediction and imputation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning has always been one of the most prominent problems in machine learning. Leveraging the advances in deep learning, variational auto-encoders (VAEs) have recently been applied to several challenging datasets to extract meaningful representations. Various extensions to vanilla VAE have achieved state-of-the-art performance in hierarchical organization of latent spaces, disentanglement and semi-supervised learning <ref type="bibr" target="#b28">(Tschannen et al., 2018)</ref>. VAE based techniques usually assume a static data, in which each data item is associated with a single latent code. Hence, auto-encoder models for sequential data have been overlooked. More recently, there have been attempts to use recurrent neural network (RNN) encoders and decoders for tasks such as representation learning, classification and forecasting <ref type="bibr" target="#b26">(Srivastava et al., 2015;</ref><ref type="bibr" target="#b21">Lotter et al., 2016;</ref><ref type="bibr" target="#b13">Hsu et al., 2017;</ref><ref type="bibr" target="#b19">Li and Mandt, 2018)</ref>. Other than neural ordinary differential equations (ODEs) <ref type="bibr" target="#b4">(Chen et al., 2018b)</ref> and Gaussian process prior VAEs (GPPVAE) , aforementioned methods operate in discrete-time, which is in contrast to most of the real-world datasets, and fail to produce plausible long-term forecasts <ref type="bibr" target="#b15">(Karl et al., 2016)</ref>.</p><p>In this paper, we propose ODE 2 VAEs that extend VAEs for sequential data with a latent space governed by a continuous-time probabilistic ODE. We propose a powerful second order ODE that allows modelling the latent dynamic ODE state decomposed as position and momentum. To handle uncertainty in dynamics and avoid overfitting, we parameterise our latent continuous-time dynamics with deep Bayesian neural networks and optimize the model using variational inference. We show state-of-the-art performance in learning, reproducing and forecasting high-dimensional sequential systems, such as image sequences. An implementation of our experiments and generated video sequences are provided at https://github.com/cagatayyildiz/ODE2VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic second-order ODEs</head><p>We tackle the problem of learning low-rank latent representations of possibly high-dimensional sequential data trajectories. We assume data sequences x 0:N := (x 0 , x 1 , . . . , x N ) with individual frames x k ∈ R D observed at time points t 0 , . . . , t N . We will present the methodology for a single data sequence x 0:N for notational simplicity, but it is straighforward to extend our method to multiple sequences. The observations are often at discrete spacings, such as individual images in a video sequence, but our model also generalizes to irregular sampling.</p><p>We assume that there exists an underlying generative low-dimensional continuous-time dynamical system, which we aim to uncover. Our goal is to learn latent representations z t ∈ R d of the sequence dynamics with d D, and reconstruct observations x t ∈ R D for missing frame imputation and forecasting the system past observed time t N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ordinary differential equations</head><p>In discrete-time sequential systems the state sequence z 0 , z 1 , . . . is indexed by a discrete variable k ∈ Z, and the state progression is governed by a transition function on the change ∆z k = z k − z k−1 . Examples of such models are auto-regressive models, Markov chains, recurrent models and neural network layers.</p><p>In contrast, continuous-time sequential systems model the state function z t : T → R d of a continuous, real-valued time variable t ∈ T = R. The state evolution is governed by a first-order time derivativė</p><formula xml:id="formula_0">z t := dz t dt = h(z t ),<label>(1)</label></formula><p>that drives the system state forward in infinitesimal steps over time. The differential h : R d → R d induces a differential field that covers the input space. Given an initial location vector z 0 ∈ R d , the system then follows an ordinary differential equation (ODE) model with state solutions</p><formula xml:id="formula_1">z T = z 0 + T 0 h(z t )dt.<label>(2)</label></formula><p>The state solutions are in practise computed by solving this initial value problem with efficient numericals solvers, such as Runge-Kutta <ref type="bibr" target="#b24">(Schober et al., 2019)</ref>. Recently several works have proposed learning ODE systems h parametrised as neural networks <ref type="bibr" target="#b4">(Chen et al., 2018b)</ref> or as Gaussian processes .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bayesian second-order ODEs</head><p>First-order ODEs are incapable of modelling high-order dynamics 1 , such as acceleration or the motion of a pendulum. Furthermore, ODEs are deterministic systems unable to account for uncertainties in the dynamics. We tackle both issues by introducing Bayesian neural second-order ODEs</p><formula xml:id="formula_2">z t := d 2 z t d 2 t = f W (z t ,ż t ),<label>(3)</label></formula><p>which can be reduced to an equivalent system of two coupled first-order ODEs</p><formula xml:id="formula_3">ṡ t = v ṫ v t = f W (s t , v t ) , s T v T = s 0 v 0 + T 0 v t f W (s t , v t ) f W (zt) dt,<label>(4)</label></formula><p>where (with a slight abuse of notation) the state tuple z t = (s t , v t ) decomposes into the state position s t , which follows the state velocity (momentum) v t . The velocity or evolution of change is governed by a neural network f W (s t , v t ) with a collection of weight parameters W = {W } L =1 over its L layers and the bias terms. We assume a prior p(W) on the weights resulting in a Bayesian neural network (BNN). Each weight sample, in turn, results in a deterministic ODE trajectory (see <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>The BNN acceleration field f W : R d × R d → R d depends on both state and velocity. For instance, in a pendulum system the accelerationz depends on both its current location and velocity. The system is now driven forward from starting position s 0 and velocity v 0 , with the BNN determining only how the velocity v t evolves. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Second order ODE flow</head><p>The ODE systems are denoted as continuous normalizing flows when they are applied on random variables z t <ref type="bibr" target="#b23">(Rezende et al., 2014;</ref><ref type="bibr" target="#b3">Chen et al., 2018a;</ref><ref type="bibr" target="#b9">Grathwohl et al., 2018)</ref>. This allows following the progression of its density through the ODE. Using the instantaneous change of variable theorem <ref type="bibr" target="#b3">(Chen et al., 2018a)</ref>, we obtain the instantaneous change of variable for our second order ODEs as</p><formula xml:id="formula_4">∂ log q(z t |W) ∂t = −Tr df W (z t ) dz t dt = −Tr ∂vt ∂st ∂vt ∂vt ∂f W (st,vt) ∂st ∂f W (st,vt) ∂vt = −Tr ∂f W (s t , v t ) ∂v t ,<label>(5)</label></formula><p>which results in the log densities over time,</p><formula xml:id="formula_5">log q(z T |W) = log q(z 0 |W) − T 0 Tr ∂f W (s t , v t ) ∂v t dt.<label>(6)</label></formula><p>3 ODE 2 VAE model</p><p>In this section we propose a novel dynamic VAE formalism for sequential data by introducing a second order Bayesian neural ODE model in the latent space to model the data dynamics. We start by reviewing the standard VAE models and then extend it to our ODE 2 VAE model.</p><p>With auto-encoders, we aim to learn latent representations z ∈ R d for complex observations x ∈ R D parameterised by θ, where often d D. The posterior p θ (z|x) ∝ p θ (x|z)p(z) is proportional to the prior p(z) of the latent variable and the decoding likelihood p θ (x|z). Parameters θ could be optimized by maximizing the marginal log likelihood but that generally involves intractable integrals. In variational auto-encoders (VAE) an amortized variational approximation q φ (z|x) ≈ p θ (z|x) with parameters φ is used instead <ref type="bibr" target="#b14">(Jordan et al., 1999;</ref><ref type="bibr" target="#b17">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b23">Rezende et al., 2014)</ref>. Variational inference that minimizes the Kullback-Leibler divergence, or equivalently maximizes the evidence lower bound (ELBO), results in efficient inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dynamic model</head><formula xml:id="formula_6">s 0 ∼ p(s 0 ) (7) v 0 ∼ p(v 0 )<label>(8)</label></formula><formula xml:id="formula_7">s t = s 0 + t 0 v τ dτ (9) v t = v 0 + t 0 f true (s τ , v τ )dτ (10) x i ∼ p(x i |s i ) i ∈ [0, N ]<label>(11)</label></formula><p>Building upon the ideas from black-box ODEs and variational auto-encoders, we propose to infer continuous-time latent position and velocity trajectories that live in a much lower dimensional space but still match the data well (see <ref type="figure" target="#fig_1">Fig. 2</ref> for illustration). For this, consider a generative model that consists of three components: (i) a distribution for the initial position p(s 0 ) and velocity p(v 0 ) in the latent space , (ii) true (unknown) dynamics defined by an acceleration field, and (iii) a decoding likelihood p(x i |s i ).</p><p>The generative model is given in Eqs. 7-11. Note that the decoding likelihood is defined only from the position variable. Velocity thus serves as an auxiliary variable, driving the position forward. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational inference</head><p>As with standard auto-encoders, optimization of ODE 2 VAE model parameters with respect to marginal likelihood would result in intractability and thus we resort to variational inference (see <ref type="figure" target="#fig_1">Fig. 2</ref>). We first combine the latent position and velocity components into a single vector z t := (s t , v t ) for notational clarity, and assume the following factorized variational approximation for the unobserved quantities q(W, z 0:N |x 0:N ) = q(W)q enc (z 0 |x 0:N )q ode (z 1:N |x 0:N , z 0 , W). As decribed in subsection 2.2, true dynamics are approximated by a BNN parameterized by W with the following variational approximation: q(W) = N (W|m, sI). We use an amortized variational approximation for the latent initial position and velocity</p><formula xml:id="formula_8">q enc (z 0 |x 0:N ) = q enc s 0 v 0 x 0:N = N µ s (x 0 ) µ v (x 0:m ) , diag(σ s (x 0 )) 0 0 diag(σ v (x 0:m )) ,<label>(12)</label></formula><p>where µ s , µ v , σ s , σ v are encoding neural networks. The encoder for the initial position depends solely on the first item in the data sequence x 0 , whereas the encoder for the initial velocity depends on multiple data points x 0:m , where m ≤ N is the amortized inference length. We use neural network encoders and decoders whose architectures depend on the application (see the supplementary document for details). The variational approximation for the latent dynamics q ode (z 1:N |x 0:N , z 0 , W) is defined implicitly via the instantaneous change of variable for the second order ODEs shown in Eq. 5. The initial density is given by the encoder q enc (z 0 |x 0 ), and density for later points can be solved by numerical integration using Eq. 6. Note that we treat the entire latent trajectory evaluated at observed time points, Z ≡ z 0:N , as a latent variable, and the latent trajectory samples z 1:N are solved conditioned on the ODE initial values z 0 and BNN parameter values W. Finally, evidence lower bound (ELBO) becomes as follows (for brevity we define X ≡ x 0:N ):</p><formula xml:id="formula_9">log p(X) ≥ − KL[q(W , Z|X)||p(W, Z)] + E q(W,Z|X) [log p(X|W, Z)] ELBO (13) = −E q(W,Z|X) log q(W)q(Z|W, X) p(W)p(Z) + E q(W,Z|X) [log p(X|W, Z)] (14) = − KL[q(W )||p(W)] + E q(W,Z|X) − log q(Z|W, X) p(Z) + log p(X|W, Z) (15) = − KL[q(W )||p(W)] ODE regularization + E qenc(z0|X) − log q enc (z 0 |X) p(z 0 ) + log p(x 0 |z 0 ) VAE loss + N i=1 E qode(W,zi|X,z0) − log q ode (z i |W, X) p(z i ) + log p(x i |z i ) dynamic loss (16)</formula><p>where the prior distribution p(W, z 0 ) is a standard Gaussian. The prior density follows Eq. 6 with f W replaced by the unknown f true , which causes p(z t ), t &gt; 1 to be intractable. 2 Thus, we resort to a simplifying assumption and place a standard regularizing Gaussian prior over z 1:N .</p><p>We now examine each term in Eq. 16. The first term is the BNN weight penalty, which helps avoiding overfitting. The second term is the standard VAE bound, meaning that VAE is retrieved for sequences of length 1. The only (but major) difference between the second and the third terms is that the expectation is computed with respect to the variational distribution induced by the second order ODE. Finally, we optimize the Monte Carlo estimate of Eq. 16 with respect to variational posterior {m, s}, encoder and decoder parameters, and also make use of reparameterization trick to tackle uncertanties in both the initial latent states and in the acceleration dynamics (Kingma and Welling, 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Penalized variational loss function</head><p>A well-known pitfall of VAE models is that optimizing the ELBO objective does not necessarily result in accurate inference <ref type="bibr" target="#b1">(Alemi et al., 2017)</ref>. Several recipes have already been proposed to counteract the imbalance between the KL term and reconstruction likelihood <ref type="bibr" target="#b31">(Zhao et al., 2017;</ref><ref type="bibr" target="#b11">Higgins et al., 2017)</ref>. In this work, we borrow the ideas from <ref type="bibr" target="#b11">Higgins et al. (2017)</ref> and weight the KL[q(W )||p(W)] term resulting from the BNN with a constant factor β. We choose to fix β to the ratio between the latent space dimensionality and number of weight parameters, β = |q|/|W|, in order to counter-balance the penalties on latent variables W and z i .</p><p>Our variational model utilizes encoders only for obtaining the initial latent distribution. In cases of long input sequences, dynamic loss term can easily dominate VAE loss, which may cause the encoders to underfit. The underfitting may also occur in small data regimes or when the distribution of initial data points differs from data distribution. In order to tackle this, we propose to minimize the distance between the encoder distribution and the distribution induced by the ODE flow (Eqs. 12 and 6). At the end, we have an alternative, penalized target function, which we call ODE 2 VAE-KL:</p><formula xml:id="formula_10">L ODE 2 VAE = −β KL[q(W )||p(W)] + E q(W,Z|X) − log q(Z|W, X) p(Z) + log p(X|W, Z)<label>(17)</label></formula><p>− γE q(W) [KL[q ode (Z|X)||q enc (Z|W, X)]] . We choose the constant γ by cross-validation. In practice, we found out that an annealing scheme in which γ is gradually increased helps optimization, which is also used in <ref type="bibr" target="#b15">(Karl et al., 2016;</ref><ref type="bibr" target="#b22">Rezende and Mohamed, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Related work</head><p>Despite the recent VAE and GAN breakthroughs, little attention has been paid to deep generative architectures for sequential data. Existing VAE-based sequential models rely heavily on RNN encoders and decoders <ref type="bibr" target="#b5">(Chung et al., 2015;</ref><ref type="bibr" target="#b25">Serban et al., 2017)</ref>, with very few interest in stochastic models <ref type="bibr" target="#b7">(Fraccaro et al., 2016)</ref>. Some research has been carried out to approximate latent dynamics by LSTMs <ref type="bibr" target="#b21">(Lotter et al., 2016;</ref><ref type="bibr" target="#b13">Hsu et al., 2017;</ref><ref type="bibr" target="#b19">Li and Mandt, 2018)</ref>, which results in observations to be included in latent transition process. Consequently, the inferred latent space and dynamics do not fully reflect the observed phenomena and usually fail to produce decent long term predictions <ref type="bibr" target="#b15">(Karl et al., 2016)</ref>. In addition, RNNs are shown to be incapable of accurately modeling nonuniformly sampled sequences <ref type="bibr" target="#b4">(Chen et al., 2018b)</ref>, despite the recent efforts that incorporate time information in RNN architectures <ref type="bibr" target="#b18">(Li et al., 2017;</ref><ref type="bibr" target="#b30">Xiao et al., 2018)</ref>.</p><p>Recently, neural ODEs introduced learning ODE systems with neural network architectures, and proposed it for the VAE latent space as well for simple cases <ref type="bibr" target="#b4">(Chen et al., 2018b)</ref>. In Gaussian process prior VAE, a GP prior is placed in the latent space over a sequential index . To the best of our knowledge, there is no work connecting second order ODEs and Bayesian neural networks with VAE models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We illustrate the performance of our model on three different datasets: human motion capture (see the acknowledgements), rotating <ref type="bibr">MNIST (Casale et al., 2018)</ref> and bouncing balls <ref type="bibr" target="#b27">(Sutskever et al., 2009</ref>). Our goal is twofold: First, given a walking or bouncing balls sequence, we aim to predict the future sensor readings and frames. Second, we would like to interpolate an unseen rotation angle from a sequence of rotating digits. The competing techniques are specified in each section. For all methods, we have directly applied the public implementations provided by the authors. Also, we have tried several values for the hyper-parameters with the same rigor and we report the best results. To numerically compare the models, we sample 50 predictions per test sequence and report the mean and standard deviation of the mean squared error (MSE) over future frames. We include the mean MSE of mean predictions (instead of trajectory samples) in the supplementary. We implement our model in Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. Encoder, differential function and the decoder parameters are jointly optimized with Adam optimizer (Kingma and Ba, 2014) with learning rate 0.001. We use Tensorflow's own odeint fixed function, which implements fourth order Runge-Kutta method, for solving the ODE systems on a time grid that is five times denser than the observed time points. Neural network hyperparameters, chosen by cross-validation, are detailed in the supplementary material. We also include ablation studies with deterministic NNs and first order dynamics in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CMU walking data</head><p>To demonstrate that our model can capture arbitrary dynamics from noisy observations, we experiment on two datasets extracted from CMU motion capture library. First, we use the dataset in , which consists of 43 walking sequences of several subjects, each of which is fitted separately. The first two-third of each sequence is reserved for training and validation, and the rest is used for testing. Second dataset consists of 23 walking sequences of subject 35 , which is partitioned into 16 training, 3 validation and 4 test sequences. We followed the preprocessing described in <ref type="bibr" target="#b29">Wang et al. (2008)</ref>, after which we were left with 50 dimensional joint angle measurements.</p><p>We compare our ODE 2 VAE against a GP-based state space model GPDM <ref type="bibr" target="#b29">(Wang et al., 2008</ref>), a dynamic model with latent GP interpolation VGPLVM , two black-box ODE solvers npODE  and neural ODEs <ref type="bibr" target="#b4">(Chen et al., 2018b)</ref>, as well as an RNN-based deep generative model DTSBN-S . In test mode, we input the first three frames and the models predict future observations. GPDM and VGPLVM are not applied to the second dataset since GPDM optimizes its latent space for input trajectories and hence does not allow simulating dynamics from any random point, and VGPLVM implementation does not support multiple input sequences.</p><p>The results are presented in <ref type="table" target="#tab_1">Table 2</ref>. First, we reproduce the results in  by obtaining the same ranking among GPDM, VGPLVM and npODE. Next, we see that DTSBN-S is not able to predict the distant future accurately, which is a well-known problem with RNNs. As expected, all models attain smaller test errors on the second, bigger dataset. We observe that neural ODE usually perfectly fits the training data but failed to extrapolate on the first dataset. This overfitting problem is not surprising considering the fact that only ODE initial value distribution is penalized. On the contrary, our ODE 2 VAE regularizes its entire latent trajectory and also samples from the acceleration field, both of which help tackling overfitting problem. We demonstrate latent state trajectory samples and reconstructions from our model in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rotating MNIST</head><p>Next, we contrast our ODE 2 VAE against recently proposed Gaussian process prior VAE (GPPVAE) , which replaces the commonly iid Gaussian prior with a GP and thus performs latent regression. We repeat the experiment in  by constructing a dataset by rotating the images of handwritten "3" digits. We consider the same number of rotation angles <ref type="formula" target="#formula_0">(16)</ref>, training and validation sequences (360&amp;40), and leave the same rotation angle out for testing (see the first row of <ref type="figure" target="#fig_8">Figure 4b</ref> for the test angle). In addition, four rotation angles are randomly removed from each rotation sequence to introduce non-uniform sequences and missing data (an example training sequence is visualized in the first row of <ref type="figure" target="#fig_8">Figure 4a</ref>).</p><p>Test errors on the unseen rotation angle are given in <ref type="table" target="#tab_2">Table 3</ref>. During test time, GPPVAE encodes and decodes the images from the test angle, and the reconstruction error is reported. On the other hand, ODE 2 VAE only encodes the first image in a given sequence, performs latent ODE integration starting from the encoded point, and decodes at given time points -without seeing the test image even in test mode. In that sense, our model is capable of generating images with arbitrary rotation angles. Also note that both models make use of the angle/time information in training and test mode. An example input sequence with missing values and corresponding reconstructions are illustrated in <ref type="figure" target="#fig_8">Figure 4a</ref>, where we see that ODE 2 VAE nicely fills in the gaps. Also, <ref type="figure" target="#fig_8">Figure 4b</ref> demonstrates our model is capable of accurately learning and rotating different handwriting styles.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bouncing balls</head><p>As a third showcase, we test our model on bouncing balls dataset, a standard benchmark used in generative temporal modeling literature <ref type="bibr" target="#b12">Hsieh et al., 2018;</ref><ref type="bibr" target="#b20">Lotter et al., 2015)</ref>. The dataset consists of video frames of three balls bouncing within a rectangular box and also colliding with each other. The exact locations of the balls as well as physical interaction rules are to be inferred from the observed sequences. We make no prior assumption on visual aspects such as ball count, mass, shape or on the underlying physical dynamics. <ref type="figure" target="#fig_8">Figure 4: Panel (a)</ref> shows a training sequence with missing values (first row) and its reconstruction (second row). First row in panel (b) demonstrates test angles from different sequences, i.e., handwriting styles, and below are model predictions.</p><p>We have generated a training set of 10000 sequences of length 20 frames and a test set of 500 sequences using the implementation provided with <ref type="bibr" target="#b27">Sutskever et al. (2009)</ref>. Each frame is 32x32x1 and pixel values vary between 0 and 1. We compare our method against DTSBN-S (  and decompositional disentangled predictive auto-encoder (DDPAE) <ref type="bibr" target="#b12">(Hsieh et al., 2018)</ref>, both of which conduct experiments on the same dataset. In test mode, first three frames of an input sequence are given as input and per pixel MSE on the following 10 frames are computed. We believe that measuring longer forecast errors is more informative about the inference of physical phenomena than reporting one-step-ahead prediction error, which is predominantly used in current literature <ref type="bibr" target="#b20">Lotter et al., 2015)</ref>. <ref type="figure" target="#fig_2">Figures 3 and 5</ref>. The RNN-based DTSBN-S nicely extrapolates a few frames but quickly loses track of ball locations and the error escalates. DDPAE achieves a much smaller error over time; however, we empirically observed that the reconstructed images are usually imperfect (here, generated balls are bigger than the originals), and also the model sometimes fails to simulate ball collisions as in <ref type="figure" target="#fig_3">Figure 5</ref>. Our ODE 2 VAE generates long and accurate forecasts and significantly improves the current state-of-the-art by almost halving the error. We empirically found out that a CNN encoder that takes channel-stacked frames as input yields smaller prediction error than an RNN encoder. We leave the investigation of better encoder architectures as an interesting future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictive errors and example reconstructions are visualized in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We have presented an extension to VAEs for continuous-time dynamic modelling. We decompose the latent space into position and velocity components, and introduce a powerful neural second order differential equation system. As shown empirically, our variational inference framework results in Bayesian neural network that helps tackling overfitting problem. We achieve state-of-the-art performance in long-term forecasting and imputation of high-dimensional image sequences.</p><p>There are several directions in which our work can be extended. Considering divergences different than KL would lead to Wasserstein auto-encoder formulations <ref type="bibr">(Tolstikhin et al., 2017)</ref>. The latent ODE flow can be replaced by stochastic flow, which would result in an even more robust model. Proposed second order flow can also be combined with generative adversarial networks to produce real-looking videos.   , except for having BNNs, and for NeuralODE placing a variational distribution on initial value q(x 0 ), while ODE 1 VAE models the posterior over full trajectory q(x 0:T ).</p><p>ODE 1 VAE vs ODE 2 VAE: We performed a new comparison study of ODE 1 VAE against ODE 2 VAE on bouncing balls dataset. The experimental setup is kept the same, except that the number of convolutional filters is reduced so that the impact of differential function choice becomes more apparent. <ref type="table" target="#tab_0">Table 1</ref> shows the resulting MSE over 10 frame ahead predictions. Note that ODE 2 VAE models the accelerationv t = f (s t , v t ) : R 2d → R d whereas 1st-order systems learnż t = f (z t ) :</p><formula xml:id="formula_11">R d → R d .</formula><p>Results show that the 2nd-order dynamics results in far better accuracy, even if the first order dynamics has more flops (d = 50). We will include ablation studies in the paper. <ref type="table" target="#tab_0">Table 1</ref> shows comparable performance of BNNs and NNs on bouncing balls. In order to demonstrate the benefit of using a BNN, we repeat the CMU walking experiment with a NN differential function. The MSE achieved by ODE 2 VAE-NN over three test sequences is 9.96, whereas ODE 2 VAE-BNN error improves to 9.43.    3 Experiment details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NN vs BNN:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CMU mocap</head><p>We consider two different datasets. Here is a link to the first one (with 43 sequences) and here is a link to the second dataset. We set γ = 1. We tried out the architecture in <ref type="figure" target="#fig_8">Figure 4</ref> with 1/2 hidden layers, 30/50 hidden units, tanh/relu/no activation functions. We found out that 2 hidden layers, 30 units and tanh performs the best. Each experiment is executed on a standard laptop for around 3 hours. The latent dimensionality is fixed to 6 for all models, i.e., s t , v t ∈ R 3 .</p><p>We visualize the position trajectories in <ref type="figure" target="#fig_1">Figure 2</ref> for cases in which either encoder/BNN variational posteriors are sampled or the mean values are used. Note that latent field that is considered in our work corresponds to the right-most panel, whereas neural ODEs considers the second one.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Rotating MNIST</head><p>Here is the dataset. We set γ = 1. We tried out 4/8/12 as the number of layers in the first layers of encoders and 8/12/16 as the last layer of the decoder. The code is executed on NVIDIA Tesla V100 GPUs for around 4 hours. The latent dimensionality is fixed to 16 for all models, i.e., s t , v t ∈ R 8 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bouncing balls</head><p>Here is the dataset. We set γ = 0.001. We tried out 8/16/32 as the number of layers in the first layers of encoders and 16/32 as the last layer of the decoder. We also experimented with relu and tanh activations. The code is executed on NVIDIA Tesla V100 GPUs for around 3 days. The latent dimensionality is fixed to 50 for all models, i.e., s t , v t ∈ R 2 5. Also note that we obtained the same error when s t , v t ∈ R 5 0. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of dynamical systems. A continuous-time system underlying a discrete-time model (a) can be extended to a 2nd-order ODE with velocity component (b). A Bayesian ODE characterises uncertain differential dynamics (c), with the corresponding position-velocity phase diagram (d). The gray arrows in (d) indicate the BNN f W (s t , v t ) mean field wrt p(W).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A schematic illustration of ODE 2 VAE model. Position encoder (µ s , σ s ) maps the first item x 0 of a high-dimensional data sequence into a distribution of the initial position s 0 in a latent space. Velocity encoder (µ v , σ v ) maps the first m high-dimensional data items x 0:m into a distribution of the initial velocity v 0 in a latent space. Probabilistic latent dynamics are implemented by a second order ODE modelf W parameterised by a Bayesian deep neural network (W). Data points in the original data domain are reconstructed by a decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Bouncing balls errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>An example test sequence from bouncing ball experiment. Top row is the original sequence. Each model takes the first three frames as input and predicts the further frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Mean prediction errors on bouncing balls dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Example latent trajectories from CMU mocap experiment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of our method against neural ODEs on CMU mocap data set. Each panel demonstrates a sensor measurement plotted over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>CMU mocap walking data experiment neural architectures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Rotating MNIST experiment neural architectures Figure 6: Bouncing balls experiment neural architectures Jack M Wang, David J Fleet, and Aaron Hertzmann. Gaussian process dynamical models for human motion. IEEE transactions on pattern analysis and machine intelligence, 30(2):283-298, 2008.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of VAE-based models GPPVAE uses a latent GP prior but only a discrete case was demonstrated in Casale et al. (2018).</figDesc><table><row><cell>Method</cell><cell>Stochastic Higher order Continuous-time dynamics state</cell><cell>Reference</cell></row><row><cell>VAE VRNN SRNN GPPVAE DSAE Neural ODE ODE 2 VAE</cell><cell>*</cell><cell>Kingma and Welling (2013) Chung et al. (2015) Fraccaro et al. (2016) Casale et al. (2018) Li and Mandt (2018) Chen et al. (2018b) current work</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average MSE on future frames</figDesc><table><row><cell></cell><cell cols="2">Test error</cell><cell></cell></row><row><cell>Model</cell><cell>Mocap-1</cell><cell>Mocap-2</cell><cell>Reference</cell></row><row><cell>GPDM VGPLVM DTSBN-S NPODE NEURALODE ODE 2 VAE ODE 2 VAE-KL</cell><cell>126.46 ± 34 142.18 ± 1.92 80.21 ± 0.04 45.74 87.23 ± 0.02 93.07 ± 0.72 15.99 ± 4.16</cell><cell>N/A N/A 34.86 ± 0.02 22.96 22.49 ± 0.88 10.06 ± 1.4 8.09 ± 1.95</cell><cell>Wang et al. (2008) Damianou et al. (2011) Gan et al. (2015) Heinonen et al. (2018) Chen et al. (2018b) current work current work</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Average prediction errors on test angle</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MODEL GPPVAE-DIS GPPVAE-JOINT ODE 2 VAE ODE 2 VAE-KL</cell><cell>TEST ERROR 0.0188 ± 0.0003 0.0194 ± 0.00006 0.0288 ± 0.00005 0.0309 ± 0.00002</cell><cell>Predictive MSE</cell><cell>0 0.05 0.1 0.15</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>Time 11</cell><cell>13</cell><cell>15</cell><cell>17</cell><cell>19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We tested a new ODE 1 VAE variant where the latent space is governed by 1storder ODE system. ODE 1 VAE is similar to the NeuralODE</figDesc><table><row><cell>Supplementary Material for ODE 2 VAE: Deep generative second order ODEs with Bayesian neural networks</cell></row><row><cell>Çagatay Yıldız 1 , Markus Heinonen 1,2 , Harri Lähdesmäki 1 Department of Computer Science Aalto University, Finland, FI-00076 {cagatay.yildiz, markus.o.heinonen, harri.lahdesmaki}@aalto.fi</cell></row><row><cell>1 Ablation studies</cell></row><row><cell>1st-order baseline:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparison of neural network (NN) and Bayesian neural network (BNN) ODE's with different latent dimensionalities on BOUNCING BALL experiment. Adding 2nd order momentum achieves superior performance, while BNN's have a smaller impact.Below, we report the MSEs of mean trajectories, which are obtained with mean model predictions (e.g., for our model, when the mean value from the encoder distribution and variational posterior is used).</figDesc><table><row><cell></cell><cell cols="2">Latent dimensions d</cell><cell cols="2">Test MSE</cell></row><row><cell>Model</cell><cell>1st-order state</cell><cell>2nd-order momentum</cell><cell>NN</cell><cell>BNN</cell></row><row><cell>ODE 1 VAE ODE 2 VAE</cell><cell>25 50 25</cell><cell>--25</cell><cell>45 36 26</cell><cell>43 35 27</cell></row><row><cell>2 Extra results</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Average mean MSE on future mocap frames</figDesc><table><row><cell></cell><cell cols="2">Test error</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Dataset 1 Dataset 2</cell><cell>Reference</cell></row><row><cell>GPDM VGPLVM DTSBN-S NPODE NEURALODE ODE 2 VAE ODE 2 VAE-KL</cell><cell>57.52 128.03 78.39 45.74 97.74 32.19 30.72</cell><cell>N/A N/A 37.20 22.96 21.60 17.20 6.48</cell><cell>Wang et al. (2008) Damianou et al. (2011) Gan et al. (2015) Heinonen et al. (2018) Chen et al. (2018) current work current work</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>: Mean prediction errors on test angle of rotating MNIST dataset ( taken from Casale et al. (2018)) MODEL TEST ERROR GPPVAE-DIS GPPVAE-JOINT ODE 2 VAE ODE 2 VAE-KL 0.0184 0.0204 0.0280 0.0306</cell><cell>Predictive MSE</cell><cell>0 0.05 0.1 0.15</cell><cell>1</cell><cell>3</cell><cell>Time 5</cell><cell>7</cell><cell>9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Time-dependent differential functions f (z, t) can indirectly approximate higher-order dynamics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although our variational approximation model assumes deterministic second-order dynamics, the underlying true model may also have more complex or stochastic dynamics.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>The data used in this project was obtained from mocap.cs.cmu.edu. The database was created with funding from NSF EIA-0196217. The calculations presented above were performed using computer resources within the Aalto University School of Science Science-IT project. This work has been supported by the Academy of Finland grants no. 311584 and 313271.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00464</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Fixing a broken elbo. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian process prior variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Paolo Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Saglietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Listgarten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10369" to="10380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Continuous-time flows for efficient inference and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational gaussian process dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2510" to="2518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep temporal sigmoid belief networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2467" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Betterncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning unknown ODE models with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagatay</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Mannerström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Intosalmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Lähdesmäki</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/heinonen18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled and interpretable representations from sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1878" to="1889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep variational bayes filters: Unsupervised learning of state space models from raw data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Soelch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06432</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: Amethod for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Learn. Representations</title>
		<meeting>3rd Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Time-dependent representation for neural event sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00065</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Disentangled sequential autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02991</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual structure using predictive generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06380</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A probabilistic model for the numerical solution of initial value problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="122" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04681</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The recurrent temporal restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01558</idno>
	</analytic>
	<monogr>
		<title level="m">Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Recent advances in autoencoder-based representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05069</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gaussian process dynamical models for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="283" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning conditional generative models for temporal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02262</idno>
		<title level="m">Infovae: Information maximizing variational autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gaussian process prior variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Paolo Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Saglietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Listgarten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10369" to="10380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Variational gaussian process dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2510" to="2518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep temporal sigmoid belief networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2467" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning unknown ODE models with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagatay</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Mannerström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Intosalmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Lähdesmäki</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/heinonen18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
