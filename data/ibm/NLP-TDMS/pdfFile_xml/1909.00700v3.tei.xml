<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training-Time-Friendly Network for Real-Time Object Detection *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Zheng</surname></persName>
							<email>zhengtuzju@gmail.comyangzheng@fabu.aimemoiry</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<email>dcai@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fabu Inc</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba-Zhejiang University Joint Institute of Frontier Technologies</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Training-Time-Friendly Network for Real-Time Object Detection *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern object detectors can rarely achieve short training time, fast inference speed, and high accuracy at the same time. To strike a balance among them, we propose the Training-Time-Friendly Network (TTFNet). In this work, we start with light-head, single-stage, and anchor-free designs, which enable fast inference speed. Then, we focus on shortening training time. We notice that encoding more training samples from annotated boxes plays a similar role as increasing batch size, which helps enlarge the learning rate and accelerate the training process. To this end, we introduce a novel approach using Gaussian kernels to encode training samples. Besides, we design the initiative sample weights for better information utilization. Experiments on MS COCO show that our TTFNet has great advantages in balancing training time, inference speed, and accuracy. It has reduced training time by more than seven times compared to previous real-time detectors while maintaining state-of-the-art performances. In addition, our super-fast version of TTFNet-18 and TTFNet-53 can outperform SSD300 and YOLOv3 by less than one-tenth of their training time, respectively. The code has been made available at https://github.com/ZJULearning/ttfnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Accuracy, inference speed, and training time of object detectors have been widely concerned and continuously improved. However, little work can strike a good balance among them. Intuitively, detectors with faster inference speed should have a shorter training time. Nevertheless, in fact, most real-time detectors require longer training time than non-real-time ones. The high-accuracy detectors can be roughly classified into one of the two types -those suffer from slow inference speed, and those require a large amount of training time.</p><p>The first type of networks <ref type="bibr" target="#b14">(Ren et al. 2015;</ref><ref type="bibr" target="#b11">Lin et al. 2017b;</ref>) generally rely on the heavy detection head or complex post-processing. Although these designs are beneficial for accuracy improvement and fast convergence, they significantly slow down the inference speed.</p><p>Therefore, this type of network is typically not suitable for real-time applications.</p><p>To speed up the inference, researchers strive to simplify the detection head and post-processing while retaining accuracy <ref type="bibr" target="#b12">(Liu et al. 2016;</ref><ref type="bibr" target="#b12">Redmon and Farhadi 2018)</ref>. In a recent study named CenterNet (Zhou, <ref type="bibr" target="#b18">Wang, and Krähenbühl 2019)</ref>, the inference time is further shortened -almost the same as the time consumed by the backbone network. However, all these networks inevitably require long training time. This is because these networks are difficult to train due to the simplification, making them heavily dependent on the data augmentation and long training schedule. For example, CenterNet needs 140-epochs training on public dataset MS COCO <ref type="bibr" target="#b9">(Lin et al. 2014)</ref>. In contrast, the first type of network usually requires 12 epochs.</p><p>In this work, we focus on shortening the training time while retaining state-of-the-art real-time detection performances. Previous study <ref type="bibr" target="#b4">(Goyal et al. 2017</ref>) has reported that a larger learning rate can be adopted if the batch size is larger, and they follow a linear relationship under most conditions. We notice that encoding more training samples from annotated boxes is similar to increasing the batch size. Since the time spent on encoding features and calculating losses is negligible compared with that of feature extraction, we can safely attain faster convergence basically without additional overhead. In contrast, CenterNet, which merely focuses on the object center for size regression, loses the opportunity to utilize the information near the object center. This design is confirmed to be the main reason for the slow convergence according to our experiments.</p><p>To shorten the training time, we propose a novel approach using Gaussian kernels to encode training samples for both localization and regression. It allows the network to make better use of the annotated boxes to produce more supervised signals, which provides the prerequisite for faster convergence. Specifically, a sub-area around the object center is constructed via the kernel, and then dense training samples are extracted from this area. Besides, the Gaussian probabilities are treated as the weights of the regression samples to emphasize those samples near the object center. We further apply appropriate normalization to take advantage of more information provided by large boxes and retain the infor-mation given by small boxes. Our approach can reduce the ambiguous and low-quality samples without requiring any other components, e.g., Feature Pyramid Network (FPN) <ref type="bibr" target="#b10">(Lin et al. 2017a</ref>). Moreover, it does not require any offset predictions to aid in correcting the results, which is effective, unified, and intuitive.</p><p>Together with the light-head, single-stage, and anchorfree designs, this paper presents the first object detector that achieves a good balance among training time, inference speed, and accuracy. Our TTFNet reduces training time by more than seven times compared to CenterNet and other popular real-time detectors while retaining state-of-the-art performances. Besides, the super-fast version of TTFNet-18 and TTFNet-53 can achieve 25.9 AP / 112 FPS only after 1.8 hours and 32.9 AP / 55 FPS after 3.1 hours of training on 8 GTX 1080Ti, which is the shortest training time to reach these performances on MS COCO currently as far as we know. Furthermore, TTFNet-18 and TTFNet-53 can achieve 30.3 AP / 113 FPS after 19 hours and 36.2 AP / 55 FPS after 32 hours when training from scratch, and the long-training version of TTFNet-53 can achieve 39.3 AP / 57 FPS after 31 hours of training. These performances are very competitive compared to any other state-of-the-art object detectors.</p><p>Our contributions can be summarized as follows: • We discuss and validate the similarity between the batch size and the number of encoded samples produced by annotated boxes. Further, we experimentally verify the main reason for the slow convergence of advanced real-time detector CenterNet. • We propose a novel and unified approach which uses Gaussian kernels to produce training samples for both center localization and size regression in anchor-free detectors. It shows great advantages over previous designs. • Without bells and whistles, our detector has reduced training time by more than seven times compared to previous real-time detectors while keeping state-of-the-art realtime detection performance. Besides, the performances of from-scratch-training and long-training version are also very significant. • The proposed detector is friendly to researchers, especially for who only have limited computing resources. Besides, it is suitable for training time-sensitive tasks such as Neural Architecture Search (NAS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Single Stage Detectors. YOLO ) and SSD <ref type="bibr" target="#b12">(Liu et al. 2016</ref>) achieve satisfying performances and make the single-stage network gain attention. Since focal loss is proposed <ref type="bibr" target="#b11">(Lin et al. 2017b)</ref> to solve the imbalance between positive and negative examples, single-stage detectors are considered promising to achieve similar accuracy as two-stage ones. However, after that, the accuracy of singlestage ones stagnates for a long time until CornerNet (Law and Deng 2018) is introduced. CornerNet is a keypointbased single-stage detector, which outperforms a range of two-stage detectors in accuracy. Its design opens a new door for the object detection task.</p><p>Anchor Free Design. DenseBox <ref type="bibr" target="#b5">(Huang et al. 2015)</ref> is the first anchor-free detector, and then UnitBox <ref type="bibr" target="#b17">(Yu et al. 2016)</ref> upgrades DenseBox for better performance. YOLO is the first successful universal anchor-free detector. However, anchor-based methods <ref type="bibr" target="#b14">(Ren et al. 2015;</ref><ref type="bibr" target="#b12">Liu et al. 2016)</ref> can achieve higher recalls, which offers more potential for performance improvement. Thus, YOLOv2 <ref type="bibr" target="#b12">(Redmon and Farhadi 2017)</ref> abandons the previous anchor-free design and adopts the anchor design. Yet, CornerNet brings the anchorfree designs back into spotlight. Recently proposed Center-Net (Duan et al. 2019) reduces the false detection in Cor-nerNet, which further improves the accuracy. Apart from corner-based anchor-free design, many anchor-free detectors relying on FPN are proposed such as FCOS  and FoveaBox <ref type="bibr" target="#b7">(Kong et al. 2019)</ref>. GARPN <ref type="bibr" target="#b16">(Wang et al. 2019a</ref>) and FSAF (Zhu, He, and Savvides 2019) also adopt the anchor-free design in their methods. On the contrary, CenterNet (Zhou, Wang, and Krähenbühl 2019) does not rely on complicated decoding strategies or heavy head designs, which can outperform popular real-time detectors <ref type="bibr" target="#b12">(Liu et al. 2016;</ref><ref type="bibr" target="#b12">Redmon and Farhadi 2018)</ref> while having faster inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>We notice that encoding more training samples plays a similar role as increasing the batch size, and both of them can provide more supervised signals for each training step. The training samples refer to the features encoded by the annotated box. Reviewing the formulation of Stochastic Gradient Descent (SGD), the weight updating expression can be described as:</p><formula xml:id="formula_0">w t+1 = w t − η 1 n x∈B l(x, w t )<label>(1)</label></formula><p>where w is the weight of the network, B is a mini-batch sampled from the training set, n = |B| is the mini-batch size, η is the learning rate and l(x, w) is the loss computed from the labeled image x.</p><p>As for object detection, the image x may incorporate multiple annotated boxes, and these boxes will be encoded to training sample s ∈ S x . m x = |S x | indicates the number of samples produced by all the boxes in image x. Therefore (1) can be formulated as:</p><formula xml:id="formula_1">w t+1 = w t − η 1 n x∈B 1 m x s∈Sx l(s, w t )<label>(2)</label></formula><p>For simplify, suppose m x is same for each image x in a mini-batch B. Focusing on the individual training sample s, (2) can be rewritten as:</p><formula xml:id="formula_2">w t+1 = w t − η 1 nm s∈B l(s, w t )<label>(3)</label></formula><p>Linear Scaling Rule is empirically found in <ref type="bibr" target="#b4">(Goyal et al. 2017)</ref>. It claims that the learning rate should be multiplied by k if the batch size is multiplied by k, unless the network is rapidly changing, or very large mini-batch is adopted. Namely, executing k iterations with small mini-batches B j <ref type="figure">Figure 1</ref>: Experiments on CenterNet-R18. We increase the learning rate by 1.5x and then remove the complex data augmentation. (a) Increasing learning rate will lead to a consistent decline in AP while (b) eliminating data augmentation will lead to obvious overfitting. and learning rate η is basically equivalent to executing 1 iteration with large mini-batches ∪ j∈[0,k) B j and learning rate kη, only if we can assume l(x, w t ) ≈ l(x, w t+j ) for j &lt; k. This condition is usually met under large-scale, realworld data.</p><p>Instead of focusing on the labeled images x as in <ref type="bibr" target="#b4">(Goyal et al. 2017)</ref>, we focus on the training samples s here. The mini-batch size can be treated as |B| = nm according to (3). Although the encoded samples s ∈ S x have a strong correlation, they are still able to contribute information with differences. We can qualitatively draw a similar conclusion: when the number of encoded training samples in each mini-batch is multiplied by k, multiply the learning rate by l, where 1 ≤ l ≤ k.</p><p>CenterNet (Zhou, Wang, and Krähenbühl 2019), which is several times faster than most detectors in inference, suffers from long training time. It uses complex data augmentations in training. Although the augmentations allow models to have stable accuracy improvements, they cause slow convergence. To rule out their impact on convergence speed, we increase the learning rate and remove the augmentations. As shown in <ref type="figure">Figure 1</ref>, the larger learning rate cannot help CenterNet converge faster, and removing the augmentations leads to a bad performance. According to our conclusion above, we believe that it is because CenterNet merely encodes a single regression sample at the object center during training. This design makes CenterNet heavily rely on the data augmentations and long training schedule, leading to unfriendly training time.</p><p>To reduce the network's dependence on data augmentation while reducing training time, we presume that a better strategy of encoding regression samples is needed. Under the guidance of this motive, we propose our approach in the next section. More comprehensive experiments in our ablation study can further validate the superiority of our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Approach</head><p>Background CenterNet treats object detection as consisting of two parts -center localization and size regression. For localization, it adopts the Gaussian kernel as in CornerNet to produce a heat-map, which enables the network to produce higher activations near the object center. For regression, it defines the pixel at the object center as a training sample and directly predicts the height and width of the object. It also predicts the offset to recover the discretization error caused by the output stride. Since the network can produce higher activations near the object center in inference, the time-consuming NMS can be replaced by other components with negligible overhead.</p><p>In order to eliminate the need for the NMS, we adopt a similar strategy for center localization. Specifically, we further consider the aspect ratio of the box in the Gaussian kernel since the strategy that does not consider it in CenterNet is obviously sub-optimal.</p><p>As for size regression, mainstream approaches treat pixels in the whole box  or the sub-rectangle area of the box <ref type="bibr" target="#b7">(Kong et al. 2019)</ref> as training samples. We propose to treat all pixels in a Gaussian-area as training samples. Besides, weights calculated by object size and Gaussian probability are applied to these samples for better information utilization. Note that our approach does not require any other predictions to help correct the error, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, which is more simple and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian Kernels for Training</head><p>Given an image, our network separately predicts featurê</p><formula xml:id="formula_3">H ∈ R N ×C× H r × W r andŜ ∈ R N ×4× H r × W r .</formula><p>The former is used to indicate where the object center may exist, and the latter is used to attain the information related to the object size. N , C, H, W , r are batch size, number of categories, the height and width of the input image, and output stride. We set C = 80 and r = 4 in our experiments, and we omit N later for simplify. Gaussian kernels are used in both localization and regression in our approach, and we define scalar α and β to control the kernel size, respectively. Object Localization. Given m-th annotated box belongs to c m -th category, firstly it is linearly mapped to the feature-map scale. Then, 2D Gaussian kernel</p><formula xml:id="formula_4">K m (x, y) = exp(− (x−x0) 2 2σ 2 x − (y−y0) 2 2σ 2 y ) is adopted to produce H m ∈ R 1× H r × W r , where σ x = αw 6 , σ y = αh 6 .</formula><p>Finally, we update c m -th channel in H by applying element-wise maximum with H m . The produced H m is decided by the parameter α, center location (x 0 , y 0 ) m , and box size (h, w) m . We use ( x r , y r ) to force the center to be in the pixel as in Cen-terNet. α = 0.54 is set in our network, and it's not carefully selected.</p><p>The peak of the Gaussian distribution, also the pixel at the box center, is treated as the positive sample while any other pixel is treated as the negative sample. We use modified focal loss as <ref type="bibr" target="#b8">(Law and Deng 2018;</ref><ref type="bibr" target="#b18">Zhou, Wang, and Krähenbühl 2019)</ref>.</p><p>Given the predictionĤ and localization target H, we have</p><formula xml:id="formula_5">L loc = 1 M xyc (1 −Ĥ ijc ) α f log(Ĥ ijc ) if H ijc = 1 (1 − H ijc ) β fĤ α f ijc log(1 −Ĥ ijc ) else</formula><p>(4) where α f and β f are hyper-parameters in focal loss <ref type="bibr" target="#b11">(Lin et al. 2017b</ref>) and its modified version <ref type="bibr" target="#b8">(Law and Deng 2018;</ref><ref type="bibr" target="#b18">Zhou, Wang, and Krähenbühl 2019)</ref>, respectively. M stands for the number of annotated boxes. We set α f = 2 and β f = 4.</p><p>Size Regression. Given m-th annotated box on the feature-map scale, another Gaussian kernel is adopted to produce S m ∈ R 1× H r × W r . The kernel size is controlled by β as mentioned above. Note that we can use the same kernel to save computation when α and β are the same. The non-zero part in S m is named Gaussian-area A m , as shown in <ref type="figure" target="#fig_1">Figure  3</ref>. Since A m is always inside the m-box, it is also named sub-area in the rest of the paper.</p><p>Each pixel in the sub-area is treated as a regression sample. Given pixel (i, j) in the area A m and output stride r, the regression target is defined as the distances from (ir, jr) to four sides of m-th box, represented as a 4-dim vector (w l , h t , w r , h b ) m ij . The predicted box at (i, j) can be represented as:x 1 = ir −ŵ l s,ŷ 1 = jr −ĥ t s, x 2 = ir +ŵ r s,ŷ 2 = jr +ĥ b s.</p><p>where s is a fixed scalar used to enlarge the predicted results for easier optimization. s = 16 is set in our experiments. Note that the predicted box (x 1 ,ŷ 1 ,x 2 ,ŷ 2 ) is on image scale rather than feature-map scale.</p><p>If a pixel is not contained in any sub-areas, it will be ignored during training. If a pixel is contained in multiple subareas -an ambiguous sample, its training target is set to the object with the smaller area.</p><p>Given the predictionŜ and regression target S, we gather training targets S ∈ R Nreg×4 from S and corresponding prediction resultsŜ ∈ R Nreg×4 fromŜ, where N reg stands for the number of regression samples. For all these samples, we decode the predicted boxes and corresponding annotated boxes of samples as in <ref type="formula" target="#formula_6">(5)</ref>, and we use GIoU <ref type="bibr" target="#b15">(Rezatofighi et al. 2019)</ref> for loss calculation.</p><formula xml:id="formula_7">L reg = 1 N reg (i,j)∈Am GIoU(B ij , B m ) × W ij<label>(6)</label></formula><p>whereB ij stands for the decoded box (x 1 ,ŷ 1 ,x 2 ,ŷ 2 ) ij and B m = (x 1 , y 1 , x 2 , y 2 ) m is the corresponding m-th annotated box on image scale. W ij is the sample weight, which is used to balance the loss contributed by each sample.</p><p>Due to the large scale variance of objects, large objects may produce thousands of samples, whereas small objects may only produce a few. After normalizing the loss contributed by all samples, the losses contributed by small objects are even negligible, which will harm the detection performance on small objects. Therefore, sample weight W ij plays an important role in balancing losses. Suppose (i, j) is inside the sub-area A m of m-th annotated box, we have:</p><formula xml:id="formula_8">W ij =      log(a m ) × G m (i, j) (x,y)∈Am G m (x, y) (i, j) ∈ A m 0 (i, j) / ∈ A (7) where G m (i, j)</formula><p>is the Gaussian probabilities at (i, j) and a m is the area of the m-th box. This scheme can make good use of more annotation information contained in large objects and preserve that of small objects. It also can emphasize these samples near the object center, reducing the effect of ambiguous and low-quality samples, which will be discussed in our ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total Loss</head><p>The total loss L is composed of localization loss L loc and regression loss L reg , weighted by two scalar. Specifically, L = w loc L loc + w reg L reg , where w loc = 1.0 and w reg = 5.0 in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Design</head><p>The architecture of TTFNet is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We use ResNet and DarkNet <ref type="bibr" target="#b12">(Redmon and Farhadi 2018)</ref>   The up-sampled features then separately go through two heads for different goals. The localization head produces high activations on those positions near the object center while the regression head directly predicts the distance from those positions to the four sides of the box. Since the object center corresponds to the local maximum at the feature map, we can safely suppress non-maximum values with the help of 2D max-pooling as in <ref type="bibr" target="#b8">(Law and Deng 2018;</ref><ref type="bibr" target="#b18">Zhou, Wang, and Krähenbühl 2019)</ref>. Then we use the positions of local maximums to gather regression results. Finally, the detection results can be attained.</p><p>Our approach makes efficient use of annotation information contained in large and medium-sized objects, but the promotion is limited for small objects that contain little information. In order to improve the detection performance on small objects in a short training schedule, we add the shortcut connections to introduce high-resolution but lowlevel features. The shortcut connections introduce the features from stage 2, 3, and 4 of the backbone, and each connection is implemented by 3 × 3 convolution layers. The number of the layers are set to 3, 2, and 1 for stage 2, 3, and 4, and ReLU follows each layer except for the last one in the shortcut connnection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Setting</head><p>Dataset. Our experiments are based on the challenging MS COCO 2017 benchmark. We use the Train split (115K images) for training and report the performances on Val split (5K images).</p><p>Training Details. We use ResNet and DarkNet as the backbone for experiments. We resize the images to 512×512 and do not keep the aspect ratio. Only the random flip is used for data augmentation in training unless the long training schedule is adopted. We use unfrozen BN but freeze all parameters of stem and stage 1 in the backbone. For ResNet, the initial learning rate is 0.016, and the mini-batch size is 128. For DarkNet, the initial learning rate is 0.015, and the mini-batch size is 96. The learning rate is reduced by a factor of 10 at epoch 18 and 22, respectively. Our network is trained with SGD for 24 epochs. For the super-fast version, the training schedule is halved. For the long-training version, the training schedule is increased by five times(i.e., 120-epochs training in total). Weight decay and momentum are set as 0.0004 and 0.9, respectively. For bias parameters in the network, their weight decay is set to 0, and their learning rate is doubled. Warm-up is applied for the first 500 steps. We initialize our backbone networks with the weights pre-trained on ImageNet <ref type="bibr" target="#b1">(Deng et al. 2009)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We use the super-fast TTNet-53 in our ablation study. The AP is reported on COCO 5k-val, and the inference speed is measured on the converged model with 1 GTX 1080Ti. Regression Weight W ij . Each annotated box produces multiple training samples in training, so how to balance the losses produced by samples becomes a problem. Treating all samples equally will lead to poor precision, as shown in Table 1. The poor result is caused by the number of samples produced by large objects is hundreds of times larger than that of small objects, which makes the losses contributed by small objects almost negligible. Another straightforward method is to normalize the losses produced by each annotated box. Namely, all these samples have same weight 1 nm if m-th annotated box produces n m samples. Still, it leads to sub-optimal results since it loses the chance to utilize the more information contained in large boxes.</p><p>To address this problem, we adopt the logarithm of the box area together with the normalized Gaussian probability as the sample weight. The results are listed in <ref type="table" target="#tab_0">Table 1</ref>, which show that our strategy can greatly handle the issues above. Note that introducing Gaussian probability in the weight can bring other benefits, which will be discussed next.</p><p>Benefits of Gaussian Design in Regression. We introduce the Gaussian probability in the regression weight, which can reduce the impact of ambiguous samples and lowquality samples more elegantly and efficiently. The ambiguous sample refers to the sample located in the overlapped area, and the low-quality sample refers to the sample that is far away from the object center.</p><p>Specifically, multiple objects are spatially overlapped sometimes, and thus it is hard for anchor-free detectors to decide the regression target in the overlapping area, which is called ambiguity. To alleviate it, previous work either places object of different scale in different level by using FPN <ref type="bibr" target="#b7">Kong et al. 2019)</ref>, or produces just one training sample based on the annotated box (Zhou, <ref type="bibr" target="#b18">Wang, and Krähenbühl 2019)</ref>, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Previous work ) also has noticed the impact of low-quality sample, and it suppresses low-quality samples by introducing the "center-ness" prediction. However, these solutions can only reduce the ambiguous samples or the low-quality samples. Besides, they have some side effects, such as leading to slow convergence speed or inference speed.     <ref type="table">Table 5</ref>: Results of different kernel size β. α = 0.54 and Gaussian kernel is used to produce regression samples. 1x stands for 12-epochs training and 2x stands for 24-epochs training.</p><p>Our Gaussian design can reduce both of the two types of samples without any side effects. It produces a sub-area inside the box, and the relative size of the sub-area is affected by the hyper-parameter β. Larger β utilizes more annotated information but also brings more ambiguous samples and low-quality samples.</p><p>Firstly, we use a more mundane form, i.e., rectangle as the sub-area to analyze the relationship between precision and β. In particular, β = 0 means only the box center is  <ref type="table">Table 6</ref>: AP after adopting long training schedule. We use data augmentation to prevent overfitting.</p><p>treated as a regression sample as in CenterNet, while β = 1 means all pixels in the rectangle box are treated as regression samples. We train a series of networks with changing β from 0.01 to 0.9. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the AP first rises and then falls as β increases. The rise indicates the annotated information near the object center also matters -the AP when β = 0.3 is much higher than that when β = 0.01. Therefore, the strategy of CenterNet that merely considers the object center is sub-optimal. The decline is caused by the increased ambiguous samples and low-quality samples.</p><p>To find out the main factor, we conduct experiments with the class-aware regression. Results show that we still meet the obvious accuracy degradation even the class-aware regression has reduced the impact of the ambiguity. It reveals that the main reason of the decline is caused by those low-quality samples. So, then, we propose the approach that uses Gaussian kernel to produce sub-area for training samples. Our approach not only emphasizes the samples near the object center but also alleviates the ambiguity. As shown in <ref type="table" target="#tab_3">Table 3</ref>, using the Gaussian sub-area achieves better results than using rectangular sub-area.</p><p>Considering the Aspect Ratio of the Box in Gaussian Kernel. CenterNet adopts the same strategy as CornetNet to produce heat-map without considering the aspect ratio of the box. According to our experiments, considering the ratio can improve precision consistently, as shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Shortcut Connection. We introduce the shortcut connection for achieving higher precision. The results when using different settings are listed in <ref type="table" target="#tab_4">Table 4</ref>. We choose the combination of 3, 2, 1 for stage 2, 3, 4, and it is not carefully selected.</p><p>The Effect of Sample Number on the Learning Rate. To verify the similarity between the batch size and the number of training samples encoded by the annotated boxes, we conduct experiments by changing β and learning rate.</p><p>As shown in <ref type="table">Table 5</ref>, we can observe that larger β guarantees a larger learning rate and better performance. Besides, the trend is more noticeable when β is smaller since there are fewer ambiguous and low-quality samples. In other words, having more samples is like enlarging the batch size, which helps to increase the learning rate further.</p><p>Training from Scratch. The from-scratch-training usually requires a longer training schedule. We set the total   training epochs to 120 here. As shown in <ref type="table">Table 6</ref>, fromscratch-training models can achieve performances comparable to those having a pre-trained backbone. Moreover, much better performance can be achieved when using a long training schedule, but it takes much longer training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared with State-of-the-Arts Detectors</head><p>Our TTFNet adopts ResNet-18/34 and DarkNet-53 as the backbone, and they are marked as TTFNet-18/34/53. As shown in <ref type="table" target="#tab_7">Table 7</ref>, our network can be more than seven times faster than other real-time detectors in training time while achieving state-of-the-art results with real-time inference speed. Compared with SSD300, our super-fast TTFNet-18 can achieve slightly higher precision, but our training time is ten times less, and the inference is more than two times faster. As for YOLOv3, our TTFNet-53 can achieve 2 points higher precision in just one-tenth training time, and it is almost two times faster than YOLOv3 in inference. The superfast TTFNet-53 can reach the precision of YOLOv3 in just one-twentieth training time.</p><p>As for the recently proposed anchor-free detector, our TTFNet shows great advantages. FCOS can achieve high precision without requiring long training time, but its slow inference speed will limit its mobile application. We list the performance of adopting lighter backbone such as ResNet18/34 in advanced RetinaNet and FCOS. Unfortunately, they can not achieve comparable performance due to the heavy head design. As for the real-time detector Cen-terNet, it has very fast inference speed and high precision, but it requires long training time. Our TTFNet only needs one-seventh training time compared with CenterNet, and it is superior in balancing training time, inference speed, and accuracy.</p><p>More Comparisons with CenterNet. CenterNet achieves 37.4 AP after being trained for 140 epochs when using DLA34(2018) as the backbone. We notice that CenterNet uses specially customized up-sampling layers for DLA34. For comparison between CenterNet and TTFNet when using DLA34, we replace the up-sampling layers in TTFNet with the ones in CenterNet, and therefore our changes in network structures cannot be applied. We use the same training hyper-parameters as TTFNet-53. The results in <ref type="table" target="#tab_8">Table 8</ref> show that our approach can bring significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We empirically show that more training samples help enlarge the learning rate and propose the novel method of using the Gaussian kernel for training. It is an elegant and efficient solution for balancing training time, inference speed, and accuracy, which can provide more potentials and possibilities for training-time-sensitive tasks <ref type="bibr" target="#b20">(Zoph and Le 2017;</ref><ref type="bibr" target="#b21">Zoph et al. 2018;</ref><ref type="bibr" target="#b3">Ghiasi, Lin, and Le 2019;</ref><ref type="bibr" target="#b17">Wang et al. 2019b;</ref><ref type="bibr" target="#b2">Gao et al. 2019)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture and Pipeline of TTFNet. Features are extracted by a backbone network and then up-sampled to 1/4 resolution of the original image. Then, the features are used for localization and regression tasks. For localization, the network can produce higher activations near the object center. For regression, all samples inside the Gaussian-area of the object can directly predict the distance to four sides of the box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Different strategies for defining training samples. Each pixel in the dark area corresponds to a training sample. In (d), the darker the color, the greater the sample weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>as the backbone in our experiments. The features extracted by the backbone are up-sampled to 1/4 resolution of the original image, which is implemented by Modulated Deformable Convolution (MDCN) (Zhu et al. 2019) and up-sample layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>MDCN layers are followed by Batch Normalization (BN)<ref type="bibr" target="#b6">(Ioffe and Szegedy 2015)</ref> and ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>unless specified. Our experiments are based on open source detection toolbox mmdetection (Chen et al. 2019) with 8 GTX 1080Ti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Different settings of regression weights W ij . Norm stands for equally treating n batches of samples produced by n objects, Sqrt and Log stand for multiplying the sample weight by square root or logarithm of the box area, and Gaussian stands for using Gaussian probabilities in sample weights. β is set to 0.54 in the experiments.</figDesc><table><row><cell>+ Norm</cell><cell></cell></row><row><cell>+ Sqrt</cell><cell></cell></row><row><cell>+ Log</cell><cell></cell></row><row><cell>+ Gaussian</cell><cell></cell></row><row><cell>AP</cell><cell>27.2 31.7 31.2 32.0 31.6 32.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of changing β from 0.01 to 0.9 and adopting class-aware regression. Note that the sub-area here is a rectangle, and Norm+Log is used as the sample weights in the experiments. Ratio stands for the relative number of ambiguous samples in the training set.</figDesc><table><row><cell>β</cell><cell cols="2">w/ Gaussian w/ Aspect Ratio AP Ratio %</cell></row><row><cell>0.3</cell><cell>32.2</cell><cell>3.66</cell></row><row><cell>0.3</cell><cell>32.5</cell><cell>3.65</cell></row><row><cell>0.54</cell><cell>32.0</cell><cell>8.01</cell></row><row><cell>0.54</cell><cell>32.7</cell><cell>7.57</cell></row><row><cell>0.54</cell><cell>32.9</cell><cell>7.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of different kernels for producing samples.</figDesc><table><row><cell cols="7">Gaussian stands for producing the regression samples using</cell></row><row><cell cols="7">Gaussian kernel, and Aspect Ratio stands for considering the</cell></row><row><cell cols="7">aspect ratio of the box in the kernel. β = 0.54 is set to be</cell></row><row><cell cols="7">consistent with α = 0.54, which allows us to share the same</cell></row><row><cell cols="7">Gaussian kernel for both localization and regression. Ratio</cell></row><row><cell cols="7">stands for the relative number of ambiguous samples in the</cell></row><row><cell>training set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stage 2</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell></row><row><cell>Stage 3</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>2</cell><cell>3</cell></row><row><cell>Stage 4</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>AP</cell><cell cols="6">29.0 32.0 32.8 32.8 32.9 33.2</cell></row><row><cell>FPS</cell><cell cols="6">58.7 55.5 54.8 54.6 54.4 54.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="7">: Speed-Accuracy tradeoffs when using different set-</cell></row><row><cell cols="3">tings in shortcut connection.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR</cell><cell>6e-3</cell><cell></cell><cell cols="2">1.2e-2</cell><cell cols="2">1.8e-2</cell></row><row><cell>Schedule</cell><cell>1x</cell><cell>2x</cell><cell>1x</cell><cell>2x</cell><cell>1x</cell><cell>2x</cell></row><row><cell cols="6">β 1 = 0.01 29.9 33.4 29.2 33.1 2.9</cell><cell>6.0</cell></row><row><cell cols="7">β 2 = 0.03 30.1 33.5 29.4 33.1 7.4 20.2</cell></row><row><cell cols="7">β 3 = 0.1 30.9 33.7 30.0 33.8 28.1 32.7</cell></row><row><cell cols="7">β 4 = 0.2 31.0 33.8 31.8 34.4 30.6 34.0</cell></row><row><cell cols="7">β 5 = 0.4 31.8 34.3 32.6 35.0 32.2 35.2</cell></row><row><cell cols="7">β 6 = 0.54 31.9 34.1 32.5 35.0 32.6 35.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>TTFNet vs. other state-of-the-art one-stage detectors. TT stands for training time. * indicates that the result is not presented in the original paper. fast stands for the super-fast version and 10x stands for the long-training version. All the training time is measured on 8 GTX 1080Ti, and all the inference speed is measured using converged models on 1 GTX 1080Ti.</figDesc><table><row><cell>Method</cell><cell cols="3">Backbone Schedule w/Augmentation AP</cell></row><row><cell>CenterNet</cell><cell>R18</cell><cell>2x</cell><cell>20.0</cell></row><row><cell>CenterNet</cell><cell>R18</cell><cell>2x</cell><cell>20.8</cell></row><row><cell>TTFNet</cell><cell>R18</cell><cell>2x</cell><cell>28.1</cell></row><row><cell>CenterNet</cell><cell>R18</cell><cell>11.67x</cell><cell>28.1</cell></row><row><cell>TTFNet</cell><cell>R18</cell><cell>10x</cell><cell>31.8</cell></row><row><cell>CenterNet</cell><cell>DLA34</cell><cell>2x</cell><cell>26.2</cell></row><row><cell>CenterNet</cell><cell>DLA34</cell><cell>2x</cell><cell>31.6</cell></row><row><cell>TTFNet</cell><cell>DLA34</cell><cell>2x</cell><cell>34.9</cell></row><row><cell>CenterNet</cell><cell>DLA34</cell><cell>11.67x</cell><cell>37.4</cell></row><row><cell>TTFNet</cell><cell>DLA34</cell><cell>10x</cell><cell>38.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>TTFNet vs. CenterNet.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CoRR abs/1906.07155</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Centernet: Keypoint triplets for object detection. CoRR abs/1904.08189</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NDDR-CNN: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">;</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<ptr target="CoRRabs/1706.02677" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno>abs/1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kong</surname></persName>
		</author>
		<idno>abs/1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
	<note>and Deng</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Yolov3: An incremental improvement</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Redmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rezatofighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FCOS: fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<idno>abs/1904.01355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2965" to="2974" />
		</imprint>
	</monogr>
	<note>Region proposal by guided anchoring</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NAS-FCOS: fast neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
	<note>Unitbox: An advanced object detection network</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
	<note>2018 IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno>abs/1903.00621</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
