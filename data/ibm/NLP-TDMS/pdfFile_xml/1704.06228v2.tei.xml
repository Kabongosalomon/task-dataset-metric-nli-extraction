<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Action Detection with Structured Segment Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Action Detection with Structured Segment Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting actions in untrimmed videos is an important yet challenging task. In this paper, we present the structured segment network (SSN), a novel framework which models the temporal structure of each action instance via a structured temporal pyramid. On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions and determining completeness. This allows the framework to effectively distinguish positive proposals from background or incomplete ones, thus leading to both accurate recognition and localization. These components are integrated into a unified network that can be efficiently trained in an end-to-end fashion. Additionally, a simple yet effective temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals. On two challenging benchmarks, THUMOS14 and ActivityNet, our method remarkably outperforms previous state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling actions with various temporal structures. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Temporal action detection has drawn increasing attention from the research community, owing to its numerous potential applications in surveillance, video analytics, and other areas <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b36">38]</ref>. This task is to detect human action instances from untrimmed, and possibly very long videos. Compared to action recognition, it is substantially more challenging, as it is expected to output not only the action category, but also the precise starting and ending time points.</p><p>Over the past several years, the advances in convolutional neural networks have led to remarkable progress in video analysis. Notably, the accuracy of action recognition <ref type="figure">Figure 1</ref>. Importance of modeling stage structures in action detection. We slide window detectors through a video clip with an action instance of "Tumbling" (green box). Top: The detector builds features without any stage structure of the action, e.g. average pooling throughout the window. It produces high responses whenever it sees any discriminative snippet related to tumbling, making it hard to localize the instance. Bottom: SSN detector utilizes stage structures (starting, course, and ending) via structured temporal pyramid pooling. Its response is only significant when the window is well aligned.</p><p>has been significantly improved <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b50">52</ref>]. Yet, the performances of action detection methods remain unsatisfactory <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b40">42]</ref>. For existing approaches, one major challenge in precise temporal localization is the large number of incomplete action fragments in the proposed temporal regions. Traditional snippet based classifiers rely on discriminative snippets of actions, which would also exist in these incomplete proposals. This makes them very hard to distinguish from valid detections (see <ref type="figure">Fig. 1</ref>). We argue that tackling this challenge requires the capability of temporal structure analysis, or in other words, the ability to identify different stages e.g. starting, course, and ending, which to-gether decide the completeness of an actions instance.</p><p>Structural analysis is not new in computer vision. It has been well studied in various tasks, e.g. image segmentation <ref type="bibr" target="#b19">[21]</ref>, scene understanding <ref type="bibr" target="#b15">[17]</ref>, and human pose estimation <ref type="bibr" target="#b0">[2]</ref>. Take the most related object detection for example, in deformable part based models (DPM) <ref type="bibr" target="#b7">[9]</ref>, the modeling of the spatial configurations among parts is crucial. Even with the strong expressive power of convolutional networks <ref type="bibr" target="#b11">[13]</ref>, explicitly modeling spatial structures, in the form of spatial pyramids <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b13">15]</ref>, remains an effective way to achieve improved performance, as demonstrated in a number of state-of-the-art object detection frameworks, e.g. Fast R-CNN <ref type="bibr" target="#b10">[12]</ref> and region-based FCN <ref type="bibr" target="#b22">[24]</ref>.</p><p>In the context of video understanding, although temporal structures have played an crucial role in action recognition <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b51">53]</ref>, their modeling in temporal action detection was not as common and successful. Snippet based methods <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b40">42]</ref> often process individual snippets independently without considering the temporal structures among them. Later works attempt to incorporate temporal structures, but are often limited to analyzing short clips. S-CNN <ref type="bibr" target="#b36">[38]</ref> models the temporal structures via the 3D convolution, but its capability is restricted by the underlying architecture <ref type="bibr" target="#b43">[45]</ref>, which is designed to accommodate only 16 frames. The methods based on recurrent networks <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b25">27]</ref> rely on dense snippet sampling and thus are confronted with serious computational challenges when modeling long-term structures. Overall, existing works are limited in two key aspects. First, the tremendous amount of visual data in videos restricts their capability of modeling long-term dependencies in an end-to-end manner. Also, they neither provide explicit modeling of different stages in an activity (e.g. starting and ending) nor offer a mechanism to assess the completeness, which, as mentioned, is crucial for accurate action detection.</p><p>In this work, we aim to move beyond these limitations and develop an effective technique for temporal action detection. Specifically, we adopt the proven paradigm of "pro-posal+classification", but take a significant step forward by utilizing explicit structural modeling in the temporal dimension. In our model, each complete activity instance is considered as a composition of three major stages, namely starting, course, and ending. We introduce structured temporal pyramid pooling to produce a global representation of the entire proposal. Then we introduce a decomposed discriminative model to jointly classify action categories and determine completeness of the proposals, which work collectively to output only complete action instances. These components are integrated into a unified network, called structured segment network (SSN). We adopt the sparse snippet sampling strategy <ref type="bibr" target="#b50">[52]</ref>, which overcomes the computational issue for long-term modeling and enables efficient end-to-end training of SSN. Additionally, we propose to use multi-scale grouping upon the temporal actionness signal to generate action proposals, achieving higher temporal recall with less proposals to further boost the detection performance.</p><p>The proposed SSN framework excels in the following aspects: 1) It provides an effective mechanism to model the temporal structures of activities, and thus the capability of discriminating between complete and incomplete proposals. 2) It can be efficiently learned in an end-to-end fashion (5 to 15 hours over a large video dataset, e.g. ActivityNet), and once trained, can perform fast inference of temporal structures.</p><p>3) The method achieves superior detection performance on standard benchmark datasets, establishing new state-of-the-art for temporal action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action Recognition. Action recognition has been extensively studied in the past few years <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b56">58]</ref>. Earlier methods are mostly based on hand-crafted visual features <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b45">47]</ref>. In the past several years, the wide adoption of convolutional networks (CNNs) has resulted in remarkable performance gain. CNNs are first introduced to this task in <ref type="bibr" target="#b18">[20]</ref>. Later, two-stream architectures <ref type="bibr" target="#b38">[40]</ref> and 3D-CNN <ref type="bibr" target="#b43">[45]</ref> are proposed to incorporate both appearance and motion features. These methods are primarily frame-based and snippet-based, with simple schemes to aggregate results. There are also efforts that explore long-range temporal structures via temporal pooling or RNNs <ref type="bibr" target="#b48">[50,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b3">5]</ref>. However, most methods assume welltrimmed videos, where the action of interest lasts for nearly the entire duration. Hence, they don't need to consider the issue of localizing the action instances. Object Detection. Our action detection framework is closely related to object detection frameworks <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b33">35]</ref> in spatial images, where detection is performed by classifying object proposals into foreground classes and a background class. Traditional object proposal methods rely on dense sliding windows <ref type="bibr" target="#b7">[9]</ref> and bottom-up methods that exploit low-level boundary cues <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b57">59]</ref>. Recent proposal methods based on deep neural networks show better average recall while requiring less candidates <ref type="bibr" target="#b33">[35]</ref>. Deep models also introduce great modeling capacity for capturing object appearances. With strong visual features, spatial structural modeling <ref type="bibr" target="#b21">[23]</ref> remains a key component for detection. In particular, the RoI pooling <ref type="bibr" target="#b10">[12]</ref> is introduced to model the spatial configuration of object with minimal extra cost. The idea is further reflected in R-FCN <ref type="bibr" target="#b22">[24]</ref> where the spatial configuration is handled with the position sensitive pooling. Temporal Action Detection. Previous works on activity detection mainly use sliding windows as candidates and focus on designing hand-crafted feature representations for classification <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b16">18]</ref>. Recent works incorporate deep networks into the detection frameworks and Tumbling Instance <ref type="figure">Figure 2</ref>. An overview of the structured segment network framework. On a video from ActivityNet <ref type="bibr" target="#b6">[8]</ref> there is a candidate region (green box). We first build the augmented proposal (yellow box) by extending it. The augmented proposal is divided into starting (orange), course (green), and ending (blue) stages. An additional level of pyramid with two sub-parts is constructed on the course stage. Features from CNNs are pooled within these five parts and concatenated to form the global region representations. The activity classifier and the completeness classifier operate on the the region representations to produce activity probability and class conditional completeness probability. The final probability of the proposal being positive instance is decided by the joint probability from these two classifiers. During training, we sparsely sample L = 9 snippets from evenly divided segments to approximate the dense temporal pyramid pooling.</p><p>obtain improved performance <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b1">3]</ref>. S-CNN <ref type="bibr" target="#b36">[38]</ref> proposes a multi-stage CNN which boosts accuracy via a localization network. However, S-CNN relies on C3D <ref type="bibr" target="#b43">[45]</ref> as the feature extractor, which is initially designed for snippetwise action classification. Extending it to detection with possibly long action proposals needs enforcing an undesired large temporal kernel stride. Another work <ref type="bibr" target="#b54">[56]</ref> uses Recurrent Neural Network (RNN) to learn a glimpse policy for predicting the starting and ending points of an action. Such sequential prediction is often time-consuming for processing long videos and it does not support joint training of the underlying feature extraction CNN. Our method differs from these approaches in that it explicitly models the action structure via structural temporal pyramid pooling. By using sparse sampling, we further enable efficient end-toend training. Note there are also works on spatial-temporal detection <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b30">32]</ref> and temporal video segmentation <ref type="bibr" target="#b14">[16]</ref>, which are beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structured Segment Network</head><p>The proposed structured segment network framework, as shown in <ref type="figure">Figure 2</ref>, takes as input a video and a set of temporal action proposals. It outputs a set of predicted activity instances each associated with a category label and a temporal range (delimited by a starting point and an ending point). From the input to the output, it takes three key steps. First, the framework relies on a proposal method to produce a set of temporal proposals of varying durations, where each proposal comes with a starting and an ending time. The proposal methods will be discussed in detail in Section 5. Our framework considers each proposal as a composition of three consecutive stages, starting, course, and ending, which respectively capture how the action starts, proceeds, and ends. Thus upon each proposal, structured temporal pyramid pooling (STPP) are performed by 1) splitting the proposal into the three stages; 2) building temporal pyramidal representation for each stage; 3) building global representation for the whole proposal by concatenating stagelevel representations. Finally, two classifiers respectively for recognizing the activity category and assessing the completeness will be applied on the representation obtained by STPP and their predictions will be combined, resulting in a subset of complete instances tagged with category labels. Other proposals, which are considered as either belonging to background or incomplete, will be filtered out. All the components outlined above are integrated into a unified network, which will be trained in an end-to-end way. For training, we adopt the sparse snippet sampling strategy <ref type="bibr" target="#b50">[52]</ref> to approximate the temporal pyramid on dense samples. By exploiting the redundancy among video snippets, this strategy can substantially reduce the computational cost, thus allowing the crucial modeling of long-term temporal structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Three-Stage Structures</head><p>At the input level, a video can be represented as a sequence of T snippets, denoted as (S t ) T t=1 . Here, one snip-pet contains several consecutive frames, which, as a whole, is characterized by a combination of RGB images and an optical flow stack <ref type="bibr" target="#b38">[40]</ref>. Consider a given set of N proposals</p><formula xml:id="formula_0">P = {p i = [s i , e i ]} N i=1</formula><p>. Each proposal p i is composed of a starting time s i and an ending time e i . The duration of p i is thus d i = e i − s i . To allow structural analysis and particularly to determine whether a proposal captures a complete instance, we need to put it in a context. Hence, we augment each proposal p i into p i = [s i , e i ] with where s i = s i −d i /2 and e i = e i + d i /2. In other words, the augmented proposal p i doubles the span of p i by extending beyond the starting and ending points, respectively by d i /2. If a proposal accurately aligns well with a groundtruth instance, the augmented proposal will capture not only the inherent process of the activity, but also how it starts and ends. Following the three-stage notion, we divide the augmented proposal p i into three consecutive intervals:</p><formula xml:id="formula_1">p s i = [s i , s i ], p c i = [s i , e i ], and p e i = [e i , e i ]</formula><p>, which are respectively corresponding to the starting, course, and ending stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Structured Temporal Pyramid Pooling</head><p>As mentioned, the structured segment network framework derives a global representation for each proposal via temporal pyramid pooling. This design is inspired by the success of spatial pyramid pooling <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b13">15]</ref> in object recognition and scene classification. Specifically, given an augmented proposal p i divided into three stages p s i , p c i , and p e i , we first compute the stage-wise feature vectors f s i , f c i , and f e i respectively via temporal pyramid pooling, and then concatenate them into a global representation. Specifically, a stage with interval [s, e] would cover a series of snippets, denoted as {S t |s ≤ t ≤ e}. For each snippet, we can obtain a feature vector v t . Note that we can use any feature extractor here. In this work, we adopt the effective two-stream feature representation first proposed in <ref type="bibr" target="#b38">[40]</ref>. Based on these features, we construct a L-level temporal pyramid where each level evenly divides the interval into B l parts. For the i-th part of the l-th level, whose interval is [s li , e li ], we can derive a pooled feature as</p><formula xml:id="formula_2">u (l) i = 1 |e li − s li + 1| e li t=s li v t .</formula><p>(1)</p><p>Then the overall representation of this stage can be obtained by concatenating the pooled features across all parts at all levels as f c i = (u (l) i |l = 1, . . . , L, i = 1, . . . , B l ). We treat the three stages differently. Generally, we observed that the course stage, which reflects the activity process itself, usually contains richer structure e.g. this process itself may contain sub-stages. Hence, we use a two-level pyramid, i.e. L = 2, B 1 = 1, and B 2 = 2, for the course stage, while using simpler one-level pyramids (which essentially reduce to standard average pooling) for starting and ending pyramids. We found empirically that this setting strikes a good balance between expressive power and complexity. Finally, the stage-wise features are combined via concatenation. Overall, this construction explicitly leverages the structure of an activity instance and its surrounding context, and thus we call it structured temporal pyramid pooling (STPP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Activity and Completeness Classifiers</head><p>On top of the structured features described above, we introduce two types of classifiers, an activity classifier and a set of completeness classifiers. Specifically, the activity classifier A classifies input proposals into K + 1 classes, i.e. K activity classes (with labels 1, . . . , K) and an additional "background" class (with label 0). This classifier restricts its scope to the course stage, making predictions based on the corresponding feature f c i . The completeness classifiers {C k } K k=1 are a set of binary classifiers, each for one activity class. Particularly, C k predicts whether a proposal captures a complete activity instance of class k, based on the global representation {f s i , f c i , f e i } induced by STPP. In this way, the completeness is determined not only on the proposal itself but also on its surrounding context.</p><p>Both types of classifiers are implemented as linear classifiers on top of high-level features. Given a proposal p i , the activity classifier will produce a vector of normalized responses via a softmax layer. From a probabilistic view, it can be considered as a conditional distribution P (c i |p i ), where c i is the class label. For each activity class k, the corresponding completeness classifier C k will yield a probability value, which can be understood as the conditional probability P (b i |c i , p i ), where b i indicates whether p i is complete. Both outputs together form a joint distribution. When c i ≥ 1, P (c i , b i |p i ) = P (c i |p i ) · P (b i |c i , p i ). Hence, we can define a unified classification loss jointly on both types of classifiers. With a proposal p i and its label c i :</p><formula xml:id="formula_3">L cls (c i , b i ; p i ) = − log P (c i |p i ) − 1 (ci≥1) log P (b i |c i , p i ).</formula><p>(2) Here, the completeness term P (b i |c i , p i ) is only used when c i ≥ 1, i.e. the proposal p i is not considered as part of the background. Note that these classifiers together with STPP are integrated into a single network that is trained in an endto-end way.</p><p>During training, we collect three types of proposal samples: (1) positive proposals, i.e. those overlap with the closest groundtruth instances with at least 0.7 IoU; (2) background proposals, i.e. those that do not overlap with any groundtruth instances; and (3) incomplete proposals, i.e. those that satisfy the following criteria: 80% of its own span is contained in a groundtruth instance, while its IoU with that instance is below 0.3 (in other words, it just covers a small part of the instance). For these proposal types, we respectively have (c i &gt; 0, b i = 1), c i = 0, and (c i &gt; 0, b i = 0). Each mini-batch is ensured to contain all three types of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Location Regression and Multi-Task Loss</head><p>With the structured information encoded in the global features, we can not only make categorical predictions, but also refine the proposal's temporal interval itself by location regression. We devise a set of location regressors {R k } K k=1 , each for an activity class. We follow the design in RCNN <ref type="bibr" target="#b11">[13]</ref>, but adapting it for 1D temporal regions. Particularly, for a positive proposal p i , we regress the relative changes of both the interval center µ i and the span φ i (in log-scale), using the closest groundtruth instance as the target. With both the classifiers and location regressors, we define a multi-task loss over an training sample p i , as:</p><formula xml:id="formula_4">L cls (c i , b i ; p i ) + λ · 1 (ci≥1 &amp; bi=1) L reg (µ i , φ i ; p i ). (3)</formula><p>Here, L reg uses the smooth L 1 loss function <ref type="bibr" target="#b10">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Efficient Training and Inference with SSN</head><p>The huge amount of frames poses a serious challenge in computational cost to video analysis. Our structured segment network also faces this challenge. This section presents two techniques which we use to reduce the cost and enable end-to-end training.</p><p>Training with sparse sampling. The structured temporal pyramid, in its original form, rely on densely sampled snippets. This would lead to excessive computational cost and memory demand in end-to-end training over long proposals -in practice, proposals that span over hundreds of frames are not uncommon. However, dense sampling is generally unnecessary in our framework. Particularly, the pooling operation is essentially to collect feature statistics over a certain region. Such statistics can be well approximated via a subset of snippets, due to the high redundancy among them.</p><p>Motivated by this, we devise a sparse snippet sampling scheme. Specifically, given a augmented proposal p i , we evenly divide it into L = 9 segments, randomly sampling only one snippet from each segment. Structured temporal pyramid pooling is performed for each pooling region on its corresponding segments. This scheme is inspired by the segmental architecture in <ref type="bibr" target="#b50">[52]</ref>, but differs in that it operates within STPP instead of a global average pooling. In this way, we fix the number of features needed to be computed regardless of how long the proposal is, thus effectively reducing the computational cost, especially for modeling long-term structures. More importantly, this enables end-to-end training of the entire framework over a large number of long proposals.</p><p>Inference with reordered computation. In testing, we sample video snippets with a fixed interval of 6 frames, and construct the temporal pyramid thereon. The original formulation of temporal pyramid first computes pooled features and then applies the classifiers and regressors on top which is not efficient. Actually, for each video, hundreds of proposals will be generated, and these proposals can significantly overlap with each other -therefore, a considerable portion of the snippets and the features derived thereon are shared among proposals.</p><p>To exploit this redundancy in the computation, we adopt the idea introduced in position sensitive pooling <ref type="bibr" target="#b22">[24]</ref> to improve testing efficiency. Note that our classifiers and regressors are both linear. So the key step in classification or regression is to multiply a weight matrix W with the global feature vector f . Recall that f itself is a concatenation of multiple features, each pooled over a certain interval. Hence the computation can be written as Wf = j W j f j , where j indexes different regions along the pyramid. Here, f j is obtained by average pooling over all snippet-wise features within the region r j . Thus, we have</p><formula xml:id="formula_5">W j f j = W j · E t∼rj [v t ] = E t∼rj [W j v t ] .<label>(4)</label></formula><p>E t∼rj denotes the average pooling over r j , which is a linear operation and therefore can be exchanged with the matrix multiplication. Eq (4) suggests that the linear responses w.r.t. the classifiers/regressors can be computed before pooling. In this way, the heavy matrix multiplication can be done in the CNN for each video over all snippets, and for each proposal, we only have to pool over the network outputs. This technique can reduce the processing time after extracting network outputs from around 10 seconds to less than 0.5 second per video on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Temporal Region Proposals</head><p>In general, SSN accepts arbitrary proposals, e.g. sliding windows <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b55">57]</ref>. Yet, an effective proposal method can produce more accurate proposals, and thus allowing a small number of proposals to reach a certain level of performance. In this work, we devise an effective proposal method called temporal actionness grouping (TAG).</p><p>This method uses an actionness classifier to evaluate the binary actionness probabilities for individual snippets. The use of binary actionness for proposals is first introduced in spatial action detection by <ref type="bibr" target="#b49">[51]</ref>. Here we utilize it for temporal action detection.</p><p>Our basic idea is to find those continuous temporal regions with mostly high actionness snippets to serve as proposals. To this end, we repurpose a classic watershed algorithm <ref type="bibr" target="#b35">[37]</ref>, applying it to the 1D signal formed by a sequence of complemented actionness values, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Imagine the signal as 1D terrain with heights and basins. This algorithm floods water on this terrain with different "water level" (γ), resulting in a set of "basins" covered by water, denoted by G(γ). Intuitively, each "basin" corresponds to a temporal region with high actionness.</p><p>The ridges above water then form the blank areas between basins, as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Given a set of basins G(γ), we devise a grouping scheme similar to <ref type="bibr" target="#b32">[34]</ref>, which tries to connect small basins into proposal regions. The scheme works as follows: it begins with a seed basin, and consecutively absorbs the basins that follow, until the fraction of the basin durations over the total duration (i.e. from the beginning of the first basin to the ending of the last) drops below a certain threshold τ . The absorbed basins and the blank spaces between them are then grouped to form a single proposal. We treat each basin as seed and perform the grouping procedure to obtain a set of proposals denoted by G (τ, γ). Note that we do not choose a specific combination of τ and γ. Instead we uniformly sample τ and γ from ∈ (0, 1) with an even step of 0.05. The combination of these two thresholds leads to multiple sets of regions. We then take the union of them. Finally, we apply non-maximal suppression to the union with IoU threshold 0.95, to filter out highly overlapped proposals. The retained proposals will be fed to the SSN framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>We conducted experiments to test the proposed framework on two large-scale action detection benchmark datasets: ActivityNet <ref type="bibr" target="#b6">[8]</ref> and THUMOS14 <ref type="bibr" target="#b17">[19]</ref>. In this section we first introduce these datasets and other experimental settings and then investigate the impact of different components via a set of ablation studies. Finally we compare the performance of SSN with other state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Settings</head><p>Datasets. ActivityNet <ref type="bibr" target="#b6">[8]</ref> has two versions, v1.2 and v1.3. The former contains 9682 videos in 100 classes, while the latter, which is a superset of v1.2 and was used in the ActivityNet Challenge 2016, contains 19994 videos in 200 classes. In each version, the dataset is divided into three disjoint subsets, training, validation, and testing, by 2:1:1. THUMOS14 <ref type="bibr" target="#b17">[19]</ref> has 1010 videos for validation and 1574 videos for testing. This dataset does not provide the training set by itself. Instead, the UCF101 <ref type="bibr" target="#b41">[43]</ref>, a trimmed video dataset is appointed as the official training set. Following the standard practice, we train out models on the validation set and evaluate them on the testing set. On these two sets, 220 and 212 videos have temporal annotations in 20 classes, respectively. 2 falsely annotated videos ("270","1496") in the test set are excluded in evaluation. In our experiments, we compare with our method with the states of the art on both THUMOS14 and ActivityNet v1.3, and perform ablation studies on ActivityNet v1.2.</p><p>Implementation Details. We train the structured segment network in an end-to-end manner, with raw video frames and action proposals as the input. Two-stream CNNs <ref type="bibr" target="#b38">[40]</ref> are used for feature extraction. We also use the spatial and temporal streams to harness both the appearance and motion features. The binary actionness classifiers underlying the TAG proposals are trained with <ref type="bibr" target="#b50">[52]</ref> on the training subset of each dataset. We use SGD to learn CNN parameters in our framework, with batch size 128 and momentum 0.9. We initialize the CNNs with pre-trained models from Im-ageNet <ref type="bibr" target="#b2">[4]</ref>. The initial learning rates are set to 0.001 for RGB networks and 0.005 for optical flow networks. In each minibatch, we keep the ratio of three types of proposals, namely positive, background, and incomplete, to be 1:1:6. For the completeness classifiers, only the samples with loss values ranked in the first 1/6 of a minibatch are used for calculating gradients, which resembles online hard negative mining <ref type="bibr" target="#b37">[39]</ref>. On both versions of ActivityNet, the RGB and optical flow branches of the two-stream CNN are respectively trained for 9.5K and 20K iterations, with learning rates scaled down by 0.1 after every 4K and 8K iterations, respectively. On THUMOS14, these two branches are respectively trained for 1K and 6K iterations, with learning rates scaled down by 0.1 per 400 and 2500 iterations.</p><p>Evaluation Metrics. As both datasets originate from contests, each dataset has its own convention of reporting performance metrics. We follow their conventions, reporting mean average precision (mAP) at different IoU thresholds.   IoU is used for comparing results from different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Studies</head><p>Temporal Action Proposal. We compare the performance of different action proposal schemes in three aspects, i.e. recall, quality, and detection performance. Particularly, we compare our TAG scheme with common sliding windows as well as other state-of-the-art proposal methods, including SCNN-prop, a proposal networks presented in <ref type="bibr" target="#b36">[38]</ref>, TAP <ref type="bibr" target="#b5">[7]</ref>, DAP <ref type="bibr" target="#b4">[6]</ref>. For the sliding window scheme, we use 20 exponential scales starting from 0.3 second long and step sizes of 0.4 times of window lengths. We first evaluate the average recall rates, which are summarized in <ref type="table">Table 1</ref>. We can see that TAG proposal have higher recall rates with the same number of proposals. Then we investigate the quality of its proposals. We plot the recall rates from different proposal methods at different IoU thresholds in <ref type="figure" target="#fig_3">Fig. 4</ref>. We can see TAG retains relatively high recall at high IoU thresholds, demonstrating that the proposals from TAG are generally more accurate. In experiments we also tried applying the actionness classifier trained on ActivityNet v1.2 directly on THUMOS14. We can still achieve a reasonable average recall of 39.6%, while the one Average mAP (%) (1)-0 (1,2)-0 (1)- trained on THUMOS14 achieves 48.9% in <ref type="table">Table 1</ref>. Finally, we evaluate the proposal methods in the context of action detection. The detection mAP values using sliding window proposals and TAG proposals are shown in <ref type="table">Table 3</ref>. The results confirm that, in most cases, the improved proposals can result in improved detection performance.</p><p>Structured Temporal Pyramid Pooling. Here we study the influence of different pooling strategies in STPP. We denote one pooling configuration as (B 1 , . . . , B K )−A, where K refers to the number of pyramid levels for the course stage and B 1 , . . . , B K the number of regions in each level. A = 1 indicates we use augmented proposal and model the starting and ending stage, while A = 0 indicates we only use the original proposal (without augmentation). Additionally we compare two within-region pooling methods: average and max pooling. The results are summarized in <ref type="table" target="#tab_1">Table 2</ref>. Note that these configurations are evaluated in the stage-wise training scenario. We observe that cases where A = 0 have inferior performance, showing that the introduction of the stage structure is very important for accurate detection. Also, increasing the depth of the pyramids for the course stage can give slight performance gain. Based on these results, we fix the configuration to (1, 2) − 1 in later experiments.</p><p>Classifier Design. In this work, we introduced the activity and completeness classifiers which work together to classify the proposal. We verify the importance of this decomposed design by studying another design that replaces it with a single set of classifiers, for which both background and incomplete samples are uniformly treated as negative.</p><p>We perform similar negative sample mining for this setting.</p><p>The results are summarized in <ref type="table">Table 3</ref>. We observe that using only one classifier to distinguish positive samples from both background and incomplete would lead to worse result even with negative mining, where mAP decreased from 23.7% to 17.9%. We attribute this performance gain to the different natures of the two negative proposal types, which require different classifiers to handle.</p><p>Location Regression &amp; Multi-Task Learning. Because of the contextual information contained in the starting and ending stages of the global region features, we are able to perform location regression. We measure the contribution of this step to the detection performance in <ref type="table">Table 3</ref>. From the results we can see that the location regression and multitask learning, where we train the classifiers and the regres-  <ref type="table">Table 3</ref>. Ablation study on ActivityNet <ref type="bibr" target="#b6">[8]</ref> v1.2. Overall, end-toend training is compared against stage wise training. We evaluate the performance using both sliding window proposals ("SW") and TAG proposals ("TAG"), measured by mean average precision (mAP). Here, "STPP" refers to structure temporal pyramid pooling. "Act. + Comp." refers to the use of two classifiers design. "Loc. Reg" denotes the use the location regression.</p><p>sors together in an end-to-end manner, always improve the detection accuracy.</p><p>Training: Stage-wise v.s. End-to-end. While the structured segment network is designed for end-to-end training, it is also possible to first densely extract features and train the classifiers and regressors with SVM and ridge regression, respectively. We refer to this training scheme as stagewise training. We compare the performance of end-to-end training and stage-wise training in <ref type="table">Table 3</ref>. We observe that models from end-to-end training can slightly outperform those learned with stage-wise training under the same settings. This is remarkable as we are only sparsely sampling snippets in end-to-end training, which also demonstrates the importance of jointly optimizing the classifiers and feature extractors and justifies our framework design. Besides, end-to-end training has another major advantage that it does not need to store the extracted features for the training set, which could become quite storage intensive as training data grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison with the State of the Art</head><p>Finally, we compare our method with other state-of-theart temporal action detection methods on THUMOS14 <ref type="bibr" target="#b17">[19]</ref> and ActivityNet v1.3 <ref type="bibr" target="#b6">[8]</ref>, and report the performances using the metrics described above. Note that the average action duration in THUMOS14 and ActivityNet are 4 and 50 seconds. And the average video duration are 233 and 114 seconds, respectively. This reflects the distinct natures of these datasets in terms of the granularities and temporal structures of the action instances. Hence, strong adaptivity is required to perform consistently well on both datasets.</p><p>THUMOS14. On THUMOS 14, We compare with the contest results <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b34">36]</ref> and those from recent works, including the methods that use segment-based 3D CNN <ref type="bibr" target="#b36">[38]</ref>, score pyramids <ref type="bibr" target="#b55">[57]</ref>, and recurrent reinforcement learning <ref type="bibr" target="#b54">[56]</ref>. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. In most cases, the proposed method outperforms previous state-of-the-art methods by over 10% in absolute mAP values.  <ref type="table">Table 5</ref>. Action detection results on ActivityNet v1.3, measured by mean average precision (mAP) for different IoU thresholds α and the average mAP of IoU thresholds from 0.5 to 0.95.</p><p>ActivityNet. The results on the testing set of ActivityNet v1.3 are shown in <ref type="table">Table 5</ref>. For references, we list the performances of highest ranked entries in the ActivityNet 2016 challenge. We submit our results to the test server of Ac-tivityNet v1.3 and report the detection performance on the testing set. The proposed framework, using a single model instead of an ensemble, is able to achieve an average mAP of 28.28 and perform well at high IOU thresholds, i.e., 0.75 and 0.95. This clearly demonstrates the superiority of our method. Visualization of the detection results can be found in the supplementary materials [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we presented a generic framework for temporal action detection, which combines a structured temporal pyramid with two types of classifiers, respectively for predicting activity class and completeness. With this framework, we achieved significant performance gain over stateof-the-art methods on both ActivityNet and THUMOS14. Moreover, we demonstrated that our method is both accurate and generic, being able to localize temporal boundaries precisely and working well for activity classes with very different temporal structures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the temporal actionness grouping process for proposal generation. Top: Actionness probabilities as a 1D signal sequence. Middle: The complement signal. We flood it with different levels γ. Bottom: Regions obtained by different flooding levels. By merging the regions according to the grouping criterion, we get the final set of proposals (in orange color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>On both versions of ActivityNet, the IoU thresholds are {0.5, 0.75, 0.95}. The average of mAP values with IoU thresholds [0.5:0.05:0.95] is used to compare the performance between different methods. On THUMOS14, the IoU thresholds are {0.1, 0.2, 0.3, 0.4, 0.5}. The mAP at 0.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Recall rate at different tIoU thresholds on ActivityNet v1.2. High recall rates at high IoU thresholds (&gt; 0.7) indicate better proposal quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison between different temporal pooling settings. The setting (1,2)-1 is used in the SSN framework. Please refer to Sec. 6.2 for the definition of these settings.</figDesc><table><row><cell>1 (1,2)-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Wang et. al. [48] 18.2 17.0 14.0 11.7 8.3 Oneata et. al. [31] 36.6 33.6 27.0 20.8 14.4 Richard et. al. [36] 39.7 35.7 30.0 23.2 15.2 S-CNN [38] 47.7 43.5 36.3 28.7 19.0 Yeung et. al. [56] 48.9 44.0 36.0 26.4 17.1 Yuan et. al. [57] 51.4 42.6 33.6 26.1 18.8 SSN 60.3 56.2 50.6 40.8 29.1 SSN* 66.0 59.4 51.9 41.0 29.8 Action detection results on THUMOS14, measured by mAP at different IoU thresholds α. The upper half of the table shows challenge results back in 2014. "SSN*" indicates metrics calculated in the PASCAL-VOC style used by ActivityNet [8]. Average Wang et. al. [54] 42.48 2.88 0.06 14.62 Singh et. al. [41] 28.67 17.78 2.88 17.68 Singh et. al. [42] 36.40 11.05 0.14 17.83 SSN 43.26 28.70 5.63 28.28</figDesc><table><row><cell></cell><cell cols="3">THUMOS14, mAP@α</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell cols="5">ActivityNet v1.3 (testing), mAP@α</cell><cell></cell></row><row><cell>Method</cell><cell>0.5</cell><cell cols="2">0.75 0.95</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at http://yjxiong.me/others/ssn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tuytelaars. Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2782" to="2795" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint segmentation and classification of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3265" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action localization by tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bag-of-fragments: Selecting and encoding video fragments for event detection and recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cappallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="437" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal activity detection in untrimmed videos with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="392" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The lear submission at thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THUMOS Action Recognition Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The watershed transform: Definitions, algorithms and parallelization strategies. Fundamenta informaticae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Roerdink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meijster</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="187" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno>abs/1607.01979</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combining the right features for complex event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2696" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">THUMOS Action Recognition Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Latent hierarchical model of temporal structure for complex activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="810" to="822" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Actionness estimation using hybrid fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal pyramid pooling based convolutional neural network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">UTS at activitynet 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AcitivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3164" to="3172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
