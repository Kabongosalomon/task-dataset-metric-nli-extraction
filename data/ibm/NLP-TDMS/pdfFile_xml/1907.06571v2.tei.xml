<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADVERSARIAL VIDEO GENERATION ON COMPLEX DATASETS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
							<email>aidanclark@google.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
							<email>jeffdonahue@google.com</email>
							<affiliation key="aff1">
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
							<email>simonyan@google.com</email>
							<affiliation key="aff2">
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADVERSARIAL VIDEO GENERATION ON COMPLEX DATASETS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative models of natural images have progressed towards high fidelity samples by the strong leveraging of scale. We attempt to carry this success to the field of video modeling by showing that large Generative Adversarial Networks trained on the complex Kinetics-600 dataset are able to produce video samples of substantially higher complexity and fidelity than previous work. Our proposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and higher resolution videos by leveraging a computationally efficient decomposition of its discriminator. We evaluate on the related tasks of video synthesis and video prediction, and achieve new state-of-the-art Fréchet Inception Distance for prediction for Kinetics-600, as well as state-of-the-art Inception Score for synthesis on the UCF-101 dataset, alongside establishing a strong baseline for synthesis on Kinetics-600.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern deep generative models can produce realistic natural images when trained on high-resolution and diverse datasets <ref type="bibr">(Brock et al., 2019;</ref><ref type="bibr">Karras et al., 2018;</ref><ref type="bibr">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr">Menick &amp; Kalchbrenner, 2019;</ref><ref type="bibr">Razavi et al., 2019)</ref>. Generation of natural video is an obvious further challenge <ref type="figure">Figure 2</ref>: Generated video samples with interesting behavior. In raster-scan order: a) On-screen generated text with further lines appearing.. b) Zooming in on an object. c) Colored detail from a pen being left on paper. d) A generated camera change and return.</p><p>for generative modeling, but one that is plagued by increased data complexity and computational requirements. For this reason, much prior work on video generation has revolved around relatively simple datasets, or tasks where strong temporal conditioning information is available.</p><p>We focus on the tasks of video synthesis and video prediction (defined in Section 2.1), and aim to extend the strong results of generative image models to the video domain. Building upon the state-of-the-art BigGAN architecture <ref type="bibr">(Brock et al., 2019)</ref>, we introduce an efficient spatio-temporal decomposition of the discriminator which allows us to train on Kinetics-600 -a complex dataset of natural videos an order of magnitude larger than other commonly used datasets. The resulting model, Dual Video Discriminator GAN (DVD-GAN), is able to generate temporally coherent, high-resolution videos of relatively high fidelity <ref type="figure" target="#fig_0">(Figure 1</ref>).</p><p>Our contributions are as follows:</p><p>• We propose DVD-GAN -a scalable generative model of natural video which produces high-quality samples at resolutions up to 256 × 256 and lengths up to 48 frames.</p><p>• We achieve state of the art for video synthesis on UCF-101 and prediction on Kinetics-600.</p><p>• We establish class-conditional video synthesis on Kinetics-600 as a new benchmark for generative video modeling, and report DVD-GAN results as a strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 VIDEO SYNTHESIS AND PREDICTION</head><p>The exact formulation of the video generation task can differ in the type of conditioning signal provided. At one extreme lies unconditional video synthesis where the task is to generate any video following the training distribution. Another extreme is occupied by strongly-conditioned models, including generation conditioned on another video for content transfer <ref type="bibr">(Bansal et al., 2018;</ref><ref type="bibr" target="#b23">Zhou et al., 2019)</ref>, per-frame segmentation masks , or pose information <ref type="bibr" target="#b16">(Walker et al., 2017;</ref><ref type="bibr" target="#b14">Villegas et al., 2017;</ref>. In the middle ground there are tasks which are more structured than unconditional generation, and yet are more challenging from a modeling perspective than strongly-conditional generation (which gets a lot of information about the generated video through its input). The objective of class-conditional video synthesis is to generate a video of a given category (e.g., "riding a bike") while future video prediction is concerned with generation of continuing video given initial frames. These problems differ in several aspects, but share a common requirement of needing to generate realistic temporal dynamics, and in this work we focus on these two problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GENERATIVE ADVERSARIAL NETWORKS</head><p>Generative Adversarial Networks (GANs) <ref type="bibr">(Goodfellow et al., 2014)</ref> are a class of generative models defined by a minimax game between a Discriminator D and a Generator G. The original objective was proposed by <ref type="bibr">Goodfellow et al. (2014)</ref>, and many improvements have since been suggested, mostly targeting improved training stability <ref type="bibr" target="#b1">(Arjovsky et al., 2017;</ref><ref type="bibr" target="#b22">Zhang et al., 2018;</ref><ref type="bibr">Brock et al., 2019;</ref><ref type="bibr">Gulrajani et al., 2017;</ref><ref type="bibr">Miyato et al., 2018)</ref>. We use the hinge formulation of the objective <ref type="bibr">(Lim &amp; Ye, 2017;</ref><ref type="bibr">Brock et al., 2019)</ref> which is optimized by gradient descent (ρ is the elementwise ReLU function):</p><formula xml:id="formula_0">D: min D E x∼data(x) ρ(1 − D(x)) + E z∼p(z) ρ(1 + D(G(z))) , G: max G E z∼p(z) D(G(z)) .</formula><p>GANs have well-known limitations including a tendency towards limited diversity in generated samples (a phenomenon known as mode collapse) and the difficulty of quantitative evaluation due to the lack of an explicit likelihood measure over the data. Despite these downsides, GANs have produced some of the highest fidelity samples across many visual domains <ref type="bibr">(Karras et al., 2018;</ref><ref type="bibr">Brock et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">KINETICS-600</head><p>Kinetics is a large dataset of 10-second high-resolution YouTube clips <ref type="bibr">(Kay et al., 2017;</ref><ref type="bibr">DeepMind, 2018</ref>) originally created for the task of human action recognition. We use the second iteration of the dataset, <ref type="bibr">Kinetics-600 (Carreira et al., 2018)</ref>, which consists of 600 classes with at least 600 videos per class for a total of around 500,000 videos. 1 Kinetics videos are diverse and unconstrained, which allows us to train large models without being concerned with the overfitting that occurs on small datasets with fixed objects interacting in specified ways <ref type="bibr">(Ebert et al., 2017;</ref><ref type="bibr">Blank et al., 2005)</ref>. Among prior work, the closest dataset (in terms of subject and complexity) which is consistently used is <ref type="bibr">UCF-101 (Soomro et al., 2012)</ref>. We focus on Kinetics-600 because of its larger size (almost 50x more videos than UCF-101) and its increased diversity (600 instead of 101 classes -not to mention increased intra-class diversity). Nevertheless for comparison with prior art we train on UCF-101 and achieve a state-of-the-art Inception Score there. Kinetics contains many artifacts expected from YouTube, including cuts (as in <ref type="figure">Figure 2d</ref>), title screens and visual effects. Except when specifically described, we choose frames with stride 2 (meaning we skip every other frame). This allows us to generate videos with more complexity without incurring higher computational cost.</p><p>To the best of our knowledge we are the first to consider generative modelling of the entirety of the Kinetics video dataset 2 , although a small subset of Kinetics consisting of 4,000 selected and stabilized videos (via a SIFT + RANSAC procedure) has been used in at least two prior papers <ref type="bibr">(Li et al., 2018;</ref><ref type="bibr">Balaji et al., 2018)</ref>. Due to the heavy pre-processing and stabilization present, as well as the sizable reduction in dataset size (two orders of magnitude) we do not consider these datasets comparable to the full Kinetics-600 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">EVALUATION METRICS</head><p>Designing metrics for measuring the quality of generative models (GANs in particular) is an active area of research <ref type="bibr">(Sajjadi et al., 2018;</ref><ref type="bibr">Barratt &amp; Sharma, 2018)</ref>. In this work we report the two most commonly used metrics, Inception Score (IS) <ref type="bibr">(Salimans et al. (2016)</ref>) and Fréchet Inception Distance (FID) <ref type="bibr">(Heusel et al., 2017)</ref>. The standard instantiation of these metrics is intended for generative image models, and uses an Inception model <ref type="bibr" target="#b7">(Szegedy et al., 2016)</ref> for image classification or feature extraction. For videos, we use the publicly available Inflated 3D Convnet (I3D) network trained on Kinetics-600 (Carreira &amp; Zisserman, 2017). Our Fréchet Inception Distance is therefore very similar to the Fréchet Video Distance (FVD) <ref type="bibr" target="#b12">(Unterthiner et al., 2018)</ref>, although our implementation is different and more aligned with the original FID metric. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DUAL VIDEO DISCRIMINATOR GAN</head><p>Our primary contribution is Dual Video Discriminator GAN (DVD-GAN), a generative video model of complex human actions built upon the state-of-the-art BigGAN architecture <ref type="bibr">(Brock et al., 2019)</ref> while introducing scalable, video-specific generator and discriminator architectures. An overview of 1 Kinetics is occasionally pruned and so we cannot give an exact size. 2 In parallel with the concurrent work of <ref type="bibr" target="#b18">Weissenborn et al. (2019)</ref>. <ref type="bibr">3</ref> We use 'avgpool' features (rather than logits) by default, our I3D model is trained on Kinetics-600 (rather than Kinetics-400), and we pre-calculate ground-truth statistics on the entire training set. the DVD-GAN architecture is given in <ref type="figure" target="#fig_1">Figure 3</ref> and a detailed description is in Appendix B.2. Unlike some of the prior work, our generator contains no explicit priors for foreground, background or motion (optical flow); instead, we rely on a high-capacity neural network to learn this in a data-driven manner. While DVD-GAN contains sequential components (RNNs), it is not autoregressive in time or in space. In other words, the pixels of each frame do not directly depend on other pixels in the video, as would be the case for auto-regressive models or models generating one frame at a time.</p><p>Generating long and high resolution videos is a heavy computational challenge: individual samples from Kinetics-600 (just 10 seconds long) contain upwards of 16 million pixels which need to be generated in a consistent fashion. This is a particular challenge to the discriminator. For example, a generated video might contain an object which leaves the field of view and incorrectly returns with a different color. Here, the ability to determine this video is generated is only possible by comparing two different spatial locations across two (potentially distant) frames. Given a video with length T , height H, and width W , discriminators that process the entire video would have to process all H × W × T pixels -limiting the size of the model and the size of the videos being generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DUAL DISCRIMINATORS</head><p>DVD-GAN tackles this scale problem by using two discriminators: a Spatial Discriminator D S and a Temporal Discriminator D T . D S critiques single frame content and structure by randomly sampling k full-resolution frames and judging them individually. We use k = 8 and discuss this choice in Section 4.3. D S 's final score is the sum of the per-frame scores. The temporal discriminator D T must provide G with the learning signal to generate movement (something not evaluated by D S ). To make the model scalable, we apply a spatial downsampling function φ(·) to the whole video and feed its output to D T . We choose φ to be 2 × 2 average pooling, and discuss alternatives in Section 4.3. This results in an architecture where the discriminators do not process the entire video's worth of pixels, since D S processes only k × H × W pixels and D T only T × H 2 × W 2 . For a 48 frame video at 128 × 128 resolution, this reduces the number of pixels to process per video from 786432 to 327680: a 58% reduction. Despite this decomposition, the discriminator objective is still able to penalize almost all inconsistencies which would be penalized by a discriminator judging the entire video. D T judges any temporal discrepancies across the entire length of the video, and D S can judge any high resolution details. The only detail the DVD-GAN discriminator objective is unable to reflect is the temporal evolution of pixels within a 2 × 2 window. We have however not noticed this affecting the generated samples in practice. DVD-GAN's D S is similar to the per-frame discriminator D I in MoCoGAN <ref type="bibr" target="#b10">(Tulyakov et al., 2018)</ref>. However MoCoGAN's analog of D T looks at full resolution videos, whereas D S is the only source of learning signal for high-resolution details in DVD-GAN. For this reason, D S is essential when φ is not the identity, unlike in MoCoGAN where the additional per-frame discriminator is less crucial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RELATED WORK</head><p>Generative video modeling is a widely explored problem which includes work on VAEs <ref type="bibr">(Babaeizadeh et al., 2018;</ref><ref type="bibr">Denton &amp; Fergus, 2018;</ref><ref type="bibr">Lee et al., 2018;</ref><ref type="bibr">Hsieh et al., 2018)</ref>, auto-regressive models <ref type="bibr">(Ranzato et al., 2014;</ref><ref type="bibr" target="#b4">Srivastava et al., 2015;</ref><ref type="bibr">Kalchbrenner et al., 2017;</ref><ref type="bibr" target="#b18">Weissenborn et al., 2019)</ref>, normalizing flows <ref type="bibr">(Kumar et al., 2019), and</ref><ref type="bibr">GANs (Mathieu et al., 2015;</ref><ref type="bibr" target="#b15">Vondrick et al., 2016;</ref><ref type="bibr">Saito et al., 2017;</ref><ref type="bibr">Saito &amp; Saito, 2018)</ref>. Much prior work considers decompositions which model the texture and spatial consistency of objects separately from their temporal dynamics. One approach is to split G into foreground and background models <ref type="bibr" target="#b15">(Vondrick et al., 2016;</ref><ref type="bibr" target="#b3">Spampinato et al., 2018)</ref>, while another considers explicit or implicit optical flow in either G or D <ref type="bibr">(Saito et al., 2017;</ref><ref type="bibr">Ohnishi et al., 2018)</ref>. Similar to DVD-GAN, MoCoGAN <ref type="bibr" target="#b10">(Tulyakov et al., 2018)</ref> discriminates individual frames in addition to a discriminator which operates on fixed-length K-frame slices of the whole video (where K &lt; T ). Though this potentially reduces the number of pixels to discriminate to <ref type="bibr" target="#b10">Tulyakov et al. (2018)</ref> describes discriminating sliding windows, which increases the total number of pixels. Other models follow this approach by discriminating groups of frames <ref type="bibr" target="#b20">(Xie et al., 2018;</ref><ref type="bibr" target="#b5">Sun et al., 2018;</ref><ref type="bibr">Balaji et al., 2018)</ref>.</p><formula xml:id="formula_1">(H × W ) + (K × H × W ),</formula><p>TGANv2 <ref type="bibr">(Saito &amp; Saito, 2018)</ref> proposes "adaptive batch reduction" for efficient training, an operation which randomly samples subsets of videos within a batch and temporal subwindows within each video. This operation is applied throughout TGANv2's G, with heads projecting intermediate feature maps directly to pixel space before applying batch reduction, and corresponding discriminators evaluating these lower resolution intermediate outputs. An effect of this choice is that TGANv2 discriminators only evaluate full-length videos at very low resolution. We show in <ref type="figure">Figure 6</ref> that a similar reduction in DVD-GAN's resolution when judging full videos leads to a loss in performance. We expect further reduction (towards the resolution at which TGANv2 evaluates the entire length of video) to lead to further degradation of DVD-GAN's quality. Furthermore, this method is not easily adapted towards models with large batch sizes divided across a number of accelerators, with only a small batch size per replica.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND ANALYSIS</head><p>A detailed description of our training setup is in Appendix B.3. Each DVD-GAN was trained on TPU pods (Google, 2018) using between 32 and 512 replicas with an Adam (Kingma &amp; Ba, 2014) optimizer. Video Synthesis models are trained for around 300,000 learning steps, whilst Video Prediction models are trained for up to 1,000,000 steps. Most models took between 12 and 96 hours to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CLASS-CONDITIONAL VIDEO SYNTHESIS</head><p>Our primary results concern the problem of Class-Conditional Video Synthesis. We provide our results for the UCF-101 and Kinetics-600 datasets. With Kinetics-600 emerging as a new benchmark for generative video modelling, our results establish a strong baseline for future work.  In <ref type="table" target="#tab_0">Table 1</ref> we show the main result of this paper: benchmarks for Video Synthesis on Kinetics-600.</p><p>We consider a range of resolutions and video lengths, and measure Inception Score and Fréchet Inception Distance (FID) for each (as described in Section 2.4). We further measure each model along a truncation curve, which we carry out by calculating FID and IS statistics while varying the standard deviation of the latent vectors between 0 and 1. There is no prior work with which to quantitatively compare these results (for comparative experiments see Section 4.1.2 and Section 4.2.1), but we believe these samples to show a level of fidelity not yet achieved in datasets as complex as Kinetics-600 (see samples from each row in Appendix E.1). Because all videos are resized for the I3D network (to 224 × 224), it is meaningful to compare metrics across equal length videos at different resolutions. Neither IS nor FID are comparable across videos of different lengths, and should be treated as separate metrics.</p><p>Generating longer and larger videos is a more challenging modeling problem, which is conveyed by the metrics (in particular, comparing 12-frame videos across 64 × 64, 128 × 128 and 256 × 256 resolutions). Nevertheless, DVD-GAN is able to generate plausible videos at all resolutions and with length spanning up to 4 seconds (48 frames). As can be seen in Appendix E.1, smaller videos display high quality textures, object composition and movement. At higher resolutions, generating coherent objects becomes more difficult (movement consists of a much larger number of pixels), but high-level details of the generated scenes are still extremely coherent, and textures (even complicated ones like a forest backdrop in <ref type="figure" target="#fig_0">Figure 1a</ref>) are generated well. It is further worth noting that the 48-frame models do not see more high resolution frames than the 12-frame model (due to the fixed choice of k = 8 described in Section 3.1), yet nevertheless learn to generate high resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">VIDEO SYNTHESIS ON UCF-101</head><p>We further verify our results by testing the same model on UCF-101 (Soomro et al., 2012), a smaller dataset of 13,320 videos of human actions across 101 classes that has previously been used for video synthesis and prediction <ref type="bibr">(Saito et al., 2017;</ref><ref type="bibr">Saito &amp; Saito, 2018;</ref><ref type="bibr" target="#b10">Tulyakov et al., 2018)</ref>. Our model produces samples with an IS of 32.97, significantly outperforming the state of the art (see <ref type="table" target="#tab_1">Table 2</ref> for quantitative comparison and Appendix C.1 for more details). VGAN <ref type="bibr" target="#b15">(Vondrick et al., 2016)</ref> 8.31 ± .09 <ref type="bibr">TGAN (Saito et al., 2017)</ref> 11.85 ± .07 MoCoGAN <ref type="bibr" target="#b10">(Tulyakov et al., 2018)</ref> 12.42 ± .03 ProgressiveVGAN <ref type="bibr" target="#b0">(Acharya et al., 2018)</ref> 14.56 ± .05 TGANv2 <ref type="bibr">(Saito &amp; Saito, 2018)</ref> 24.34 ± .35 DVD-GAN (ours) 32.97 ± 1.7  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FUTURE VIDEO PREDICTION</head><p>Future Video Prediction is the problem of generating a sequence of frames which directly follow from one (or a number) of initial conditioning frames. Both this and video synthesis require G to learn to produce realistic scenes and temporal dynamics, however video prediction further requires G to analyze the conditioning frames and discover elements in the scene which will evolve over time. In this section, we use the Fréchet Video Distance exactly as <ref type="bibr" target="#b12">Unterthiner et al. (2018)</ref>: using the logits of an I3D network trained on Kinetics-400 as features. This allows for direct comparison to prior work. Our model, DVD-GAN-FP (Frame Prediction), is slightly modified to facilitate the changed problem, and details of these changes are given in Appendix B.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">FRAME-CONDITIONAL KINETICS</head><p>For direct comparison with concurrent work on autoregressive video models <ref type="bibr" target="#b18">(Weissenborn et al., 2019)</ref> we consider the generation of 11 frames of Kinetics-600 at 64 × 64 resolution conditioned on 5 frames, where the videos for training are not taken with any frame skipping. We show results for all these cases in <ref type="table" target="#tab_3">Table 4</ref>. Our frame-conditional model DVD-GAN-FP outperforms the prior work on frame-conditional prediction for Kinetics. The final row labeled DVD-GAN corresponds to 16-frame class-conditional Video Synthesis samples, generated without frame conditioning and without frame skipping. The FVD of this video synthesis model is notably better.</p><p>On the one hand, we hypothesize that the synthesis model has an easier generative task: it can choose to generate (relatively) simple samples for each class, rather than be forced to continue frames taken from videos which are class outliers, or contain more complicated details. On the other hand, a certain portion of the FID/FVD metric undoubtedly comes from the distribution of objects and backgrounds present in the dataset, and so it seems that the prediction model should have a handicap in the metric by being given the ground truth distribution of backgrounds and objects with which to continue videos. The synthesis model's improved performance on this task seems to indicate that the advantage of being able to select videos to generate is greater than the advantage of having a ground truth distribution of starting frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">BAIR ROBOT PUSHING</head><p>We further test future video prediction on the single-class BAIR Robot Pushing Dataset (Ebert et al., 2017), a dataset of stationary videos of a robot arm moving around a set of changing objects. In order for direct comparison with previous results reported in <ref type="bibr" target="#b12">Unterthiner et al. (2018)</ref>, we consider generating 15 frames conditioned on a single starting frame. Like on prediction with Kinetics, we report FVD exactly as in <ref type="bibr" target="#b12">Unterthiner et al. (2018)</ref>, with ground truth statistics and conditioning <ref type="figure">Figure 6</ref>: The effect of φ in D T (left two) and k in D S (right two). FID is similar for any choice of φ, while IS declines as downsampling increases. Increasing k improves both with diminishing returns.</p><p>frames taken from the 256-video dev set. Results are reported in <ref type="table" target="#tab_2">Table 3</ref>. Scores are taken from <ref type="bibr" target="#b12">Unterthiner et al. (2018)</ref>. DVD-GAN-FP outperforms all prior adversarial models trained on this dataset, but performs slightly worse than Video Transformer, a concurrently developed autoregressive model <ref type="bibr" target="#b18">Weissenborn et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DUAL DISCRIMINATOR INPUT</head><p>We analyze several choices for k (the number of frames per sample in the input to D S ) and φ (the downsampling function for D T ). We expect setting φ to the identity or k = T to result in the best model, but we are interested in the maximally compressive k and φ that reduce discriminator input size (and the amount of computation), while still producing a high quality generator. For φ, we consider: 2 × 2 and 4 × 4 average pooling, the identity (no downsampling), as well as a φ which takes a random half-sized crop of the input video (as in Saito &amp; Saito <ref type="formula">(2018)</ref>). Results can be seen in <ref type="figure">Figure 6</ref>. For each ablation, we train three identical DVD-GANs with different random initializations on 12-frame clips of Kinetics-600 at 64 × 64 resolution for 100,000 steps. We report mean and standard deviation (via the error bars) across each group for the whole training period. For k, we consider 1, 2, 8 and 10 frames. We see diminishing effect as k increases, so settle on k = 8. We note the substantially reduced IS of 4 × 4 downsampling as opposed to 2 × 2, and further note that taking half-sized crops (which results in the same number of pixels input to D T as 2 × 2 pooling) is also notably worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We approached the challenging problem of modeling natural video by introducing a GAN capable of capturing the complexity of a large video dataset. We showed that on UCF-101 and frame-conditional Kinetics-600 it quantitatively achieves the new state of the art, alongside qualitatively producing sample videos with high complexity and diversity. We further wish to emphasize the benefit of training generative models on large and complex video datasets, such as Kinetics-600, and envisage the strong baselines we established on this dataset with DVD-GAN will be used as a reference point by the generative modeling community moving forward. While much remains to be done before realistic videos can be consistently generated in an unconstrained setting, we believe DVD-GAN is a step in that direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SEPARABLE ATTENTION</head><p>A previous version of this draft contained Separable Attention, a module which allows self-attention <ref type="bibr" target="#b13">(Vaswani et al., 2017)</ref> to be applied to spatio-temporal features which are too large for the quadratic memory cost of vanilla self-attention. Though DVD-GAN as proposed in this draft does not contain this module, we include a formal definition to aid understanding.</p><p>Self-attention on a single batch element X of shape <ref type="bibr">[N, C]</ref> (where N is the number of spatial positions and C is the number of features per location) can be given as: Separable Attention recognizes the natural decomposition of N = H × W × T by attending over each axis separately and in order. That is, first each feature is replaced with the result of a self-attention pass which only considers other features at the same H/W location (but across different frames), then the result of that layer (which contains cross-temporal information) is processed by a second self-attention layer which attends to features at different heights (but at the same width-point, and at the same frame), and then finally one which attends over width. The Python pseudocode 4 below implements this module assuming that X is given with the interior axes already separated (i.e., X is of shape  <ref type="figure" target="#fig_0">× 128</ref> resolution. This is done by a bilinear resize such that the video's smallest dimension is mapped to 128 pixels (maintaining aspect ratio). From this we take a random 128-pixel crop along the other dimension. We use the same procedure to construct datasets of different resolutions for Kinetics-600. All three datasets contain videos with more frames than we generate, so we take a random sequence of consecutive frames from the resized output.  <ref type="figure">Figure 7</ref>: The residual blocks for G and D S /D T . See <ref type="figure" target="#fig_1">Figure 3</ref> for the icons and B.2 for more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ARCHITECTURE DESCRIPTION</head><p>Our model adopts many architectural choices from Brock et al. <ref type="formula">(2019)</ref> including our nomenclature for describing network width, which is determined by the product of a channel multiplier ch with a constant for each layer in the network. The layer-wise constants for G are <ref type="bibr">[8,</ref><ref type="bibr">8,</ref><ref type="bibr">8,</ref><ref type="bibr">4,</ref><ref type="bibr">2]</ref> for 64 × 64 videos and <ref type="bibr">[8,</ref><ref type="bibr">8,</ref><ref type="bibr">8,</ref><ref type="bibr">4,</ref><ref type="bibr">2,</ref><ref type="bibr">1]</ref> for 128 × 128. The width of the i-th layer is given by the product of ch and the i-th constant and all layers prior to the residual network in G use the initial layer's multiplier and we refer to the product of that and ch as ch 0 . ch in DVD-GAN is 128 for videos with 64 × 64 resolution and 96 otherwise. The corresponding ch lists for both D T and D S are <ref type="bibr">[2,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16,</ref><ref type="bibr">16]</ref> for 64 × 64 resolution and <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16,</ref><ref type="bibr">16]</ref> for 128 × 128.</p><p>The input to G consists of a Gaussian latent noise z ∼ N (0, I) and a learned linear embedding e(y) of the desired class y. Both inputs are 120-dimensional vectors. G starts by computing an affine transformation of [z; e(y)] to a [4, 4, ch 0 ]-shaped tensor (in <ref type="figure" target="#fig_1">Figure 3</ref> this is represented as a 1 × 1 convolution). [z; e(y)] is used as the input to all class-conditional Batch Normalization layers throughout G (the gray line in <ref type="figure">Figure 7)</ref>. This is then treated as the input (at each frame we would like to generate) to a Convolutional Gated Recurrent Unit <ref type="bibr">(Ballas et al., 2015;</ref><ref type="bibr" target="#b6">Sutskever et al., 2011</ref>) whose update rule for input x t and previous output h t−1 is given by the following:</p><formula xml:id="formula_2">r = σ(W r 3 [h t−1 ; x t ] + b r ) u = σ(W u 3 [h t−1 ; x t ] + b u ) c = ρ(W c 3 [x t ; r h t−1 ] + b c ) h t = u h t−1 + (1 − u) c</formula><p>In these equations σ and ρ are the elementwise sigmoid and ReLU functions respectively, the n operator represents a convolution with a kernel of size n × n, and the operator is an elementwise multiplication. Brackets are used to represent a feature concatenation. This RNN is unrolled once per frame. The output of this RNN is processed by two residual blocks (whose architecture is given by <ref type="figure">Figure 7)</ref>. The time dimension is combined with the batch dimension here, so each frame proceeds through the blocks independently. The output of these blocks has width and height dimensions which are doubled (we skip upsampling in the first block). This is repeated a number of times, with the output of one RNN + residual group fed as the input to the next group, until the output tensors have the desired spatial dimensions. We do not reduce over the time dimension when calculating Batch Normalization statistics. This prevents the network from utilizing the Batch Normalization layers to pass information between timesteps.</p><p>The spatial discriminator D S functions almost identically to BigGAN's discriminator, though an overview of the residual blocks is given in <ref type="figure">Figure 7</ref> for completeness. A score is calculated for each of the uniformly sampled k frames (we default to k = 8) and the D S output is the sum over per-frame scores. The temporal discriminator D T has a similar architecture, but pre-processes the real or generated video with a 2 × 2 average-pooling downsampling function φ. Furthermore, the first two residual blocks of D T are 3-D, where every convolution is replaced with a 3-D convolution with a kernel size of 3 × 3 × 3. The rest of the architecture follows BigGAN <ref type="figure" target="#fig_0">(Brock et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 TRAINING DETAILS</head><p>Sampling from DVD-GAN is very efficient, as the core of the generator architecture is a feed-forward convolutional network: two 64 × 64 48-frame videos can be sampled in less than 150ms on a single  <ref type="bibr" target="#b22">(Zhang et al., 2018)</ref> for all weight layers (approximated by the first singular value) and orthogonal initialization of weights <ref type="bibr">(Saxe et al., 2013)</ref>. Sampling is carried out using the exponential moving average of G's weights, which is accumulated with decay γ = 0.9999 starting after 20,000 training steps. The model is optimized using Adam (Kingma &amp; Ba, 2014) with batch size 512 and a learning rate of 1 · 10 −4 and 5 · 10 −4 for G and D respectively. Class conditioning in D (Miyato &amp; Koyama, 2018) is projection-based whereas G relies on class-conditional Batch Normalization <ref type="bibr">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr">De Vries et al., 2017;</ref><ref type="bibr">Dumoulin et al., 2017)</ref>: equivalent to standard Batch Normalization without a learned scale and offset, followed by an elementwise affine transformation where each parameter is a function of the noise vector and class conditioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ARCHITECTURE EXTENSION TO VIDEO PREDICTION</head><p>In order to provide results on future video prediction problems we describe a simple modification to DVD-GAN to facilitate the added conditioning. A diagram of the extended model is in <ref type="figure" target="#fig_5">Figure 8</ref>.</p><p>Given C conditioning frames, our modified DVD-GAN-FP passes each frame separately through a deep residual network identical to D S . The (near) symmetric design of G and D S 's residual blocks mean that each output from a D-style residual block has a corresponding intermediate tensor in G of the same spatial resolution. After each block the resulting features for each conditioning frame are stacked in the channel dimension and passed through a 3 × 3 convolution and ReLU activation. The resulting tensor is used as the initial state for the Convolutional GRU in the corresponding block in G. Note that the frame conditioning stack reduces spatial resolution while G increases resolution. Therefore the smallest features of the conditioning frames (which have been through the most layers) are input earliest in G and the larger features (which have been through less processing) are input to G towards the end. D T operates on the concatenation of the conditioning frames and the output of G, meaning that it does not receive any extra information detailing that the first C frames are special. However to reduce wasted computation we do not sample the first C frames for D S on real or generated data. This technically means that D S will never see the first few frames from real videos at full resolution, but this was not an issue in our experiments. Finally, our video prediction variant does not condition on any class information, allowing us to directly compare with prior art. This is achieved by settling the class id of all samples to 0. 2018; <ref type="bibr" target="#b10">Tulyakov et al., 2018)</ref>. We report Inception Score (IS) calculated with a C3D network <ref type="bibr" target="#b9">(Tran et al., 2015)</ref> for quantitative comparison with prior work. 5 Our model produces samples with an IS of 32.97, significantly outperforming the state of the art (see <ref type="table" target="#tab_1">Table 2</ref>). The DVD-GAN architecture on UCF-101 is identical to the model used for Kinetics, and is trained on 16-frame 128 × 128 clips from UCF-101.</p><p>However, it is worth mentioning that our improved score is, at least partially, due to memorization of the training data. In <ref type="figure" target="#fig_6">Figure 9</ref> we show interpolation samples from our best UCF-101 model. Like interpolations in Appendix E.2, we sample 2 latents (left and rightmost columns) and show samples from the linear interpolation in latent space along each row. Here we show 4 such interpolations (the first frame from each video). Unlike Kinetics-600 interpolations, which smoothly transition from one sample to the other, we see abrupt jumps in the latent space between highly distinct samples, and little intra-video diversity between samples in each group. It can be further seen that some generated samples highly correlate with samples from the training set.</p><p>We show this both as a failure of the Inception Score metric, the commonly reported value for classconditional video synthesis on UCF-101, but also as strong signal that UCF-101 is not a complex or diverse enough dataset to facilitate interesting video generation. Each class is relatively small, and reuse of clips from shared underlying videos means that the intra-class diversity can be restricted to just a handful of videos per class. This suggests the need for larger, more diverse and challenging datasets for generative video modelling, and we believe that Kinetics-600 provides a better benchmark for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MISCELLANEOUS EXPERIMENTS</head><p>Here we detail a number of modifications or miscellaneous results we experimented with which did not produce a conclusive result.</p><p>• We experimented with several variations of normalization which do not require calculating statistics over a batch of data. Group Normalization <ref type="bibr" target="#b19">(Wu &amp; He, 2018)</ref> performed best, almost on a par with (but worse than) Batch Normalization. We further tried Layer Normalization <ref type="bibr">(Lei Ba et al., 2016)</ref>, Instance Normalization <ref type="bibr" target="#b11">(Ulyanov et al., 2016)</ref>, and no normalization, but found that these significantly underperformed Batch Normalization.</p><p>• We found that removing the final Batch Normalization in G, which occurs after the ResNet and before the final convolution, caused a catastrophic failure in learning. Interestingly, just removing the Batch Normalization layers within G's residual blocks still led to good (though slightly worse) generative models. In particular, variants without Batch Normalization in the residual blocks often achieve significantly higher IS (up to 110.05 for 64 × 64 12 frame samples -twice normal). But these models had substantially worse FID scores (1.22 for the aforementioned model) -and produced qualitatively worse video samples.</p><p>• Early variants of DVD-GAN contained Batch Normalization which normalized over all frames of all batch elements. This gave G an extra channel to convey information across time. It took advantage of this, with the result being a model which required batch statistics in order to produce good samples. We found that the version which normalizes over timesteps independently worked just as well and without the dependence on statistics.</p><p>• Models based on the residual blocks of BigGAN-deep trained faster (in wall clock time) but slower with regards to metrics, and struggled to reach the accuracy of models based on BigGAN's residual blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E GENERATED SAMPLES</head><p>It is difficult to accurately convey complicated generated video through still frames. Where provided, we recommend readers view the generated videos themselves via the provided links. We refer to videos within these batches by row/column number where the video in the 0th row and column is in the top left corner.</p><p>E.1 SYNTHESIS SAMPLES <ref type="figure" target="#fig_0">Figure 10</ref>: The first frames from a random batch of samples from DVD-GAN trained on 12 frames of 64 × 64 Kinetics-600. Full samples at https://drive.google.com/file/d/ 155F1lkHA5fMAd7k4W3CQvTsi1eKQDhGb/view?usp=sharing.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 INTERPOLATION SAMPLES</head><p>We expect G to produce samples of higher quality from latents near the mean of the distribution (zero). This is the idea behind the Truncation Trick <ref type="bibr">(Brock et al., 2019)</ref>. Like BigGAN, we find that DVD-GAN is amenable to truncation. We also experiment with interpolations in the latent space and in the class embedding. In both cases, interpolations are evidence that G has learned a relatively smooth mapping from the latent space to real videos: this would be impossible for a network that has only memorized the training data, or which is only capable of generating a few exemplars per class. Note that while all latent vectors along an interpolation are valid (and therefore G should produce a reasonable sample), at no point during training is G asked to generate a sample halfway between two classes. Nevertheless G is able to interpolate between even very distinct classes. <ref type="figure" target="#fig_0">Figure 15</ref>: An example intra-class interpolation. Each column is a separate video (the vertical axis is the time dimension). The left and rightmost columns are randomly sampled latent vectors and are generated under a shared class. Columns in between represent videos generated under the same class across the linear interpolation between the two random samples. Note the smooth transition between videos at all six timesteps displayed here. <ref type="figure" target="#fig_0">Figure 16</ref>: An example of class interpolation. As before, each column is a sequence of timesteps of a single video. Here, we sample a single latent vector, and the left and rightmost columns represent generating a video of that latent under two different classes. Columns in between represent videos of that same latent generated across an interpolation of the class embedding. Even though at no point has DVD-GAN been trained on data under an interpolated class, it nevertheless produces reasonable samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Selected frames from videos generated by a DVD-GAN trained on Kinetics-600 at 256×256, 128 × 128, and 64 × 64 resolutions (top to bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Simplified architecture diagram of G (left) and D S /D T (right). More details in B.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Each row is the first frame of 15 videos from a random class, all from the same checkpoint. The classes are: cooking scallops, changing wheel (not on bike), calculating, dribbling basketball.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>All 48 frames (in raster-scan order) from a 64 × 64 sample from watermelon cutting class.4.1.1 KINETICS-600 RESULTS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Self</head><label></label><figDesc>Attention(X) = sof tmax XQ(XK) T XV where Q, K, V are parameters all of shape [C, C] and the softmax is taken over the final axis. Batched self-attention is identical, except X has a leading batch axis and matrix multiplications are batched (i.e. the XQ multiplies two tensors of shape [B, N, C] and [C, C] and results in shape [B, N, C]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>An architecture diagram describing the changes for the frame conditional model. TPU core. The dual discriminator D is updated twice for every update of G (Heusel et al., 2017) and we use Spectral Normalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>C FURTHER EXPERIMENTS C.1 UCF-101 UCF-101 (Soomro et al., 2012) is a dataset of 13,320 videos of human actions across 101 classes that has previously been used for video synthesis and prediction (Saito et al., 2017; Saito &amp; Saito, The first frames of interpolations between UCF-101 samples. Each row is a separate interpolation. Contrast with samples in Appendix E.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>The first frames from a random batch of samples from DVD-GAN trained on 48 frames of 64 × 64 Kinetics-600. Full samples at https://drive.google.com/file/d/ 1FjOQYdUuxPXvS8yeOhXdPQMapUQaklLi/view?usp=sharing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>The first frames from a random batch of samples from DVD-GAN trained on 12 frames of 128 × 128 Kinetics-600. Full samples at https://drive.google.com/file/ d/165Yxuvvu3viOy-39LhhSDGtczbWphj_i/view?usp=sharingFigure 13: The first frames from a random batch of samples from DVD-GAN trained on 48 frames of 128 × 128 Kinetics-600. Full samples at https://drive.google.com/file/ d/1P8SsWEGP6tEGPPNPH-iVycOlN6vpIgE8/view?usp=sharing. The sample in row 1, column 5 is a stereotypical example of a degenerate sample occasionally produced by DVD-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>The first frames from a random batch of samples from DVD-GAN trained on 12 frames of 256 × 256 Kinetics-600. Full samples at https://drive.google.com/file/ d/1RGRVKCpVaG8z3p9GBCamRk4apiIR7jUc/view?usp=sharing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>FID/IS for DVD-GAN on Kinetics-600 Video Synthesis. We present the scores of the model taken at the point in training when the best FID was attained. The "No Truncation" columns contain the scores obtained without the truncation trick. The "With Truncation" columns contain the scores obtained at the truncation level which results in the best Inception Score.</figDesc><table><row><cell>(# Frames / Resolution)</cell><cell cols="2">No Truncation</cell><cell cols="2">With Truncation</cell></row><row><cell></cell><cell cols="2">FID (↓) IS (↑)</cell><cell cols="2">FID (↓) IS (↑)</cell></row><row><cell>12/64 × 64</cell><cell>0.85</cell><cell>53.81</cell><cell>7.13</cell><cell>187.23</cell></row><row><cell>12/128 × 128</cell><cell>1.16</cell><cell>77.45</cell><cell>13.04</cell><cell>246.18</cell></row><row><cell>12/256 × 256</cell><cell>2.05</cell><cell>62.78</cell><cell>10.17</cell><cell>162.44</cell></row><row><cell>48/64 × 64</cell><cell>13.75</cell><cell cols="2">104.09 47.86</cell><cell>264.12</cell></row><row><cell>48/128 × 128</cell><cell>28.44</cell><cell>81.41</cell><cell>45.79</cell><cell>188.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>IS on UCF-101 with C3D.</figDesc><table><row><cell>Method</cell><cell>IS (↑)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>FVD on BAIR.</figDesc><table><row><cell>Method</cell><cell>FVD (↓)</cell></row><row><cell>SVP-FP</cell><cell>315.5</cell></row><row><cell>CDNA</cell><cell>296.5</cell></row><row><cell>SV2P</cell><cell>262.5</cell></row><row><cell>SAVP</cell><cell>116.4</cell></row><row><cell cols="2">DVD-GAN-FP (ours) 109.8</cell></row><row><cell>Video Transformer</cell><cell>94 ± 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>DVD-GAN-FP's FVD scores on Video Prediction for 16 frames of Kinetics-600 without frame skipping. The final row represents a Video Synthesis model generating 16 frames.</figDesc><table><row><cell>Method</cell><cell cols="2">Training Set FVD (↓) Test Set FVD (↓)</cell></row><row><cell cols="2">Video Transformer (Weissenborn et al., 2019) -</cell><cell>170 ± 5</cell></row><row><cell>DVD-GAN-FP</cell><cell>68.66 ± 0.78</cell><cell>69.15 ± 1.16</cell></row><row><cell>DVD-GAN</cell><cell>32.3 ± 0.82</cell><cell>31.1 ± 0.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Separable Attention crucially reduces the asymptotic memory cost from O((HW T ) 2 ) to max O(H 2 W T ), O(HW 2 T ), O(HW T 2 ) while still allowing the result of the module to contain features at each location accumulated from all other features at any spatio-temporal location. we randomly shuffle the training set for each model replica independently. Experiments on the BAIR Robot Pushing dataset are conducted in the native resolution of 64 × 64, where for UCF-101 we operate at a (downsampled) 128</figDesc><table><row><cell>[B, H, W, T, C]).</cell></row><row><cell>def self_attention(x, q, k, v):</cell></row><row><cell>xq, xk, xv = np.matmul(x, q), np.matmul(x, k), np.matmul(x, v)</cell></row><row><cell>qv_correlations = np.matmul(xq, np.transpose(xk))</cell></row><row><cell>return np.matmul(np.softmax(qv_correlations, axis=−1), xv)</cell></row><row><cell>def separable_attention(x, q1, k1, v1, q2, k2, v2, q3, k3, v3):</cell></row><row><cell>b, h, w, t, c = x.shape</cell></row><row><cell># Apply attention over time.</cell></row><row><cell>x = np.reshape(x, [b * h * w, t, c])</cell></row><row><cell>x = self_attention(x, q1, k1, v1)</cell></row><row><cell># Apply attention over height.</cell></row><row><cell>x = np.reshape(x, [b * w * t, h, c])</cell></row><row><cell>x = self_attention(x, q2, k2, v2)</cell></row><row><cell># Apply attention over width.</cell></row><row><cell>x = np.reshape(x, [b * h * t, w, c])</cell></row><row><cell>x = self_attention(x, q3, k3, v3)</cell></row><row><cell>return x</cell></row><row><cell>B EXPERIMENT METHODOLOGY</cell></row><row><cell>B.1 DATASET PROCESSING</cell></row><row><cell>For all datasets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>1 Frame 0 Block x2 x Depth Times x Depth Times</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To be completely correct, a similar function operating on numpy arrays must properly transpose the axes before each reshape to ensure data is formatted in the proper order after the reshape operation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use the Chainer<ref type="bibr" target="#b8">(Tokui et al., 2015)</ref> implementation of Inception Score for C3D available at https://github.com/pfnet-research/tgan.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Eric Noland and João Carreira for help with the Kinetics dataset and Marcin Michalski and Karol Kurach for helping us acquire data and models for Fréchet Video Distance comparison. We would further like to thank Sander Dieleman, Jacob Walker and Tim Harley for useful discussions and feedback on the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards high resolution video generation with progressive growing of sliced Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danda</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02419</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P D&amp;apos;</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Murabito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vos-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09092</idno>
		<title level="m">Adversarial learning of visual-temporal dynamics for unsupervised dense prediction in videos</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A two-stream variational adversarial network for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01037</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking the Inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Systems for ML and Open Source Software at NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717</idno>
		<title level="m">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to generate long-term future via hierarchical prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungryull</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06601</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02634</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">tempoGAN: A temporally coherent, volumetric GAN for super-resolution fluid flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyu</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Thuerey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pose guided human video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<title level="m">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00129</idno>
		<title level="m">Dance dance generation: Motion transfer for internet videos</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
