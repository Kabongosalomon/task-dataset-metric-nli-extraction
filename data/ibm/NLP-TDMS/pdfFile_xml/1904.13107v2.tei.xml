<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolutional Networks with EigenPooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
							<email>mayao4@msu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">IBM T. J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Convolutional Networks with EigenPooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>ACM Reference Format: Tang. 2019. Graph Convolutional Networks with EigenPooling. In Proceedings of ACM Confer-ence (Conference&apos;17). ACM, New York, NY, USA, 9 pages. https://doi.org/10. 1145/nnnnnnn.nnnnnnn</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate the graph representation from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator EigenPooling based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework EigenGCN for graph classification. Theoretical analysis is provided to understand EigenPooling from both local and global perspectives. Experimental results of the graph classification task on 6 commonly used benchmarks demonstrate the effectiveness of the proposed framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate the graph representation from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator EigenPooling based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework EigenGCN for graph classification. Theoretical analysis is provided to understand EigenPooling from both local and global perspectives. Experimental results of the graph classification task on 6 commonly used benchmarks demonstrate the effectiveness of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have witnessed increasing interests in generalizing neural networks for graph structured data. The stream of research Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference'17, July 2017, Washington, DC, USA Â© 2019 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn on this topic is usually under the name of "Graph Neural Networks" <ref type="bibr" target="#b33">[34]</ref>, which typically involves transforming, propagating and aggregating node features across the graph. Among them, some focus on node-level representation learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref> while others investigate learning graph-level representation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48]</ref>. While standing from different perspectives, these methods have been proven to advance various graph related tasks. The methods focusing on node representation learning have brought improvement to tasks such as node classification <ref type="bibr">[14-16, 18, 22, 35]</ref> and link prediction <ref type="bibr" target="#b34">[35]</ref> and those methods working on graph-level representation learning have mainly facilitated graph classification. In this paper, we work on graph level representation learning with a focus on the task of graph classification.</p><p>The task of graph classification is to predict the label of a given graph utilizing its associated features and graph structure. Graph Neural Networks can extract graph representation while using all associated information. Majority of existing graph neural networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref> have been designed to generate good node representations, and then globally summarize the node representations as the graph representation. These methods are inherently "flat" since they treat all the nodes equivalently when generating graph representation using the node representations. In other words, the entire graph structure information is totally neglected during this process. However, nodes are naturally of different statuses and roles in a graph, and they should contribute differently to the graph level representation. Furthermore, graphs often have different local structures (or subgraphs), which contain vital graph characteristics. For instance, in a graph of a protein, atoms (nodes) are connected via bonds (edges); some local structures, which consist of groups of atoms and their direct bonds, can represent some specific functional units, which, in turn, are important to tell the functionality of the entire protein <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37]</ref>. These local structures are also not captured during the global summarizing process. To generate the graph representation which preserves the local and global graph structures, a hierarchical pooling process, analogous to the pooling process in conventional convolutional neural (CNN) networks <ref type="bibr" target="#b22">[23]</ref>, is needed.</p><p>There are very recent works investigating the pooling procedure for graph neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>. These methods group nodes into subgraphs (supernodes), coarsen the graph based on these subgraphs and then the entire graph information is reduced to the coarsened graph by generating features of supernodes from their corresponding nodes in subgraphs. However, when pooling the features for supernodes, average pooling or max pooling have been usually adopted where the structures of these group nodes (the local structures) are still neglected. With the local structures, the nodes in the subgraphs are of different statuses and roles when they contribute to the supernode representations. It is challenging to design a general pooling operator while incorporating the local structure information as 1) the subgraphs may contain different numbers of nodes, thus a fixed size pooling operator cannot work for all subgraphs; and 2) the subgraphs could have very different structures, which may require different approaches to summarize the information for the supernode representation. To address the aforementioned challenges, we design a novel pooling operator EigenPooling based on the eigenvectors of the subgraphs, which naturally have the same size of each subgraph and can effectively capture the local structures when summarizing node features for supernodes. EigenPooling can be used as pooling layers to stack with any graph neural network layers to form a novel framework EigenGCN for graph classification. Our major contributions can be summarized as follows:</p><p>â¢ We introduce a novel pooling operator EigenPooling, which can naturally summarize the subgraph information while utilizing the subgraph structure; â¢ We provide theoretical understandings on EigenPooling from both local and global perspectives; â¢ We incorporate pooling layers based on EigenPooling into existing graph neural networks as a novel framework EigenGCN for representation learning for graph classification; and â¢ We conduct comprehensive experiments on numerous realworld graph classification benchmarks to demonstrate the effectiveness of the proposed pooling operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED FRAMEWORK -EigenGCN</head><p>In this paper, we aim to develop a Graph Neural Networks (GNN) model, which consists of convolutional layers and pooling layers, to learn graph representations such that graph level classification can be applied. Before going to the details, we first introduced some notations and the problem setting.</p><p>Problem Setting: A graph can be represented as G = {E, V}, where V = {v 1 , . . . , v N } is the set of N nodes and E is the set of edges. The graph structure information can also be represented by an adjacency matrix A â R N ÃN . Furthermore, each node in the graph is associated with node features and we use X â R N Ãd to denote the node feature matrix, where d is the dimension of features. Note that this node feature matrix can also be viewed as a ddimensional graph signal <ref type="bibr" target="#b37">[38]</ref> defined on the graph G. In the graph classification setting, we have a set of graphs {G i }, each graph G i is associated with a label y i . The task of the graph classification is to take the graph (structure information and node features) as input and predict its corresponding label. To make the prediction, it is important to extract useful information from both graph structure and node features. We aim to design graph convolution layers and EigenPooling to hierarchically extract graph features, which finally learns a vector representation of the input graph for graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">An Overview of EigenGCN</head><p>In this work, we build our model based on Graph Convolutional Networks (GCN) <ref type="bibr" target="#b21">[22]</ref>, which has been demonstrated to be effective in node-level representation learning. While the GCN model is originally designed for semi-supervised node classification, we only discuss the part for node representation learning but ignoring the classification part. The GCN is stacked by several convolutional layers and a single convolutional layer can be written as:</p><formula xml:id="formula_0">F i+1 = ReLU (D â 1 2ÃD â 1 2 F i W i )<label>(1)</label></formula><p>where F i+1 â R N Ãd i +1 is the output of the i-th convolutional layer for i &gt; 0 and F 0 = X denotes the input node features. A total number of I convolutional layers are stacked to learn node representations and the output matrix F I can be viewed as the final node representations learned by the GCN model. As we described above, the GCN model has been designed for learning node representations. In the end, the output of the GCN model is a matrix instead of a vector. The procedure of the GCN is rather "flat", as it can only "pass message" between nodes through edges but cannot summarize the node information into the higher level graph representation. A simple way to summarize the node information to generate graph level representation is global pooling. For example, we could use the average of the node representations as the graph representation. However, in this way, a lot of key information is ignored and the graph structure is also totally overlooked during the pooling process.</p><p>To address this challenge, we propose eigenvector based pooling layers EigenPooling to hierarchically summarize node information and generate graph representation. An illustrative example is demonstrated in <ref type="figure">Figure 1</ref>. In particular, several pooling layers are added between convolutional layers. Each of the pooling layers pools the graph signal defined on a graph into a graph signal defined on a coarsened version of the input graph, which consists of fewer nodes. Thus, the design of the pooling layers consists of two components: 1) graph coarsening, which divides the graph into a set of subgraphs and form a coarsened graph by treating subgraphs as supernodes; and 2) transform the original graph signal information into the graph signal defined on the coarsened graph with EigenPooling. We coarsen the graph based on a subgraph partition. Given a subgraph partition with no overlaps between subgraphs, we treat each of the subgraphs as a supernode. To form a coarsened graph of the supernodes, we determine the connectivity between the supernodes by the edges across the subgraphs. During the pooling process, for each of the subgraphs, we summarize the information of the graph signal on the subgraph to the supernode. With graph coarsening, we utilize the graph structure information to form coarsened graphs, which makes it possible to learn representations level by level in a hierarchical way. With EigenPooling, we can learn node features of the coarsened graph that exploits the subgraph structure as well as the node features of the input graph. <ref type="figure">Figure 1</ref> shows an illustrative example, where a binary graph classification is performed. In this illustrative example, the graph is coarsened three times and finally becomes a single supernode. The input is a graph signal (the node features), which can be multidimensional. For the ease of illustration, we do not show the node features on the graph. Two convolutional layers are applied to the graph signal. Then, the graph signal is pooled to a signal defined on the coarsened graph. This procedure (two convolution layers and one pooling layer) is repeated two more times and the graph signal is finally pooled to a signal on a single node. This pooled signal  <ref type="figure">Figure 1</ref>: An illustrative example of the general framework on the single node, which is a vector, can be viewed as the graph representation. The graph representation then goes through several fully connected layers and the prediction is made upon the output of the last layer. Next, we introduce details of graph coarsening and EigenPooling of EigenGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Coarsening</head><p>In this subsection, we introduce how we perform the graph coarsening. As we mentioned in the previous subsection, the coarsening process is based on subgraph partition. There are different ways to separate a given graph to a set of subgraphs with no overlapping nodes. In this paper, we adopt spectral clustering to obtain the subgraphs, so that we can control the number of the subgraphs, which, in turn, determines the pooling ratio. We leave other options as future work. Given a set of subgraphs, we treat them as supernodes and build the connections between them as similar in <ref type="bibr" target="#b39">[40]</ref>. An example of the graph coarsening and supernodes is shown in <ref type="figure">Figure 1</ref>, where a subgraph and its supernodes are denoted using the same color. Next, we introduce how to mathematically describe the subgraphs, supernodes, and their relations.</p><p>Let c be a partition of a graph G, which consists of K connected subgraphs {G (k) } K k=1 . For the graph G, we have the adjacency matrix A â R N ÃN and the feature matrix X â R N Ãd . Let N k denote the number of nodes in the subgraph G (k ) and Î (k ) is the list of nodes in subgraph G <ref type="bibr">(k )</ref> . Note that each of the subgraph can be also viewed as a supernode. For each subgraph G (k ) , we can define a sampling operator C (k ) â R N ÃN k as follows:</p><formula xml:id="formula_1">C (k) [i, j] = 1 if and only if Î (k ) (j) = v i ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">C (k) [i, j] denotes the element in the (i, j)-th position of C (k) [i, j] and Î (k) (j) is the j-th element in the node list Î (k )</formula><p>. This operator provides a relation between nodes in the subgraph G (k ) and the nodes in the original graph. Given a single dimensional graph signal x â R N Ã1 defined on the original entire graph, the induced signal that is only defined on the subgraph G (k ) can be written as</p><formula xml:id="formula_3">x (k) = (C (k ) ) T x.<label>(3)</label></formula><p>On the other hand, we can also use C (k) to up-sample a graph signal x (k ) defined only on the subgraph G (k) to the entire graph G bÈ³</p><formula xml:id="formula_4">x = C (k ) x (k ) .<label>(4)</label></formula><p>It keeps the values of the nodes in the subgraph untouched while setting the values of all the other nodes that do not belong to the subgraph to 0. The operator can be applied to multi-dimensional signal X â R N Ãd in a similar way. The induced adjacency matrix</p><formula xml:id="formula_5">A (k) â R N k ÃN k of the subgraph G (k )</formula><p>, which only describes the connection within the subgraph G (k ) , can be obtained as</p><formula xml:id="formula_6">A (k ) = (C (k ) ) T AC (k ) .<label>(5)</label></formula><p>The intra-subgraph adjacency matrix of the graph G, which only consists of the edges inside each subgraph, can be represented as</p><formula xml:id="formula_7">A int = K k=1 C (k ) A (k ) (C (k ) ) T .<label>(6)</label></formula><p>Then the inter-subgraph adjacency matrix of graph G, which only consists of the edges between subgraphs, can be represented as</p><formula xml:id="formula_8">A ex t = A â A int .</formula><p>Let G coar denote the coarsened graph, which consists of the supernodes and their connections. We define the assignment matrix S â R N ÃK , which indicates whether a node belongs to a specific subgraph as:</p><formula xml:id="formula_9">S[i, j] = 1 if and only if v i â Î (j) .</formula><p>Then, the adjacency matrix of the coarsened graph is given as</p><formula xml:id="formula_10">A coar = S T A ex t S.<label>(7)</label></formula><p>With Graph Coarsening, we can obtain the connectivity of G coar , i.e., A coar . Obviously, A coar encodes the network structure information of G. Next, we describe how to obtain the node features X coar of G coar using EigenPooling. With A coar and X coar , we can stack more layers of GCN-GraphCoarsening-EigenPooling to learn higher level representations of the graph for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Eigenvector-Based Pooling -EigenPooling</head><p>In this subsection, we introduce EigenPooling, aiming to obtain X coar that encodes network structure information and node features of G. Globally, the pooling operation is to transform a graph signal defined on a given graph to a corresponding graph signal defined on the coarsened version of this graph. It is expected that the important information of the original graph signal can be largely preserved in the transformed graph signal. Locally, for each of the subgraph, we summarize the features of the nodes in this subgraph to a single representation of the supernode. It is necessary to consider the structure of the subgraph when we perform the summarizing, as the subgraph structure also encodes important information. However, common adopted pooling methods such as max pooling <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b47">48]</ref> or average pooling <ref type="bibr" target="#b10">[11]</ref> ignored the graph structure. In some works <ref type="bibr" target="#b29">[30]</ref>, the subgraph structure is used to find a canonical ordering of the nodes, which is, however, very difficult and expensive. In this work, to use the structure of the subgraphs, we design the pooling operator based on the graph spectral theory by facilitating the eigenvectors of the Laplacian matrix of the subgraph. Next, we first briefly review the graph Fourier transform and then introduce the design of EigenPooling based on graph Fourier transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Graph Fourier Transform. Given a graph G = {E, V} with</head><p>A â R N ÃN being the adjacency matrix and X â R N Ãd being the node feature matrix. Without loss of generality, for the following description, we consider d = 1, i.e., x â R N Ã1 , which can be viewed as a single dimensional graph signal defined on the graph G <ref type="bibr" target="#b32">[33]</ref>. This is the spatial view of a graph signal, which maps each node in the graph to a scalar value (or a vector if the graph signal is multidimensional). Analogous to the classical signal processing, we can define graph Fourier transform <ref type="bibr" target="#b37">[38]</ref> and spectral representation of the graph signal in the spectral domain. To define the graph signal in the spectral domain, we need to use the Laplacian matrix <ref type="bibr" target="#b5">[6]</ref> </p><formula xml:id="formula_11">L = Dâ A, where D is the diagonal degree matrix with D[i, i] = N j=1 A[i, j].</formula><p>The Laplacian matrix L can be used to define the "smoothness" of a graph signal <ref type="bibr" target="#b37">[38]</ref> as follows:</p><formula xml:id="formula_12">s(x) = x T Lx = 1 2 N i, j A[i, j](x[i] â x[j]) 2 .<label>(8)</label></formula><p>s(x) measures the smoothness of the graph signal x. The smoothness of a graph signal depends on how dramatically the value of connected nodes can change. The smaller s(x), the more smooth it is. For example, for a connected graph, a graph signal with the same value on all the nodes has a smoothness of 0, which means "extremely smooth" with no change.</p><p>As L is a real symmetric semi-positive definite matrix, it has a completed set of orthonormal eigenvectors {u l } N l =1 . These eigenvectors are also known as the graph Fourier modes <ref type="bibr" target="#b37">[38]</ref>, which are associated with the ordered real non-negative eigenvalues {Î» l } N l . Given a graph signal x, the graph Fourier transform can be obtained as followsx</p><formula xml:id="formula_13">= U T x,<label>(9)</label></formula><p>where U = [u 1 , . . . , u N ] â R N ÃN is the matrix consists of the eigenvectors of L. The vectorx obtained after the transform is the representation of the graph signal in the spectral domain. Correspondingly, the inverse graph Fourier transform, which transfers the spectral representation back to the spatial representation, can be denoted as:</p><formula xml:id="formula_14">x = Ux.<label>(10)</label></formula><p>Note that we can also view each the eigenvector u l of the Laplacian matrix L as a graph signal, and its corresponding eigenvalue Î» l can measure its "smoothness". For any of the eigenvector u l , we have:</p><formula xml:id="formula_15">s(u l ) = u T l Lu l = u T l Î» l u l = Î» l .<label>(11)</label></formula><p>The eigenvectors (or Fourier modes) are a set of base signals with different "smoothness" defined on the graph G. Thus, the graph Fourier transform of a graph signal x can be also viewed as linearly decomposing the signal x into the set of base signals.x can be viewed as the coefficients of the linear combination of the base signals to obtain the original signal x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.2</head><p>The Design of Pooling Operators. Since graph Fourier transform can transform graph signal to spectral domain which takes into consideration both graph structure and graph signal information, we adopt graph Fourier transform to design pooling operators, which pool the graph signal defined on a given graph G to a signal defined on its coarsened version G coar . The design of the pooling operator is based on graph Fourier transform of the subgraphs {G k } K k =1 . Let L (k ) denote the Laplacian matrix of the subgraph G (k ) . We denote the eigenvectors of the Laplacian matrix L (k ) as</p><formula xml:id="formula_16">u (k ) 1 , . . . , u (k) N k</formula><p>. We then use the up-sampling operator C (k ) to upsample these eigenvectors (base signals on this subgraph) into the entire graph and get the up-sampled version as:</p><formula xml:id="formula_17">u (k ) l = C (k ) u (k ) l , l = 1 . . . N k .<label>(12)</label></formula><p>With the up-sampled eigenvectors, we organize them into matrices to form pooling operators. Let Î l â R N ÃK denote the pooling operator consisting of all the l-th eigenvectors from all the subgraphs</p><formula xml:id="formula_18">Î l = [Å« (1) l , . . . ,Å« (K ) l ]<label>(13)</label></formula><p>Note that the subgraphs are not necessary all with the same number of nodes, which means that the number of eigenvectors can be different. Let N max = max k =1, ..., K N k be the largest number of nodes among all the subgraphs. Then, for a subgraph G (k ) with N k nodes, we set u</p><formula xml:id="formula_19">(k ) l for N k &lt; l â¤ N max as 0 â R N k Ã1 .</formula><p>The pooling process with l-th pooling operator Î l can be described as</p><formula xml:id="formula_20">X l = Î T l X<label>(14)</label></formula><p>where X l â R K Ãd is the pooled result using the l-th pooling operator. The k-th row of X l contains the information pooled from the k-th subgraph, which is the representation of the k-th supernode.</p><p>Following this construction, we build a set of N max pooling operators. To combine the information pooled by different pool operators, we can concatenate them together as follows:</p><formula xml:id="formula_21">X pool ed = [X 0 , . . . , X N max ].<label>(15)</label></formula><p>where X pool ed â R K Ãd Â·N max is the final pooled results. For efficient computation, instead of using the results pooled by all the pooling operators, we can choose to only use the first H of them as follows:</p><formula xml:id="formula_22">X coar = X pool ed = [X 0 , . . . , X H ].<label>(16)</label></formula><p>In Section 3.1 and Section 3.2, we will show that with H âª N max , we can still preserve most of the information. We will further empirically investigate the effect of choice of H in Section 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL ANALYSIS OF EigenPooling</head><p>In this section, we provide a theoretical analysis of EigenPooling by understanding it from local and global perspectives. We prove that the pooling operation can preserve useful information to be processed by the following GCN layers. We also verify that EigenGCN is permutation invariant, which lays the foundation for graph classification with EigenGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Local View of EigenPooling</head><p>In this subsection, we analyze the pooling operator from a local perspective focusing on a specific subgraph G (k ) . For the subgraph G (k ) , the pooling operator tries to summarize the nodes' features and form a representation for the corresponding supernode of the subgraph. For a pooling operator Î l , the part that is effective on the subgraph G (k ) , is only the up-sampled eigenvectorÅ« (k ) l as the other eigenvectors have 0 values on the subgraph G (k ) . Without the loss of generality, let's consider a single dimensional graph signal</p><p>x â R N Ã1 defined on the G, the pooling operation on G (k ) can be represented as:</p><formula xml:id="formula_23">(Å« (k) l ) T x = (u (k ) l ) T x (k ) ,<label>(17)</label></formula><p>which is the Fourier coefficient of the Fourier mode u (k ) l of the subgraph G (k ) . Thus, from a local perspective, the pooling process is a graph Fourier transform of the graph signal defined on the subgraph. As we introduced in the Section 2.3.1, each of the Fourier modes (eigenvectors) is associated with an eigenvalue, which measures its smoothness. The Fourier coefficient of the corresponding Fourier mode provides the information to indicate the importance of this Fourier mode to the signal. The coefficient summarizes the graph signal information utilizing both the node features and the subgraph structure as the smoothness is related to both of them. Each of the coefficients characterizes a different property (smoothness) of the graph signal. Using the first H coefficients while discarding the others means that we focus more on the "smoother" part of the graph signal, which is common in a lot of applications such as signal denoising and compression <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40]</ref>. Therefore, we can use the squared summation of the coefficients to measure how much information can be preserved as shown in the following theorem. [l] Â· u l .</p><p>Since U is orthogonal, we have</p><formula xml:id="formula_24">â¥x â² â¥ 2 2 â¥xâ¥ 2 2 = â¥ H l =1x [l] Â· u l â¥ 2 2 â¥ N l =1x [l] Â· u l â¥ 2 2 = â¥x[1 : H ]â¥ 2 2 â¥xâ¥ 2 2<label>(18)</label></formula><p>which completes the proof. â¡ It is common that for natural graph signal that the magnitude of the spectral form of the graph signal is concentrated on the first few coefficients <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38]</ref>, which means that</p><formula xml:id="formula_25">â¥x[1:H ] â¥ 2 2 â¥x â¥ 2 2 â 1 for H âª N k .</formula><p>In other words, by using the first H coefficients, we can preserve the majority of the information while reducing the computational cost. We will empirically verify it in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Global View of EigenPooling</head><p>In this subsection, we analyze the pooling operators from a global perspective focusing on the entire graph G. The pooling operators we constructed can be viewed as a filterbank <ref type="bibr" target="#b39">[40]</ref>. Each of the filters in the filterbank filters the given graph signal and obtains a new graph signal. In our case, the filtered signal is defined on the coarsened graph G coar . Without the loss of generality, we consider a single dimensional signal x â R N Ã1 of G, then the filtered signals are {x l } N max l =1 . Next, we describe some key properties of these pooling operators.</p><p>Property 1: Perfect Reconstruction: The first property is that when N max number of filters are used, the input graph signal can be perfectly reconstructed from the filtered signals. together with the pooling operators</p><formula xml:id="formula_26">{Î l } N max l =1 as x = N max l =1 Î l x l .</formula><p>Proof. With the definition of Î l given in Eq.(13), we have</p><formula xml:id="formula_27">N max l =1 Î l x l = N max l =1 K k =0Å« (k ) l Â· x l [k] = K k =0 N max l =1Å« (k ) l Â· x l [k] (19) From Eq.(14), we know that x l [k] = (Å« (k ) l ) T x. Together with the fact thatÅ« (k ) l = C (k ) u (k ) l in Eq.(12), we can rewrite N max l =1Å« (k ) l Â· x l [k] as N max l =1Å« (k) l Â· x l [k] = C (k) ( N max l =1 u (k ) l u (k ) l T )C (k ) T x (20) Obviously, N max l =1 u (k ) l u (k ) l T = N K l =1 u (k ) l u (k ) l T = I, since that {u (k ) l } N k l =1 are orthonormal and {u (k ) l } N max l =N k +1 are all 0 vectors. Thus, N max l =1Å« (k ) l Â· x l [k] = C (k ) x (k ) . Substitute this to Eq.(19), we arrive at N max l =1 Î l x l = K k =1 C (k ) x (k) = x<label>(21)</label></formula><p>which completes the proof. â¡ can preserve all the information from x. Thus, together with graph coarsening, eigenvector pooling can preserve the signal information of the input graph and can enlarge the receptive filed, which allows us to finally learn a vector representation for graph classification.</p><p>Property 2: Energy/Information Preserving The second property is that the filtered signals preserve all the energy when N max filters are chosen. To show this, we first give the following lemma, which serves as a tool for demonstrating property 2. Proof. By definition, we know that, for the same k, i.e, the same subgraph, u  l with different k are also orthogonal to each other as they only have non-zero values on different subgraphs. â¡ With the above lemma, we can further conclude that the â 2 norm of graph signal x is equal to the summation of the â 2 norm of the pooled signals {x l } N max l =1 . The proof is given as follows:</p><p>Lemma 3.4. The â 2 norm of the graph signal x is equal to the summation of the â 2 norm of the pooled signals {x l } N max l =1 :</p><formula xml:id="formula_28">||x|| 2 2 = N max l =1 ||x l || 2 2 (22)</formula><p>Proof. From Lemma 3.2 and 3.3, we have</p><formula xml:id="formula_29">â¥xâ¥ 2 2 = â¥ N max l =1 Î l x l â¥ 2 2 = K k =0 N max l =1Å« (k ) l Â· x l [k] 2 2 = K k =0 N max l =1 x 2 l [k] = N max l =1 ||x l || 2 2</formula><p>which completes the proof. â¡ Property 3: Approximate Energy Preserving Lemma 3.4 describes the energy preserving when N max number of filters are chosen. In practice, we only need H âª N max of filters for efficient computation. Next we show that even with H number of filters, the filtered signals preserve most of the energy/information.</p><formula xml:id="formula_30">Theorem 3.5. Let x â² = H l =1</formula><p>Î l x l be the graph signal reconstructed only using the first H pooling operators and pooled signals {x l } H l =1 .</p><p>Then the ratio</p><formula xml:id="formula_31">N H l =1 | |x l | | 2 2 Nmax l =1 | |x l | | 2 2</formula><p>can measure the portion of information being preserved by this x â² .</p><p>Proof. As shown in Lemma 3.2,</p><formula xml:id="formula_32">â¥xâ¥ 2 2 = N max l =1 â¥x l â¥ 2 2 . Similarly, we can show that â¥x â² â¥ 2 2 = H l =1 â¥x l â¥ 2 2 .</formula><p>The portion of the information being preserved can be represented as</p><formula xml:id="formula_33">â¥x â² â¥ 2 2 â¥xâ¥ 2 2 = N H l =1 ||x l || 2 2 N max l =1 ||x l || 2 2 .<label>(23)</label></formula><p>which completes the proof. â¡</p><p>Since for natural graph signals, the magnitude of the spectral form of the graph signal is concentrated in the first few coefficients <ref type="bibr" target="#b32">[33]</ref>, which means that even with H filters, EigenPooling can preserve the majority of the information/energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Permutation Invariance of EigenGCN</head><p>EigenGCN takes the adjacency matrix A and the node feature matrix X as input and aims to learn a vector representation of the graph. The nodes in a graph do not have a specific ordering, i.e., A and X can be permuted. Obviously, for the same graph, we want EigenGCN to extract the same representation no matter which permutation of A and X are used as input. Thus, in this subsection, we prove that EigenGCN is permutation invariant, which lays the foundation of using EigenGCN for graph classification. Theorem 3.6. Let P = {0, 1} nÃn be any permutation matrix, then EigenGCN(A, X) = EigenGCN(PAP T , PX), i.e., EigenGCN is permutation invariant.</p><p>Proof. In order to prove that EigenGCN is permutation invariant, we only need to show that it's key components GCN, graph coarsening and EigenPooling are permutation invariant. For GCN, before permutation, the output is F = ReLU (D â 1 2ÃD â 1 2 XW i ). With permutation, the output becomes</p><formula xml:id="formula_34">ReLU (PD â 1 2ÃD â 1 2 P T )(PX)W = ReLU (PD â 1 2ÃD â 1 2 XW) = PF</formula><p>where we have used P T P = I. This shows that the effect of permutation on GCN only permutes the order of the node representations but doesn't change the value of the representations. Second, the graph coarsening is done by spectral clustering with A. No matter which permutation we have, the detected subgraphs will not change. Finally, EigenPooling summarizes the information within each subgraph. Since the subgraph structures are not affected by the permutation and the representation of each node in the subgraphs is also not affected by the permutation, we can see that the supernodes' representations after EigenPooling are not affected by the permutation. In addition, the inter-connectivity of supernodes is not affected since it's determined by spectral clustering. Thus, we can say that one step of GCN-Graph Coarsening-EigenPooling is permutation invariant. Since finally EigenGCN learns one vector representation of the input graph, we can conclude that the vector representation is the same under any permutation P. â¡</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>In this section, we conduct experiments to evaluate the effectiveness of the proposed framework EigenGCN. Specifically, we aim to answer two questions:</p><p>â¢ Can EigenGCN improve the graph classification performance by the design of EigenPooling? â¢ How reliable it is to use H number of filters for pooling?</p><p>We begin by introducing datasets and experimental settings. We then compare EigenGCN with representative and state-of-the-art baselines for graph classification to answer the first question. We further conduct analysis on graph signals to verify the reasonableness of using H filters, which answers the second question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data sets</head><p>To verify whether the proposed framework can hierarchically learn good graph representations for classification, we evaluate EigenGCN on 6 widely used benchmark data sets for graph classification <ref type="bibr" target="#b20">[21]</ref>, which includes three protein graph data sets, i.e., ENZYMES <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>, PROTEINS <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>, and D&amp;D <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref>; one mutagen data set Mutagenicity <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref> (We denoted as MUTAG in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>); and two data sets that consist of chemical compounds screened for activity against non-small cell lung cancer and ovarian cancer cell lines, NCI1 and NCI109 <ref type="bibr" target="#b43">[44]</ref>. Some statistics of the data sets can be found in <ref type="table" target="#tab_0">Table 1</ref>. From the table, we can see that the used data sets contain varied numbers of graphs and have different graph sizes. We include data sets of different domains, sample and graph sizes to give a comprehensive understanding of how EigenGCN performs with data sets under various conditions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Experimental Settings</head><p>To compare the performance of graph classification, we consider some representative and state-of-the-art graph neural network models with various pooling layers. Next, we briefly introduce these baseline approaches as well as the experimental settings for them.</p><p>â¢ GCN <ref type="bibr" target="#b21">[22]</ref> is a graph neural network framework proposed for semi-supervised node classification. It learns node representations by aggregating information from neighbors. As the GCN model does not consist of a pooling layer, we directly pool the learned node representations as the graph representation. We use it as a baseline to compare whether a hierarchical pooling layer is necessary. â¢ GraphSage <ref type="bibr" target="#b17">[18]</ref> is similar as the GCN and provides various aggregation method. As similar in GCN, we directly pool the learned node representations as the graph representation. â¢ SET2SET. This baseline is also built upon GCN, it is also "flat" but uses set2set architecture introduced in <ref type="bibr" target="#b42">[43]</ref> instead of averaging over all the nodes. We select this method to further show whether a hierarchical pooling layer is necessary no matter average or other pooling methods are used. â¢ DGCNN <ref type="bibr" target="#b48">[49]</ref> is built upon the GCN layer. The features of nodes are sorted before feeding them into traditional 1-D convolutional and dense layers <ref type="bibr" target="#b48">[49]</ref>. This method is also "flat" without a hierarchical pooling procedure. â¢ Diff-pool <ref type="bibr" target="#b47">[48]</ref> is a graph neural network model designed for graph level representation learning with differential pooling layers. It uses node representations learned by an additional convolutional layer to learn the subgraphs (supernodes) and coarsen the graph based on it. We select this model as it achieves state-of-art performance on the graph classification task. â¢ EigenGCN-H represents various variants of the proposed framework EigenGCN, where H denotes the number of pooling operators we use for EigenPooling. In this evaluation, we choose H = 1, 2, 3. For each of the data sets, we randomly split it to 3 parts, i.e., 80% as training set, 10% as validation set and 10% as testing set. We repeat the randomly splitting process 10 times, and the average performance of the 10 different splits are reported. The parameters of baselines are chosen based on their performance on the validate set. For the proposed framework, we use the 9 splits of the training set and validation set to tune the structure of the graph neural network as well as the learning rate. The same structure and learning rate are then used for all 9 splits.</p><p>Following previous work <ref type="bibr" target="#b47">[48]</ref>, we adopt the widely used evaluation metric, i.e., Accuracy, for graph classification to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance on Graph Classification</head><p>Each experiment is run 10 times and the average graph classification performance in terms of accuracy is reported in <ref type="table" target="#tab_1">Table 2</ref>. From the table, We make the following observations: â¢ Diff-pool and the EigenGCN framework perform better than those methods without a hierarchical pooling procedure in most of the cases. Aggregating the node information hierarchically can help learn better graph representations. â¢ The proposed framework EigenGCN shares the same convolutional layer with GCN, GraphSage, and SET2SET. However, the proposed framework (with different H ) outperforms them in most of the data sets. This further indicates the necessity of the hierarchical pooling procedure. In other words, the proposed EigenPooling can indeed help the graph classification performance. â¢ In most of the data sets, we can observe that the variants of the EigenGCN with more eigenvectors achieve better performance than those with fewer eigenvectors. Including more eigenvectors, which suggests that we can preserve more information during pooling, can help learn better graph representations in most of the cases. In some of data sets, including more eigenvector does not bring any improvement in performance or even make the performance worse. Theoretically, we are able to preserve more information by using more eigenvectors. However, noise signals may be also preserved, which can be filtered when using fewer eigenvectors. â¢ The proposed EigenGCN achieves the state-of-the-art or at least comparable performance on all the data sets, which shows the effectiveness of the proposed framework EigenGCN.</p><p>To sum up, EigenPooling can help learn better graph representation and the proposed framework EigenGCN with EigenPooling can achieve state-of-the-art performance in graph classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Understanding Graph Signals</head><p>In this subsection, we investigate the distribution of the Fourier coefficients on signals in real data. We aim to show that for natural graph signals, most of the information/energy concentrates on the first few Fourier models (or eigenvectors). This paves us a way to only use H filters in EigenPooling. Specifically, given one data set, for each graph G i with N i nodes and its associated signal X i â R N i Ãd , we first calculate the graph Fourier transform and obtain the coefficientsX i â R N i Ãd . We then calculate the following ratio: r H i =   </p><p>Note that if H &gt; N i , we set r H i = 1. We visualize the ratio for each of the data set up to H = 40 in <ref type="figure" target="#fig_8">Figure 2</ref>. As shown in <ref type="figure" target="#fig_8">Figure 2</ref>, for most of the data set, the magnitude of the coefficients concentrated in the first few coefficients, which demonstrates the reasonableness of using only H âª N max filters in EigenPooling. In addition, using H filters can save computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>In recent years, graph neural network models, which try to extend deep neural network models to graph structured data, have attracted increasing interests. These graph neural network models have been applied to various applications in many different areas. In <ref type="bibr" target="#b21">[22]</ref>, a graph neural network model that tries to learn node representation by aggregating the node features from its neighbors, is applied to perform semi-supervised node classification. Similar methods were later proposed to further enhance the performance by including attention mechanism <ref type="bibr" target="#b41">[42]</ref>. GraphSage <ref type="bibr" target="#b47">[48]</ref>, which allows more flexible aggregation procedure, was designed for the same task. There are some graph neural networks models designed to reason the dynamics of physical systems where the model is applied to predict future states of nodes given their previous states <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Most of the aforementioned methods can fit in the framework of "message passing" neural networks <ref type="bibr" target="#b16">[17]</ref>, which mainly involves transforming, propagating and aggregating node features across the graph through edges. Another stream of graph neural networks was developed based on the graph Fourier transform <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>. The features are first transferred to the spectral domain, next filtered with learnable filters and then transferred back to the spatial domain. The connection between these two streams of works is shown in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>. Graph neural networks have also been extended to different types of graphs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> and applied to various applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Comprehensive surveys on graph neural networks can be found in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>However, the design of the graph neural network layers is inherently "flat", which means the output of pure graph neural network layers is node representations for all the nodes in the graph. To apply graph neural networks to the graph classification task, an approach to summarize the learned node representations and generate the graph representation is needed. A simple way to generate the graph representations is to globally combine the node representations. Different combination approaches have been investigated, which include averaging over all node representation as the graph representation <ref type="bibr" target="#b10">[11]</ref>, adding a "virtual node" connected to all the nodes in the graph and using its node representation as the graph representation <ref type="bibr" target="#b24">[25]</ref>, and using conventional fully connected layers or convolutional layers after arranging the graph to the same size <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>. However, these global pooling methods cannot hierarchically learn graph representations, thus ignoring important information in the graph structure. There are a few recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> investigating learning graph representations with a hierarchical pooling procedure. These methods usually involve two steps 1) coarsen a graph by grouping nodes into supernode to form a hierarchical structure and 2) learn supernode representations level by level and finally obtain the graph representation. These methods use mean-pooling or max-pooling when they generate supernodes representation, which neglects the important structure information in the subgraphs. In this paper, we propose a pooling operator based on local graph Fourier transform, which utilizes the subgraph structure as well as the node features for generating the supernode representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we design EigenPooling, a pooling operator based on local graph Fourier transform, which can extract subgraph information utilizing both node features and structure of the subgraph. We provide a theoretical analysis of the pooling operator from both local and global perspectives. The pooling operator together with a subgraph-based graph coarsening method forms the pooling layer, which can be incorporated into any graph neural networks to hierarchically learn graph level representations. We further proposed a graph neural network framework EigenGCN by combining the proposed pooling layers with the GCN convolutional layers. Comprehensive graph classification experiments were conducted on 6 commonly used graph classification benchmarks. Our proposed framework achieves state-of-the-art performance on most of the data sets, which demonstrates its effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 3 . 1 . 2 | |x| | 2 2</head><label>3122</label><figDesc>Let x be a graph signal defined on the graph G and U = [u 1 , . . . , u N ] be the Fourier modes of this graph. Letx be the corresponding Fourier coefficients, i.e.,x = U T x. Let x â² = H l =1x [l] Â· u l be the signal re-constructed using only the first H Fourier modes. Then | |x[1:H ] | | 2 can measure the information being preserved by this re-construction. Herex[1 : H ] denotes the vector consisting of the first H elements ofx. Proof. According to Eq.(10), x can be written as x = N l =1x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 3 . 2 .</head><label>32</label><figDesc>The graph signal x can be perfectly reconstructed from its filtered signals {x l } N max l =1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>From Lemma 3 . 2 ,</head><label>32</label><figDesc>we know if N max number of filters are chosen, the filtered signals {x l } N max l =1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 3 . 3 .</head><label>33</label><figDesc>All the columns in the operators {Î l } N max l =1 are orthogonal to each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>l</head><label></label><figDesc>, l = 1, . . . N max are orthogonal to each other, which meansÅ« (k ) l , l = 1, . . . N max are also orthogonal to each other. In addition, all theÅ«</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>Understanding graph signals set and obtain r H = i r H i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets</figDesc><table><row><cell></cell><cell cols="2">ENZYMES PROTEINS</cell><cell>D&amp; D</cell><cell cols="3">NCI1 NCI109 MUTAG</cell></row><row><cell># graphs</cell><cell>600</cell><cell>1,113</cell><cell>1,178</cell><cell>4,110</cell><cell>4,127</cell><cell>4,337</cell></row><row><cell>mean |V|</cell><cell>32.63</cell><cell>39.06</cell><cell cols="2">284.32 29.87</cell><cell>29.68</cell><cell>30.32</cell></row><row><cell># classes</cell><cell>6</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Data sets</cell><cell></cell><cell></cell></row><row><cell>Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">ENZYMES PROTEINS</cell><cell>D&amp;D</cell><cell>NCI1</cell><cell cols="2">NCI109 MUTAG</cell></row><row><cell>GCN</cell><cell>0.440</cell><cell>0.740</cell><cell>0.759</cell><cell>0.725</cell><cell>0.707</cell><cell>0.780</cell></row><row><cell>GraphSage</cell><cell>0.554</cell><cell>0.746</cell><cell>0.766</cell><cell>0.732</cell><cell>0.703</cell><cell>0.785</cell></row><row><cell>SET2SET</cell><cell>0.380</cell><cell>0.727</cell><cell>0.745</cell><cell>0.715</cell><cell>0.686</cell><cell>0.764</cell></row><row><cell>DGCNN</cell><cell>0.410</cell><cell>0.732</cell><cell>0.778</cell><cell>0.729</cell><cell>0.723</cell><cell>0.788</cell></row><row><cell>Diff-pool</cell><cell>0.636</cell><cell>0.759</cell><cell>0.780</cell><cell>0.760</cell><cell>0.741</cell><cell>0.806</cell></row><row><cell>EigenGCN-1</cell><cell>0.650</cell><cell>0.751</cell><cell>0.775</cell><cell>0.760</cell><cell>0.746</cell><cell>0.801</cell></row><row><cell>EigenGCN-2</cell><cell>0.645</cell><cell>0.754</cell><cell>0.770</cell><cell>0.767</cell><cell>0.748</cell><cell>0.789</cell></row><row><cell>EigenGCN-3</cell><cell>0.645</cell><cell>0.766</cell><cell cols="2">0.786 0.770</cell><cell>0.749</cell><cell>0.795</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>SchÃ¶nauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Signal denoising on graphs via graph filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kovacevic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In GlobalSIP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><forename type="middle">Chung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
		<title level="m">Spectral graph theory. Number 92</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">MichaÃ«l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Signed graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="929" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMB</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">AlÃ¡n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07243</idno>
		<title level="m">Graph Neural Networks for Social Recommendation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SplineCNN: Fast geometric deep learning with continuous B-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph representation learning via hard and channel-wise attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph U-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 36th International Conference on Machine Learning</title>
		<meeting>The 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Derivation and validation of toxicophores for mutagenicity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Kazius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><surname>Bursi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMC</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Benchmark Data Sets for Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="http://graphkernels.cs.tu-dortmund.de" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10627</idno>
		<title level="m">Dynamic graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-dimensional Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chara</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 SIAM International Conference on Data Mining. SIAM</title>
		<meeting>the 2019 SIAM International Conference on Data Mining. SIAM</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="657" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perfect reconstruction two-channel wavelet filter banks for graph structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="2786" to="2799" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">IAM graph database repository for graph based pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaspar</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR International Workshops on SPR and (SSPR)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01242</idno>
		<title level="m">Graph networks as learnable physics engines for inference and control</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>JosÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1644" to="1656" />
		</imprint>
	</monogr>
	<note>n. d.. n. d.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BRENDA, the enzyme database: updates and major new developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ida</forename><surname>Schomburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antje</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Gremse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Schomburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="431" to="433" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Subgraph-based filterbanks for graph signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Borgnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="3827" to="3840" />
		</imprint>
	</monogr>
	<note>n. d.. n. d.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Know-evolve: Deep temporal reasoning for dynamic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>VeliÄkoviÄ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph Attention Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via semantic embeddings and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6857" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An End-to-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<title level="m">Deep learning on graphs: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph Neural Networks: A Review of Methods and Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
