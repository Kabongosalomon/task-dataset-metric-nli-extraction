<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REVISITING DISTRIBUTIONAL CORRESPONDENCE INDEXING: A PYTHON REIMPLEMENTATION AND NEW EXPERIMENTS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-10-23">October 23, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Istituto di Scienza e Tecnologie dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Istituto di Scienza e Tecnologie dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Istituto di Scienza e Tecnologie dell&apos;Informazione Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<postCode>56124</postCode>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">REVISITING DISTRIBUTIONAL CORRESPONDENCE INDEXING: A PYTHON REIMPLEMENTATION AND NEW EXPERIMENTS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-23">October 23, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Transfer Learning · Domain Adaptation · Text Classification · Sentiment Classification · Cross-Domain Classification · Cross-Lingual Classification · Python</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces PyDCI, a new implementation of Distributional Correspondence Indexing (DCI) written in Python. DCI is a transfer learning method for cross-domain and cross-lingual text classification for which we had provided an implementation (here called JaDCI) built on top of JaTeCS, a Java framework for text classification. PyDCI is a stand-alone version of DCI that exploits scikit-learn and the SciPy stack. We here report on new experiments that we have carried out in order to test PyDCI, and in which we use as baselines new high-performing methods that have appeared after DCI was originally proposed. These experiments show that, thanks to a few subtle ways in which we have improved DCI, PyDCI outperforms both JaDCI and the above-mentioned high-performing methods, and delivers the best known results on the two popular benchmarks on which we had tested DCI, i.e., MultiDomainSentiment (a.k.a. MDS -for cross-domain adaptation) and Webis-CLS-10 (for cross-lingual adaptation). PyDCI, together with the code allowing to replicate our experiments, is available at https://github.com/AlexMoreo/pydci . With respect to JaDCI, PyDCI introduces a few modifications in the way DCI is implemented that, although subtle, bring about a significant improvement in the effectiveness of the method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional Correspondence Indexing (DCI) is a pivot-based feature-transfer domain adaptation method for crossdomain and cross-lingual text classification. DCI was first described in <ref type="bibr" target="#b1">[Esuli and Moreo, 2015]</ref>, and later improved and extended in <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref>; it was formerly implemented in Java as part of the JaTeCS (Java Text Categorization System) framework <ref type="bibr" target="#b2">[Esuli et al., 2017]</ref>, and this implementation (henceforth called JaDCI) was made publicly available. 1 JaTeCS is a complex package, since it makes available many functionalities for text analytics research. A drawback of JaDCI is thus that, for the researcher wishing to replicate the results of <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref> or simply wishing to use JaDCI, a substantive effort in installing and properly configuring the entire JaTeCS framework is thus needed.</p><p>In this paper we present PyDCI, a new implementation of the DCI method written in Python and built on top of the SciPy stack and scikit-learn toolkit. Python has become the preferred programming language for computer scientists. In the fields of machine learning and data mining its use has also been promoted by the appearance of Python-based environments such as SciPy and scikit-learn, whose potential and ease of use have attracted the interest of practitioners. Our reimplementation is thus in line with these trends.</p><p>The rest of this paper is structured as follows. In Section 2 we describe the main modifications to DCI that our new implementation introduces. In Section 3 we report on new experiments that we have run using PyDCI, and show that, thanks to the modifications above, PyDCI delivers new state-of-the-art results on two popular benchmark datasets, i.e., MultiDomainSentiment (hereafter MDS -for cross-domain adaptation) and Webis-CLS-10 (for cross-lingual adaptation). These results represent a clear improvement over the ones originally obtained with JaDCI and presented in <ref type="bibr" target="#b6">Moreo et al. [2016a]</ref>, and also over the ones obtained by recent high-performing methods that have appeared <ref type="bibr" target="#b3">Ganin et al. [2016]</ref>, <ref type="bibr" target="#b5">Li et al. [2017]</ref>, <ref type="bibr" target="#b11">Xu and Yang [2017]</ref>, <ref type="bibr" target="#b13">Zhou et al. [2016]</ref>, or that we have become aware of <ref type="bibr" target="#b12">Yang et al. [2015]</ref>, after DCI was originally proposed. Section 4 concludes, hinting at future developments.</p><p>We make PyDCI publicly available via GitHub. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Implementation Changes</head><p>For reasons of brevity we do not re-explain DCI from scratch; we refer the interested reader to <ref type="bibr" target="#b8">[Moreo et al., 2018]</ref> for a concise description, or to <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref> for the full-blown presentation.</p><p>The main modifications that PyDCI introduces with respect to JaDCI are the following:</p><p>1. Document Standardization: In DCI, feature vectors and document vectors (i.e., the vectors that represent the features and the vectors that represent the documents, respectively) are post-processed via L2-normalization.</p><p>In <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref> we had witnessed improvements when applying standardization to the feature vectors (i.e., translating and scaling each dimension so that it is approximatelly normally distributed in N (0, 1) -see <ref type="bibr">[Moreo et al., 2016a, p. 144]</ref>). In PyDCI we give the user the option to apply standardization also to each dimension of the document vectors before training the classifier. All experiments we report in this paper are run with this option activated. 2. Classifier Optimization: In PyDCI we use scikit-learn's implementation of linear SVMs (LinearSVC, which is in turn based on the liblinear package 3 ), instead of using Joachims' SVM light package 4 as we had done in JaDCI. This allows us to leverage scikit-learn's GridSearchCV utility in order to optimize SVM's C parameter (which determines the trade-off between training error and the margin) via grid search optimization, which allows us to effortlessly tune the classifier. In the new experiments using PyDCI we let parameter C range in {10 i } −5≤i≤5 , while in the JaDCI experiments we had simply relied on the default value that SVM light attributes to C. 3. Increase in the Number of Pivots: We increase the number of pivots from 100 (the value we had used in <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref>) to 1,000 in the cross-domain experiments and to 450 in the cross-lingual experiments. This brings about a significant improvement in performance, that does not come at a significant cost in execution time (as instead had happened with the previous implementation). We limit the number of pivots to 450 in the cross-lingual case (instead of 1,000) since in this case each pivot requires a translation to the target language 5 which is assumed to have a cost; we thus set the number of pivots to 450 as was done in previous research (e.g., in <ref type="bibr" target="#b9">[Prettenhofer and Stein, 2010]</ref>). We discuss below in more detail the impact on performance that the variation in the number of pivots has.</p><p>We should also mention that PyDCI relies on scikit-learn (while JaDCI relied on JaTeCS) for many preprocessingrelated aspects (e.g., term weighting), which also may cause some (hard to track) differences in performance with respect to JaDCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effectiveness on Cross-Domain Classification and Cross-Lingual Classification</head><p>In this section we report the results we have obtained in rerunning with PyDCI the same experiments we had run with JaDCI, and whose results had been reported in <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref>. The datasets we use are arguably the most popular  <ref type="bibr" target="#b0">[Blitzer et al., 2007]</ref> for cross-domain adaptation 6 and Webis-CLS-10 <ref type="bibr" target="#b9">[Prettenhofer and Stein, 2010]</ref> for cross-lingual adaptation 7 . A complete description of the datasets and the standard experimental protocol followed in each case can be found either in the original publications describing the datasets <ref type="bibr">[Blitzer et al., 2007, Prettenhofer and</ref><ref type="bibr" target="#b9">Stein, 2010]</ref> or in <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref>. <ref type="table" target="#tab_2">Tables 1 and 2</ref> show the values of classification accuracy (i.e., the fraction of correctly classified documents) we obtain for cross-domain and cross-lingual classification experiments, respectively. We focus on Linear and Cosine (columns 9-10), two parameter-free probabilistic and kernel-based distributional correspondence functions (DCFs) investigated in <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref>. For each such DCF we show a direct comparison against the values we had obtained with JaDCI (Columns 7-8). We also report two baselines:</p><p>• LOWER (Column 3), a classifier that directly trains on the "source" training examples and tests on the "target" unlabeled examples without performing any sort of adaptation at all. Such a classifier should thus act as a lower bound for any reasonable adaptation endeavour.</p><p>• UPPER (Column 4), a classifier that trains on the "target" training examples and tests on the "target" unlabeled examples without performing any sort of adaptation at all. 8 Such a classifier should thus act as an upper bound for any reasonable adaptation endeavour.</p><p>The baselines use exactly the same learner we use for PyDCI (LinearSVC with the C parameter optimized via grid search). For each (problem, dataset) pair we also report the accuracy obtained by what, to the best of our knowledge, is today the best-performing known method on this (problem, dataset) pair (Column 5 -labelled as "SOTA", which stands for "State Of The Art" -reports the name of the method and Column 6 reports the accuracy score, taken from the original paper). Boldface indicates the best score for each (problem, dataset) pair; shadowed cells indicate the PyDCI scores that outperform the best-known results.</p><p>Note that, aside from SDA <ref type="bibr" target="#b4">[Glorot et al., 2011]</ref>, all the baselines in the "SOTA" column had not been used as baselines in our original work on DCI; the reason is that these methods were published after DCI appeared in print <ref type="bibr" target="#b3">[Ganin et al., 2016</ref><ref type="bibr" target="#b5">, Li et al., 2017</ref><ref type="bibr" target="#b11">, Xu and Yang, 2017</ref><ref type="bibr" target="#b13">, Zhou et al., 2016</ref>, or that we were unaware of them <ref type="bibr" target="#b12">[Yang et al., 2015]</ref>.</p><p>PyDCI outperforms JaDCI in most cases, and outperforms also the best-performing method in the literature, which is not always the same for each (problem,dataset) pair, with very few exceptions. PyDCI obtains 7 out of 13 best results on MDS (including best averaged accuracy) when equipped with the Cosine DCF, and 5 out of 10 best results in Webis-CLS-10 when using the Linear DCF (including best averaged accuracy). In agreement with with <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref>, Cosine proved the best performing DCF, yielding the best results overall and surpassing the best accuracy obtained by any other method in 17 cases out of 23 (across the two datasets, and also including the average results). With respect to the previously best-performing system, PyDCI(Cosine) brings about a reduction in error of +10.2% on MDS and +9.6% on Webis-CLS-10. 6 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/ 7 https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/data/ corpus-webis-cls-10/ 8 In MDS there is only one labelled set available for each domain (see <ref type="bibr" target="#b0">[Blitzer et al., 2007]</ref>). In this case we report the accuracy of a 5-fold cross-validation on the test set.   <ref type="table">Table 3</ref>: Average accuracy obtained using PyDCI (using the Cosine DCF) with or without document standardization and classifier optimization; percentages indicate relative improvement of the "with" configuration with respect to the corresponding "without" configuration.</p><p>On the very same (problem,dataset) pairs we have also run experiments in order to evaluate the impact of modifications 1 (Document Standardization) and 2 (Classifier Optimization) mentioned in Section 2. Concerning document standardization, we have rerun all the PyDCI experiments described in <ref type="table" target="#tab_2">Tables 1 and 2</ref> without applying document standardization.</p><p>The results are reported in the first two rows of <ref type="table">Table 3</ref>, and indicate, on average, a relative improvement in accuracy of +0.2% on MDS and +8.6% on Webis-CLS-10; document standardization thus appears to be clearly beneficial.</p><p>Concerning classifier optimization, we have rerun all the PyDCI experiments described in <ref type="table" target="#tab_2">Tables 1 and 2</ref> without applying classifier optimization. The results are reported in the last two rows of <ref type="table">Table 3</ref>, and indicate, on average, a relative improvement in accuracy of +9.7% on MDS and +11.8% on Webis-CLS-10; also classifier optimization is thus (unsurprisingly) clearly beneficial. 9 <ref type="table">Table 4</ref> reports classification accuracy values obtained in the domain adaptation setting proposed in <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref>, in which both domain and language differ between the source and target (i.e., when the classification task is simultaneously cross-domain and cross-lingual). In <ref type="table">Table 4</ref> we include the results we had obtained in <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref> for the Cross-Lingual Structural Correspondence Learning (SCL) method <ref type="bibr" target="#b9">[Prettenhofer and Stein, 2010]</ref> (which we use here as a baseline), using its authors' code 10 (see <ref type="bibr" target="#b10">[Prettenhofer and Stein, 2011]</ref>). The reason why we use SCL as a baseline is that, although newer approaches have been tested in this setting, none of them, to the best of our knowledge, has outperformed SCL so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effectiveness on Cross-Domain Cross-Lingual Classification</head><p>The results in <ref type="table">Table 4</ref> confirm the superiority of PyDCI over JaDCI. In this case, though, the differences in performance between the "Cosine" counterparts is less pronounced. Between the PyDCI variants, Linear performs slightly better than Cosine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistical Significance</head><p>We have subjected our experiments to thorough statistical significance testing, by running a two-tailed t-test on paired examples across all runs (cross-domain and/or cross-lingual). The test reveals that the PyDCI versions of Linear and 9 Note that the results obtained by PyDCI without document standardization and classifier optimization are different from the ones obtained by JaDCI, the main reason being that SVM light and LinearSVC choose different default parameters for their SVM learner. 10 https://github.com/pprett/nut  <ref type="table">Table 4</ref>: Cross-domain cross-lingual classification on the Webis-CLS-10 dataset. In the first two columns, for conciseness we use the following notation: E (English), G (German), F (French), and J (Japanese) for languages; and B (Books), D (DVD), and M (Music) for domains. E.g., GB stands for German-Books.</p><p>Cosine outperform, in a statistically significant sense, the corresponding JaDCI versions (at a confidence level of α = 0.005).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Efficiency</head><p>One important aspect of DCI in general, and of PyDCI in particular, is its efficiency. <ref type="figure" target="#fig_0">Figure 1</ref> reports the computation times we have recorded 11 in order to measure the efficiency of PyDCI. While the best-performing methods from the literature rely on computationally expensive optimizations (most of them are deep-learning-based), none of the experiments we have presented so far required more than 35 seconds to run. The time for preprocessing the documents is not included. Results for MDS are averages across 5 folds (see <ref type="bibr" target="#b0">[Blitzer et al., 2007]</ref> for further details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effectiveness vs. Efficiency Trade-off</head><p>In this section we analyse the trade-off between effectiveness (in terms of classification accuracy) and time efficiency (in terms of seconds). In this experiment, we vary the number of pivots in the range <ref type="bibr">[10,</ref><ref type="bibr">25,</ref><ref type="bibr">50,</ref><ref type="bibr">100,</ref><ref type="bibr">250,</ref><ref type="bibr">500,</ref><ref type="bibr">1000,</ref><ref type="bibr">1500,</ref><ref type="bibr">2000,</ref><ref type="bibr">2500,</ref><ref type="bibr">5000]</ref>. For the Webis-CLS-10 we bound this range to 1500 pivots since, for some tasks it was impossible to extract more than 1500 pivots. <ref type="figure" target="#fig_1">Figure 2</ref> shows the average accuracy (left) and computation times for MDS and Webis-CLS-10. Cross-lingual Classification in Webis-CLS-10 As npivots increases, PyDCI surpasses the best average accuracy reported for any other method in both datasets. In particular, and in accordance with <ref type="bibr" target="#b6">[Moreo et al., 2016a]</ref>, PyDCI equipped with the Cosine DCF does so with only 100 pivots. In this case, and in contrast with JaDCI, classification accuracy increases noticeably when more pivots are taken into account; this might be a side effect of the modifications discussed in Section 2. In any case, the method seems to reach a plateau for higher values of npivots, allowing the Cosine variant to reach new peaks of classification accuracy of 0.839 (when npivots = 2500) in MDS, and 0.840 (when npivots = 1000) in Webis-CLS-10. Regarding the efficiency of the method, PyDCI exhibits a quasi-linear trend in time complexity, e.g., when the number of pivots is doubled, the execution time is roughly doubled too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have presented PyDCI, a (Python-based) revision of our previous (Java-based) implementation of DCI. This new implementation incorporates changes that, although subtle, nonetheless allow the method to deliver improved results that outperform the currently known best-performing methods. The efficiency tests we have carried out speak clearly about the efficiency of PyDCI, which requires roughly half a minute to undertake any of the domain adaptation tasks in our experiments.</p><p>In a preliminary study DCI was also tested in transductive scenarios <ref type="bibr" target="#b7">[Moreo et al., 2016b]</ref>. PyDCI does not support transductive classification; this is something we plan to address in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Computation times for the MDS (left) and Webis-CLS-10 (right) datasets. Values include the time required for pivot selection (Pivot), DCI projection (DCI), and SVM training and optimization (SVM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Effectiveness (left) and efficiency (right) of PyDCI as a function of the number of pivots, for cross-domain (top) and cross-lingual (bottom) sentiment classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Cross-lingual classification on the Webis-CLS-10 dataset.</figDesc><table><row><cell>MDS</cell><cell>Webis-CLS-10</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/AlexMoreo/pydci 3 https://www.csie.ntu.edu.tw/~cjlin/liblinear/ 4 http://svmlight.joachims.org/ 5 A word translator oracle is used to automate the translation work in the experiments, following the indications of<ref type="bibr" target="#b9">[Prettenhofer and Stein, 2010]</ref> and the bilingual dictionaries they released.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">The experiments were run on a machine equipped with a 8-core processor AMD FX-8350 at 4GHz with 32 GB of RAM under Ubuntu 16.04 (LTS).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007)</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007)<address><addrLine>Prague, CZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional correspondence indexing for cross-language text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th European Conference on Information Retrieval (ECIR 2015)</title>
		<meeting>the 37th European Conference on Information Retrieval (ECIR 2015)<address><addrLine>Wien, AT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">JaTeCS: An open-source Java Text Categorization System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Fagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06802</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning<address><addrLine>Bellevue, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end adversarial memory network for crossdomain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017)</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence (IJCAI 2017)<address><addrLine>Melbourne, AU</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2237" to="2243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributional correspondence indexing for cross-lingual and cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="131" to="163" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transductive distributional correspondence indexing for cross-domain topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Italian Information Retrieval Workshop (IIR 2016)</title>
		<meeting>the 7th Italian Information Retrieval Workshop (IIR 2016)<address><addrLine>Venezia, IT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributional correspondence indexing for cross-lingual and cross-domain sentiment classification (Extended Abstract)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/802</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI 2018)</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence (IJCAI 2018)<address><addrLine>Stockholm, SE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5647" to="5651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-language text classification using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010)<address><addrLine>Uppsala, SE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-lingual adaptation using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-lingual distillation for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-domain feature learning in multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="78" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment classification with bilingual document representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1403" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
