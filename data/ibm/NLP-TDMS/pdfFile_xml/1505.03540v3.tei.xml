<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Brain Tumor Segmentation with Deep Neural Networks $</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Sherbrooke</orgName>
								<address>
									<settlement>Sherbrooke</settlement>
									<region>Qc</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">bÉ cole Normale supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Davy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">eÉ cole Polytechnique de Montréal</orgName>
								<orgName type="institution">dÉ cole polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country>France, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Biard</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">eÉ cole Polytechnique de Montréal</orgName>
								<orgName type="institution">dÉ cole polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country>France, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">eÉ cole Polytechnique de Montréal</orgName>
								<orgName type="institution">dÉ cole polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country>France, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">eÉ cole Polytechnique de Montréal</orgName>
								<orgName type="institution">dÉ cole polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country>France, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">eÉ cole Polytechnique de Montréal</orgName>
								<orgName type="institution">dÉ cole polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country>France, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Sherbrooke</orgName>
								<address>
									<settlement>Sherbrooke</settlement>
									<region>Qc</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">bÉ cole Normale supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Sherbrooke</orgName>
								<address>
									<settlement>Sherbrooke</settlement>
									<region>Qc</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">bÉ cole Normale supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<address>
									<settlement>Twitter</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Brain Tumor Segmentation with Deep Neural Networks $</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Brain tumor segmentation, deep neural networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data.</p><p>We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test dataset reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the United States alone, it is estimated that 23,000 new cases of brain cancer will be diagnosed in 2015 <ref type="bibr" target="#b1">2</ref> . While gliomas are the most common brain tumors, they can be less aggressive (i.e. low grade) in a patient with a life expectancy of several years, or more aggressive (i.e. high grade) in a patient with a life expectancy of at most 2 years.</p><p>Although surgery is the most common treatment for brain tumors, radiation and chemotherapy may be used to slow the growth of tumors that cannot be physically removed. Magnetic resonance imaging (MRI) provides detailed images of the brain, and is one of the most common tests used to diagnose brain tumors. All the more, brain tumor segmentation from MR images can have great impact for improved diagnostics, growth rate prediction and treatment planning.</p><p>While some tumors such as meningiomas can be easily segmented, others like gliomas and glioblastomas are much more difficult to localize. These tumors (together with their surrounding edema) are often diffused, poorly contrasted, and extend tentacle-like structures that make them difficult to segment. Another fundamental difficulty with segmenting brain tumors is that they can appear anywhere in the brain, in almost any shape and size. Furthermore, unlike images derived from X-ray computed tomography (CT) scans, the scale of voxel values in MR images is not standardized. Depending on the type of MR machine used (1.5, 3 or 7 tesla) and the acquisition protocol (field of view value, voxel resolution, gradient strength, b0 value, etc.), the same tumorous cells may end up having drastically different grayscale values when pictured in different hospitals.</p><p>Healthy brains are typically made of 3 types of tissues: the white matter, the gray matter, and the cerebrospinal fluid. The goal of brain tumor segmentation is to detect the location and extension of the tumor regions, namely active tumorous tissue (vascularized or not), necrotic tissue, and edema (swelling near the tumor). This is done by identifying abnormal areas when compared to normal tissue. Since glioblastomas are infiltrative tumors, their borders are often fuzzy and hard to distinguish from healthy tissues. As a solution, more than one MRI modality is often employed, e.g. T1 (spin-lattice relaxation), T1-contrasted (T1C), T2 (spin-spin relaxation), proton density (PD) contrast imaging, diffusion MRI (dMRI), and fluid attenuation inversion recovery (FLAIR) pulse sequences. The con-trast between these modalities gives almost a unique signature to each tissue type.</p><p>Most automatic brain tumor segmentation methods use handdesigned features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>. These methods implement a classical machine learning pipeline according to which features are first extracted and then given to a classifier whose training procedure does not affect the nature of those features. An alternative approach for designing task-adapted feature representations is to learn a hierarchy of increasingly complex features directly from in-domain data. Deep neural networks have been shown to excel at learning such feature hierarchies <ref type="bibr" target="#b6">[7]</ref>. In this work, we apply this approach to learn feature hierarchies adapted specifically to the task of brain tumor segmentation that combine information across MRI modalities.</p><p>Specifically, we investigate several choices for training Convolutional Neural Networks (CNNs), which are Deep Neural Networks (DNNs) adapted to image data. We report their advantages, disadvantages and performance using well established metrics. Although CNNs first appeared over two decades ago <ref type="bibr" target="#b28">[29]</ref>, they have recently become a mainstay of the computer vision community due to their record-shattering performance in the ImageNet Large-Scale Visual Recognition Challenge <ref type="bibr" target="#b26">[27]</ref>. While CNNs have also been successfully applied to segmentation problems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>, most of the previous work has focused on non-medical tasks and many involve architectures that are not well suited to medical imagery or brain tumor segmentation in particular. Our preliminary work on using convolutional neural networks for brain tumor segmentation together with two other methods using CNNs was presented in BRATS'14 workshop. However, those results were incomplete and required more investigation (More on this in chapter 2).</p><p>In this paper, we propose a number of specific CNN architectures for tackling brain tumor segmentation. Our architectures exploit the most recent advances in CNN design and training techniques, such as Maxout <ref type="bibr" target="#b17">[18]</ref> hidden units and Dropout <ref type="bibr" target="#b41">[42]</ref> regularization. We also investigate several architectures which take into account both the local shape of tumors as well as their context.</p><p>One problem with many machine learning methods is that they perform pixel classification without taking into account the local dependencies of labels (i.e. segmentation labels are conditionally independent given the input image). To account for this, one can employ structured output methods such as conditional random fields (CRFs), for which inference can be computationally expensive. Alternatively, one can model label dependencies by considering the pixel-wise probability estimates of an initial CNN as additional input to certain layers of a second DNN, forming a cascaded architecture. Since convolutions are efficient operations, this approach can be significantly faster than implementing a CRF.</p><p>We focus our experimental analysis on the fully-annotated MICCAI brain tumor segmentation (BRATS) challenge 2013 dataset <ref type="bibr" target="#b14">[15]</ref> using the well defined training and testing splits, thereby allowing us to compare directly and quantitatively to a wide variety of other methods.</p><p>Our contributions in this work are four fold:</p><p>1. We propose a fully automatic method with results currently ranked second on the BRATS 2013 scoreboard; 2. To segment a brain, our method takes between 25 seconds and 3 minutes, which is one order of magnitude faster than most state-of-the-art methods. 3. Our CNN implements a novel two-pathway architecture that learns about the local details of the brain as well as the larger context. We also propose a two-phase training procedure which we have found is critical to deal with imbalanced label distributions. Details of these contributions are described in Sections 3.1.1 and 3.2. 4. We employ a novel cascaded architecture as an efficient and conceptually clean alternative to popular structured output methods. Details on those models are presented in Section 3.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>As noted by Menze et al. <ref type="bibr" target="#b31">[32]</ref>, the number of publications devoted to automated brain tumor segmentation has grown exponentially in the last several decades. This observation not only underlines the need for automatic brain tumor segmentation tools, but also shows that research in that area is still a work in progress.</p><p>Brain tumor segmentation methods (especially those devoted to MRI) can be roughly divided in two categories: those based on generative models and those based on discriminative models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Generative models rely heavily on domain-specific prior knowledge about the appearance of both healthy and tumorous tissues. Tissue appearance is challenging to characterize, and existing generative models usually identify a tumor as being a shape or a signal which deviates from a normal (or average) brain <ref type="bibr" target="#b8">[9]</ref>. Typically, these methods rely on anatomical models obtained after aligning the 3D MR image on an atlas or a template computed from several healthy brains <ref type="bibr" target="#b11">[12]</ref>. A typical generative model of MR brain images can be found in Prastawa et al. <ref type="bibr" target="#b36">[37]</ref>. Given the ICBM brain atlas, the method aligns the brain to the atlas and computes posterior probabilities of healthy tissues (white matter, gray matter and cerebrospinal fluid) . Tumorous regions are then found by localizing voxels whose posterior probability is below a certain threshold. A post-processing step is then applied to ensure good spatial regularity. Prastawa et al. <ref type="bibr" target="#b37">[38]</ref> also register brain images onto an atlas in order to get a probability map for abnormalities. An active contour is then initialized on this map and iterated until the change in posterior probability is below a certain threshold. Many other active-contour methods along the same lines have been proposed <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36]</ref>, all of which depend on left-right brain symmetry features and/or alignment-based features. Note that since aligning a brain with a large tumor onto a template can be challenging, some methods perform registration and tumor segmentation at the same time <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Other approaches for brain tumor segmentation employ discriminative models. Unlike generative modeling approaches, these approaches exploit little prior knowledge on the brain's anatomy and instead rely mostly on the extraction of [a large number of] low level image features, directly modeling the relationship between these features and the label of a given voxel. These features may be raw input pixels values <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref>, local histograms <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref> texture features such as Gabor filterbanks <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43]</ref>, or alignment-based features such as interimage gradient, region shape difference, and symmetry analysis <ref type="bibr" target="#b32">[33]</ref>. Classical discriminative learning techniques such as SVMs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30]</ref> and decision forests <ref type="bibr" target="#b47">[48]</ref> have also been used. Results from the 2012, 2013 and 2014 editions of the MICCAI-BRATS Challenge suggest that methods relying on random forests are among the most accurate <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>One common aspect with discriminative models is their implementation of a conventional machine learning pipeline relying on hand-designed features. For these methods, the classifier is trained to separate healthy from non-heatlthy tissues assuming that the input features have a sufficiently high discriminative power since the behavior the classifier is independent from nature of those features. One difficulty with methods based on hand-designed features is that they often require the computation of a large number of features in order to be accurate when used with many traditional machine learning techniques. This can make them slow to compute and expensive memory-wise. More efficient techniques employ lower numbers of features, using dimensionality reduction or feature selection methods, but the reduction in the number of features is often at the cost of reduced accuracy.</p><p>By their nature, many hand-engineered features exploit very generic edge-related information, with no specific adaptation to the domain of brain tumors. Ideally, one would like to have features that are composed and refined into higher-level, taskadapted representations. Recently, preliminary investigations have shown that the use of deep CNNs for brain tumor segmentation makes for a very promising approach (see the BRATS 2014 challenge workshop papers of Davy et al. <ref type="bibr" target="#b10">[11]</ref>, Zikic et al. <ref type="bibr" target="#b48">[49]</ref>, Urban et al. <ref type="bibr" target="#b44">[45]</ref>). All three methods divide the 3D MR images into 2D <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b48">49]</ref> or 3D patches <ref type="bibr" target="#b44">[45]</ref> and train a CNN to predict its center pixel class. Urban et al. <ref type="bibr" target="#b44">[45]</ref> as well as Zikic et al. <ref type="bibr" target="#b48">[49]</ref> implemented a fairly common CNN, consisting of a series of convolutional layers, a non-linear activation function between each layer and a softmax output layer. Our work here 3 extends our preliminary results presented in Davy et al. <ref type="bibr" target="#b10">[11]</ref> using a two-pathway architecture, which we use here as a building block.</p><p>In computer vision, CNN-based segmentation models have typically been applied to natural scene labeling. For these tasks, the inputs to the model are the RGB channels of a patch from a color image. The work in Pinheiro and Collobert <ref type="bibr" target="#b34">[35]</ref> uses a basic CNN to make predictions for each pixel and further improves the predictions by using them as extra information in the input of a second CNN model. Other work <ref type="bibr" target="#b12">[13]</ref> involves several distinct CNNs processing the image at different resolutions. The final per-pixel class prediction is made by integrating information learned from all CNNs. To produce a smooth segmentation, these predictions are regularized using a more global superpixel segmentation of the image. Like our work, other recent work has exploited convolution operations in the final layer of a network to extend traditional CNN architectures for semantic scene segmentation <ref type="bibr" target="#b30">[31]</ref>. In the medical imaging domain in general there has been comparatively less work using CNNs for segmentation. However, some notable recent work by Huang and Jain <ref type="bibr" target="#b22">[23]</ref> has used CNNs to predict the boundaries of neural tissue in electron microscopy images. Here we explore an approach with similarities to the various approaches discussed above, but in the context of brain tumor segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Convolutional Neural Network Approach</head><p>Since the brains in the BRATS dataset lack resolution in the third dimension, we consider performing the segmentation slice by slice from the axial view. Thus, our model processes sequentially each 2D axial image (slice) where each pixel is associated with different image modalities namely; T1, T2, T1C and FLAIR. Like most CNN-based segmentation models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13]</ref>, our method predicts the class of a pixel by processing the M×M patch centered on that pixel. The input X of our CNN model is thus an M × M 2D patch with several modalities.</p><p>The main building block used to construct a CNN architecture is the convolutional layer. Several layers can be stacked on top of each other forming a hierarchy of features. Each layer can be understood as extracting features from its preceding layer into the hierarchy to which it is connected. A single convolutional layer takes as input a stack of input planes and produces as output some number of output planes or feature maps. Each feature map can be thought of as a topologically arranged map of responses of a particular spatially local nonlinear feature extractor (the parameters of which are learned), applied identically to each spatial neighborhood of the input planes in a sliding window fashion. In the case of a first convolutional layer, the individual input planes correspond to different MRI modalities (in typical computer vision applications, the individual input planes correspond to the red, green and blue color channels). In subsequent layers, the input planes typically consist of the feature maps of the previous layer.</p><p>Computing a feature map in a convolutional layer (see </p><formula xml:id="formula_0">O s = b s + r W sr * X r<label>(1)</label></formula><p>where X r is the r th input channel, W sr is the sub-kernel for that channel, * is the convolution operation and b s is a bias term <ref type="bibr" target="#b3">4</ref> . In other words, the affine operation being performed for each feature map is the sum of the application of R different 2-dimensional N × N convolution filters (one per input channel/modality), plus a bias term which is added pixel-wise to each resulting spatial position. Though the input to this operation is a M × M × R 3-dimensional tensor, the spatial topology being considered is 2-dimensional in the X-Y axial plane of the original brain volume. Whereas traditional image feature extraction methods rely on a fixed recipe (sometimes taking the form of convolution with a linear e.g. Gabor filter bank), the key to the success of convolutional neural networks is their ability to learn the weights and biases of individual feature maps, giving rise to data-driven, customized, task-specific dense feature extractors. These parameters are adapted via stochastic gradient descent on a surrogate loss function related to the misclassification error, with gradients computed efficiently via the backpropagation algorithm <ref type="bibr" target="#b39">[40]</ref>. Special attention must be paid to the treatment of border pixels by the convolution operation. Throughout our architecture, we employ the so-called valid-mode convolution, meaning that the filter response is not computed for pixel positions that are less than N/2 pixels away from the image border. An N × N filter convolved with an M × M input patch will result in a Q × Q output, where Q = M − N + 1. In <ref type="figure" target="#fig_1">Figure 1</ref>, M = 7, N = 3 and thus Q = 5. Note that the size (spatial width and height) of the kernels are hyperparameters that must be specified by the user. 2. Non-linear activation function: To obtain features that are non-linear transformations of the input, an element-wise non-linearity is applied to the result of the kernel convolution. There are multiple choices for this non-linearity, such as the sigmoid, hyperbolic tangent and rectified linear functions <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Recently, Goodfellow et al. <ref type="bibr" target="#b17">[18]</ref> proposed a Maxout nonlinearity, which has been shown to be particularly effective at modeling useful features. Maxout features are associated with multiple kernels W s . This implies each Maxout map Z s is associated with K feature maps : <ref type="figure" target="#fig_1">Figure 1</ref>, the Maxout maps are associated with K = 2 feature maps. Maxout fea-tures correspond to taking the max over the feature maps O, individually for each spatial position:</p><formula xml:id="formula_1">{O s , O s+1 , ..., O s+K−1 }. Note that in</formula><formula xml:id="formula_2">Z s,i, j = max O s,i, j , O s+1,i, j , ..., O s+K−1,i, j<label>(2)</label></formula><p>where i, j are spatial positions. Maxout features are thus equivalent to using a convex activation function, but whose shape is adaptive and depends on the values taken by the kernels. 3. Max pooling: This operation consists of taking the maximum feature (neuron) value over sub-windows within each feature map. This can be formalized as follows:</p><formula xml:id="formula_3">H s,i, j = max p Z s,i+p, j+p ,<label>(3)</label></formula><p>where p determines the max pooling window size. The sub-windows can be overlapping or not ( <ref type="figure" target="#fig_1">Figure 1</ref> shows an overlapping configuration). The max-pooling operation shrinks the size of the feature map. This is controlled by the pooling size p and the stride hyper-parameter, which corresponds to the horizontal and vertical increments at which pooling sub-windows are positioned. Let S be the stride value and Q × Q be the shape of the feature map before max-pooling. The output of the max-pooling operation would be of size D × D, where D = (Q − p)/S + 1.</p><p>In <ref type="figure" target="#fig_1">Figure 1</ref>, since Q = 5, p = 2, S = 1, the max-pooling operation results into a D = 4 output feature map. The motivation for this operation is to introduce invariance to local translations. This subsampling procedure has been found beneficial in other applications <ref type="bibr" target="#b26">[27]</ref>.</p><p>Convolutional networks have the ability to extract a hierarchy of increasingly complex features which makes them very appealing. This is done by treating the output feature maps of a convolutional layer as input channels to the subsequent convolutional layer.</p><p>From the neural network perspective, feature maps correspond to a layer of hidden units or neurons. Specifically, each coordinate within a feature map corresponds to an individual neuron, for which the size of its receptive field corresponds to the kernel's size. A kernel's value also represents the weights of the connections between the layer's neurons and the neurons in the previous layer. It is often found in practice that the learned kernels resemble edge detectors, each kernel being tuned to a different spatial frequency, scale and orientation, as is appropriate for the statistics of the training data.</p><p>Finally, to perform a prediction of the segmentation labels, we connect the last convolutional hidden layer to a convolutional output layer followed by a non-linearity (i.e. no pooling is performed). It is necessary to note that, for segmentation purposes, a conventional CNN will not yield an efficient test time since the output layer is typically fully connected. By using a convolution at the end, for which we have an efficient implementation, the prediction at test time for a whole brain will be 45 times faster. The convolution uses as many kernels as there are different segmentation labels (in our case five). Each kernel thus acts as the ultimate detector of tissue from one of the segmentation labels. We use the softmax nonlinearity which normalizes the result of the kernel convolutions into a multinominal distribution over the labels. Specifically, let a be the vector of values at a given spatial position, it computes softmax(a) = exp(a)/Z where Z = c exp(a c ) is a normalization constant. More details will be discussed in Section 4.</p><p>Noting Y as the segmentation label field over the input patch X, we can thus interpret each spatial position of the convolutional output layer as providing a model for the likelihood distribution p(Y i j |X), where Y i j is the label at position i, j. We get the probability of all labels simply by taking the product of each conditional p(Y|X) = i j p(Y i j |X). Our approach thus performs a multiclass labeling by assigning to each pixel the label with the largest probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Architectures</head><p>Our description of CNNs so far suggests a simple architecture corresponding to a single stack of several convolutional layers. This configuration is the most commonly implemented architecture in the computer vision literature. However, one could imagine other architectures that might be more appropriate for the task at hand.</p><p>In this work, we explore a variety of architectures by using the concatenation of feature maps from different layers as another operation when composing CNNs. This operation allows us to construct architectures with multiple computational paths, which can each serve a different purpose. We now describe the two types of architectures that we explore in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Two-pathway architecture</head><p>This architecture is made of two streams: a pathway with smaller 7 × 7 receptive fields and another with larger 13 × 13 receptive fields. We refer to these streams as the local pathway and the global pathway, respectively. The motivation for this architectural choice is that we would like the prediction of the label of a pixel to be influenced by two aspects: the visual details of the region around that pixel and its larger "context", i.e. roughly where the patch is in the brain.</p><p>The full architecture along with its details is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. We refer to this architecture as the TwoPathCNN. To allow for the concatenation of the top hidden layers of both pathways, we use two layers for the local pathway, with 3 × 3 kernels for the second layer. While this implies that the effective receptive field of features in the top layer of each pathway is the same, the global pathway's parametrization more directly and flexibly models features in that same area. The concatenation of the feature maps of both pathways is then fed to the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Cascaded architectures</head><p>One disadvantage of the CNNs described so far is that they predict each segmentation label separately from each other. This is unlike a large number of segmentation methods in the literature, which often propose a joint model of the segmentation labels, effectively modeling the direct dependencies between spatially close labels. One approach is to define a conditional random field (CRF) over the labels and perform meanfield message passing inference to produce a complete segmen-tation. In this case, the final label at a given position is effectively influenced by the models beliefs about what the label is in the vicinity of that position.</p><p>On the other hand, inference in such joint segmentation methods is typically more computationally expensive than a simple feed-forward pass through a CNN. This is an important aspect that one should take into account if automatic brain tumor segmentation is to be used in a day-to-day practice.</p><p>Here, we describe CNN architectures that both exploit the efficiency of CNNs, while also more directly model the dependencies between adjacent labels in the segmentation. The idea is simple: since we'd like the ultimate prediction to be influenced by the model's beliefs about the value of nearby labels, we propose to feed the output probabilities of a first CNN as additional inputs to the layers of a second CNN. Again, we do this by relying on the concatenation of convolutional layers. In this case, we simply concatenate the output layer of the first CNN with any of the layers in the second CNN. Moreover, we use the same two-pathway structure for both CNNs. This effectively corresponds to a cascade of two CNNs, thus we refer to such models as cascaded architectures.</p><p>In this work, we investigated three cascaded architectures that concatenate the first CNN's output at different levels of the second CNN:</p><p>• Input concatenation: In this architecture, we provide the first CNN's output directly as input to the second CNN.</p><p>They are thus simply treated as additional image channels of the input patch. The details are illustrated in <ref type="figure" target="#fig_3">Figure 3a</ref>.</p><p>We refer to this model as InputCascadeCNN.</p><p>• Local pathway concatenation: In this architecture, we move up one layer in the local pathway and perform concatenation to its first hidden layer, in the second CNN. The details are illustrated in <ref type="figure" target="#fig_3">Figure 3b</ref>. We refer to this model as LocalCascadeCNN.</p><p>• Pre-output concatenation: In this last architecture, we move to the very end of the second CNN and perform concatenation right before its output layer. This architecture is interesting, as it is similar to the computations made by one pass of mean-field inference <ref type="bibr" target="#b45">[46]</ref> in a CRF whose pairwise potential functions are the weights in the output kernels. From this view, the output of the first CNN is the first iteration of mean-field, while the output of the second CNN would be the second iteration. The difference with regular mean-field however is that our CNN allows the output at one position to be influenced by its previous value, and the convolutional kernels are not the same in the first and second CNN. The details are illustrated in <ref type="figure" target="#fig_3">Figure 3c</ref>. We refer to this model as MFCascadeCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>Gradient Descent. By interpreting the output of the convolutional network as a model for the distribution over segmentation labels, a natural training criteria is to maximize the probability of all labels in our training set or, equivalently, to minimize the  negative log-probability − log p(Y|X) = i j − log p(Y i j |X) for each labeled brain.</p><p>To do this, we follow a stochastic gradient descent approach by repeatedly selecting labels Y i j at a random subset of patches within each brain, computing the average negative log-probabilities for this mini-batch of patches and performing a gradient descent step on the CNNs parameters (i.e. the kernels at all layers).</p><p>Performing updates based only on a small subset of patches allows us to avoid having to process a whole brain for each update, while providing reliable enough updates for learning. In practice, we implement this approach by creating a dataset of mini-batches of smaller brain image patches, paired with the corresponding center segmentation label as the target.</p><p>To further improve optimization, we implemented a so-called momentum strategy which has been shown successful in the past <ref type="bibr" target="#b26">[27]</ref>. The idea of momentum is to use a temporally averaged gradient in order to damp the optimization velocity:</p><formula xml:id="formula_4">V i+1 = µ * V i − α * ∇W i W i+1 = W i + V i+1</formula><p>where W i stands for the CNNs parameters at iteration i, ∇W i the gradient of the loss function at W i , V is the integrated velocity initialized at zero, α is the learning rate, and µ the momentum coefficient. We define a schedule for the momentum µ where the momentum coefficient is gradually increased during training. In our experiments the initial momentum coefficient was set to µ = 0.5 and the final value was set to µ = 0.9.</p><p>Also, the learning rate α is decreased by a factor at every epoch. The initial learning rate was set to α = 0.005 and the decay factor to 10 −1 .</p><p>Two-phase training. Brain tumor segmentation is a highly data imbalanced problem where the healthy voxels (i.e. label 0) comprise 98% of total voxels. From the remaining 2% pathological voxels, 0.18% belongs to necrosis (label 1), 1.1% to edema (label 2), 0.12% to non-enhanced (label 3) and 0.38% to enhanced tumor (label 4). Selecting patches from the true distribution would cause the model to be overwhelmed by healthy patches and causing problem when training out CNN models. Instead, we initially construct our patches dataset such that all labels are equiprobable. This is what we call the first training phase. Then, in a second phase, we account for the un-balanced nature of the data and re-train only the output layer (i.e. keeping the kernels of all other layers fixed) with a more representative distribution of the labels. This way we get the best of both worlds: most of the capacity (the lower layers) is used in a balanced way to account for the diversity in all of the classes, while the output probabilities are calibrated correctly (thanks to the re-training of the output layer with the natural frequencies of classes in the data).</p><p>Regularization. Successful CNNs tend to be models with a lot of capacity, making them vulnerable to overfitting in a setting like ours where there clearly are not enough training examples. Accordingly, we found that regularization is important in obtaining good results. Here, regularization took several forms. First, in all layers, we bounded the absolute value of the kernel weights and applied both L1 and L2 regularization to prevent overfitting. This is done by adding the regularization terms to the negative log-probability (i.e. − log p(Y|X) + λ 1 W 1 + λ 2 W 2 , where λ 1 and λ 2 are coefficients for L1 and L2 regularization terms respectively). L1 and L2 affect the parameters of the model in different ways, while L1 encourages sparsity, L2 encourages small values. We also used a validation set for early stopping, i.e. stop training when the validation performance stopped improving. The validation set was also used to tune the other hyper-parameters of the model. The reader shall note that the hyper-parameters of the model which includes using or not L2 and/or L1 coefficients were selected by doing a grid search over range of parameters. The chosen hyper-parameters were the ones for which the model performed best on a validation set.</p><p>Moreover, we used Dropout <ref type="bibr" target="#b41">[42]</ref>, a recent regularization method that works by stochastically adding noise in the computation of the hidden layers of the CNN. This is done by multiplying each hidden or input unit by 0 (i.e. masking) with a certain probability (e.g. 0.5), independently for each unit and training update. This encourages the neural network to learn (c) Cascaded architecture, using pre-output concatenation, which is an architecture with properties similar to that of learning using a limited number of mean-field inference iterations in a CRF (MFCascadeCNN). features that are useful "on their own", since each unit cannot assume that other units in the same layer won't be masked as well and co-adapt its behavior. At test time, units are instead multiplied by one minus the probability of being masked. For more details, see Srivastava et al. <ref type="bibr" target="#b41">[42]</ref>.</p><p>Considering the large number of parameters our model has, one might think that even with our regularization strategy, the 30 training brains from BRATS 2013 are too few to prevent overfitting. But as will be shown in the results section, our model generalizes well and thus do not overfit. One reason for this is the fact that each brain comes with 200 2d slices and thus, our model has approximately 6000 2D images to train on. We shall also mention that by their very nature, MRI images of brains are very similar from one patient to another. Since the variety of those images is much lower than those in realimage datasets such as CIFAR and ImageNet, a fewer number of training samples is thus needed.</p><p>Cascaded Architectures. To train a cascaded architecture, we start by training the TwoPathCNN with the two phase stochastic gradient descent procedure described previously. Then, we fix the parameters of the TwoPathCNN and include it in the cascaded architecture (be it the InputCascadeCNN, the Local-CascadeCNN, or the MFCascadeCNN) and move to training the remaining parameters using a similar procedure. It should be noticed however that for the spatial size of the first CNN's output and the layer of the second CNN to match, we must feed to the first CNN a much larger input. Thus, training of the second CNN must be performed on larger patches. For example in the InputCascadeCNN <ref type="figure" target="#fig_3">(Figure 3a)</ref>, the input size to the first model is of size 65 × 65 which results into an output of size 33 × 33. Only in this case the outputs of the first CNN can be concatenated with the input channels of the second CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation details</head><p>Our implementation is based on the Pylearn2 library <ref type="bibr" target="#b16">[17]</ref>.</p><p>Pylearn2 is an open-source machine learning library specializing in deep learning algorithms. It also supports the use of GPUs, which can greatly accelerate the execution of deep learning algorithms.</p><p>Since CNN's are able to learn useful features from scratch, we applied only minimal pre-processing. We employed the same pre-processing as Tustison et al., the winner of the 2013 BRATS challenge <ref type="bibr" target="#b31">[32]</ref>. The pre-processing follows three steps. First, the 1% highest and lowest intensities are removed. Then, we apply an N4ITK bias correction <ref type="bibr" target="#b2">[3]</ref> to T1 and T1C modalities. The data is then normalized within each input channel by subtracting the channel's mean and dividing by the channel's standard deviation.</p><p>As for post-processing, a simple method based on connected components was implemented to remove flat blobs which might appear in the predictions due to bright corners of the brains close to the skull.</p><p>The hyper-parameters of the different architectures (kernel and max pooling size for each layer and the number of layers) can be seen in <ref type="figure" target="#fig_3">Figure 3</ref>. Hyper-parameters were tuned using grid search and cross-validation on a validation set (see Bengio <ref type="bibr" target="#b5">[6]</ref>). The chosen hyper-parameters were the ones for which the model performed best on the validation set. For max pooling, we always use a stride of 1. This is to keep per-pixel accuracy during full image prediction. We observed in practice that max pooling in the global path does not improve accuracy. We also found that adding additional layers to the architectures or increasing the capacity of the model by adding additional feature maps to the convolutional blocks do not provide any meaningful performance improvement.</p><p>Biases are initialized to zero except for the softmax layer for which we initialized them to the log of the label frequencies.</p><p>The kernels are randomly initialized from U (−0.005, 0.005).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training takes about 3 minutes per epoch for the TwoPathCNN model on an NVIDIA Titan black card.</head><p>At test time, we run our code on a GPU in order to exploit its computational speed. Moreover, the convolutional nature of the output layer allows us to further accelerate computations at test time. This is done by feeding as input a full image and not individual patches. Therefore, convolutions at all layers can be extended to obtain all label probabilities p(Y i j |X) for the entire image. With this implementation, we are able to produce a segmentation in 25 seconds per brain on the Titan black card with the TwoPathCNN model. This turns out to be 45 times faster than when we extracted a patch at each pixel and processed them individually for the entire brain.</p><p>Predictions for the MFCascadeCNN model, the LocalCas-cadeCNN model, and InputCascadeCNN model take on average 1.5 minutes, 1.7 minutes and 3 minutes respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>The experiments were carried out on real patient data obtained from the 2013 brain tumor segmentation challenge (BRATS2013), as part of the MICCAI conference <ref type="bibr" target="#b14">[15]</ref>. The BRATS2013 dataset is comprised of 3 sub-datasets. The training dataset, which contains 30 patient subjects all with pixelaccurate ground truth (20 high grade and 10 low grade tumors); the test dataset which contains 10 (all high grade tumors) and the leaderboard dataset which contains 25 patient subjects (21 high grade and 4 low grade tumors). There is no ground truth provided for the test and leaderboard datasets. All brains in the dataset have the same orientation. For each brain there exists 4 modalities, namely T1, T1C, T2 and Flair which are coregistered. The training brains come with groundtruth for which 5 segmentation labels are provided, namely non-tumor, necrosis, edema, non-enhancing tumor and enhancing tumor. Please note that we could not use the BRATS 2014 dataset due to problems with both the system performing the evaluation and the quality of the labeled data. For these reasons T1 T2</p><p>T1-enhanced Flair GT <ref type="figure">Figure 4</ref>: The first four images from left to right show the MRI modalities used as input channels to various CNN models and the fifth image shows the ground truth labels where edema, enhanced tumor, necrosis, non-enhanced tumor.</p><p>the old BRATS 2014 dataset has been removed from the official website and, at the time of submitting this manuscript, the BRATS website still showed: "Final data for BRATS 2014 to be released soon". Furthermore, we have even conducted an experiment where we trained our model with the old 2014 dataset and made predictions on the 2013 test dataset; however, the performance was worse than our results mentioned in this paper. For these reasons, we decided to focus on the BRATS 2013 data. As mentioned in Section 3, we work with 2D slices due to the fact that the MRI volumes in the dataset do not posses an isotropic resolution and the spacing in the third dimension is not consistent across the data. We explored the use of 3D information (by treating the third dimension as extra input channels or by having an architecture which takes orthogonal slices from each view and makes the prediction on the intersecting center pixel), but that didn't improve performance and made our method very slow.</p><p>Note that as suggested by Krizhevsky et al. <ref type="bibr" target="#b26">[27]</ref>, we applied data augmentation by flipping the input images. Unlike what was reported by Zeiler and Fergus <ref type="bibr" target="#b46">[47]</ref>, it did not improve the overall accuracy of our model.</p><p>Quantitative evaluation of the models performance on the test set is achieved by uploading the segmentation results to the online BRATS evaluation system <ref type="bibr" target="#b13">[14]</ref>. The online system provides the quantitative results as follows: The tumor structures are grouped in 3 different tumor regions. This is mainly due to practical clinical applications. As described by Menze et al. <ref type="bibr" target="#b31">[32]</ref>, tumor regions are defined as:</p><p>a) The complete tumor region (including all four tumor structures).</p><p>b) The core tumor region (including all tumor structures exept "edema").</p><p>c) The enhancing tumor region (including the "enhanced tumor" structure).</p><p>For each tumor region, Dice (identical to F measure), Sensitivity and Specificity are computed as follows :</p><formula xml:id="formula_5">Dice(P, T ) = |P 1 ∧ T 1 | (|P 1 | + |T 1 |)/2 , S ensitivity(P, T ) = |P 1 ∧ T 1 | |T 1 | , S peci f icity(P, T ) = |P 0 ∧ T 0 | |T 0 | ,</formula><p>where P represents the model predictions and T represents the ground truth labels. We also note as T 1 and T 0 the subset of voxels predicted as positives and negatives for the tumor region in question. Similarly for P 1 and P 0 . The online evaluation system also provides a ranking for every method submitted for evaluation. This includes methods from the 2013 BRATS challenge published in <ref type="bibr" target="#b31">[32]</ref> as well as anonymized unpublished methods for which no reference is available. In this section, we report experimental results for our different CNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The TwoPathCNN architecture</head><p>As mentioned previously, unlike conventional CNNs, the TwoPathCNN architecture has two pathways: a "local" path focusing on details and a "global" path more focused on the context. To better understand how joint training of the global and local pathways benefits the performance, we report results on each pathway as well as results on averaging the outputs of each pathway when trained separately. Our method also deals with the unbalanced nature of the problem by training in two phases as discussed in Section 3.2. To see the impact of the two phase training, we report results with and without it. We refer to the CNN model consisting of only the local path (i.e. conventional CNN architecture) as LocalPathCNN, the CNN model consisting of only the global path as GlobalPathCNN, the model averaging the outputs of the local and global paths (i.e. LocalPathCNN and GlobalPathCNN) as AverageCNN and the two-pathway CNN architecture as TwoPathCNN. The second training phase is noted by appending '*' to the architecture name. Since the second phase training has a substantial effect and always improves the performance, we only report results on GlobalPathCNN and AverageCNN with the second phase. <ref type="table" target="#tab_2">Table 1</ref> presents the quantitative results of these variations. This table contains results for the TwoPathCNN with one and two training phases, the common single path CNN (i.e. Lo-calPathCNN) with one and two training phases, the Global-PathCNN* which is a single path CNN model following the global pathway architecture and the output average of each of the trained single-pathway models (AverageCNN*). Without much surprise, the single path with one training phase CNN was ranked last with the lowest scores on almost every region. Using a second training phase gave a significant boost to that model with a rank that went from 15 to 9. Also, the table shows that joint training of the local and global paths yields better performance compared to when each pathway is trained separately and the outputs are averaged. One likely explanation is that by joint training the local and global paths, the model allows the two pathways to co-adapt. In fact, the AverageCNN* performs worse than the LocalPathCNN* due to the fact that the Glob-alPathCNN* performs very badly. The top performing method in the uncascaded models is the TwoPathCNN* with a rank of 4.</p><p>Also, in some cases results are less accurate over the Enhancing region than for the Core and Complete regions. There are 2 main reasons for that. First, borders are usually diffused and there are no clear cut between enhanced tumor and nonenhanced tissues. This creates problems for both user labeling, ground truth, as well as the model. The second reason is that the model learns what it sees in the ground truth. Since the labels are created by different people and since the borders are not clear, each user has a slightly different interpretation of the borders of the enhanced tumor and so sometimes we see overly thick enhanced tumor in the ground truth. <ref type="figure" target="#fig_5">Figure 5</ref> shows representation of low level features in both local and global paths. As seen from this figure, features in the local path include more edge detectors while the ones in the global path are more localized features. Unfortunately, visualizing the learned mid/high level features of a CNN is still very much an open research problem. However, we can study the impact these features have on predictions by visualizing the segmentation results of different models. The segmentation results on two subjects from our validation set, produced by different variations of the basic model can be viewed in <ref type="figure" target="#fig_8">Figure 7</ref>  <ref type="bibr" target="#b4">5</ref> . As shown in the figure, the two-phase training procedure allows the model to learn from a more realistic distribution of labels and thus removes false positives produced by the model which trains with one training phase. Moreover, by having two pathways, the model can simultaneously learn the global contextual features as well as the local detailed features. This gives the advantage of correcting labels at a global scale as well as recognizing fine details of the tumor at a local scale, yielding a better segmentation as oppose to a single path architecture which results in smoother boundaries. Joint training of the two convolutional pathways and having two training phases achieves better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cascaded architectures</head><p>We now discuss our experiments with the three cascaded architectures namely InputCascadeCNN, LocalCascadeCNN and MFCascadeCNN. <ref type="table" target="#tab_3">Table 2</ref> provides the quantitative results for each architecture. <ref type="figure" target="#fig_8">Figure 7</ref> also provides visual examples of the segmentation generated by each architecture.</p><p>We find that the MFCascadeCNN* model yields smoother boundaries between classes. We hypothesize that, since the <ref type="bibr" target="#b4">5</ref> It is important to note that we do not train the model on the validation set and thus the quality of the results is not due to overfitting neurons in the softmax output layer are directly connected to the previous outputs within each receptive field, these parameters are more likely to learn that the center pixel label should have a similar label to its surroundings.</p><p>As for the LocalCascadeCNN* architecture, while it resulted in fewer false positives in the complete tumor category, the performance in other categories (i.e. tumor core and enhanced tumor) did not improve. <ref type="figure" target="#fig_6">Figure 8</ref> shows segmentation results from the same brains (as in <ref type="figure" target="#fig_8">Figure 7</ref>) in Sagittal and Coronal views. The InputCas-cadeCNN* model was used to produce these results. As seen from this figure, although the segmentation is performed on Axial view but the output is consistent in Coronal and Sagittal views. Although subjects in <ref type="figure" target="#fig_5">Figure 5</ref> and <ref type="figure" target="#fig_7">Figure 6</ref> are from our validation set for which the model is not trained on and the segmentation results from these subjects can give a good estimate of the models performance on a test set, however, for further clarity we visualise the models performance on two subjects from BRATS-2013 testst. These results are shown in <ref type="figure">Figure 9</ref> in Saggital (top) and Axial (bottom) views.</p><p>To better understand the process for which InputCas-cadeCNN* learns features, we present in <ref type="figure" target="#fig_7">Figure 6</ref> the progression of the model by making predictions at every few epochs on a subject from our validation set.</p><p>Overall, the best performance is reached by the InputCas-cadeCNN* model. It improves the Dice measure on all tumor regions. With this architecture, we were able to reach    the second rank on the BRATS 2013 scoreboard. While MF-CascadeCNN*, TwoPathCNN* and LocalCascadeCNN* are all ranked 4, the inner ranking between these three models is noted as 4a, 4b and 4c respectively.  <ref type="table" target="#tab_8">Table 4</ref> shows that our method outperforms other approaches on this dataset. We also compare our top performing method in <ref type="table" target="#tab_6">Table 5</ref> with state-of-the-art methods on BRATS-2012, "4 label" test set as mentioned in <ref type="bibr" target="#b31">[32]</ref>. As seen from this table, our method out performs other methods in the tumor Core category and gets competitive results on other categories.</p><p>Let us mention that Tustison's method takes 100 minutes to compute predictions per brain as reported in <ref type="bibr" target="#b31">[32]</ref>, while the InputCascadeCNN* takes 3 minutes, thanks to the fully convolutional architecture and the GPU implementation, which is over 30 times faster than the winner of the challenge. The TwoPathCNN* has a performance close to the state-of-the-art. However, with a prediction time of <ref type="bibr" target="#b24">25</ref>   <ref type="bibr" target="#b42">[43]</ref>, their method takes 70 minutes to process a subject, which is about 23 times slower than our method.</p><p>Regarding other methods using CNNs, Urban et al. <ref type="bibr" target="#b44">[45]</ref> used an average of two 3D convolutional networks with dice measures of 0.87, 0.77, 0.73 for Complete, Core and Enhancing tumor regions on BRATS 2013 test dataset with a prediction time of about 1 minute per model which makes for a total of 2 minutes. Again, since they do not report Specificity and Sensitivity measures, we can not make a full comparison. However, based on their dice scores our TwoPathCNN* is similar in per- Using our best performing method, we took part in the BRATS 2015 challenge. The BRATS 2015 training dataset comprises of 220 subjects with high grade and 54 subjects with low grade gliomas. There are 53 subjects with mixed high and low grade gliomas for testing. Every participating group had 48 hours from receiving the test subjects to process them and submit their segmentation results to the online evaluation system. BRATS'15 contains the training data of 2013. The ground truth for the rest of the training brains is generated by a voted average of segmented results of the top performing methods in BRATS'13 and BRATS'12. Some of these automatically generated ground truths have been refined manually by a user.</p><p>Because distribution of the intensity values in this dataset is very variable from one subject to another, we used a 7 fold cross validation for training. At test time, a voted average of these models was made to make prediction for each subject in the test dataset. The results of the challenge are presented in <ref type="figure" target="#fig_1">Figure 10</ref>. The semi-automatic methods participating in the challenge have been highlighted in grey. Please note since these results are not yet publicly available, we refrain from disclosing the name of the participants. In this figure the semi-automatic methods are highlighted in gray. As seen from the figure, our method ranks either first or second on Complete tumor and tumor Core categories and gets competitive results on active tumor category. Our method has also less outliers than most other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we presented an automatic brain tumor segmentation method based on deep convolutional neural networks. We considered different architectures and investigated their impact on the performance. Results from the BRATS 2013 online evaluation system confirms that with our best model we managed to improve on the currently published state-of-the-art method both on accuracy and speed as presented in MICCAI 2013. The high performance is achieved with the help of a novel two-pathway architecture (which can model both the local details and global context) as well as modeling local label dependencies by stacking two CNN's. Training is based on a two phase procedure, which we've found allows us to train CNNs efficiently when the distribution of labels is unbalanced. Thanks to the convolutional nature of the models and by using an efficient GPU implementation, the resulting segmentation system is very fast. The time needed to segment an entire brain with any of the these CNN architectures varies between 25 seconds and 3 minutes, making them practical segmentation methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig- ure 1 ) 1 .</head><label>11</label><figDesc>consists of the following three steps: Convolution of kernels (filters): Each feature map O s is associated with one kernel (or several, in the case of Maxout). The feature map O s is computed as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A single convolution layer block showing computations for a single feature map. The input patch (here 7 × 7), is convolved with series of kernels (here 3 × 3) followed by Maxout and max-pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Two-pathway CNN architecture (TwoPathCNN). The figure shows the input patch going through two paths of convolutional operations. The feature-maps in the local and global paths are shown in yellow and orange respectively. The convolutional layers used to produce these feature-maps are indicated by dashed lines in the figure. The green box embodies the whole model which in later architectures will be used to indicate the TwoPathCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Cascaded architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 4 shows an example of the data as well as the ground truth. In total, the model iterates over about 2.2 million examples of tumorous patches (this consists of all the 4 sub-tumor classes) and goes through 3.2 million of the healthy patches. As mentioned before during the first phase training, the distribution of examples introduced to the model from all 5 classes is uniform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Randomly selected filters from the first layer of the model. From left to right the figure shows visualization of features from the first layer of the global and local path respectively. Features in the local path include more edge detectors while the global path contains more localized features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Visual results from our top performing model, In-putCascadeCNN* on Coronal and Sagittal views. The subjects are the same as in Figure 7. In every sub-figure, the top row represents the Sagital view and the bottom row represents the Coronal view. The color codes are as follows: edema, enhanced tumor, necrosis, non-enhanced tumor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Progression of learning in InputCascadeCNN*. The stream of figures on the first row from left to right show the learning process during the first phase. As the model learns better features, it can better distinguish boundaries between tumor sub-classes. This is made possible due to uniform label distribution of patches during the first phase training which makes the model believe all classes are equiprobable and causes some false positives. This drawback is alleviated by training a second phase (shown in second row from left to right) on a distribution closer to the true distribution of labels. The color codes are as follows: edema, enhanced tumor, necrosis, non-enhanced tumor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Visual results from our CNN architectures from the Axial view. For each sub-figure, the top row from left to right shows T1C modality, the conventional one path CNN, the Conventional CNN with two training phases, and the TwoPathCNN model. The second row from left to right shows the ground truth, LocalCascadeCNN model, the MFCascadeCNN model and the InputCascadeCNN. The color codes are as follows: edema, enhanced tumor, necrosis, non-enhanced tumor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Our BRATS'15 challenge results using InputCas-cadeCNN*. Dice scores and negative log Hausdorff distances are presented for the three tumor categories. Since the results of the challenge are not yet publicly available, we are unable to disclose the name of the participants. The semi-automatic methods are highlighted in gray. In each sub-figure, the methods are ranked based on the mean value. The mean is presented in green, the median in red and outliers in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance of the TwoPathCNN model and variations. The second phase training is noted by appending '*' to the architecture name. The 'Rank' column represents the ranking of each method in the online score board at the time of submission.</figDesc><table><row><cell>Rank</cell><cell>Method</cell><cell></cell><cell>Dice</cell><cell></cell><cell></cell><cell>Specificity</cell><cell></cell><cell></cell><cell>Sensitivity</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="9">Complete Core Enhancing Complete Core Enhancing Complete Core Enhancing</cell></row><row><cell>4</cell><cell>TwoPathCNN*</cell><cell>0.85</cell><cell>0.78</cell><cell>0.73</cell><cell>0.93</cell><cell>0.80</cell><cell>0.72</cell><cell>0.80</cell><cell>0.76</cell><cell>0.75</cell></row><row><cell>9</cell><cell>LocalPathCNN*</cell><cell>0.85</cell><cell>0.74</cell><cell>0.71</cell><cell>0.91</cell><cell>0.75</cell><cell>0.71</cell><cell>0.80</cell><cell>0.77</cell><cell>0.73</cell></row><row><cell>10</cell><cell>AverageCNN*</cell><cell>0.84</cell><cell>0.75</cell><cell>0.70</cell><cell>0.95</cell><cell>0.83</cell><cell>0.73</cell><cell>0.77</cell><cell>0.74</cell><cell>0.73</cell></row><row><cell>14</cell><cell>GlobalPathCNN*</cell><cell>0.82</cell><cell>0.73</cell><cell>0.68</cell><cell>0.93</cell><cell>0.81</cell><cell>0.70</cell><cell>0.75</cell><cell>0.65</cell><cell>0.70</cell></row><row><cell>14</cell><cell>TwoPathCNN</cell><cell>0.78</cell><cell>0.63</cell><cell>0.68</cell><cell>0.67</cell><cell>0.50</cell><cell>0.59</cell><cell>0.96</cell><cell>0.89</cell><cell>0.82</cell></row><row><cell>15</cell><cell>LocalPathCNN</cell><cell>0.77</cell><cell>0.64</cell><cell>0.68</cell><cell>0.65</cell><cell>0.52</cell><cell>0.60</cell><cell>0.96</cell><cell>0.87</cell><cell>0.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of the cascaded architectures. The reported results are from the second phase training. The 'Rank' column shows the ranking of each method in the online score board at the time of submission.</figDesc><table><row><cell>Rank</cell><cell>Method</cell><cell></cell><cell>Dice</cell><cell></cell><cell></cell><cell>Specificity</cell><cell></cell><cell></cell><cell>Sensitivity</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="9">Complete Core Enhancing Complete Core Enhancing Complete Core Enhancing</cell></row><row><cell>2</cell><cell>InputCascadeCNN*</cell><cell>0.88</cell><cell>0.79</cell><cell>0.73</cell><cell>0.89</cell><cell>0.79</cell><cell>0.68</cell><cell>0.87</cell><cell>0.79</cell><cell>0.80</cell></row><row><cell>4-a</cell><cell>MFCascadeCNN*</cell><cell>0.86</cell><cell>0.77</cell><cell>0.73</cell><cell>0.92</cell><cell>0.80</cell><cell>0.71</cell><cell>0.81</cell><cell>0.76</cell><cell>0.76</cell></row><row><cell>4-c</cell><cell>LocalCascadeCNN*</cell><cell>0.88</cell><cell>0.76</cell><cell>0.72</cell><cell>0.91</cell><cell>0.76</cell><cell>0.70</cell><cell>0.84</cell><cell>0.80</cell><cell>0.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>shows how our implemented architectures compare with currently published state-of-the-art methods as mentioned in [32] 6 . The table shows that InputCascadeCNN* out performs Tustison et al. the winner of the BRATS 2013 challenge and is ranked first in the table. Results from the BRATS-2013 leaderboard presented in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>seconds, it is over 200 times faster than Tustison's method. Other top methods in the table are that of Meier et al and Reza et al with processing times of 6 and 90 minutes respectively. Recently Subbanna et al. [43] published competitive results on the BRATS 2013 dataset, reporting dice measures of 0.86, 0.86, 0.77 for Complete, Core and Enhancing tumor regions. Since they do not report Specificity and Sensitivity measures, a completely fair comparison with that method is not possible. However, as mentioned in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our top implemented architectures with the state-of-the-art methods on the BRATS-2012 "4 label" test set as discussed in<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Dice</cell><cell></cell></row><row><cell></cell><cell cols="3">Complete Core Enhancing</cell></row><row><cell>InputCascadeCNN*</cell><cell>0.81</cell><cell>0.72</cell><cell>0.58</cell></row><row><cell>Subbanna</cell><cell>0.75</cell><cell>0.70</cell><cell>0.59</cell></row><row><cell>Zhao</cell><cell>0.82</cell><cell>0.66</cell><cell>0.42</cell></row><row><cell>Tustison</cell><cell>0.75</cell><cell>0.55</cell><cell>0.52</cell></row><row><cell>Festa</cell><cell>0.62</cell><cell>0.50</cell><cell>0.61</cell></row><row><cell cols="4">formance while taking only 25 seconds, which is four times</cell></row><row><cell cols="4">faster. And the InputCascadeCNN* is better or equal in accu-</cell></row><row><cell cols="4">racy while having the same processing time. As for [49], they</cell></row><row><cell cols="4">do not report results on BRATS 2013 test dataset. However,</cell></row><row><cell cols="4">their method is very similar to the LocalPathCNN which, ac-</cell></row><row><cell cols="3">cording to our experiments, has worse performance.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparison of our implemented architectures with the state-of-the-art methods on the BRATS-2013 test set.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Dice</cell><cell></cell><cell></cell><cell>Specificity</cell><cell></cell><cell></cell><cell>Sensitivity</cell><cell></cell></row><row><cell></cell><cell cols="9">Complete Core Enhancing Complete Core Enhancing Complete Core Enhancing</cell></row><row><cell>InputCascadeCNN*</cell><cell>0.88</cell><cell>0.79</cell><cell>0.73</cell><cell>0.89</cell><cell>0.79</cell><cell>0.68</cell><cell>0.87</cell><cell>0.79</cell><cell>0.80</cell></row><row><cell>Tustison</cell><cell>0.87</cell><cell>0.78</cell><cell>0.74</cell><cell>0.85</cell><cell>0.74</cell><cell>0.69</cell><cell>0.89</cell><cell>0.88</cell><cell>0.83</cell></row><row><cell>MFCascadeCNN*</cell><cell>0.86</cell><cell>0.77</cell><cell>0.73</cell><cell>0.92</cell><cell>0.80</cell><cell>0.71</cell><cell>0.81</cell><cell>0.76</cell><cell>0.76</cell></row><row><cell>TwoPathCNN*</cell><cell>0.85</cell><cell>0.78</cell><cell>0.73</cell><cell>0.93</cell><cell>0.80</cell><cell>0.72</cell><cell>0.80</cell><cell>0.76</cell><cell>0.75</cell></row><row><cell>LocalCascadeCNN*</cell><cell>0.88</cell><cell>0.76</cell><cell>0.72</cell><cell>0.91</cell><cell>0.76</cell><cell>0.70</cell><cell>0.84</cell><cell>0.80</cell><cell>0.75</cell></row><row><cell>LocalPathCNN*</cell><cell>0.85</cell><cell>0.74</cell><cell>0.71</cell><cell>0.91</cell><cell>0.75</cell><cell>0.71</cell><cell>0.80</cell><cell>0.77</cell><cell>0.73</cell></row><row><cell>Meier</cell><cell>0.82</cell><cell>0.73</cell><cell>0.69</cell><cell>0.76</cell><cell>0.78</cell><cell>0.71</cell><cell>0.92</cell><cell>0.72</cell><cell>0.73</cell></row><row><cell>Reza</cell><cell>0.83</cell><cell>0.72</cell><cell>0.72</cell><cell>0.82</cell><cell>0.81</cell><cell>0.70</cell><cell>0.86</cell><cell>0.69</cell><cell>0.76</cell></row><row><cell>Zhao</cell><cell>0.84</cell><cell>0.70</cell><cell>0.65</cell><cell>0.80</cell><cell>0.67</cell><cell>0.65</cell><cell>0.89</cell><cell>0.79</cell><cell>0.70</cell></row><row><cell>Cordier</cell><cell>0.84</cell><cell>0.68</cell><cell>0.65</cell><cell>0.88</cell><cell>0.63</cell><cell>0.68</cell><cell>0.81</cell><cell>0.82</cell><cell>0.66</cell></row><row><cell>TwoPathCNN</cell><cell>0.78</cell><cell>0.63</cell><cell>0.68</cell><cell>0.67</cell><cell>0.50</cell><cell>0.59</cell><cell>0.96</cell><cell>0.89</cell><cell>0.82</cell></row><row><cell>LocalPathCNN</cell><cell>0.77</cell><cell>0.64</cell><cell>0.68</cell><cell>0.65</cell><cell>0.52</cell><cell>0.60</cell><cell>0.96</cell><cell>0.87</cell><cell>0.80</cell></row><row><cell>Festa</cell><cell>0.72</cell><cell>0.66</cell><cell>0.67</cell><cell>0.77</cell><cell>0.77</cell><cell>0.70</cell><cell>0.72</cell><cell>0.60</cell><cell>0.70</cell></row><row><cell>Doyle</cell><cell>0.71</cell><cell>0.46</cell><cell>0.52</cell><cell>0.66</cell><cell>0.38</cell><cell>0.58</cell><cell>0.87</cell><cell>0.70</cell><cell>0.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our top implemented architectures with the state-of-the-art methods on the BRATS-2013 leaderboard set.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Dice</cell><cell></cell><cell></cell><cell>Specificity</cell><cell></cell><cell></cell><cell>Sensitivity</cell><cell></cell></row><row><cell></cell><cell cols="9">Complete Core Enhancing Complete Core Enhancing Complete Core Enhancing</cell></row><row><cell>InputCascadeCNN*</cell><cell>0.84</cell><cell>0.71</cell><cell>0.57</cell><cell>0.88</cell><cell>0.79</cell><cell>0.54</cell><cell>0.84</cell><cell>0.72</cell><cell>0.68</cell></row><row><cell>Tustison</cell><cell>0.79</cell><cell>0.65</cell><cell>0.53</cell><cell>0.83</cell><cell>0.70</cell><cell>0.51</cell><cell>0.81</cell><cell>0.73</cell><cell>0.66</cell></row><row><cell>Zhao</cell><cell>0.79</cell><cell>0.59</cell><cell>0.47</cell><cell>0.77</cell><cell>0.55</cell><cell>0.50</cell><cell>0.85</cell><cell>0.77</cell><cell>0.53</cell></row><row><cell>Meier</cell><cell>0.72</cell><cell>0.60</cell><cell>0.53</cell><cell>0.65</cell><cell>0.62</cell><cell>0.48</cell><cell>0.88</cell><cell>0.69</cell><cell>0.6</cell></row><row><cell>Reza</cell><cell>0.73</cell><cell>0.56</cell><cell>0.51</cell><cell>0.68</cell><cell>0.64</cell><cell>0.48</cell><cell>0.79</cell><cell>0.57</cell><cell>0.63</cell></row><row><cell>Cordier</cell><cell>0.75</cell><cell>0.61</cell><cell>0.46</cell><cell>0.79</cell><cell>0.61</cell><cell>0.43</cell><cell>0.78</cell><cell>0.72</cell><cell>0.52</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It is important to note that while we did participate in the BRATS 2014 challenge, we could not report complete and fair experiments for it at the time of submitting this manuscript. See Section 5 for a discussion on this point.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Since the convolutional layer is associated to R input channels, X contains M × M × R gray-scale values and thus each kernel W s contains N × N × R weights. Accordingly, the number of parameters in a convolutional block of consisting of S feature maps is equal to R × M × M × S .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Please note that the results mentioned inTable 3andTable 4are from methods competing in the BRATS 2013 challenge for which a static table is provided [https://www.virtualskeleton.ch/BRATS/StaticResults2013]. Since then, other methods have been added to the score board but for which no reference is available.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Road scene segmentation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision -Volume Part VII</title>
		<meeting>the 12th European Conference on Computer Vision -Volume Part VII<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="376" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Glioma dynamics and computational models: A review of segmentation, registration, and in silico growth algorithms and their clinical applications 3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Clatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Capelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duffau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advanced normalization tools (ants)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insight J</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fully automatic segmentation of brain tumor images using support vector machine classification in combination with hierarchical conditional random field regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of mri-based medical image analysis for brain tumor studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in medicine and biology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="97" to="129" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic tumor segmentation using knowledge-based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldgof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Velthuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Silbiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d variational brain tumor segmentation using a high dimensional feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cobzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jgersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murtha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>in proc of BRATS-MICCAI</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fully automatic brain tumor segmentation from multiple mr sequences using hidden markov fields and variational em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dojat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Forbes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>in proc of BRATS-MICCAI</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multimodal Brain Tumor Segmentation (BRATS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<ptr target="http://martinos.org/qtim/miccai2013/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<ptr target="http://www.braintumorsegmentation.org" />
	</analytic>
	<monogr>
		<title level="j">Challenge Manuscripts</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain adaptation for largescale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.4214</idno>
		<title level="m">Pylearn2: a machine learning research library</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
	<note>Maxout networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extremely randomized trees based brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stieltjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Meinzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc of BRATS Challenge -MICCAI</title>
		<meeting>of BRATS Challenge -MICCAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tumor-cut: Segmentation of brain tumors on contrast enhanced mr images for radiosurgery applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hamamci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kucuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="790" to="804" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient interactive brain tumor segmentation as within-brain knn classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.0354</idno>
		<title level="m">Deep and wide multiscale recursive networks for robust image labeling</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?, in: Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on, IEEE</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d brain tumor segmentation in mri using fuzzy classification, symmetry analysis and spatially constrained deformable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khotanlou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Colliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="1457" to="1473" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">ilastik for multi-modal brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>in proc of BRATS-MICCAI</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>in: NIPS</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal brain tumor image segmentation using glistr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc of BRATS Challenge -MICCAI</title>
		<meeting>of BRATS Challenge -MICCAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Segmenting brain tumor with conditional random fields and support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bistritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of Workshop on Computer Vision for Biomedical Image Applications</title>
		<meeting>of Workshop on Computer Vision for Biomedical Image Applications</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Leemput</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Medical Imaging</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ants andárboles, in: in proc of BRATS Challenge -MICCAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tustison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wintermark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Avants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Joint tumor segmentation and dense deformable registration of brain mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duffau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chemouny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MICCAI</publisher>
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d variational brain tumor segmentation using dirichlet priors on a clustered feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Popuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cobzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jgersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="493" to="506" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A brain tumor segmentation framework based on outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bullit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Anaylsis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="275" to="283" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust estimation for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bullitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention-MICCAI 2003</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="530" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Appearanceand context-sensitive features for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc of BRATS Challenge -MICCAI</title>
		<meeting>of BRATS Challenge -MICCAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segmenting brain tumors using alignment-based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Levner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bistritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf on Machine Learning and Applications</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Iterative multilevel mrf leveraging context and voxel information for brain tumour segmentation in mri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Subbanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic gabor and mrf segmentation of brain tumours in mri volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Subbanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc of MICCAI</title>
		<meeting>of MICCAI</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="751" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multi-modal brain tumor segmentation using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>in proc of BRATS-MICCAI</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A generalized mean field algorithm for variational inference in exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Nineteenth conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Decision forests for tissue-specific segmentation of high-grade gliomas in multi-channel mr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention-MICCAI 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Segmentation of brain tumor tissues with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>in proc of</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
