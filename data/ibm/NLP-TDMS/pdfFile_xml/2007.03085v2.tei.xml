<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wasserstein Distances for Stereo Disparity Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Wasserstein Distances for Stereo Disparity Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing approaches to depth or disparity estimation output a distribution over a set of pre-defined discrete values. This leads to inaccurate results when the true depth or disparity does not match any of these values. The fact that this distribution is usually learned indirectly through a regression loss causes further problems in ambiguous regions around object boundaries. We address these issues using a new neural network architecture that is capable of outputting arbitrary depth values, and a new loss function that is derived from the Wasserstein distance between the true and the predicted distributions. We validate our approach on a variety of tasks, including stereo disparity and depth estimation, and the downstream 3D object detection. Our approach drastically reduces the error in ambiguous regions, especially around object boundaries that greatly affect the localization of objects in 3D, achieving the state-of-the-art in 3D object detection for autonomous driving. Our code will be available at https://github.com/Div99/W-Stereo-Disp.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction <ref type="figure">Figure 1</ref>: The effect of our continuous disparity network (CDN). We show a person (green box) in front of a wall. The blue 3D points are obtained using PSMNet <ref type="bibr" target="#b3">[4]</ref>. The red points from our CDN model are much better aligned with the shape of the objects: they do not suffer the streaking artifacts near edges. Yellow points are from the ground truth LiDAR. (One floor square is 1m×1m.)</p><p>Depth estimation from stereo images is a longstanding task in computer vision <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref>. It is a key component of many downstream problems, ranging from 3D object detection in autonomous vehicles <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref> to graphics applications such as novel view generation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b54">55]</ref>. The importance of this task in practical applications has led to a flurry of recent research. Convolutional networks have now superseded more classical techniques and led to significant improvements in accuracy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>These techniques estimate depth by finding accurate pixel correspondences and estimating the disparity between their X-coordinates, which is inversely proportional to depth. Because pixels have integral coordinates, so does the estimated disparity -causing even the resulting depth estimates to be discrete. This introduces inaccuracy, as the ground truth disparity and depth are naturally real-valued. This discrepancy is typically addressed by predicting a categorical distribution over a fixed set of discrete values, and then computing the expected depth from this distribution, which can in theory be any arbitrary real value (within the range of the set) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57]</ref>.</p><p>In this paper, we argue that such a design choice may lead to inaccurate depth estimates, especially around object boundaries. For example, in <ref type="figure">Figure 1</ref> we show the pixels (back-projected into 3D for each pre-defined discrete disparity value (e.g., {1, 2, 3, 4}), turning a categorical distribution (magenta bars) to a continuous distribution (red bars), from which we can output the mode disparity for accurate estimation.</p><p>using the depth estimates) along the boundary between a person in the foreground at 30m depth and a wall in the background at 70m depth. The predicted depth distribution of these border pixels is likely to be multi-modal, having two peaks around 30 and 70 meters. Simply taking the mean outputs a low probability value in between the two modes (e.g., 50m). Such "smoothed" depth estimates can have a strong negative impact on subsequent 3D object detection, as they "smear" the pedestrian around the edges towards the background (note the many blue points between the wall and the pedestrian). A bounding box including all these trailing points, far from the actual person, would strongly misrepresent the scene's geometry. What may further aggravate the problem is how the distribution is usually learned. Existing approaches mostly learn the distribution via a regression loss: minimizing the distance between the mean value and the ground truth <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">54]</ref>. In other words, there is no direct supervision to teach the model to assign higher probabilities around the truth depth.</p><p>To address these issues, we propose a novel neural network architecture for stereo disparity estimation that is capable of outputting a distribution over arbitrary disparity values, from which we can directly take the mode and bypass the mean. As with existing work, our model predicts a probability for each disparity value in a pre-defined, discrete set. Additionally, it predicts a real-valued offset for each discrete value. This is a simple architectural modification, but it has a profound impact. With these offsets, the output is converted from a discrete categorical distribution to a continuous distribution over disparity values: a mixture of Dirac delta functions, centered at the pre-defined discrete values shifted by predicted offsets <ref type="bibr" target="#b0">1</ref> . This simple addition of predicted offsets allows us to use the mode as the prediction during inference, instead of the mean, guaranteeing that the predicted depth has a high estimated probability. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates our model, continuous disparity network (CDN).</p><p>Next, we propose a novel loss function that provides a more informative objective during training. Concretely, we allow uni-or multi-modal ground truth depth distributions (obtained from nearby pixels) and represent them as (mixtures of) Dirac delta functions. The learning objective is then to minimize the divergence between the predicted and the ground truth distributions. Noting that the two distributions might not have a common support, we apply the Wasserstein distance <ref type="bibr" target="#b40">[41]</ref> to measure the divergence. While computing the exact Wasserstein distance of arbitrary distributions can be timeconsuming, computing it for one-dimensional distributions (e.g., distributions of one-dimensional disparity) enjoys efficient solutions, creating negligible training overhead.</p><p>Our proposed approach is both mathematically well-founded and practically extremely simple. It is compatible with most existing stereo depth or disparity estimation approaches -we only need to add an additional offset branch and replace the commonly used regression loss by the Wasserstein distance. We validate our approach using multiple existing stereo networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57]</ref> on three tasks: stereo disparity estimation <ref type="bibr" target="#b26">[27]</ref>, stereo depth estimation <ref type="bibr" target="#b8">[9]</ref>, and 3D object detection <ref type="bibr" target="#b8">[9]</ref>. The last is a downstream task using stereo depth as the input to detect objects in 3D. We conduct comprehensive experiments and show that our algorithm leads to significant improvement in all three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Stereo techniques rely on two cameras oriented parallel and translated horizontally relative to each other <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b55">56]</ref>. In this setting, for a pixel (u, v) in one image, the corresponding pixel in the second image is constrained to be at (u + D(u, v), v), where D(u, v) is called the disparity of the pixel. The disparity is inversely proportional to the depth Z(u, v) :</p><formula xml:id="formula_0">D(u, v) = f ×b Z(u,v)</formula><p>, where b is the translation between the cameras (called the baseline) and f is the focal length of the cameras. Stereo depth estimation techniques typically first estimate disparity in units of pixels and then exploit the reciprocal relationship to approximate depth. The basic approach is to compare pixels (u, v) in the left image I l with pixels (u, v + d) in the right image I r for different values of d, and find the best match. Since pixel coordinates are constrained to be integers, d is constrained to be an integer as well. The estimated disparity is thus an integer, forcing the estimated depth to be one of a few discrete values.</p><p>Instead of producing a single integer-valued disparity value, modern pipelines produce a distribution over these possible disparities <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>. They do this by constructing a 4D disparity feature volume, C disp , in which C disp (u, v, d, :) is a feature vector that captures the difference in appearance between I l (u, v) and I r (u, v + d). This feature vector can be, for instance, the concatenation of the feature vectors of the two pixels, in turn obtained by running a convolutional network on each image. The disparity feature volume is then passed through a series of 3D convolutional layers, culminating in a cost for each disparity value d for each pixel, S disp (u, v, d) <ref type="bibr" target="#b3">[4]</ref>. By taking softmax along the disparity dimension, one can turn S disp (u, v, d) into a probability distribution <ref type="bibr" target="#b25">[26]</ref>. Because we only consider integral disparity values, this distribution is a categorical distribution over the possible disparity values (e.g., d ∈ {0, · · · , 191}). One can then obtain the disparity D(u, v), for example, by argmax d softmax(−S disp (u, v, d)). However, in order to obtain continuous disparity estimates beyond integer-valued disparities, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b56">57]</ref> apply the following weighted combination (i.e., mean),</p><formula xml:id="formula_1">D(u, v) = d softmax(−S disp (u, v, d)) × d.<label>(1)</label></formula><p>The whole neural network can be learned end-to-end, including the image feature extractor and 3D convolution kernels, to minimize the disparity error (on one image)</p><formula xml:id="formula_2">(u,v)∈A (D(u, v) − D (u, v)),<label>(2)</label></formula><p>where is the smooth L1 loss, D is the ground truth map, and A contains pixels with ground truths.</p><p>Recently, <ref type="bibr" target="#b53">[54]</ref> argue that learning with Equation 2 may over-emphasize nearby depths, and accordingly propose to learn the network directly to minimize the depth loss. Specifically, they constructed depth cost volume S depth (u, v, z), rather than S disp (u, v, d), and predicted the continuous depth by</p><formula xml:id="formula_3">Z(u, v) = z softmax(−S depth (u, v, z)) × z.<label>(3)</label></formula><p>The entire network is learned to minimize the distance to the ground truth depth map Z</p><formula xml:id="formula_4">(u,v)∈A (Z(u, v) − Z (u, v)).<label>(4)</label></formula><p>In this paper, we argue that the design choices to output continuous values (Equation 1 and Equation 3) can be harmful to pixels in ambiguous regions, and the objective functions for learning the networks (Equation 2 and Equation 4) do not directly match the predicted distribution to the true one. The most similar work to ours is <ref type="bibr" target="#b57">[58]</ref>, which learns the network with a distribution matching loss on softmax(−S disp (u, v, d)); however, they still need to apply Equation 1 to obtain continuous estimates. Luo et al. <ref type="bibr" target="#b25">[26]</ref> also learned the network by distribution matching, but applied post-processing (e.g., semi global block matching) to obtain continuous estimates. Stereo-based 3D object detection. 3D object detection has attracted significant attention recently, especially for the application of self-driving cars <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>. While many algorithms rely on the expensive LiDAR sensor as input <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>, several recent papers have shown promising accuracy using the much cheaper stereo images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>. One particular framework is Pseudo-LiDAR <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref>, which converts stereo depth estimates into a 3D point cloud that can be inputted to any existing LiDAR-based detector, achieving the state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Disparity Estimation</head><p>For brevity, in the following we mainly discuss disparity estimation. The same technique can easily be applied to depth estimation, which is usually adapted from their disparity estimation counterparts.</p><p>As reviewed in section 2, many existing stereo networks output a distribution of disparities at each pixel. This distribution is a categorical distribution over discrete disparity values: discrete because they are estimated as the difference in X-coordinates of corresponding pixels, and as such are integers. Stereo techniques then compute the mean of the distribution to obtain a continuous estimate that is not limited to integral values.  We point out two disadvantages of taking the mean estimate. First, the mean value can deviate from the mode and may wrongly predict values of low probability when the predicted distribution is multi-modal (see <ref type="figure" target="#fig_2">Figure 3</ref>). Such multi-modal distributions appear frequently at pixels around the object boundaries. While they collectively occupy only a tiny portion of image pixels, recent studies have shown their particular importance in the downstream tasks like 3D object detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>. For instance, let us consider a street scene where a car 30m away (a disparity of, say, 10 pixels) is driving on the road towards the camera, with the sky as the background. The pixels on the car boundary can either take a disparity of around 10 pixels (for the car) or a disparity of 0 pixels (for the sky). Simply taking the mean likely produces arbitrary disparity estimates between these values, producing depth estimates that are neither on the car nor on the background. The downstream 3D object detector can, therefore, wrongly predict the car orientation and size, potentially leading to accidents. Second, the physical meaning of the mean value is by no means aligned with the true disparity: uncertainty in correspondence might yield a 40% chance of a disparity of 10 pixels and a 60% chance for a disparity of 20 pixels, but this does not mean that the disparity should be 16 pixels.</p><p>Instead, a more straightforward way to simultaneously model the uncertainty and output continuous disparity estimates is to extend the support of the output distribution beyond integers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Continuous disparity network (CDN)</head><p>To this end, we propose a new neural network architecture and output representation for disparity estimation. The output of our network will still be a set of discrete values with corresponding probabilities, but the discrete values will not be restricted to integers. The key idea is to start with integral disparity values, and predict offsets in addition to probabilities.</p><p>Denote by D the set of integral disparity values. As above, disparity estimation techniques produce a cost S disp (u, v, d) for every d ∈ D. A softmax converts this cost into a probability distribution:</p><formula xml:id="formula_5">p(d|u, v) = softmax(−S disp (u, v, d)) if d ∈ D, 0 otherwise.<label>(5)</label></formula><p>We propose to add a sub-network b(u, v, d) that predicts an offset disparity value for each integral disparity value d ∈ D at each pixel (u, v). We use this to displace the probability mass at d ∈ D to</p><formula xml:id="formula_6">d = d + b(u, v, d).</formula><p>This results in the following probability distribution:</p><formula xml:id="formula_7">p(d |u, v) = d∈D p(d|u, v)δ(d − (d + b(u, v, d))),<label>(6)</label></formula><p>which is a mixture of Dirac delta functions over arbitrary disparity values d . In other words,p has |D| supports, each located at</p><formula xml:id="formula_8">d + b(u, v, d) with a weight p(d|u, v). The resulting continuous disparity estimate D(u, v) at (u, v) is the mode ofp(d |u, v).</formula><p>Our network design with a sub-network for offset prediction is reminiscent of G-RMI pose estimator <ref type="bibr" target="#b29">[30]</ref> and one-stage 2D object detectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. The former predicts the heatmaps (at fixed locations) and offsets for each keypoint; the latter parameterizes the predicted bounding box coordinates by the anchor box location plus the predicted offset. One may also interpret our approach as a coarse-to-fine depth prediction, first picking the bin centered around argmax d∈D p(d|u, v) and then locally adjusting it by an offset.</p><p>In our implementation, the sub-network b(u, v, d) shares its feature and computation with S disp (u, v, d) except for the last block of fully-connected or convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning with Wasserstein distances</head><p>We propose to train our disparity network such that the mixture of Dirac delta functions <ref type="table" target="#tab_5">(Equation 6)</ref> is directly learned to match the ground truth distribution. Concretely, we represent the distribution of ground truth disparity at a pixel (u, v), p (d |u, v), as a Dirac delta function centered at the ground</p><formula xml:id="formula_9">truth disparity d = D (u, v): p (d |u, v) = δ(d − d ).</formula><p>We then employ a learning objective to minimize the divergence (distance) betweenp(d |u, v) and p (d |u, v). There are many popular divergence measures between distributions, such as Kullback-Leibler divergence, Jensen-Shannon divergence, total Variation, the Wasserstein distance, etc. In this paper, we choose the Wasserstein distance for one particular reason:p(d |u, v) and p (d |u, v) may not have any common supports.</p><p>The Wasserstein-p distance between two distributions µ, ν over a metric space (X, d) is defined as</p><formula xml:id="formula_10">W p (µ, ν) = inf γ∈Γ(µ,ν) E γ d(x, y) p 1/p ,<label>(7)</label></formula><p>where Γ(µ, ν) denotes the set of all the joint distributions γ(x, y) whose marginal distributions γ(x) and γ(y) are exactly µ and ν, respectively. Intuitively, γ(x, y) indicates how much "mass" to be transported from x to y in order to transform the distribution µ to ν.</p><p>Estimating the Wasserstein distance is usually non-trivial and requires solving a linear programming problem. One particular exception is when µ and ν are both distributions of one-dimensional variables, which is the case for our distribution over disparity values 2 . Specifically, when ν is a Dirac delta function whose support is located at y , the Wasserstein-p distance can be simplified as</p><formula xml:id="formula_11">W p (µ, ν) = (E µ E ν x − y p ) 1/p = (E µ x − y p ) 1/p .<label>(8)</label></formula><p>By pluggingp(d |u, v) and p (d |u, v) into µ and ν respectively, we obtain</p><formula xml:id="formula_12">W p (p, p ) = (Ep d − d p ) 1/p = d∈D p(d|u, v) d + b(u, v, d) − d p 1/p (9) = d∈D softmax(−S disp (u, v, d)) d + b(u, v, d) − d p 1/p</formula><p>, based on which we can learn the conventional disparity network (red) and the additional offset sub-network (blue) jointly (i.e., by minimizing <ref type="formula">Equation 9</ref>). We focus on W 1 and W 2 2 distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extension: learning with multi-modal ground truths</head><p>One particular advantage of learning to match the distributions is the capability of allowing multiple ground truth values (i.e., a multi-modal ground truth distribution) at a single pixel location. Denote D as the set of ground truth disparity values at a pixel (u, v), the ground truth distribution becomes</p><formula xml:id="formula_13">p (d |u, v) = d ∈D 1 |D | δ(d − d ).<label>(10)</label></formula><p>Since p (d |u, v) is not a Dirac delta function, we can no longer apply Equation 8 but the following equation for comparing two one-dimensional distributions <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref> </p><formula xml:id="formula_14">W p (p, p ) = 1 0 P −1 (x) − P −1 (x) p dx 1/p ,<label>(11)</label></formula><p>whereP and P are the cumulative distribution functions (CDFs) ofp and p , respectively. For the case p = 1, we can rewrite Equation 11 as [40]</p><formula xml:id="formula_15">W 1 (p, p ) = R P (d ) − P (d ) dd .<label>(12)</label></formula><p>We note that, both Equation <ref type="bibr" target="#b10">11</ref> and Equation 12 can be computed efficiently.</p><p>While existing datasets do not provide multi-modal ground truths directly, we investigate the following procedure to construct them. For each pixel, we consider a k × k neighborhood and create a multimodal distribution by setting the center-pixel disparity with a weight α and the remaining ones each with 1−α k×k−1 . We set k = 3 and α = 0.8 in the experiment. Our empirical study shows that using a multi-modal ground truth leads to a much faster model convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparisons to related work</head><p>Kendall et al. <ref type="bibr" target="#b13">[14]</ref> discussed the use of means or modes. They employed pre-scaling to sharpen the predicted probability, which might resolve the multi-modal issue but makes the prediction concentrate on discrete disparity values. In contrast, we do not prevent predicting a multi-modal distribution, especially for pixels whose disparities are inherently multi-modal. We output the mode (after an offset), which is what Kendall et al. <ref type="bibr" target="#b13">[14]</ref> hoped to achieve. We note that 3D convolutions can smooth the estimation but cannot guarantee uni-modal distributions.</p><p>Compared to G-RMI pose estimator and one-stage 2D object detectors mentioned in subsection 3.1, our work learns the two (sub-)networks jointly using a single objective function rather than a combination of two separated ones. See the supplementary material for more comparisons. Liu et al. <ref type="bibr" target="#b24">[25]</ref> propose to use the Wasserstein loss for pose estimation to characterize the inter-class correlations; however, they do not predict offsets for pre-defined discrete pose labels. Our work is also related to <ref type="bibr" target="#b1">[2]</ref>, in which the authors propose to learn the value distribution, instead of the expected value, using the Wasserstein loss for reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and metrics</head><p>Datasets. We evaluate our method on two challenging stereo benchmark datasets, i.e., Scene Flow <ref type="bibr" target="#b26">[27]</ref> and KITTI 2015 <ref type="bibr" target="#b27">[28]</ref>, and on a 3D object detection benchmark KITTI 3D <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. 1) Scene Flow <ref type="bibr" target="#b26">[27]</ref>. Scene Flow is a large synthetic dataset containing 35,454 training image pairs and 4,370 testing image pairs, where the ground truth disparity maps are densely provided, which is large enough for directly training deep neural networks.</p><p>2) KITTI 2015 <ref type="bibr" target="#b27">[28]</ref>. KITTI 2015 is a real-world dataset with street scenes captured from a driving car. KITTI 2015 contains 200 training stereo image pairs with sparse ground truth disparities obtained using LiDAR, and 200 testing image pairs with ground truth disparities held by evaluation server for submission evaluation only. Its small size makes it a challenging dataset.</p><p>3) KITTI 3D <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. KITTI 3D contains 7,481 (pairs of) images for training and 7,518 (pairs of) images for testing. We follow the same training and validation splits as suggested by Chen et al. <ref type="bibr" target="#b5">[6]</ref>, containing 3,712 and 3,769 images, respectively. For each image, KITTI provides the corresponding Velodyne LiDAR point cloud (for sparse depth ground truths), camera calibration matrices, and 3D bounding box annotations. We evaluate our approach by plugging it into existing stereo-based 3D object detectors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54]</ref>, which all require stereo depth estimation as a key component. Metrics. We evaluate our methods on three tasks: stereo disparity estimation, stereo depth estimation, and 3D object detection. We apply the corresponding standard metrics listed as follows.</p><p>1) stereo disparity, we use two standard metrics: End-Point-Error (EPE), i.e., the average difference of the predicted disparities and their true ones, and k-Pixel Threshold Error (PE), i.e., the percentage of pixels for which the predicted disparity is off the ground truth by more than k pixels. We use the 1-pixel and 3-pixel threshold errors, denoted as 1PE and 3PE. PE is robust to outliers with large disparity errors, while EPE measures errors to sub-pixel level.</p><p>2) stereo depth. We use the Root Mean Square Error (RMSE)</p><formula xml:id="formula_16">1 |A| Σ (u,v)∈A |z(u, v) − z (u, v)| 2 and Absolute Relative Error (ABSR) 1 |A| Σ (u,v)∈A |z(u,v)−z (u,v)| z (u,v)</formula><p>, where A denotes all the pixels having ground truths, and z and z are estimated depth and ground truth depth respectively.</p><p>3) 3D object detection. We focus on 3D and bird's-eye-view (BEV) localization and report the results on the official leader board and the validation set. Specifically, we focus on the "car" category, following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref>. We report the average precision (AP) at IoU thresholds 0.5 and 0.7. We denote AP for the 3D and BEV tasks by AP 3D and AP BEV , respectively. The benchmark defines for each category three cases -easy, moderate, and hard -according to the bounding box height and occlusion and truncation. In general, the easy cases correspond to cars within 30 meters of the ego-car distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>We mainly use the Wasserstein-1 distance (i.e., W 1 loss) for training our CDN model. We compare W 1 and W 2 2 losses in the supplementary material. Stereo disparity. We apply our continuous disparity network (CDN) architecture to PSMNet <ref type="bibr" target="#b3">[4]</ref> and GANet <ref type="bibr" target="#b56">[57]</ref>, namely CDN-PSMNET and CDN-GANET. To keep a fair comparison, we train the models with their default settings. For Scene Flow, the models are trained from scratch with a constant learning rate of 0.001 for 10 epochs. For KITTI 2015, the models pre-trained on Scene Flow are fine-tuned following the default strategy of the vanilla models. We consider disparities in the range of [0, 191] for both datasets. We use a uniform grid of bin size 2 pixels to create the categorical distribution (cf. Equation 5). We show the effect of bin sizes in the supplementary material. Stereo depth. We apply CDN to the SDN architecture <ref type="bibr" target="#b53">[54]</ref>, namely CDN-SDN. We follow the training procedure in <ref type="bibr" target="#b53">[54]</ref>. We consider depths in the range of [0m, 80m]. We use a uniform grid of bin size 1m to create the categorical distribution. The offset sub-network. We implement b(u, v, d) with a Conv3D-Relu-Conv3D block. It takes the 4D cost volume, before the last fully-connected or convolutional block of S disp (u, v, d), as the input. We predict a single offset b(u, v, d) ∈ [0, s] for each integral disparity value d, where s is the bin size. We achieve this by clipping. The sub-network has 30K parameters, only 0.3% w.r.t. PSMNET <ref type="bibr" target="#b3">[4]</ref>. For stereo depth, we implement b(u, v, z) ∈ [0, s] in the same way for each integral depth value z. Stereo 3D object detection. We apply CDN-SDN to PSEUDO-LIDAR ++ <ref type="bibr" target="#b53">[54]</ref>, which uses SDN to estimate depth. We fine-tune the CDN-SDN model pre-trained on Scene Flow on KITTI 3D dataset, followed by using an 3D object detector, here P-RCNN <ref type="bibr" target="#b36">[37]</ref>, to detect 3D bounding boxes of cars. We also apply CDN to DSGN <ref type="bibr" target="#b7">[8]</ref>, the state-of-the-art stereo-based 3D object detector. DSGN uses as a backbone depth estimator based on PSMNET and we replace it with our CDN version. Multi-modal ground truths. As mentioned in subsection 3.3, we create multi-modal ground truths for a pixel by considering a patch in its k × k neighborhood. We give the center-pixel disparity a weight α = 0.8, and the remaining ones an equal weight such that the total sums to 1. In this case, we use Equation 12 as the loss function. We implement a differentiable loss module in Pytorch that can be applied to a batch of image tensors. Please see the supplementary material for more details.  On KITTI 2015, CDN-GANET Deep obtains the lowest error on the foreground pixels and performs comparably to other methods on all the pixels 4 . We see a similar gain by CDN-PSMNET over PSM-NET on the foreground, which is quite surprising, as we do not specifically re-weight the loss function towards foreground pixels. Since CDN has advantages on pixels whose disparity is ambiguous and hard to estimate correctly (e.g., due to multi-modal distributions), the fact that foreground pixels have a higher error and CDN can effectively reduce it suggests that those challenging pixels are mostly in the foreground. As will be seen in 3D object detection, the improvement by CDN on foreground pixels translates to a higher accuracy on localizing objects. 3D object detection. <ref type="table" target="#tab_1">Table 2</ref> summarizes the results on the test set of KITTI 3D. Our CDN consistently improves the two mainstream approaches, namely, DSGN and PSEUDO-LIDAR. For PSEUDO-LIDAR, we achieve a 2.5%/3.0% gain on AP 3D /AP BEV Moderate (the standard metric on the leader board) against PSEUDO-LIDAR ++: the only difference is that we replace SDN by our CDN-SDN to have better depth estimates. Our approach even outperforms PSEUDO-LIDAR E2E, which fine-tunes the depth network specifically for object detection. We argue that our approach, which can automatically focus on the foregrounds, may have a similar effect as end-toend training with object detection losses. For DSGN, plugging our CDN-SDN leads to a notable 2% gain at AP 3D , attaining the highest entry of stereo-based 3D detection accuracy on the KITTI leader board.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Multi-modal (MM) ground truth. We investigate creating the multi-modal (MM) ground truths for training our models. <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref> summarize the results on Scene Flow for disparity and depth estimation, respectively. MM training slightly reduces the errors. To better understand how MM ground truths affect network training, we plot the test accuracy along the training epochs in <ref type="figure" target="#fig_3">Figure 4</ref>: CDN-PSMNET trained with MM ground truths converges much faster. We attribute this to the observations in <ref type="bibr" target="#b0">[1]</ref>: a neural network tends to learn simple and clean patterns first. We note that, for boundary pixels whose disparities are inherently multi-modal, uni-modal ground truths are indeed noisy labels. A network thus tends to ignore these pixels in the early epochs. In contrast, MM ground truths provide clean supervisions for these boundary pixels; the network thus can learn the patterns much faster. See the supplementary material for a visualization and further discussions. Ablation studies. We study different components of our approach in <ref type="table" target="#tab_5">Table 6</ref>. Methods without W 1 loss use the regression loss for optimization (cf. Equation 2) and output the mean. Methods with W 1 loss output the mode. We see that, the offset sub-network alone can hardly improve the performance. Using W 1 distance alone reduces 1PE and 3PE errors, but not EPE, suggesting that it cannot produce sub-pixel disparity estimates <ref type="bibr" target="#b4">5</ref> . Only combining the offset subnetwork and the W 1 loss produces consistent improvement over all three metrics. Disparity on boundaries. <ref type="table" target="#tab_4">Table 5</ref> shows the results: we obtain pixels on object boundaries using the OpenCV Canny edge detector with minVal/maxVal=100/200. Both CDN and training with multi-modal ground truths reduce the error significantly.</p><p>Qualitative disparity results on KITTI. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, our approach is able to estimate disparity accurately, especially along the object boundaries. Specifically, CDN-GANET Deep maintains the straight bar shape (on the right), while GANET Deep blends it with the background sky due to the mean estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Left Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GANet Deep: FG Error=0.09</head><p>GANet-CDN Deep: FG Error=0.00 In this paper we have introduced a new output representation, model architecture and loss function for depth/disparity estimation that can faithfully produce real-valued estimates of depth/disparity. We have shown that results not just in more accurate depth estimates, but also significant improvement in downstream tasks like object detection. Finally, because we explicitly output and optimize a distribution over depths, our approach can naturally take into account uncertainty and multimodality in the ground truth. More generally, our results suggest that removing suboptimalities in how we represent and optimize 3D information can have a large impact on a multitude of vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The end results of this paper are improved depth and disparity estimation, particularly on foreground objects. This is of use to self-driving cars, 3D reconstruction, and other robotics applications. In particular, it has the potential to improve the safety of these systems, as indicated by the increased 3D object detection performance. Our approach can also easily be incorporated into other depth or disparity estimation algorithms for further improvement.</p><p>While our depth predictions are significantly better, any failure has important safety considerations, such as collisions and accidents. Before deployment, appropriate safety thresholds must be cleared.</p><p>Our approach does not specifically leverage dataset biases, although being a machine learning approach, it is impacted as much as other machine learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We provide in this material the contents omitted in the main paper:</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Learning with the (approximated) KL divergence</head><p>The Kullback-Leibler (KL) divergence</p><formula xml:id="formula_17">KL(µ||ν) = E µ log(µ|ν)<label>(13)</label></formula><p>between two distributions µ and ν requires them to have the same supports: i.e., µ(d ) = 0 if ν(d ) = 0, for KL(µ||ν) to be finite.</p><p>For our case, µ(d ) = δ(d − d ) and ν(d ) = d∈D p(d|u, v)δ(d − (d + b(u, v, d))). These two measures may have different supports. To make the KL divergence applicable, we can smooth ν to form a mixture of Laplace or Gaussian distributions.</p><p>For example, smoothing ν with a Laplace distribution, Laplace(0, τ ) = 1 2τ exp − |d | τ , we get</p><formula xml:id="formula_18">ν Lap (d ) = d∈D p(d|u, v) 1 2τ exp − |d − (d + b(u, v, d))| τ .<label>(14)</label></formula><p>With ν Lap , the KL divergence reduces to the following loss Similarly, smoothing ν with a Gaussian distribution N (0, σ 2 ), we get</p><formula xml:id="formula_19">(µ, ν Lap ) = − log d∈D p(d|u, v) 1 2τ exp − |d − (d + b(u, v, d))| τ (15) ≈ − log p(d|u, v) + 1 τ d − (d + b(u, v,d)) ,<label>(16)</label></formula><formula xml:id="formula_20">ν Gau (d ) = d∈D p(d|u, v) 1 σ √ 2π exp − d − (d + b(u, v, d)) 2 2 2σ 2 .<label>(17)</label></formula><p>With ν Gau , the KL divergence reduces to the following loss</p><formula xml:id="formula_21">(µ, ν Gau ) = − log d∈D p(d|u, v) 1 σ √ 2π exp − d − (d + b(u, v, d)) 2 2 2σ 2<label>(18)</label></formula><formula xml:id="formula_22">≈ − log p(d|u, v) + 1 2σ 2 d − (d + b(u, v,d)) 2 2 ,<label>(19)</label></formula><p>whered = s d s ∈ D is the grid disparity value of the bin the true disparity d belongs to.</p><p>These formulations reduce to the conventional classification loss plus offset regression loss, commonly used for keypoint estimation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b58">59]</ref> and one-stage 2D object detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b58">59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Multi-modal ground truths</head><p>There are three reasons why multi-modal ground truths would benefit disparity or depth estimation. First, pixels are discrete: a single pixel may capture different depths. Second, real datasets need to project signals from a depth sensor (e.g., LiDAR) to a depth map. As pixels are discrete and the cameras and LiDAR might be placed differently, multiple LiDAR points of different depths may be projected to the same pixel. Third, for stereo estimation, pixels along boundaries or occluded regions cause ambiguity to the model; multi-modal ground truths offer better supervision for training, especially in early training epochs.</p><p>Conceptually, learning with multi-modal ground truths should notably improve results in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="table" target="#tab_3">Table 4</ref> of the main paper. However, in evaluation, a majority of pixels are not on the object boundaries. Besides, we still evaluate using the (likely noisy) uni-modal ground truths. To further analyze these, we show in <ref type="table" target="#tab_5">Table 6</ref> of the main paper the disparity error calculated on object boundaries: learning with multi-modal ground truths leads to a significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Multi-task learning</head><p>One way to mitigate stereo predictions at depth discontinuities is to jointly perform stereo estimation and other tasks such as semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46]</ref>, which can reason about object boundaries. The core idea is to leverage additional semantic labels to guide the model to resolve depth discontinuities (i.e., predict uni-modal distributions). Our method, in contrast, does not prevent predicting multi-modal distributions along depth discontinuities, but changes the outputting rule (i.e., argmin with a predicted offset). Our method can also capture depth discontinuities within an object or an object class. In contrast, semantic segmentation labels overlapped objects of the same class by the same label and does not directly tell their boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Offsets and distributions without common supports</head><p>While the learned offsets may lead to common supports between the predicted and ground truth distributions, we have to first come up with a loss to learn such offsets before common supports become possible. Concretely, to learn b(u, v, d) in Equation 9, we need a loss that can measure the divergence betweenp and p , which may not have common supports. We note that, this may occur even if the target distribution p is a Dirac delta function. While the KL divergence or a regression loss may be applied to learn the offsets, they need to either smooth the distributions or carefully design the loss to learn both the distribution and the offset networks. The Wasserstein distance offers a principled loss to learn the two networks jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Ablation studies on different divergences</head><p>We show the ablation study on using different divergences between distributions in <ref type="table" target="#tab_7">Table 7</ref>. For the KL divergence (subsection A.2), we use Laplace smoothing with b = 1 <ref type="figure" target="#fig_6">(Equation 16</ref>). Our results show that the Wasserstein distance is a better choice than the KL divergence for comparing the predicted and the ground truth disparity (or depth) distributions. We also see that W 2 2 distance performs worse than W 1 . We attribute this to outliers (i.e., noisy disparity labels) in a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Effect of bin sizes</head><p>CDN outputs modes and needs (a) the bin containing the truth disparity or depth to have the highest probability and (b) the offset to be accurate. The bin size balances the difficulty of (a) and (b). A  We show these effects of bin sizes on uniform grids, with disparities in the range of [0, 191] for disparity estimation in <ref type="table" target="#tab_8">Table 8</ref>. For a bin size s = 1, predicting the correct bin is harder. For a bin size s = 4, predicting the correct bin is easier, whereas predicting the correct offset becomes harder. We found s = 2 to perform well in general.</p><p>C.3 Ablation studies on α and k in multi-modal (MM) ground truths <ref type="table" target="#tab_9">Table 9</ref> shows the depth estimation error on Scene Flow using CDN-SDN-MM, with different α and k in preparing the MM ground truths (cf. <ref type="table" target="#tab_3">Table 4</ref> of the main paper). A smaller α leads to a larger error, which makes sense as it relies less on the ground truths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Learning with multi-modal (MM) ground truths</head><p>Following subsection 4.4 and <ref type="figure" target="#fig_3">Figure 4</ref> of the main paper, we train CDN-PSMNET and CDN-PSMNET MM for only two epochs and compare their disparity estimation performance. <ref type="figure" target="#fig_6">Figure 6</ref> shows the results on KITTI images. While both methods have similar predictions at smooth regions, CDN-PSMNET MM leads to much sharper and clearer object boundaries, suggesting that the multi-modal ground truths are better supervisions for learning around the boundaries in early epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Learned offsets</head><p>The offset network learns to produce the sub-grid disparity at each integral disparity values. <ref type="figure" target="#fig_7">Figure 7</ref> shows an example, in which we back-project pixels into 3D points using the estimated disparity or depth at each pixel by the mode, with or without the offset prediction. Without the offset, the 3D points can only occupy discrete depths, leading to a discontinuous, non-smooth point cloud. <ref type="figure" target="#fig_8">Figure 8</ref> shows the BEV point cloud visualization. We show the 3D points generated by SDN and CDN-SDN as well as the ground truth LiDAR points and car/pedestrian boxes. We see that, CDN-SDN generates sharper points than SDN. Specifically for pixels on the foreground objects, SDN usually predicts the depths beyond the boxes due to the mean estimates from multi-modal distributions on the boundary pixels, whereas CDN-SDN significantly alleviates the problem. We also see some failure cases of CDN-SDN: on the right image, CDN-SDN has a larger error on the background compared to SDN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Point cloud visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Depth estimation</head><p>Besides the Scene Flow dataset, we show the depth estimation error on KITTI Val: the 3,769 validation images for 3D object detection. We follow <ref type="bibr" target="#b53">[54]</ref> to train the depth estimation model and compute the depth estimation error on pixels associated with ground truth LiDAR points. <ref type="table" target="#tab_0">Table 10</ref>     and <ref type="figure">Figure 9</ref> show the results, CDN-SDN achieves lower error than SDN, which explains why CDN-SDN (and CDN-DSGN) can lead to better 3D object detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Without Offset With Offset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 3D object detection</head><p>We show in <ref type="figure">Figure 10</ref> the object detection precision-recall curves of DSGN vs. CDN-DSGN. CDN-DSGN has higher precision (vertical) values than DSGN at different recall (horizontal) values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.9 Qualitative disparity results</head><p>We show in <ref type="figure">Figure 11</ref> and <ref type="figure" target="#fig_0">Figure 12</ref> the predicted disparity maps and the foreground errors of both GANET Deep and CDN-GANET Deep on KITTI and Scene Flow. CDN generally leads to sharper and clearer object boundaries.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Continuous disparity network (CDN). We propose to predict a real-value offset (yellow arrows)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The predicted disparity posterior for a pixel on object boundaries. The uni-modal assumption can break down, leading to a mean estimate that is in a low probability region. Learning offsets allow us to predict the continuous mode. (Offsets are in [0, 1] here.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>MM training. We show the EPE and 3PE disparity errors on Scene Flow test set using CDN-PSMNET, w/ or w/o MM training. MM training leads to faster convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on disparity. The top, middle, and bottom images are the left image, the result of GANET Deep, and the result of CDN-GANET Deep, together with the foreground 3PE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>whered = s d s ∈</head><label>s</label><figDesc>D is the grid disparity value of the bin the true disparity d belongs to.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on learning with multi-modal ground truths at early epochs. The top, middle, and bottom images are the left image, the result of CDN-PSMNET, and the result of CDN-PSMNET MM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>A visualization of the 3D point cloud (from the bird's-eye view) derived from the estimated disparities or depths (by modes), with (right) and without (left) offset prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>BEV Point cloud visualization. The blue points are obtained using SDN. The red points are from our CDN-SDN model. The yellow points are from the ground truth LiDAR. The green boxes are ground truth car / pedestrian locations. The observer is at the left-hand side of the point cloud looking to the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Depth error on KITTI Val. We compute the median absolute depth error for different depth ranges on KITTI Val images using SDN and CDN-SDN. We show the object detection precision-recall curves for AP3D at moderate cases on cars. We compare DSGN (stereo images) and CDN-DSGN (stereo images).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Qualitative results on KITTI. The top, middle, and bottom images are the left image, the result of GANET Deep, and the result of CDN-GANET Deep, together with the foreground 3PE. Qualitative results on Scene Flow validation set. The first row shows the left image and the ground truth map. The second row shows the result of GANET Deep and the result of CDN-GANET Deep, together with the foreground end point errors (EPE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Disparity results. We report results on Scene Flow and KITTI 2015. For Scene Flow, end point errors (EPE) and the 1-pixel and 3-pixel threshold error rates (1PE, 3PE) are reported. For KITTI 2015 we report the standard metrics (using 3PE) for both Non-occluded and All pixels regions. Methods based on CDN are highlighted in blue. Lower is better. The best result per column in in bold. Since the baselines are mostly trained with uni-modal ground truths, we only show CDN with the same ground truths here for a fair comparison.</figDesc><table><row><cell></cell><cell cols="3">Scene Flow</cell><cell></cell><cell cols="2">KITTI 2015</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Non Occlusion 3PE</cell><cell cols="2">All Areas 3PE</cell></row><row><cell>Method</cell><cell cols="4">EPE 1PE 3PE Foreground</cell><cell>All</cell><cell>Foreground</cell><cell>All</cell></row><row><cell>MC-CNN [56]</cell><cell>3.79</cell><cell>-</cell><cell>-</cell><cell>7.64</cell><cell>3.33</cell><cell>8.88</cell><cell>3.89</cell></row><row><cell>GC-Net [14]</cell><cell cols="3">2.51 16.9 9.34</cell><cell>5.58</cell><cell>2.61</cell><cell>6.16</cell><cell>2.87</cell></row><row><cell>PSMNet [4]</cell><cell cols="3">1.09 12.1 4.56</cell><cell>4.31</cell><cell>2.14</cell><cell>4.62</cell><cell>2.32</cell></row><row><cell>SegStereo [52]</cell><cell>1.45</cell><cell>-</cell><cell>-</cell><cell>3.70</cell><cell>2.08</cell><cell>4.07</cell><cell>2.25</cell></row><row><cell>GwcNet-g [11]</cell><cell>0.77</cell><cell>8.0</cell><cell>3.30</cell><cell>3.49</cell><cell>1.92</cell><cell>3.93</cell><cell>2.11</cell></row><row><cell>HD 3 -Stereo [53]</cell><cell>1.08</cell><cell>-</cell><cell>-</cell><cell>3.43</cell><cell>1.87</cell><cell>3.63</cell><cell>2.02</cell></row><row><cell>GANet [57]</cell><cell>0.84</cell><cell>9.9</cell><cell>-</cell><cell>3.37</cell><cell>1.73</cell><cell>3.82</cell><cell>1.93</cell></row><row><cell>AcfNet [58]</cell><cell>0.87</cell><cell>-</cell><cell>4.31</cell><cell>3.49</cell><cell>1.72</cell><cell>3.80</cell><cell>1.89</cell></row><row><cell>Stereo Expansion [51]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.11</cell><cell>1.63</cell><cell>3.46</cell><cell>1.81</cell></row><row><cell>GANet Deep [57]</cell><cell>0.78</cell><cell>8.7</cell><cell>-</cell><cell>3.11</cell><cell>1.63</cell><cell>3.46</cell><cell>1.81</cell></row><row><cell>CDN-PSMNet</cell><cell>0.98</cell><cell>9.1</cell><cell>3.99</cell><cell>4.01</cell><cell>2.12</cell><cell>4.34</cell><cell>2.29</cell></row><row><cell>CDN-GANet Deep</cell><cell>0.70</cell><cell>7.7</cell><cell>2.98</cell><cell>2.79</cell><cell>1.72</cell><cell>3.20</cell><cell>1.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>3D object detection results on the KITTI leader board. We report APBEV and AP3D (in %) of the car category at IoU= 0.7. Methods with CDN are in blue. The best result of each column is in bold font.</figDesc><table><row><cell></cell><cell cols="3">BEV Detection AP (APBEV)</cell><cell cols="3">3D Detection AP (AP3D)</cell></row><row><cell>Method</cell><cell cols="2">Easy Moderate</cell><cell>Hard</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell>S-RCNN [21]</cell><cell>61.9</cell><cell>41.3</cell><cell>33.4</cell><cell>47.6</cell><cell>30.2</cell><cell>23.7</cell></row><row><cell>OC-STEREO [31]</cell><cell>68.9</cell><cell>51.5</cell><cell>43.0</cell><cell>55.2</cell><cell>37.6</cell><cell>30.3</cell></row><row><cell>DISP R-CNN [38]</cell><cell>74.1</cell><cell>52.4</cell><cell>43.8</cell><cell>59.6</cell><cell>39.4</cell><cell>32.0</cell></row><row><cell>PSEUDO-LIDAR [42]</cell><cell>67.3</cell><cell>45.0</cell><cell>38.4</cell><cell>54.5</cell><cell>34.1</cell><cell>28.3</cell></row><row><cell>PSEUDO-LIDAR ++ [54]</cell><cell>78.3</cell><cell>58.0</cell><cell>51.3</cell><cell>61.1</cell><cell>42.4</cell><cell>37.0</cell></row><row><cell>PSEUDO-LIDAR E2E [33]</cell><cell>79.6</cell><cell>58.8</cell><cell>52.1</cell><cell>64.8</cell><cell>43.9</cell><cell>38.1</cell></row><row><cell cols="2">CDN-PSEUDO-LIDAR ++ 81.3</cell><cell>61.0</cell><cell>52.8</cell><cell>64.3</cell><cell>44.9</cell><cell>38.1</cell></row><row><cell>DSGN [8]</cell><cell>82.9</cell><cell>65.0</cell><cell>56.6</cell><cell>73.5</cell><cell>52.2</cell><cell>45.1</cell></row><row><cell>CDN-DSGN</cell><cell>83.3</cell><cell>66.2</cell><cell>57.7</cell><cell>74.5</cell><cell>54.2</cell><cell>46.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Disparity multi-modal results. We report the EPE, 1PE and 3PE on Scene Flow. Methods with CDN</figDesc><table><row><cell cols="5">are highlighted in blue. The best result of each column is in bold font.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">EPE 1PE 3PE</cell><cell>Method</cell><cell cols="3">EPE 1PE 3PE</cell></row><row><cell>PSMNET [4]</cell><cell cols="3">1.09 12.1 4.56</cell><cell>GANET Deep [57]</cell><cell>0.78</cell><cell>8.7</cell><cell>-</cell></row><row><cell>CDN-PSMNET</cell><cell>0.98</cell><cell>9.1</cell><cell>3.99</cell><cell>CDN-GANET Deep</cell><cell>0.70</cell><cell>7.7</cell><cell>2.98</cell></row><row><cell cols="2">CDN-PSMNET MM 0.96</cell><cell>9.0</cell><cell>3.96</cell><cell cols="2">CDN-GANET Deep MM 0.68</cell><cell>7.7</cell><cell>2.97</cell></row><row><cell>4.3 Main results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Disparity estimation. Table 1 summarizes the results on disparity estimation. CDN-GANET Deep 3 achieves the lowest error at all three metrics on Scene Flow. It reduces the error for GANET Deep by 1.0 1PE and 0.08 EPE, both are significant. We see a similar gain for PSMNET: CDN-PSMNET reduces EPE by 0.09, demonstrating the general applicability of our approach to existing networks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Depth multi-modal results. We report the RMSE and ABSR errors on Scene Flow. The best result of each column is in bold font.</figDesc><table><row><cell>Method</cell><cell cols="2">RMSE (m) ABSR</cell></row><row><cell>SDN [54]</cell><cell>2.05</cell><cell>0.039</cell></row><row><cell>CDN-SDN</cell><cell>1.81</cell><cell>0.030</cell></row><row><cell>CDN-SDN MM</cell><cell>1.80</cell><cell>0.028</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ambiguous regions (object boundaries). report the disparity error on Scene Flow. The best result of each column is in bold font.</figDesc><table><row><cell>We Method</cell><cell>EPE 1PE</cell><cell>3PE</cell></row><row><cell>PSMNet [4]</cell><cell cols="2">3.10 20.1 11.33</cell></row><row><cell>CDN-PSMNet</cell><cell>2.10 15.3</cell><cell>8.92</cell></row><row><cell cols="2">CDN-PSMNet MM 2.08 13.2</cell><cell>8.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies. We report disparity error for CDN-PSMNET on Scene Flow. Methods without W1 loss are learned with mean regression.</figDesc><table><row><cell>Offsets W1 Loss Output EPE 1PE 3PE</cell></row><row><cell>Mean 1.09 12.1 4.56</cell></row><row><cell>Mean 1.04 12.0 4.55</cell></row><row><cell>Mode 1.20 10.5 4.21</cell></row><row><cell>Mode 0.98 9.1 3.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Appendix A: additional implementation details (cf. subsection 3.2 and subsection 4.2 of the main paper). • Appendix B: additional discussions (cf. section 3 and subsection 4.4 of the main paper). • Appendix C: additional experimental results and analysis (cf. subsection 4.2, subsection 4.3, and subsection 4.4 of the main paper). For multi-modal ground truths, we cannot use Equation 8 of the main paper for optimization. Instead, we apply the loss in Equation 12, for W 1 distance. This loss essentially computes the difference in areas between the CDFs of the two distributions. For mixtures of Dirac delta functions, it can be efficiently implemented by computing the accumulated difference between CDF histograms. It takes O(B log B) for each pixel using sorting, where B is the total number of supports of both distributions. Our implementation is adapted from scipy.stats.wasserstein_distance and we modify it to be compatible with Pytorch tensors and use CUDA to parallelize the computation over all the pixels.</figDesc><table><row><cell>A Implementation Details</cell></row><row><cell>A.1 Learning with multi-modal ground truths</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of different divergences (distances). We report the RMSE and the ABSR error for depth estimation on Scene Flow. The best result of each column is in bold font.</figDesc><table><row><cell>Method</cell><cell cols="3">Divergence RMSE (m) ABSR</cell></row><row><cell>SDN</cell><cell>-</cell><cell>2.05</cell><cell>0.04</cell></row><row><cell>CDN-SDN</cell><cell>KL</cell><cell>2.57</cell><cell>0.04</cell></row><row><cell>CDN-SDN</cell><cell>W1</cell><cell>1.81</cell><cell>0.03</cell></row><row><cell>CDN-SDN</cell><cell>W 2 2</cell><cell>1.91</cell><cell>0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison of bin sizes. We report the disparity error on Scene Flow using CDN-PSMNET model. It is the only hyper-parameter to tune and only integral values are considered.</figDesc><table><row><cell cols="4">Bin size EPE 1PE 3PE</cell></row><row><cell>1</cell><cell cols="3">1.22 13.9 4.33</cell></row><row><cell>2</cell><cell>0.98</cell><cell>9.1</cell><cell>3.99</cell></row><row><cell>4</cell><cell cols="3">1.52 26.1 4.17</cell></row><row><cell cols="4">smaller bin size makes (a) harder. A larger bin size makes (a) easier but makes (b) harder as the range</cell></row><row><cell>of offsets gets larger.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Ablation studies on the MM ground truths. We conduct experiments using CDN-SDN MM on Scene Flow (cf.Table 4of the main paper).</figDesc><table><row><cell>α 0.8 3 k RMSE ABSR 1.80 0.028 0.8 5 1.82 0.029 0.8 7 1.88 0.035 0.5 3 1.81 0.029 0.2 3 2.20 0.062</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Depth error on KITTI Val. We compare SDN and CDN-SDN models.</figDesc><table><row><cell></cell><cell cols="2">Depth errors (m)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Mean Median RMSE ABSR</cell></row><row><cell>SDN</cell><cell>0.589</cell><cell>0.128</cell><cell>3.08</cell><cell>0.044</cell></row><row><cell cols="2">CDN-SDN 0.524</cell><cell>0.093</cell><cell>3.00</cell><cell>0.042</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our work is reminiscent of G-RMI pose estimator<ref type="bibr" target="#b29">[30]</ref>, which predicts the heatmaps (at fixed locations) and offsets for each keypoint. Our work is also related to one-stage object detectors<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> that predict the class probabilities and box offsets for each anchor box.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For dealing with disparity or depth values at a pixel, our metric space naturally becomes R 1 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We apply the GANET Deep model introduced in the released code of<ref type="bibr" target="#b56">[57]</ref>, available at https://github. com/feihuzhang/GANet. The main architectures of GANET Deep and GANET are the same, while the former has some more 2D and 3D convolutional layers.<ref type="bibr" target="#b3">4</ref> There are two possible reasons that CDN-GANET Deep does not outperform GANET Deep on all the pixels. First, CDN overly focuses on foreground pixels. Second, we used the same hyper-parameters as the original GANET without specific tuning for CDN. We note that the ratio of foreground/background pixels is ∼ 0.15/0.85; the degradation by CDN on the background is ∼ 0.16 3PE, smaller than the gain on foreground.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Using a bin size s = 2 without offsets, the mode is restricted to integral values and EPE suffers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by grants from the National Science Foundation NSF (III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822, OAC-1934714), the Office of Naval Research DOD (N00014-17-1-2175), the Bill and Melinda Gates Foundation, and the Cornell Center for Materials Research with funding from the NSF MRSEC program (DMR-1719875). We are thankful for generous support by Zillow, SAP America Inc, AWS Cloud Credits for Research, Ohio Supercomputer Center, and Facebook.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing modular cnn architectures for joint depth prediction and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hosseini Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kesten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nadhamuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazakova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Platinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<ptr target="urlhttps://level5.lyft.com/dataset/,2019.3" />
		<title level="m">Lyft level 5 av dataset 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Realtime 3d object detection for automated driving using stereo vision and semantic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Königshof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><forename type="middle">Ole</forename><surname>Salscheider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geometry-aware deep network for singleimage novel view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conservative wasserstein training for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bvk</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">An invitation to statistics in wasserstein space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object-centric stereo matching for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end pseudo-lidar for image-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On wasserstein two-sample testing and related families of nonparametric tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Nicolás García Trillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-accuracy stereo depth maps using structured light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Introduction to optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Thorpe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Anytime stereo image depth estimation on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Lecture notes in statistical methods for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided predictionand-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zoomnet: Part-aware adaptive zooming neural network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient joint segmentation, occlusion labeling, stereo and flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Upgrading optical flow to 3d scene flow through optical expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">SegStereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Depth estimation meets inverse renderingfor single image novel view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Visual Media Production</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for end-to-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adaptive unimodal cost volume filtering for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
