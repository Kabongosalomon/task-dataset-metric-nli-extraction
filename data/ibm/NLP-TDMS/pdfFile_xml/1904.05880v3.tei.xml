<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Factor Graph Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Schwartz</surname></persName>
							<email>idansc@cs.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Technion</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
							<email>seunghak@csail.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT CSAIL 3 UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hazan</surname></persName>
							<email>tamir.hazan@technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Technion</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
							<email>aschwing@illinois.edu</email>
						</author>
						<title level="a" type="main">Factor Graph Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dialog is an effective way to exchange information, but subtle details and nuances are extremely important. While significant progress has paved a path to address visual dialog with algorithms, details and nuances remain a challenge. Attention mechanisms have demonstrated compelling results to extract details in visual question answering and also provide a convincing framework for visual dialog due to their interpretability and effectiveness. However, the many data utilities that accompany visual dialog challenge existing attention techniques. We address this issue and develop a general attention mechanism for visual dialog which operates on any number of data utilities. To this end, we design a factor graph based attention mechanism which combines any number of utility representations. We illustrate the applicability of the proposed approach on the challenging and recently introduced VisDial datasets, outperforming recent state-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dialog is an effective way for humans to exchange information. Due to this effectiveness it is an important research goal to develop artificial intelligence based agents for humancomputer conversation. However, when humans talk to each other, subtle details and nuances are often very important. This importance of subtle details and nuances makes development of agents for visual dialog a challenging endeavor.</p><p>Recent efforts to facilitate human-computer conversation about images focus on image captioning, visual question answering, visual question generation and very recently also visual dialog. To this end, Das et al. <ref type="bibr" target="#b9">[10]</ref> collected, curated and provided to the general public an impressive dataset, which allows to design virtual assistants that can converse. Different from image captioning datasets, such as MSCOCO <ref type="bibr" target="#b28">[29]</ref>, or visual question answering datasets, such as VQA <ref type="bibr" target="#b4">[5]</ref>, the visual dialog dataset contains short dialogs about a scene between two people. To direct the dialog, the dataset was collected by showing a caption to the first person ('questioner') which attempts to inquire more about the hidden image. The second person ('answerer') could see both the * Work conducted while the author was at Samsung Research.  <ref type="figure">Figure 1</ref>: Illustration of our factor graph attention. We show two consecutive questions in a dialog. The image attention correlates well with the question. Attention over history interactions allows our model to attend to subtle nuances. The caption focuses on the last word due to given potential priors. Attention over the answers focuses on specific options. The attended options usually correlate with the correct answer. Note: for readability, we chose to display only the top-10 answers out of 100 possible ones. image and its caption to provide answers to these questions. Beyond releasing the Visual Dialog dataset, to ensure a fair comparison, Das et al. <ref type="bibr" target="#b9">[10]</ref> propose a particular task that can be evaluated precisely. It asks the AI system to predict the next answer given the image, the question, and a history of question-answer pairs. A variety of discriminative and generative techniques have been discussed, ranging from deep nets with Long-Short-Term-Memory (LSTM) units <ref type="bibr" target="#b17">[18]</ref> to more involved ones with memory nets <ref type="bibr" target="#b50">[51]</ref> and hierarchical LSTM architectures <ref type="bibr" target="#b44">[45]</ref>.</p><p>One of the successful techniques to improve visual question answering is the attention mechanism <ref type="bibr" target="#b32">[33]</ref>. Due to the similarity of visual question answering and visual dialog, we envision similar improvements to be realizable. In fact, some approaches point in this direction and use a subset of the available data utilities to direct question answering <ref type="bibr" target="#b31">[32]</ref>. However, in visual dialog many more "data parts," i.e., the image, the question, the history and the caption are involved and have been referred to as 'modalities.' To avoid confusion with the original convention/sense of the word modality, we coin the term "utilities" to refer to different parts of the available data. Taking all utilities into account makes it computationally and conceptually much more challenging to develop an effective attention mechanism. While ignoring utilities when computing attention is always an option, we argue that subtle details and nuances can only be captured adequately if we focus on all available signals.</p><p>To address this issue we develop a general factor graph based attention mechanism which combines representations of any number of utilities. Inspired by graphical models, we use a graph based formulation to represent the attention framework, where nodes correspond to utilities and factors model their interactions. A message passing like procedure aggregates information from modalities which are connected by edges in the graph.</p><p>We demonstrate the efficacy of the proposed multi-utility attention mechanism on the challenging and recently introduced Visual Dialog dataset, realizing improvements up to 1.1% on MRR. Moreover, we examine our model behavior using question generation proposed by <ref type="bibr" target="#b20">[21]</ref>. Examples of the computed attention for visual question answering are illustrated in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years various machine learning techniques were developed to tackle cognitive-like multimodal tasks, which involve both vision and language processing. Image captioning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref> was an instrumental language+vision task, followed by visual question answering <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> and visual question generation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Instrumental to cognitive tasks are attention models, that enable interpretation of the machine's cognition and often improve performance. While attention mechanisms have been applied to visual question answering <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b57">58]</ref>, few works have addressed visual dialog because of the many different data utilities. Here, we develop an attention mechanism for visual dialog, a cognitive task that was created to imitate human-like decisions <ref type="bibr" target="#b9">[10]</ref>. We build a general attention mechanism that is capable of capturing details. In the following we briefly review visual question answering and visual dialog, focusing on the use of attention. Visual Question Answering (VQA): Visual question answering is considered a simplified version of visual dialog since it consists of a single interaction with a given image. Some discriminative approaches include a pre-trained convolutional neural network with question embedding to predict the correct answer <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b34">35]</ref>. Quickly, attention mechanisms have emerged as a tool to augment the spatial attention of the image. Yang et al. <ref type="bibr" target="#b56">[57]</ref> created a multi-step reasoning system via an attention model. Fukui et al. <ref type="bibr" target="#b13">[14]</ref> and Kim et al. <ref type="bibr" target="#b24">[25]</ref> suggested an efficient multi-modal pooling method before applying attention using a compact outer product which was later improved using the Hadamard product. Zhu et al. <ref type="bibr" target="#b57">[58]</ref> treated image attention as a structured prediction task over regions, by first generating attention beliefs via unary and pairwise potentials, for which a probability distribution is inferred via loopy belief propagation.</p><p>Alternatively, Lu et al. <ref type="bibr" target="#b32">[33]</ref> suggested to produce Co-Attention for the image and question separately, using a hierarchical formulation. Schwartz et al. <ref type="bibr" target="#b41">[42]</ref> later extended this approach for the multiple-choice VQA variant, applying attention over image, question and answer via unary, pairwise and ternary potentials. Visual Dialog: D. Geman et al. <ref type="bibr" target="#b15">[16]</ref> were among the first to generate dialogs over images. These early attempts used only street scene images, and also restricted the conversation to templated, binary questions. A discriminative and generative approach was later introduced by Das et al. <ref type="bibr" target="#b9">[10]</ref>, along with the largest visual dialog dataset, VisDial. Concurrently, GuessWhat, another visual dialog dataset was published <ref type="bibr" target="#b10">[11]</ref>. GuessWhat is a goal driven dialog dataset for object identification, while VisDial focuses on human-like interactions. For instance, in <ref type="figure">Fig. 1</ref>, the answer for the question "are kids wearing hats?" is "0 of them wearing hats," while a goaldriven interaction will answer with a simple "no." While both types of dialogs are challenging, VisDial interactions typically consider more subtle nuances. Another work by Mostafazadeh et al. <ref type="bibr" target="#b36">[37]</ref>, focuses on conversation generation around images, instead of the content visible in images.</p><p>The VisDial dataset is accompanied with three baselines. A vanilla approach which encodes the image, dialog and history separately and combines them subsequently (i.e., late fusion). A more complex approach based on a memory network <ref type="bibr" target="#b50">[51]</ref>, which maintains previous question and answer as facts in a memory bank, and learns to retrieve the appropriate fact. Lastly, a hierarchical encoding approach to capture the history <ref type="bibr" target="#b44">[45]</ref>. Seo et al. <ref type="bibr" target="#b43">[44]</ref> propose a memory network based on attention, which also addressed co-referential issues. Later, Lu et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> combined a generative and discriminative model to choose generated answers, and also proposed history attention conditioned on the image using hierarchical co-attention developed for visual question answering. Wu et al. <ref type="bibr" target="#b52">[53]</ref> apply attention over image, question and history representation using a Generative Adversarial Network (GAN) to create a more human-like response. Jain et al. <ref type="bibr" target="#b20">[21]</ref> developed a discriminative model that produces a binary score for each possible answer by concatenating representations of all utilities. While Jain et al. <ref type="bibr" target="#b20">[21]</ref> also consider all utilities for interaction prediction, our work differs in important aspects: (1) we develop an attention mechanism that weights different representations; <ref type="bibr" target="#b1">(2)</ref> when predicting an answer, we take information from other possible answers into account. Recently, Kottur et al. <ref type="bibr" target="#b26">[27]</ref> focused on visual co-reference resolution for visual dialog. Their approach relies on a weak supervision of a parser for reasoning <ref type="bibr" target="#b19">[20]</ref>, and a co-referential solver <ref type="bibr" target="#b7">[8]</ref>. While co-reference resolution is not the focus of our work, we found our attention model to exhibit some co-reference resolution abilities.</p><p>Among all attention-based techniques for Visual Dialog, the most relevant to our approach is work by Wu et al. <ref type="bibr" target="#b52">[53]</ref> and Lu et al. <ref type="bibr" target="#b31">[32]</ref>. Both generate Co-Attention over the image, the question and the history representation in a hierarchical fashion. Their hierarchical approach is based on a sequential process, computing attention for one utility first and using the obtained result to generate attention for another utility subsequently. As the ordering is important, their framework is not straightforward to extend to a general multi-utility setting.</p><p>In contrast, we develop a general attention model for any number of utilities. In the visual dialog setting, those utilities are the question in the history (10 utilities), each answer in the history (10 utilities), the caption (1 utility), the image (1 utility) and the answer representation (1 utility). To work with a total of 23 utilities, we constructed a general attention framework that may be applied to any high-order utility setting. With our general purpose attention model we improve results and achieve state-of-the-art performance.</p><p>To demonstrate the generality of the approach, we also follow Jain et al. <ref type="bibr" target="#b20">[21]</ref> and evaluate the proposed approach on choosing an appropriate question given the previous question and answer. There too we obtain state-of-the-art results. Attention in General: More generally, attention models have been applied to graphical data structures. For example, Graph Attention Networks use an MRF approach to embed graph-structured data, e.g., protein-protein interactions <ref type="bibr" target="#b47">[48]</ref>. Also, attention for non-structured tasks (e.g., chain, tree) were discussed in the past <ref type="bibr" target="#b25">[26]</ref>. These works differ from ours in important aspects: they are used to embed a structure based model, e.g., a graph, and provide a probability distribution across nodes of the graph. Instead, our model provides attention for entities within each node of the graph, e.g., the words of a question or the pixels in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Factor Graph Attention</head><p>In the following we describe a general framework to construct a multi-utility attention model using factor graphs.</p><p>The factor graph is defined over utilities, which, in the visual dialog setting, consists of an image I, an answer A, a caption C, and a history of past interactions H Qt , H At t∈{1,...,T } . We subsume all utilities within the set U = {I, A, C, H Qt , H At t∈{1,...,T } }. In our work we have 23 utilities (10 history questions, 10 history answers, the image, answer and caption). For notational convenience and to demonstrate the generality of the formulation we also refer to the set of utilities via U = {U 1 , . . . , U |U | }. Each utility U i ∈ U, for i ∈ {1, . . . , |U|} consists of basic entities, e.g., a question is composed of a sequence of words and an image is composed of spatially ordered regions.</p><p>Formally, the i-th utility U i is a d i × n i matrix which consists of n i entitiesû i ∈ U i , which are the d i -dimensional columns of the matrix. Each vectorû i ∈ U i is embedded in its respective Euclidean space, i.e.,û i ∈ R di , where d i is the embedding dimension of the i-th utility. We use the index u i ∈ {1, . . . , n i } to refer to a specific column inside the matrix U i , i.e., we extract the u i -th column viaû i = U i,ui .</p><p>The |U| nodes in the factor graph each represent attention distributions over their n i utility elements, which we call beliefs. To infer the probability we take into account two types of factors: 1) Local factors which capture information within a utility, such as their entity representation and their local interactions. 2) Joint factors which capture interactions of any subset of utilities. Due to the high number of utilities, in our attention model, we limit ourselves to pairwise factors. Next we will explain our construction of local factors and joint factors. Note, bias terms are omitted for readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Local factors</head><p>The local factors capture the local information in an employed utility U i . Each utility contains entities, i.e., words in a sentence or regions in an image. There are two types of information within a utility U i : Entity information, which is extracted from an entity's vector representationû i ∈ U i and Entity interactions, which capture dependencies between two entities, such as two words in the same question or two regions in the same image. Entity information: This representation is obtained as the result of an embedding model, such as a Long-Short-Term-Memory (LSTM) net for sentences or a convolutional layer for image regions. Each vector representationû i ∈ U i has the potential to focus the model's attention to the entity the vector is representing. The potential function ψ i (u i ) is parametrized by the i-th utility's parameters V i and v i , and is obtained via</p><formula xml:id="formula_0">ψ i (u i ) = v i relu(V iûi ). Hereby, v i ∈ R di , V i ∈ R di×di are trainable parameters.</formula><p>Recall that the index u i ∈ {1, . . . , n i } refers to a specific entity. During training we also apply a dropout operation after the first linear embedding (i.e., V iûi ). Entity interactions: The factor dependency between two elements is extracted from their vector representation. Given two indices u 1 i , u 2 i ∈ {1, . . . , n i }, we embed the two corresponding entity representation vectorsû 1 i ,û 2 i in the same Euclidean space, and compute the factor dependency on both entities using the dot product operation, i.e.,</p><formula xml:id="formula_1">ψ ii (u 1 i , u 2 i ) = L iû 1 i L iû 1 i R iû 2 i R iû 2 i , where L i ∈ R di×di , R i ∈ R di×di are trainable parameters,</formula><p>governing the left and right arguments respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint factors</head><p>Joint factors capture interactions between two elements of different utilities, e.g., between a word in the question and a region in the image. Similarly to entity interaction factors within a utility, we use</p><formula xml:id="formula_2">ψ ij (u i , u j ) = L ijûi L ijûi R jiûj R jiûj , where L ij ∈ R di×d , R ji ∈ R dj ×d are trainable parameters.</formula><p>For simplicity we let d = max{d i , d j } be the maximum dimension between the two utilities. </p><formula xml:id="formula_3">... { } { } h A t 1 { } { } h A t n h ... ... { } { } h Q t 1 { } { } h Q t n H ... c 1 c n c ... ... LSTM ... q 1 q n Q ... CNN ... i 1 i n I ... A 1 A 1 0 0 I { } H Q t { } H A t C Factor Graph Attention a Q a I a { } H A t { } a H t a C a { } H Q t a A : a 1 a Q</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="100">D scores</head><p>Multiclass cross entropy loss <ref type="figure">Figure 2</ref>: Our state-of-the-art architecture for the Visual Dialog task. Implementations details can be found in Sec. 4.</p><formula xml:id="formula_4">0 0 : 1 : 0 a I . . . { } a H 1 { } a H t a C d A Fusion Net a A . . . . . . . . . d A ⋅ t d H d C d I d Q    (Sec. 4.1)    (Sec. 4.2)    (Sec. 4.3)</formula><p>To avoid a situation where pairwise scores (e.g., image and question) negatively bias another one (e.g., image and caption), proper normalization is necessary. Since the pairwise interaction scores are generated during training, we chose a batch normalization <ref type="bibr" target="#b30">[31]</ref> operation which fixes the bias during training. Additionally, we applied an L 2 normalization on u i and u j to be of unit norm before the multiplication, i.e., we use the cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention, messages and beliefs</head><p>For each utility U i we infer the amount of attention that should be given to each of its elementsû i ∈ U i . Motivated by classical message-passing algorithms, we first collect all dependencies of a given utility element via</p><formula xml:id="formula_5">µ j→i (u i ) = uj ∈{1,...,nj } W ij (u i , u j )ψ ij (u i , u j ),</formula><p>where W ij (u i , u j ) ∈ R is a trainable parameter. We aggregate these messages from all pairwise factor dependencies and send them to a utility, in order to infer its attention belief. The inferred attention belief</p><formula xml:id="formula_6">b i (u i ) ∝ exp  ŵ i p i (u i ) + w i ψ i (u i ) + |U | j=1 w ij µ j→i (u i )   ,</formula><p>also uses local entity information.</p><p>Hereby w ij , w i are scalar weights learned per utility. These scalars reflect the importance of one utility with respect to the others. For instance, for the image belief, we find by examining these weights that the question utility is more important than the caption utility. This makes sense since we want to look at relevant places for the question. Moreover, p i is a prior potential for the i-th utility, andŵ i is a trainable parameter to calibrate the prior potential's importance. For instance, the question utility prior encourages focus of its attention onto the last word in the question, a common practice in LSTM networks. Using priors, we are able to steer the desired belief for a utility, while still allowing guidance of other utilities via pairwise interactions. We To infer the probability we aggregate two types of messages: 1) A joint factor message, constructed from interactions of entities from different utilities, e.g., ΨQ,I . 2) A local factor: learned from the entity representation, e.g., ΨQ, and the self entity interactions, e.g., ΨQ,Q. T is the number of history dialog interactions.</p><p>also experimented with priors that are updated after we infer the attention through steps, but we didn't find it to improve the results in our setup. Once the attention belief b i (u i ) is computed for each entity representationû i ∈ U i , we obtain the attended vector of this utility as the average representation. This reduces the utility representation to a single vector, which is dependent on the other utilities via the belief b i (u i ):</p><formula xml:id="formula_7">a i = ui∈{1,...,ni} b i (u i ) ·û i .</formula><p>Note that a i is the attended representation of utility U i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Visual Dialog</head><p>We use visual dialog to demonstrate the generality of the discussed attention mechanism because many utilities are available. A general overview of the approach is illustrated in <ref type="figure">Fig. 2</ref>. We detail next how the general factor graph attention model is applied to visual dialog by describing (1) the utility embeddings, (2) the attention module, and (3) the fusion of attended representations for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Utilities and Embeddings</head><p>In the following, we describe the embeddings of the image and textual utilities. Image utility: To represent the image regions, we use a conv net, pre-trained on ImageNet <ref type="bibr" target="#b11">[12]</ref>. Taking the output of the last convolutional layer we obtain a representation of 7 × 7 × 512. Specifically, 7 × 7 is the spatial dimension of the convolutional layer and 512 is the number of channels/features of the representation. Following our notation in Sec. 3, the visual utility U i has dimensions n i = 49 and d i = 512. To fine-tune this representation to our task, we feed it into another convolutional layer, with a 1 × 1 kernel, followed by a ReLU activation and a dropout. Textual utilities: Our textual utilities are the caption, the question, the possible answers and the history interactions. For each textual utility U i we embed up to n i words. Sentences with a shorter length are zero padded, while sentences of longer length are truncated. The embedding starts with a one-hot encoding representation of the word index, followed by a linear transformation. The linear transformation embeds the word index into the Euclidean space. This embedding is identical for all textual utilities. Intuitively, usage of the same embedding ensures a better consistency between the textual utilities and we also found it to improve the results.</p><p>Each embedded representation for each textual utility is fed into an LSTM layer, which yields a representation with the appropriate embedding dimension. The caption utility C and the question utility Q are generated by applying a dedicated LSTM on the respective embedded representation. In contrast, we embed all history questions H Qt t∈{1,...,T } using the same LSTM model. We also embed all history answers H At t∈{1,...,T } using another LSTM model.</p><p>The answer utility subsumes n A possible answers and it consists of the final decision of the model in our visual dialog system. Our answer utility uses the same LSTM to embed each of the n A = 100 answers separately, the embedding of each possible answer is the LSTM hidden state of the last word in the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attention module</head><p>The attention step infers the importance of each entity in each utility, using our Factor Graph Attention (see Sec. 3), and creates an attended representation. In the visual dialog setting, for each answer generation step we use an image I, a question Q, an answer A, a caption C, and a history of past interactions H Qt , H At t∈{1,...,T } (see <ref type="figure" target="#fig_0">Fig. 3</ref> for an illustration). In the following we describe the special treatment of the different entities as well as their respective priors. Group utilities and dependency-relaxation: Our factor graph attention model may have a large number of trainable parameters, as it grows quadratically with the number of utilities. To address this concern, we observe that we can group some utilities, e.g., the history answers H At t∈{1,...,T } , and the history questions H Qt t∈{1,...,T } . To take advantage of the dependency between the group of utilities, we share the factor weights across all the group utilities. For example, for two utilities U i1 , U i2 ∈ H At we enforce the parame-</p><formula xml:id="formula_8">ter sharing v i1 = v i2 , V i1 = V i2 , L i1 = L i2 , R i1 = R i2 , L i1,j = L i2,j and R j,i1 = R j,i2</formula><p>. Not only did it contribute to a reduced memory consumption, but we also observed this grouping to improve the results. We attribute the improvement to better generalization of the factors.</p><p>The answer utility U i encodes each of the possible n i answers in a d i -dimensional vector, using the LSTM hidden state at the last word. <ref type="figure">Fig. 1</ref> shows that the attention beliefs correlate with the correct answer. Note that we didn't attend separately to each possible answer. Doing so would have resulted in increased computational demand and we didn't find improved model performance. We conjecture that due the fact that the number of words within an answer is usually small, a complete attention model on each and every word of the answer does not seem to be necessary. Priors: The prior potentials for the question and caption utilities are important in practice. For both utilities we set the prior to emphasize the last word by focusing the energy onto the last hidden state index. We use a one hot vector with the high bit set for the last hidden state index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fusion Step</head><p>The fusion step, outlined in <ref type="figure">Fig. 2</ref> combines the attended representations a i from all utilities {I, A, C, H Qt , H At t∈{1,...,T } } to find the best answer. This is performed by creating a probability distribution p(u A |I, Q, C, A, H) for each answer index u A ∈ {1, . . . , n A }, where n A = 100 is the number of possible answers.</p><p>We denote by a I ∈ R d I the attended image vector, a A ∈ R d A the attended answer vector, and a C ∈ R d C the attended caption vector. We construct the attended history vector a H ∈ R d I from the attended history utilities H Qt , H At t∈{1,...,T } . For this purpose, we start by concatenating the attended vector of each history question a Qt with the concurrent history answer a At , and fuse them using a linear transformation with a bias term to obtain a t , which is a d t -dimensional vector. We then concatenate the attended history vectors a t for the entire dialog history t ∈ {1, . . . , T }, which results in an attended history representation a H ∈ R d H . Note that d H = T t=1 d t . We concatenate the image, question, caption and history attended representations, which yields an attention representation a ∈ R L of length</p><formula xml:id="formula_9">L = d I + d Q + d C + d A + d H .</formula><p>Next, we combine the image, question, caption and history attended representation a ∈ R L with the n A = 100 possible answers to compute a probability for each answer. Let U A ∈ R n A ×d A be the answer utility, with N = n A = 100 answers, while each answer is embedded in a d A -dimensional space. For each answer, we denote byû A ∈ R d A its embedded vector. We concatenate each answer embedding with the system attention (a,û A ) to obtain a (L + d A )-dimensional vector and feed it into a multi-layer perception with two layers of size (L + d A )/2 and (L + d A )/4 respectively. Between each layer we perform batch normalization followed by a ReLU activation. We used a dropout layer before the last fully connected layer. The obtained scores are turned into probabilities, for each  answer, using a softmax (·) operation, which yields the posterior probability for each answer p(u A |I, Q, C, A, H). The approach is trained using maximum likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In the following we evaluate the proposed factor graph attention (FGA) approach on the Visual dialog dataset, which we briefly describe first. Our code is publicly available 1 . Visual Dialog Dataset: We used VisDial v0.9 to train the model. The dataset consists of approx. 120k images from COCO <ref type="bibr" target="#b28">[29]</ref>. Each image is annotated with a dialog of 10 questions and corresponding answers, for a total of approx. 1.2M dialog question-answer pairs. In the discriminative setup, each question-answer pair is given 100 plausible possible answers, the model needs to choose from. We follow <ref type="bibr" target="#b9">[10]</ref> and split the data into 80k images for train, 40k for test and 3k for validation. Experimental setup: We used a batch size of 64. We set the word embedding dimension to d E = 128, and the utility embeddings to d Q = 512 and d C = 128. For each question or answer in the history we use d</p><formula xml:id="formula_10">H Q i = d H A i = 128.</formula><p>For each possible answer we use d a = 512. The lengths are set equally for all textual utilities n Q = n C = n a = n H Q = n H A = 20. The VisDial history consists of T = 10 questions with their answers. For our image representation we use the last conv layer of VGG having dimensions of 7 × 7 × 512. After flattening the 2D spatial dimension, n I = 49. The dropout parameter after the image embedding is set to 0.5, the dropout parameter before the last fc layer is set to 0.3. Training: The total amount of trainable parameters in our model is 17, 848, 416. We initialized all the weights in the model using Kaiming normal initialization <ref type="bibr" target="#b16">[17]</ref>. To train the   model we used a multi-class cross entropy loss, where each possible answer represents a class. We used Adam optimizer with a learning rate of 10 −3 . We evaluate our performance on the validation set after each epoch to determine when to stop our training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Evaluation</head><p>Evaluation metrics: Evaluating dialog systems, or any other generative tasks is challenging <ref type="bibr" target="#b29">[30]</ref>. We follow <ref type="bibr" target="#b9">[10]</ref> and evaluate each individual response at each of the T = 10 rounds in a multiple-choice setup. The model is hence evaluated on retrieval metrics: Recall@k is the percentage of questions where the human response was part of the top k predicted answers. Mean rank is the average rank allotted by a model to the human response, hence a lower score is desired. Mean Reciprocal Rank (MRR) is defined as</p><formula xml:id="formula_11">1 |Q| |Q| i=1</formula><p>1 ranki , where rank i is the rank of the human response, and Q is the set of all questions. The perfect score, i.e., MRR = 1 is achieved when the human response is consistently ranked first. Visual question answering comparison: We first compare against a variety of baselines (see Tab. 1). Note that almost all of the baselines (except LF, HRE and MN and SF-QIH-se-2) use attention, i.e., attention is an important element in any model. Note that our model uses the entire set of answers to predict each answer's score, i.e., we use p(u i |A, I, Q, C, H) This is in contrast to SF-QIH-se-2, which doesn't use attention and models p(u i |û i , I, Q, C, H). Notable as well, the current state-of-the-art model, CoAtt-GAN <ref type="bibr" target="#b52">[53]</ref>, used the largest amount of utilities to attend to, i.e., image, question and history. Because CoAtt-GAN uses a hierarchical ap- <ref type="figure">Figure 4</ref>: An illustration of question and image attention over a series of interactions for the same dialog. In addition we provide the ground truth answer, i.e., GT, and our predicted answer, i.e., A. proach, the ability to further improve the reasoning system is challenging and manual work. In contrast, our general attention mechanism allows to attend to the entire set of cues in the dataset, letting the model automatically choose the more relevant cues. We refer the readers to the appendix for analysis of utility-importance via importance score. As can be seen from Tab. 1, this results in a significant improvement of performance, even when compared to the very recently published baselines <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b26">27]</ref>. We also report an ensemble of 9 models which differ only by the initial seed. We emphasize that our approach only uses VGG16. Lastly, some baselines report to use GloVe to initialize the word embeddings, while we didn't use any pre-trained embedding weights.</p><p>Our attention model is very efficient to train. Our stateof-the-art score is achieved after only 4 epochs. Each epoch takes approximately 2 hours on a standard machine with an Nvidia Tesla M40 GPU. In contrast, CorefNMN <ref type="bibr" target="#b26">[27]</ref>, has 100M parameters and takes 33 hours to train on a Titan X. Both <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b52">53]</ref> report that more than 25 epochs 101M parameters and 50 hours were required for training. Visual question generation comparison: To assess question generation, <ref type="bibr" target="#b20">[21]</ref> proposed to predict the next question given the previous question and answer. Their introduced question prediction dataset is based on VisDial v0.9, along with a collected set of 100 question candidates. We adapted to this task, by changing the input utilities to the previous interaction (Q + A) t−1 instead of the current question Q t . Our model also improves previous state-of-theart results (see Tab. 2).</p><p>Visual Dialog Challenge: Recently, VisDial v1.0 was released as part of the Visual Dialog challenge, where 123,287 images are used for training, 2,000 images for validation, and 8,000 images for testing. For the test split each image consists of only 1 interaction, at some point of time in the dialog. Furthermore, an additional metric, normalized discounted cumulative gain (NDCG), was introduced. NDCG uses dense annotations, i.e., the entire set of candidate answers is annotated as true or wrong. The metric penalizes low ranking correct answers, addressing issues when the set of answers contains more than one plausible result.</p><p>Our submission to the challenge significantly improved all metrics except for NDCG. We report our results in Tab. 3 on test-std, a 4,000 image split, the other 4,000 image split was preserved for the challenge. While the challenge did allow use of any external resources to improve the model, we only changed our approach to use an ensemble of 5 trained Factor Graph Attention models which were initialized randomly. All other top teams used external data in form of detection features on top of ResNet-152, inspired by Top-Bottom attention <ref type="bibr" target="#b1">[2]</ref>. These features are expensive to extract, and use external detector information.</p><p>Our model used only the single ground truth answer to train. Therefore it is expected that our model isn't optimized w.r.t. the NDCG metric. However, given the small subset of densely annotated samples (2,000 out of the 123,287 train images), it is hard to carefully analyze this result. Ablation Study: We asses (1) design choices of our factor graph attention; and (2) utility ablation focusing on history and answer cues as they are a unique aspect of our work.  <ref type="figure">Figure 5</ref>: Illustration of history attention for 2 interactions. We observe small nuances of history to be useful to answer questions, and improve co-reference resolution.</p><p>(1) In Tab. 4 we see that FGA improves the MRR of a model without attention by 3% (0.6249 vs. 0.6653). This ablation study shows that attention is crucial for VisDial. Removing local-information drops MRR to 0.6425. When omitting local-interactions, i.e., a score based on interactions of embedding representations of a utility, the MRR drops to 0.6369. BatchNorm over pairwise interactions is crucial. Without BatchNorm MRR drops to 0.6301. Removing prior information, e.g., a high prior potential for the last word in the question is less crucial, dropping MRR to 0.6451.</p><p>(2) Our history attention attends separately to questions and answers in the history. In contrast, classical methods <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b43">44]</ref> attend over history locations only. Based on Tab. 5, we note that our fine-grained history attention improves MRR from 0.6494 to 0.6525. Without the answers utility, performance on MRR drops significantly from 0.6525 to 0.6294. If we attend to each word in the answers separately, i.e., 'Answers Fine-Attention,' performance drops to 0.6478. Other Datasets: When we replace the attention unit of other methods with our FGA unit we observe improvements in visual question answering (VQA) and audio-visual scene aware dialog (AVSD) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref>. For VQA v1.0 we increase validation set accuracy from 57.0 to 57.3 (no tuning) by replacing the alternating and parallel attention <ref type="bibr" target="#b32">[33]</ref>. For AVSD, we improve Hori et al. <ref type="bibr" target="#b18">[19]</ref> which report a CIDEr score of 0.733 to 0.806. We used FGA to attend to all video cues as well as the question. This differs from Hori et al. who mix the question representation with video-related cues (e.g., I3D features, optical flow and audio features), and aggregate them to generate attention. Other components remain the same. Our flexible framework is instrumental for this improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative Evaluation</head><p>Attention is an important tool not only because it boosts performance, but also because it yields a weak form of interpretability. By illustrating the attention beliefs, we can observe the reasoning process of the model. In <ref type="figure">Fig. 4</ref> we provide co-attention of image and question. The first row shows dialogs with yes/no questions. We observe the question attention to focus on the indicative word, e.g., people, animals, buildings, cars, etc., while the image attention performs detection and attends to the relevant area of the image. For the second row, again we observe plausible attention behavior. An interesting failure-case: when asked about the color of the bat, the ground-truth answer was "light brown," while our model answered "brown, black and white" instead. A possible explanation is related to the fact that the image is in black and white. The last line shows that questionanswering type of task is always debatable. For the question "what is the weather like?" the model answered "cloudy," while the ground truth is "it looks sunny and warm." While it does look sunny, the model attends to clouds and the model answer likely isn't entirely wrong.</p><p>Next, in <ref type="figure">Fig. 5</ref>, we show how attention is useful when applied over each question in the history. In the first row, for the question "is this at a skateboarder park?", the skateboard related terms in the history are given more weight. Another use case of attention is co-reference resolution. We highlight those results in the second row: the word "they" in the second question refers to people in the background, which remain the focus of the attention model.</p><p>Lastly, in <ref type="figure" target="#fig_2">Fig. 6</ref>, we evaluate question generation and let the model interact with the answer predictor. We show how complete dialogs can be generated in a discriminative manner. We first observe that attention for question generation is noisier. This seems intuitive because asking a question requires a broader focus than answering. Nonetheless, visual input is important. For the second row second image, "at what angle is the man sitting?" the model attends mostly to the man, and for the question "is it a laptop or desktop?" image attention focuses on the laptop. Also, in both cases the caption attention is useful. For instance, in the first row, the word "tie" is picked to generate two relevant questions. This nicely illustrates how the proposed model adapts to tasks, when the importance of different data cues changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We developed a general factor graph based attention mechanism which can operate on any number of utilities. We showed applicability of the proposed attention mechanism on the recently introduced visual dialog dataset and outperformed existing baselines by 1.1% on MRR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>A graphical representation of our attention unit. Each node represents an attention probability over the utilities' entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Bottom-up Features: We follow Anderson et al. [2] and use bottom-up features of 36 proposals from images. Equipped with bottom-up features as image representation our ensemble network increase MRR score on VisDial v1.0 by 2% (0.673 vs 0.693). For a single model we observe a similar boost in performance (0.6525 vs 0.6712) on VisDial v0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>is there any design on her tie ? what color is her tie ? A: purple C: a woman in a tie and blue hair . is it a laptop or desktop ?at what angle is the man sitting ? A: leaning over slightly C: a man sitting at a desk on a laptop computer HIllustration of 2 step interaction using visual question generation and illustration of the involved modalities. The classifier receives the previous question and answer, to predict a new one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance of discriminative models on VisDial v0.9. Higher is better for MRR and recall@k, while lower is better for mean rank. (*) denotes use of external knowledge.</figDesc><table><row><cell>Model</cell><cell>MRR R@1 R@5 R@10 Mean</cell></row><row><cell>LF [10]</cell><cell>0.5807 43.82 74.68 84.07 5.78</cell></row><row><cell>HRE [10]</cell><cell>0.5846 44.67 74.50 84.22 5.72</cell></row><row><cell>HREA [10]</cell><cell>0.5868 44.82 74.81 84.36 5.66</cell></row><row><cell>MN [10]</cell><cell>0.5965 45.55 76.22 85.37 5.46</cell></row><row><cell>HieCoAtt-QI [33]</cell><cell>0.5788 43.51 74.49 83.96 5.84</cell></row><row><cell>AMEM [44]</cell><cell>0.6160 47.74 78.04 86.84 4.99</cell></row><row><cell>HCIAE-NP-ATT [32]</cell><cell>0.6222 48.48 78.75 87.59 4.81</cell></row><row><cell>SF-QIH-se-2 [21]</cell><cell>0.6242 48.55 78.96 87.75 4.70</cell></row><row><cell>CorefNMN [27]*</cell><cell>0.636 50.24 79.81 88.51 4.53</cell></row><row><cell cols="2">CoAtt-GAN-w/ R inte -TF [53] 0.6398 50.29 80.71 88.81 4.47</cell></row><row><cell cols="2">CorefNMN (ResNet-152) [27]* 0.641 50.92 80.18 88.81 4.45</cell></row><row><cell>FGA (VGG)</cell><cell>0.6525 51.43 82.08 89.56 4.35</cell></row><row><cell>FGA (F-RCNNx101)</cell><cell>0.6712 54.02 83.21 90.47 4.08</cell></row><row><cell>9×FGA (VGG)</cell><cell>0.6892 55.16 86.26 92.95 3.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance on the question generation task. Higher is better for MRR and recall@k, while lower is better for mean rank.</figDesc><table><row><cell>Model</cell><cell>MRR</cell><cell cols="2">R@1 R@5 R@10 Mean</cell></row><row><cell cols="3">SF-QIH-se-2 [21] 0.4060 26.76 55.17 70.39</cell><cell>9.32</cell></row><row><cell>FGA</cell><cell cols="2">0.4138 27.42 56.33 71.32</cell><cell>9.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance of discriminative models on VisDial v1.0 teststd. Higher is better for MRR and recall@k, while lower is better for mean rank and NDCG. (*) denotes use of external knowledge.</figDesc><table><row><cell>Model</cell><cell cols="3">MRR R@1 R@5 R@10 Mean NDCG</cell></row><row><cell>LF [10]</cell><cell cols="2">0.554 40.95 72.45 82.83 5.95</cell><cell>0.453</cell></row><row><cell>HRE [10]</cell><cell cols="2">0.542 39.93 70.45 81.50 6.41</cell><cell>0.455</cell></row><row><cell>MN [10]</cell><cell cols="2">0.555 40.98 72.30 83.30 5.92</cell><cell>0.475</cell></row><row><cell cols="3">CorefNMN (ResNet-152) [27]* 0.615 47.55 78.10 88.80 4.40</cell><cell>0.547</cell></row><row><cell>NMN (ResNet-152) [20]*</cell><cell cols="2">0.588 44.15 76.88 86.88 4.81</cell><cell>0.581</cell></row><row><cell>FGA (VGG)</cell><cell cols="2">0.637 49.58 80.97 88.55 4.51</cell><cell>0.521</cell></row><row><cell>FGA (F-RCNNx101)</cell><cell>0.662 52.75 82.92 91.07</cell><cell>3.8</cell><cell>0.569</cell></row><row><cell>5×FGA (VGG)</cell><cell cols="2">0.673 53.40 85.28 92.70 3.54</cell><cell>0.545</cell></row><row><cell>5×FGA (F-RCNNx101)</cell><cell cols="2">0.693 55.65 86.73 94.05 3.14</cell><cell>0.572</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Attention-related ablation analysis.</figDesc><table><row><cell>Model</cell><cell>MRR</cell><cell cols="2">R@1 R@5 R@10 Mean</cell></row><row><cell>No Attention</cell><cell cols="2">0.6249 48.67 78.95 87.73</cell><cell>4.69</cell></row><row><cell>No BatchNorm</cell><cell cols="2">0.6301 49.23 79.65 88.32</cell><cell>4.55</cell></row><row><cell cols="3">No Local-Interactions 0.6369 50.17 79.92 88.33</cell><cell>4.55</cell></row><row><cell cols="3">No Local-Information 0.6425 50.12 81.49 89.34</cell><cell>4.37</cell></row><row><cell>No Priors</cell><cell cols="2">0.6451 50.57 81.37 89.00</cell><cell>4.47</cell></row><row><cell>FGA</cell><cell cols="2">0.6525 51.43 82.08 89.56</cell><cell>4.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Utility-related ablation analysis.</figDesc><table><row><cell>Model</cell><cell>MRR</cell><cell cols="2">R@1 R@5 R@10 Mean</cell></row><row><cell>No Answer Utility</cell><cell cols="2">0.6294 49.35 79.31 88.10</cell><cell>4.63</cell></row><row><cell>No History Attention</cell><cell cols="2">0.6449 50.74 81.07 88.86</cell><cell>4.48</cell></row><row><cell>Answers Fine-attention</cell><cell cols="2">0.6478 50.80 81.86 89.25</cell><cell>4.46</cell></row><row><cell cols="3">History No Fine-attention 0.6494 51.17 81.56 89.13</cell><cell>4.43</cell></row><row><cell>FGA</cell><cell cols="2">0.6525 51.43 82.08 89.56</cell><cell>4.35</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/idansc/fga</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A: yes , but i ca n't read any of their signs GT: yes , but i ca n't read any of their signs A: yes but too far away to read GT: yes but too far away to read</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Audio visual scene-aware dialog (avsd) challenge at dstc7</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00525</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep compositional question answering with neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diverse and Coherent Paragraph Generation from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08667</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human attention in visual question answering: Do humans and deep networks look at the same regions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Diverse and Controllable Image Captioning with Part-of-Speech Guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.12589" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? Dataset and Methods for Multilingual Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual turing test for computer vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end audio visual sceneaware dialog using multimodal attention-based video features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Winchern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1704.05526</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two can play this game: Visual dialog with discriminative question generation and answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Creativity: Generating Diverse Questions using Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2017. * equal contribution</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00887</idno>
		<title level="m">Structured attention networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Visual Question Generation as Dual Task of Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1709.07192" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Loffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical questionimage co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Captioning with Multimodal Recurrent Neural Networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Image-grounded conversations: Multimodal context for and response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08251</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating natural questions about an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">High-Order Attention Models for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Simple Baseline for Audio-Visual Scene-Aware Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual reference resolution using attention memory for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Image captioning and visual question answering based on attributes and their related external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<idno>arXiv 1603.02814</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07613</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Structured attentions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
