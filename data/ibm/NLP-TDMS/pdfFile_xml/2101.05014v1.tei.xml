<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFFECTIVE LOW-COST TIME-DOMAIN AUDIO SEPARATION USING GLOBALLY ATTENTIVE LOCALLY RECURRENT NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">W Y</forename><surname>Lam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EFFECTIVE LOW-COST TIME-DOMAIN AUDIO SEPARATION USING GLOBALLY ATTENTIVE LOCALLY RECURRENT NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech separation</term>
					<term>TasNet</term>
					<term>low-cost</term>
					<term>multi-head attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research on the time-domain audio separation networks (Tas-Nets) has brought great success to speech separation. Nevertheless, conventional TasNets struggle to satisfy the memory and latency constraints in industrial applications. In this regard, we design a low-cost high-performance architecture, namely, globally attentive locally recurrent (GALR) network. Alike the dual-path RNN (DPRNN), we first split a feature sequence into 2D segments and then process the sequence along both the intra-and inter-segment dimensions. Our main innovation lies in that, on top of features recurrently processed along the inter-segment dimensions, GALR applies a self-attention mechanism to the sequence along the inter-segment dimension, which aggregates context-aware information and also enables parallelization. Our experiments suggest that GALR is a notably more effective network than the prior work. On one hand, with only 1.5M parameters, it has achieved comparable separation performance at a much lower cost with 36.1% less runtime memory and 49.4% fewer computational operations, relative to the DPRNN. On the other hand, in a comparable model size with DPRNN, GALR has consistently outperformed DPRNN in three datasets, in particular, with a substantial margin of 2.4dB absolute improvement of SI-SNRi in the benchmark WSJ0-2mix task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Audio separation is a fundamental problem in signal processing, and the most typical problem is called "cocktail party problem" <ref type="bibr" target="#b0">[1]</ref> including multi-talker speech separation, overlapped speech-music separation, etc. Recent advances in deep learning models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> have drastically advanced state-of-the-art speech separation performances on several benchmark datasets. Currently, one outstanding category of the best-performing solutions is based on the timedomain audio separation network (TasNet) <ref type="bibr" target="#b5">[6]</ref>, which takes mixture waveforms as inputs and directly reconstruct sources by computing time-domain loss with permutation invariant training (PIT) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In particular, there were several types of TasNets: the initially proposed bi-directional long short-term memory (Bi-LSTM) based Tas-Net <ref type="bibr" target="#b5">[6]</ref>, the time convolutional network (TCN) based Conv-TasNet <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, the dual-path recurrent neural network (DPRNN) <ref type="bibr" target="#b1">[2]</ref> and the recently proposed dual-path Transformer network (DPTNet) <ref type="bibr" target="#b10">[11]</ref>.</p><p>These TasNet-based prior work has proven that a smaller window size improves the separation performance at the cost of a significantly longer 1-D feature sequence <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>. To provide a more concrete illustration, we take a 4-second 16Hz sample rate waveform input as an example in <ref type="figure" target="#fig_0">Fig. 1</ref>, where the resultant feature sequence that a TasNet (with a window size of 2 samples and hop size of 1 sample) needs to model would be as long as 64000. Learning such long-term sequential dependency poses special challenges to various conventional sequential modeling networks, including attention models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10]</ref>, and RNNs (e.g., LSTMs <ref type="bibr" target="#b14">[15]</ref> and GRUs <ref type="bibr" target="#b15">[16]</ref>), each with respective difficulties as discussed below.</p><p>Attention models <ref type="bibr" target="#b16">[17]</ref> has superiority in learning context-aware long-term dependencies, e.g., in BERT <ref type="bibr" target="#b17">[18]</ref> for natural language processing tasks. Most recently, a series of research has also attempted to apply self-attention in speech signal processing, but generally to remarkably shorter feature sequences than a raw input, e.g., framelevel acoustic features for speech recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, layer features in a U-Net architecture <ref type="bibr" target="#b11">[12]</ref> or short-time Fourier transform (STFT) features for speech enhancement <ref type="bibr" target="#b12">[13]</ref>. Nevertheless, attention models have been hardly applied to time-domain source separation tasks as we discussed above because its complexity and memory consumption per layer is quadratic to the sequence length and become unacceptable for very long sequential modeling. 1-D CNNs with fixed receptive fields that are smaller than the very long sequence length, unlike RNNs that have dynamic receptive fields, are not able to fully utilize the sequence-level dependency <ref type="bibr" target="#b8">[9]</ref>. RNNs are also limited by its nature of recursively processing and memorizing context <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. To mitigate the long sequence modeling problem for RNNs, Luo et al. <ref type="bibr" target="#b1">[2]</ref> introduced DPRNN, in which the long signal sequence is divided into shorter segments and interleave two RNNs, an inter-segment RNN and an inter-segment RNN, for local and global modeling, respectively.</p><p>To provide a better panorama about how a sequential context a TasNet (e.g., DPRNN) is dealing with looks, we plot the 4s raw waveform mixture in the upper in <ref type="figure" target="#fig_0">Fig. 1</ref>, where one of the two overlapping utterances, saying "Settle, no matter how, but settle", has been marked in red. Under the dual-path setting, it is segmented into 512 segments, each with length 250. The lower plot zooms in on the 385 th segment to show the details inside a segment around the lateral phoneme of /l/ in the second "settle". As we can see, high temporal correlation, acoustic signal structure, and continuities occur in inter-segment sequences, whilst strong discontinuities occur in inter-segment sequences. As revealed by Khandelwal et al. <ref type="bibr" target="#b22">[23]</ref>, RNNs are far more sensitive to the nearby elements than to the distant ones, and the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. This suggests that RNNs are ideally suited for inter-segment modeling, but not necessarily so for inter-segment modeling. Moreover, Ravanelli et al. <ref type="bibr" target="#b23">[24]</ref> discovered that RNNs reset the stored memory to avoid bias towards an "unrelated" history. However, unlike language modeling where information could be safely discarded when moving from one text to another semantically unrelated text, we argue that for specific tasks such as audio separation, faraway memory could potentially be important. For example, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the first "settle" (20 th -65 th segment), which is very distant from the second "settle" (360 th -405 th segment), could be more useful than nearby elements. In contrast, one strength of attention mechanisms over RNNs lies in a fully connected sequence processing strategy, where every element is connected to other elements in a sequence via a direct path without any recursively processing, memorizing reset, or update mechanisms like RNNs. Given the above example, for the second "settle", a selfattention model would have readily placed more importance to the first "settle", despite the faraway context beyond 200 segments.</p><p>Motivated by the above observation, our work revises TasNets and DPRNN to better address long-range context modeling for audio separation, leading to a lower-cost, higher-performance structure called globally attentive locally recurrent (GALR) network. We resort to the self-attention mechanism <ref type="bibr" target="#b16">[17]</ref> for inter-segment computation to model context-aware global dependencies. Meanwhile, we keep using RNNs for modeling local dependencies at the lower inter-segment context level, e.g., signal continuity, signal structure, etc, which are inherently important for waveform reconstruction.</p><p>The contribution of this paper is four-fold:</p><p>• To the best of our knowledge, this is the first work that jointly leverages the attention and recurrent mechanisms in an alternate and iterative manner, and most importantly, allows the system to take advantage of both techniques, which are complementary at modeling global long-range context and local detail dependencies, respectively.</p><p>• Our work elegantly solves the critical bottleneck of self-attention networks due to unacceptable computational and memory cost for modeling very long sequences, as we can control the global sequence length via the dual-path structure <ref type="bibr" target="#b1">[2]</ref>. Related similar work includes R-Transformer <ref type="bibr" target="#b24">[25]</ref> and DPTNet <ref type="bibr" target="#b10">[11]</ref>. Nevertheless, R-Transformer has the attention mechanism at the middle-level directly applied to the whole sequence, thus its computational cost is still quadratic to the sequence length, which hinders its application to TasNet; DPTNet also applied a dual-path setting like DPRNN to a Transformer-based architecture, but both inter-segment and inter-segment sequences are processed by a combination of attention model and RNN, leading to additional computational cost much heavier than DPRNN and our proposed GALR, which we will discuss in more details in Section 3.2.4.</p><p>• The proposed GALR model has a significantly lower cost with 42.3% reduction in model-size and with a 36.1% reduction in runtime memory cost and a 49.4% reduction in computational opera-tions, while achieving comparable or better performance comparing to DPRNN. Moreover, unlike RNNs and DPRNN, the global attentive structure can further reduce training and inference time via parallelization, as the attention can be computed for all segments in parallel and allows parallel computation for aggregating information across segments over long audio sequences.</p><p>• Finally, results show consistently higher performance over DPRNN in terms of SI-SNRi and SDRi across three different datasets, while still maintaining a lower cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODEL DESIGN</head><p>This section presents our proposed globally attentive locally recurrent (GALR) network. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the inner machinery of GALR, of which the core processing component is a stack of GALR blocks. Each GALR block contains two modeling perspectives. The first modeling perspective is responsible for modeling the local structures of input signals recurrently; the second modeling perspective aims at capturing global dependencies with the multi-head self-attention mechanism. Next, we describe each part in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Encoding Raw Signals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Encoder</head><p>In a TasNet-based separation system, an input mixture signal is represented as I half-overlapping frames, denoted by x1, ..., xI ∈ R M , where M denotes the window length. Analogous to the short-time Fourier transform (STFT), we non-linearly transform each frame xi into a D-dimensional feature vectorxi ∈ R D using a 1D gated convolutional layer:</p><formula xml:id="formula_0">x i = ReLU(U * xi),<label>(1)</label></formula><p>where * denotes the 1D convolution operation, U ∈ R D×M contains D vectors (encoder basis functions) with length M each, and ReLU(·) is the rectified linear unit used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref> to ensure the non-negativity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Segmentation</head><p>Given an encoded signal inputX ∈ R D×I , the segmentation module splitsX into S half-overlapping segments each of length K. The first and last segments are padded with zeros to create S = 2I/K + 1 equal-size segments: Ss ∈ R D×K for s = 1, ..., S. These segments are packed into a 3D tensor, denoted by T ∈ R D×S×K . Note that the size of each segment K is a hyperparameter that affects the number of segments and can be used to control the scale of the locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">GALR Blocks</head><p>The input 3D tensor T is then passed to a stack of N GALR blocks, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which is designed to decouple the mixture signal by alternating local and global sequence modeling. Each GALR block returns a 3D tensor with the same dimensionality as its input. We denote the input for block n = 1, ..., N as T (n) ∈ R D×S×K , where T (1) = T. As shown on the right of <ref type="figure" target="#fig_1">Fig. 2</ref>, a GALR block is composed of two phases of computation, a locally recurrent layer and a globally attentive layer, respectively corresponding to intersegment processing and inter-segment processing. Each of which is described in detail below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Locally Recurrent Model</head><p>A recurrent model is adopted to model the local information of the input sequence upon segmentation. To model such short-term dependencies within each segment, we employ a bi-directional LSTM of H hidden nodes:</p><formula xml:id="formula_1">L (n) = R (n) f (n) Bi-LSTM T (n) [:, s, :] + Y (n) , s = 1, ..., S , (2) where R (n) ∈ R D×2H and Y (n) ∈ R D×1 form a linear layer, L (n) ∈ R D×S×K is the output of the Bi-LSTM f (n)</formula><p>Bi-LSTM (·), and T (n) [:, s, :] ∈ R D×K refers to the local sequence within the s th segment. The output of this locally recurrent model then goes through a layer normalization operation LN(·) with a residual connection to the block's input, which in practice is critical for model regularization and training acceleration <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_2">L (n) = LN(L (n) ) + T (n) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Globally Attentive Model</head><p>We build a globally attentive model on top of the locally recurrent model to capture the long-term dependencies. Recent works in the speech community <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> have unveiled the extraordinary performance of attention mechanisms in learning long-term global dependencies <ref type="bibr" target="#b16">[17]</ref>. Here in our case, the multi-head attention mechanism especially becomes the perfect fit due to three reasons. Firstly, the inherent computational burden of attention models becomes lessproblematic since we now can control the sequence length by changing the window length of segmentation. Secondly, global dependencies across segments are directly modeled without needing to memorize segments one by one as in RNNs. Thirdly, given that the input is composed of different sources, it is sensible to use multiple attention schemes (a.k.a. heads) on the whole sequence. Before applying the attention mechanism, the output of locally recurrent model first goes through the following:</p><formula xml:id="formula_3">G (n) = LND(L (n) ) + P<label>(4)</label></formula><p>where LND(·) denotes the layer normalization applied only along the feature dimension D, P denotes the positional encoding matrix introduced in <ref type="bibr" target="#b16">[17]</ref>. For global sequence modeling, we consider the sequence of frames across all segments, i.e., G</p><formula xml:id="formula_4">(n) k [G (n) [: , s, k], s = 1, ..., S].</formula><p>In order to create J heads, we linearly map G (n) k into J different forms of query, key, and value matrices:</p><formula xml:id="formula_5">Q (n) k,j = W (n) query,j (W (n) query G (n) k + b (n) query ) (5) K (n) k,j = W (n) key,j (W (n) key G (n) k + b (n) key ) (6) V (n) k,j = W (n) value,j (W (n) value G (n) k + b (n) value )<label>(7)</label></formula><p>for k = 1, ..., K, j = 1, ..., J. , where W (n)</p><formula xml:id="formula_6">query , W (n) key , W (n) value ∈ R D×D , b (n) query , b (n) key , b (n) value ∈ R D×1 and W (n) query,j , W (n) key,j , W (n)</formula><p>value,j ∈ R D/J×D . Note that the attention parameters are not dependent on k, which means that we tie the attention weights for all K sequences, i.e, [G (n) k , k = 1, ..., K]. Tying weights is sensible here because the cross-segment sequences formed within a relatively small segment size ought to have very high correlations.</p><p>Given the query, key, and value inputs, we then compute the scaled dot-product attention following <ref type="bibr" target="#b16">[17]</ref> in J heads:</p><formula xml:id="formula_7">A (n) k,j = Softmax   Q (n) k,j K (n) k,j D/J   V (n) k,j .<label>(8)</label></formula><p>Next, the attention matrices computed at different heads are combined using an affine transformation after concatenating the matrices:</p><formula xml:id="formula_8">A (n) k = W (n) attn Concat A (n) k,1 , ..., A (n) k,J ,<label>(9)</label></formula><p>where W (n) attn ∈ R D×D is the weight matrix for the heads. The attention outputs are then concatenated back to a 3D tensor, i.e,</p><formula xml:id="formula_9">A (n) = [A (n)</formula><p>k , k = 1, ..., K]. Given the attention output, we employ a sub-layer connection with reference to the well-known Transformer model <ref type="bibr" target="#b16">[17]</ref> given bŷ</p><formula xml:id="formula_10">G (n) = LN(G (n) + Dropout(A (n) )),<label>(10)</label></formula><p>where Dropout(·) denotes the dropout regularization <ref type="bibr" target="#b27">[28]</ref> operation. Finally, the GALR block outputs a residual sum between the local model output and global model output:</p><formula xml:id="formula_11">T (n+1) =Ĝ (n) +L (n) ,<label>(11)</label></formula><p>which defines a recurrence relation between N GALR blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Low-dimension Segment Representation</head><p>Note that the attention mechanism is repeated K times in Eq. (8-9), we observe that our globally attentive model can use a lowdimension trick to reduce memory and floating-point operations, while maintaining the performance. Due to high correlations between the cross-segment sequences, we indeed can approximate the global dependencies with a down-sampled number of sequences.</p><p>In this regard, we employ two affine transformations Cmap(·) and Cinv(·) for mapping K dimensions into Q dimensions and inversely mapping Q dimensions back to K dimensions, respectively, where Q &lt;&lt; K. Mathematically, we only need to re-write Eq. (4) and Eq.</p><p>as</p><formula xml:id="formula_13">G (n) = LND(Cmap(L (n) )) + P,<label>(12)</label></formula><formula xml:id="formula_14">T (n+1) = Cinv(G (n) ) +L (n) ,<label>(13)</label></formula><p>where Cmap(·) and Cinv(·) contain affine mapping parameters in shapes Q × (K + 1) and K × (Q + 1), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Signals Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Mask Estimation</head><p>After N consecutive GALR blocks, we obtain a representation of the mixture signal that facilitates the separation of C sources. We then use a 2D convolutional layer to transform this 3D representation into C 3D tensors. Then, we transform each of the C 3D tensors back to a matrix Sc ∈ R D×L for c = 1, ..., C by applying the OverlapAdd method described in <ref type="bibr" target="#b1">[2]</ref>. After that, we have a beamforming procedure <ref type="bibr" target="#b28">[29]</ref> that applies two 1D gated convolution layers to each of the C matrices:</p><formula xml:id="formula_15">Sc = tanh(Utanh * Sc) σ(Usigmoid * Sc),<label>(14)</label></formula><p>where denotes element-wise multiplication, σ(·) is the Sigmoid function, and Utanh ∈ R D×D and Usigmoid ∈ R D×D are two parameter matrices in the 1D gated convolution. The Tanh and Sigmoid functions here act as the beam-forming filters.</p><p>To produce a mask matrix for each source, the final step is to employ a resilient linear (ReLU) mask function</p><formula xml:id="formula_16">Mc = ReLU(Urelu * Ŝc),<label>(15)</label></formula><p>where Urelu ∈ R D×D is a 1D convolution for learning mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Decoder for Waveform Reconstruction</head><p>The c th estimated mask is applied back to the initially encoded mix-tureX = [x1, ...,xL] to reconstruct source c:</p><formula xml:id="formula_17">sc = OverlapAdd(B(X Mc)),<label>(16)</label></formula><p>where B ∈ R M ×D is a matrix containing the basis signals with each column corresponding to a 1D filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Permutation Invariant Training</head><p>In a standard training framework, given a speech separation model f θ with a set of parameters denoted by θ, a loss function L(f θ (x), y) is used to penalize the divergence between the predicted outputs f θ (x) = {ŝ1, ...,ŝC } and the clean sources y = {s1, ..., sC }. As an end-to-end network, our proposed GALR model outputs waveforms of the estimated clean signals so that we can directly use the scale-invariant source-to-noise ratio (SI-SNR) <ref type="bibr" target="#b5">[6]</ref> as our maximization objection with permutation invariant training (PIT) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>:</p><formula xml:id="formula_18">LSI-SNR(f θ (x), y) = −10 log 10 Πs(ŝ) 2 2 ŝ − Πs(ŝ) 2 2 ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_19">Πa(b) = a b a 2 2</formula><p>a is the projection of b onto a. SI-SNR has also been used in many end-to-end separation models [6, 10, 30, 2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Data Preparation</head><p>We used three datasets for our experiments: (1) WSJ0-2mix, a twospeaker speech dataset <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> that consists of 30 hours of training, 10 hours of validation, and 5 hours of evaluation data and is widely used as the benchmark in monaural speech separation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, (2) Libri-2mix, a larger two-speaker audio mixture dataset generated from a publicly available English speech corpus Librispeech <ref type="bibr" target="#b35">[36]</ref> that contains 982.1 hours of speech from 2484 speakers, and (3) WSJ0-music, a speech-music mixture audio dataset generated in <ref type="bibr" target="#b34">[35]</ref>. All mixture audios were simulated by randomly combining utterances from different speakers or music clips at a sampling rate of 8 kHz with SNRs between 0 dB and 5 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Model Setup</head><p>In our implementation, we used the setting of encoder-decoder modules in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> and the segmentation module described in <ref type="bibr" target="#b1">[2]</ref>. In the middle part of the separation network, 6 consecutive blocks were used to model local and global sequences, i.e., N = 6. We fixed the number of hidden nodes (H) in Bi-LSTM to 128 as in <ref type="bibr" target="#b1">[2]</ref>. The multihead attention based global model was made of 8 heads, i.e., J = 8. In each attention layer, the dropout rate was set to 0.1. Regarding other model hyperparameters, we varied the number of filters (D), the window length (M ), and the segment size (K) as shown in <ref type="table" target="#tab_2">Table 3</ref>. Notably, when we set D = 64 as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> the model size of GALR is much smaller than that in the previous works, therefore, we also tried D = 128 to obtain a comparable model size with DPRNN. Meanwhile, we implemented the most recently proposed DPTNet <ref type="bibr" target="#b10">[11]</ref> as another reference.</p><p>It is worthwhile to explain why we omitted the configuration of M = 2 and K = 250, which corresponds to the highest SI-SNRi score in <ref type="bibr" target="#b1">[2]</ref>. Although the authors in <ref type="bibr" target="#b1">[2]</ref> reported that setting shorter window length leads to better SI-SNRi performance, the associated computational burden was not disclosed. Given a limited GPU memory, halving the window length resorts to halving the batch size and doubling the training time. We tried to run the highest scores under the setting of M = 2 for both DPRNN and GALR. However, building such systems in a small dataset like WSJ0-2mix costs more than ten days of training. For a realistic industrial development, we generally need to tackle 10-100 times larger datasets. Therefore, the corresponding training efficiency is unacceptable for most realistic  <ref type="bibr" target="#b10">[11]</ref> O(</p><formula xml:id="formula_20">KSH 2 + K 2 SD) O(KSH 2 + KS 2 D) O(S + K) DPRNN [2] O(KSH 2 ) O(KSH 2 ) O(S + K) GALR O(KSH 2 ) O(QS 2 D) O(K)</formula><p>scenarios, not to mention that the high latency at inference time is also problematic for system deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Training Details</head><p>All the models were trained on 8 NVIDIA Tesla M40 GPU devices using PyTorch <ref type="bibr" target="#b36">[37]</ref> for fair comparisons. We found that the performances of all separation models deteriorated as we used 8 GPUs in place of 1 GPU. A similar observation has been obtained in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, where this multi-GPU training approach is termed model averaging.</p><p>Note that it is impractical to use a single GPU for model training with its unacceptably long training time. For a fair part-to-part comparison, we report the performances of GALR under the same 8-GPU training condition, though the SI-SNRi can be further improved in the case of using fewer GPUs. For the benchmark WSJ0-2mix separation task, we referred to the training protocol in <ref type="bibr" target="#b1">[2]</ref>, where clipped 4-second waveforms were used for permutation invariant training <ref type="bibr" target="#b6">[7]</ref> to minimize pairwise SI-SNR loss <ref type="bibr" target="#b5">[6]</ref>. Concerning optimization, we used Adam <ref type="bibr" target="#b39">[40]</ref> optimizer with an initial learning rate of 1e −3 and a weight decaying rate of 1e −6 . The learning rate was exponentially decayed at a rate of 0.96 for every two epochs. The training was considered converged when no lower validation loss can be observed in 10 consecutive epochs. A gradient clipping method was used to ensure the maximum l2-norm of each gradient is less than 5. All models were assessed in terms of SI-SNRi and SDRi <ref type="bibr" target="#b40">[41]</ref>. First and foremost, we experimented on WSJ0-2mix to validate our hypothesis that the proposed GALR architecture is the optimal choice amongst while permuting recurrent and attention models for local and global sequence modeling. In this experiment, we chose the bi-directional LSTM as a representative recurrent model. As shown in <ref type="table" target="#tab_1">Table 2</ref>, we obtained 4 distinctive SI-SNRi scores from 4 kinds of TasNet systems. Interestingly, we found two consistent patterns: (1) in local modeling, the recurrent model performed better than the attention model; <ref type="bibr" target="#b1">(2)</ref> in global modeling, the attention model performed better than the recurrent model. Overall, the proposed GALR architecture (bottom-left) gave the best performance among the four architectures, which matches our expectations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Investigation of GALR Optimality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Time Complexity Analysis</head><p>Next, we compared the algorithmic complexities of all three models in <ref type="table" target="#tab_0">Table 1</ref>, where we reported dual-path processing complexities and the maximum path length (MPL) <ref type="bibr" target="#b16">[17]</ref> that was needed to connect any two positions in the signal sequence in Big-O notation. Considering the global-path processing complexity, the cost of DPTNet was about the sum of the costs of both DPRNN and GALR. Notably, for very long input sequences, e.g., in the case of small window length, we needed to use a larger K to improve the computational performance of both models. By introducing low-dimensional mapping with Q &lt;&lt; K, we found that GALR could significantly relieve the computational burden carried by a large K, as reported in terms of actual FLOPs in <ref type="table" target="#tab_2">Table 3</ref>. Besides, with regard to MPL, amongst the three models, only GALR managed to connect all positions with O(K), whereas both DPRNN and DPTNet required O(S + K) sequential operations. As discussed in <ref type="bibr" target="#b41">[42]</ref>, the shorter the MPL, the easier it was to learn long-term dependencies, which echoed the theoretical advantage of GALR. The clean speech signals unseen by the model are shown in red and blue. The two graphs above the signals denotes the softmax values of two selected heads attending on the target segment in green frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Visualizing Global Attention with Multiple Heads</head><p>We were also interested in understanding how the multi-head self-attention works in speech separation. To reason about the physical meaning of our global attention mechanism, we examined the values of the softmax matrices defined in Eq. 8 computed in different heads. The softmax values were plotted against the time axis with respect to the source signals, as shown in <ref type="figure">Fig. 3</ref>. Interestingly, we observed two heads where the attention values were related to the energies of the two clean speeches. This explained how multihead self-attention mechanism worked in GALR -the attention matrices of different heads were combined over stacks of GALR blocks to output an easily separable representation for the mixture signal. This attention behavior was also mimicking how humans conceptually following the speech of one talker with regards to its volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Performances in Benchmark WSJ0-2mix</head><p>As for the architecture for TasNet, we compared the result of our GALR model to the state-of-the-art DPRNN <ref type="bibr" target="#b1">[2]</ref> and DPTNet <ref type="bibr" target="#b10">[11]</ref>.</p><p>Considering only the lightest model with M = 16, we found that DPTNet costs 152% more time and 225% more memory than our proposed GALR. Considering this remarkable surge of memory consumption, we noted that DPTNet might be not practically preferable for most realistic industrial tasks; therefore we only compared GALR with DPRNN in the following tasks. We replicated the experiment in <ref type="bibr" target="#b1">[2]</ref> with the same configurations of window length and segment size. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Besides the standard separation measure SI-SNRi, we also analyzed the runtime cost of each model for processing a 1s mixture input in terms of memory measured in GPU and floating-point operations (FLOPs) approximated with a third-party module, <ref type="bibr" target="#b0">1</ref>   To validate the consistency of GALR's improvements over DPRNN, we experimented on a larger dataset Libri-2mix. In Libri-2mix, we split the Librispeech <ref type="bibr" target="#b35">[36]</ref> corpus into (1) a training set containing 12055 utterances (5 utterances per speaker) drawn from 2411 speakers, (2) a validation set containing another 7233 utterances (5 utterances per speaker) drawn from the same 2411 speakers, and 2411 speakers (3) a test set containing 4380 utterances (60 utterances per speaker) drawn from 73 speakers. Note that the separation task in Libri-2mix was much more challenging than that in WSJ0-2mix because of not only increasing training speakers from 101 to 2411 but also the limitation of only five sampled utterances per speaker for training. Despite the increased separation difficulty, we conceived that the separation scenario became more realistic as it was often hard to collect many clean utterances from the user in real-world applications.</p><p>In this large-scale separation dataset, to strike a balance between the training speed and the separation performance, we decided to use a configuration of M = 8 and K = 150 to train both DPRNN and GALR. For a fair comparison in terms of the number of parameters, we only trained the GALR with 2.3M parameters to be comparable to DPRNN. The separation results of DPRNN and GALR were 1 https://github.com/sovrasov/flops-counter.pytorch summarized and shown in <ref type="table" target="#tab_4">Table 4</ref>. We observed that GALR maintained its advantage in speed and memory over DPRNN and meanwhile achieved 0.2dB absolute improvements in SI-SNRi and SDRi. The consistent results suggest that GALR keeps performing low-cost separation without sacrificing separation efficacy. Besides two-speaker speech separation tasks, we were also interested in separating speech from a speech-music mixture. In this paper, we simulated the speech-music mixture using the speech from the WSJ0-2mix corpus and the music clips in <ref type="bibr" target="#b34">[35]</ref>. The results were shown in <ref type="table" target="#tab_5">Table 5</ref>. Comparing to the two-speaker separation, we found that GALR obtained a even greater superiority over DPRNN in the speech-music scenario, which is also very common in realworld conversation scenarios. In particular, there is a growing demand in the industry where the speech-music separation becomes critical for the deployment of many real-world signal processing systems, e.g., micro-video automatic captioning. Therefore, the consistent and larger superiority of GALR over DPRNN in the speechmusic separation task is valuable for both conventional and emerging application deployments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6.">Performances in Separating Speech-Music Mixture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>This paper introduces a novel architecture -globally attentive locally recurrent network (GALR), which combines the advantages of recurrent networks and attention mechanisms for effective, low-cost time-domain signal processing. We provide empirical evidences that the self-attention mechanism is a better candidate for modeling the long-range context sequence than the RNNs. Results across three different datasets also suggest the superiority of attention models over recurrent models in modeling global sequences, which leads to greater modeling power, reduced model size, and less runtime memory. We believe that a compact, low-cost, and effective separation system is more practical to the industry and will empower wider applications of speech separation for robust signal processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Upper: a 4s raw waveform mixture of two overlapping utterances; Lower: zooming in on the 385th segment around the lateral phoneme of /l/.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Left: the overall architecture of our GALR network. Right: detailed illustration about how the intra-and inter-segment sequences are processed in the locally recurrent layer (lower right) and the globally attentive layer (upper right) inside each GALR block, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>head 1 head 2 Fig. 3 :</head><label>23</label><figDesc>An example given by a GALR model trained on WSJ0-2mix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of dual-path processing time complexities for different block types. Block Local-path Complexity Global-path Complexity Maximum Path Length [17] DPTNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SI-SNRi results of WSJ0-2mix when permuting Bi-LSTM and attention model in local and global modeling</figDesc><table><row><cell>Approach</cell><cell cols="2">Local Bi-LSTM Local Attention</cell></row><row><cell>Global Bi-LSTM</cell><cell>15.9</cell><cell>12.3</cell></row><row><cell>Global Attention</cell><cell>17.0</cell><cell>14.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performances of DPRNN and our proposed GALR in WSJ0-2mix test set with different configurations.</figDesc><table><row><cell>TasNet</cell><cell>D</cell><cell>M</cell><cell>K</cell><cell>Q</cell><cell>Size</cell><cell cols="4">SI-SNRi SDRi Runtime Memory GFLOPs</cell></row><row><cell>DPTNet [11]</cell><cell>64</cell><cell cols="2">16 100</cell><cell>-</cell><cell>2.8M</cell><cell>15.5</cell><cell>15.7</cell><cell>419 MiB</cell><cell>12.6</cell></row><row><cell></cell><cell>64</cell><cell cols="2">16 100</cell><cell></cell><cell></cell><cell>15.9</cell><cell>16.2</cell><cell>231 MiB</cell><cell>10.7</cell></row><row><cell>DPRNN [2]</cell><cell>64</cell><cell>8</cell><cell>150</cell><cell>-</cell><cell>2.6M</cell><cell>17.0</cell><cell>17.3</cell><cell>456 MiB</cell><cell>22.2</cell></row><row><cell></cell><cell>64</cell><cell>4</cell><cell>200</cell><cell></cell><cell></cell><cell>17.9</cell><cell>18.1</cell><cell>929 MiB</cell><cell>42.3</cell></row><row><cell></cell><cell>64</cell><cell cols="3">16 100 32</cell><cell></cell><cell>16.2</cell><cell>16.5</cell><cell>161 MiB</cell><cell>5.6</cell></row><row><cell></cell><cell>64</cell><cell>8</cell><cell cols="2">150 16</cell><cell>1.5M</cell><cell>17.1</cell><cell>17.4</cell><cell>309 MiB</cell><cell>11.5</cell></row><row><cell>GALR</cell><cell cols="4">64 128 16 100 32 4 200 8</cell><cell></cell><cell>17.7 17.0</cell><cell>17.9 17.3</cell><cell>594 MiB 186 MiB</cell><cell>21.4 8.3</cell></row><row><cell></cell><cell>128</cell><cell>8</cell><cell cols="2">150 16</cell><cell>2.3M</cell><cell>18.7</cell><cell>18.9</cell><cell>363 MiB</cell><cell>16.5</cell></row><row><cell></cell><cell>128</cell><cell>4</cell><cell>200</cell><cell>8</cell><cell></cell><cell>20.3</cell><cell>20.5</cell><cell>730 MiB</cell><cell>30.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>which represents the model efficiency. On the one hand, the GALR with a model size comparable to DPRNN consistently gave superior SI-SNRi performances over DPRNN under the same configurations of window length and segment size. On the other hand, the smaller GALR attained comparable or better separation performances while requiring only 57.3% parameters and reducing up to 36.1% runtime memory and 49.4% computational operations.</figDesc><table /><note>3.2.5. Performances in Large-scale Libri-2mix</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performances of DPRNN and our proposed GALR in Libri-2mix</figDesc><table><row><cell>TasNet</cell><cell>Size</cell><cell cols="2">SI-SNRi SDRi</cell></row><row><cell cols="2">DPRNN [2] 2.6M</cell><cell>12.0</cell><cell>12.5</cell></row><row><cell>GALR</cell><cell>2.3M</cell><cell>12.2</cell><cell>12.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performances of DPRNN and our proposed GALR in WSJ0-music</figDesc><table><row><cell>TasNet</cell><cell>Size</cell><cell cols="2">SI-SNRi SDRi</cell></row><row><cell cols="2">DPRNN [2] 2.6M</cell><cell>14.5</cell><cell>14.8</cell></row><row><cell>GALR</cell><cell>2.3M</cell><cell>15.9</cell><cell>16.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherry</forename><surname>E Colin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the acoustical society of America</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain singlechannel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06379</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2092" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face landmark-based speaker-independent audio-visual speech enhancement in multi-talker environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Bergamaschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Pasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Fadiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Tikhanoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Badino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6900" to="6904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Audio-visual recognition of overlapped speech for the lrs2 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi-Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shansong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01656</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speakerindependent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13975</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention wave-u-net for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="249" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">T-gsa: Transformer with gaussian-weighted self-attention for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwon</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06762</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wave-unet: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Society for Music Information Retrieval Conference, ISMIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep self-attention networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai-Son</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.13377</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-stride self-attention for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2788" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Simplified self-attention for transformer-based end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoneng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10463</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sharp nearby, fuzzy far away: How neural language models use context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04623</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Light gated recurrent units for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="102" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rtransformer: Recurrent neural network enhanced transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05572</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6706" to="6713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fastspeech: Fast, robust and controllable text to speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fasnet: Low-latency adaptive beamforming for multi-microphone audio processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enea</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13387</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improved speech separation with time-and-frequency cross-domain joint embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene-Ping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07845</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Continuous speech recognition (csr-i) wall street journal (wsj0) news, complete. linguistic data consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pallett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">End-to-end speech separation with unfolded iterative phase reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10204</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Furcanext: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiqing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mixupbreakdown: a consistency training method for improving generalization of speech separation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Experiments on parallel training of deep neural network using model averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01239</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 ieee international conference on acoustics, speech and signal processing (icassp)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5880" to="5884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
