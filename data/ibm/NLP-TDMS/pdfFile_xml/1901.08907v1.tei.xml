<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
							<email>wanghongwei55@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
							<email>zhangfuzheng@meituan.com</email>
							<affiliation key="aff2">
								<orgName type="department">Meituan-Dianping Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
							<email>guo-my@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename></persName>
						</author>
						<title level="a" type="main">Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<note>ACM Reference Format: Guo. 2019. Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation In Proceedings of The 2019 Web Conference (WWW 2019). ACM, New York, NY, USA, 11 pages. https://doi.org/xxxxx</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender systems</term>
					<term>knowledge graph</term>
					<term>multi-task learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Collaborative filtering often suffers from sparsity and cold start problems in real recommendation scenarios, therefore, researchers and engineers usually use side information to address the issues and improve the performance of recommender systems. In this paper, we consider knowledge graphs as the source of side information. We propose MKR, a Multi-task feature learning approach for Knowledge graph enhanced Recommendation. MKR is a deep end-to-end framework that utilizes knowledge graph embedding task to assist recommendation task. The two tasks are associated by cross&amp;compress units, which automatically share latent features and learn high-order interactions between items in recommender systems and entities in the knowledge graph. We prove that cross&amp;compress units have sufficient capability of polynomial approximation, and show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning. Through extensive experiments on real-world datasets, we demonstrate that MKR achieves substantial gains in movie, book, music, and news recommendation, over state-of-theart baselines. MKR is also shown to be able to maintain a decent performance even if user-item interactions are sparse.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems (RS) aims to address the information explosion and meet users personalized interests. One of the most popular recommendation techniques is collaborative filtering (CF) <ref type="bibr" target="#b11">[11]</ref>, which utilizes users' historical interactions and makes recommendations based on their common preferences. However, CF-based methods usually suffer from the sparsity of user-item interactions and the cold start problem. Therefore, researchers propose using side information in recommender systems, including social networks <ref type="bibr" target="#b10">[10]</ref>, attributes <ref type="bibr" target="#b30">[30]</ref>, and multimedia (e.g., texts <ref type="bibr" target="#b29">[29]</ref>, images <ref type="bibr" target="#b40">[40]</ref>). Knowledge graphs (KGs) are one type of side information for RS, which usually contain fruitful facts and connections about items. Recently, researchers have proposed several academic and commercial KGs, such as NELL 1 , DBpedia 2 , Google Knowledge Graph <ref type="bibr" target="#b2">3</ref> and Microsoft Satori 4 . Due to its high dimensionality and heterogeneity, a KG is usually pre-processed by knowledge graph embedding (KGE) methods <ref type="bibr" target="#b27">[27]</ref>, which embeds entities and relations into low-dimensional vector spaces while preserving its inherent structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Existing KG-aware methods</head><p>Inspired by the success of applying KG in a wide variety of tasks, researchers have recently tried to utilize KG to improve the performance of recommender systems <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b45">45]</ref>. Personalized Entity Recommendation (PER) <ref type="bibr" target="#b39">[39]</ref> and Factorization Machine with Group lasso (FMG) <ref type="bibr" target="#b45">[45]</ref> treat KG as a heterogeneous information network, and extract meta-path/meta-graph based latent features to represent the connectivity between users and items along different types of relation paths/graphs. It should be noted that PER and FMG rely heavily on manually designed meta-paths/meta-graphs, which limits its application in generic recommendation scenarios. Deep Knowledge-aware Network (DKN) <ref type="bibr" target="#b32">[32]</ref> designs a CNN framework to combine entity embeddings with word embeddings for news recommendation. However, the entity embeddings are required in advance of using DKN, causing DKN to lack an endto-end way of training. Another concern about DKN is that it can hardly incorporate side information other than texts. RippleNet <ref type="bibr" target="#b31">[31]</ref> is a memory-network-like model that propagates users' potential preferences in the KG and explores their hierarchical interests. But the importance of relations is weakly characterized in Rip-pleNet, because the embedding matrix of a relation R can hardly be trained to capture the sense of importance in the quadratic form v ⊤ Rh (v and h are embedding vectors of two entities). Collaborative Knowledge base Embedding (CKE) <ref type="bibr" target="#b40">[40]</ref> combines CF with structural knowledge, textual knowledge, and visual knowledge in a unified framework. However, the KGE module in CKE (i.e., TransR <ref type="bibr" target="#b13">[13]</ref>) is more suitable for in-graph applications (such as KG completion and link prediction) rather than recommendation. In addition, the CF module and the KGE module are loosely coupled in CKE under a Bayesian framework, making the supervision from KG less obvious for recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The proposed approach</head><p>To address the limitations of previous work, we propose MKR, a multi-task learning (MTL) approach for knowledge graph enhanced recommendation. MKR is a generic, end-to-end deep recommendation framework, which aims to utilize KGE task to assist recommendation task <ref type="bibr" target="#b5">5</ref> . Note that the two tasks are not mutually independent, but are highly correlated since an item in RS may associate with one or more entities in KG. Therefore, an item and its corresponding entity are likely to have a similar proximity structure in RS and KG, and share similar features in low-level and non-task-specific latent feature spaces <ref type="bibr" target="#b15">[15]</ref>. We will further validate the similarity in the experiments section. To model the shared features between items and entities, we design a cross&amp;compress unit in MKR. The cross&amp;compress unit explicitly models high-order interactions between item and entity features, and automatically control the cross knowledge transfer for both tasks. Through cross&amp;compress units, representations of items and entities can complement each other, assisting both tasks in avoiding fitting noises and improving generalization. The whole framework can be trained by alternately optimizing the two tasks with different frequencies, which endows MKR with high flexibility and adaptability in real recommendation scenarios.</p><p>We probe the expressive capability of MKR and show, through theoretical analysis, that the cross&amp;compress unit is capable of approximating sufficiently high order feature interactions between items and entities. We also show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning, including factorization machines <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, deep&amp;cross network <ref type="bibr" target="#b34">[34]</ref>, and cross-stitch network <ref type="bibr" target="#b18">[18]</ref>. Empirically, we evaluate our method in four recommendation scenarios, i.e., movie, book, music, and news recommendations. The results demonstrate that MKR achieves substantial gains over state-of-theart baselines in both click-through rate (CTR) prediction (e.g., <ref type="bibr" target="#b11">11</ref>.6% AU C improvements on average for movies) and top-K recommendation (e.g., 66.4% Recall@10 improvements on average for books). MKR can also maintain a decent performance in sparse scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>It is worth noticing that the problem studied in this paper can also be modelled as cross-domain recommendation <ref type="bibr" target="#b26">[26]</ref> or transfer learning <ref type="bibr" target="#b21">[21]</ref>, since we care more about the performance of recommendation task. However, the key observation is that though cross-domain recommendation and transfer learning have single objective for the target domain, their loss functions still contain constraint terms for measuring data distribution in the source domain or similarity between two domains. In our proposed MKR, the KGE task serves as the constraint term explicitly to provide regularization for recommender systems. We would like to emphasize that the major contribution of this paper is exactly modeling the problem as multi-task learning: We go a step further than cross-domain recommendation and transfer learning by finding that the intertask similarity is helpful to not only recommender systems but also knowledge graph embedding, as shown in theoretical analysis and experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OUR APPROACH</head><p>In this section, we first formulate the knowledge graph enhanced recommendation problem, then introduce the framework of MKR and present the design of the cross&amp;compress unit, recommendation module and KGE module in detail. We lastly discuss the learning algorithm for MKR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>We formulate the knowledge graph enhanced recommendation problem in this paper as follows. In a typical recommendation scenario, we have a set of M users U = {u 1 , u 2 , ..., u M } and a set of N items V = {v 1 , v 2 , ..., v N }. The user-item interaction matrix Y ∈ R M ×N is defined according to users' implicit feedback, where y uv = 1 indicates that user u engaged with item v, such as behaviors of clicking, watching, browsing, or purchasing; otherwise y uv = 0. Additionally, we also have access to a knowledge graph G, which is comprised of entity-relation-entity triples (h, r, t). Here h, r , and t denote the head, relation, and tail of a knowledge triple, respectively. For example, the triple (Quentin Tarantino, film.director.film, Pulp Fiction) states the fact that Quentin Tarantino directs the film Pulp Fiction. In many recommendation scenarios, an item v ∈ V may associate with one or more entities in G. For example, in movie recommendation, the item "Pulp Fiction" is linked with its namesake in a KG, while in news recommendation, news with the title "Trump pledges aid to Silicon Valley during tech meeting" is linked with entities "Donald Trump" and "Silicon Valley" in a KG.</p><p>Given the user-item interaction matrix Y as well as the knowledge graph G, we aim to predict whether user u has potential interest in item v with which he has had no interaction before. Our goal is to learn a prediction functionŷ uv = F (u, v |Θ, Y, G), wherê y uv denotes the probability that user u will engage with item v, and Θ is the model parameters of function F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework</head><p>The framework of MKR is illustrated in <ref type="figure" target="#fig_1">Figure 1a</ref>. MKR consists of three main components: recommendation module, KGE module, and cross&amp;compress units. <ref type="bibr" target="#b0">(1)</ref> The recommendation module on the left takes a user and an item as input, and uses a multi-layer perceptron (MLP) and cross&amp;compress units to extract short and dense features for the user and the item, respectively. The extracted features are then fed into another MLP together to output the predicted probability. (2) Similar to the left part, the KGE module in the right part also uses multiple layers to extract features from the head and relation of a knowledge triple, and outputs the representation of the predicted tail under the supervision of a score function f and the real tail. (3) The recommendation module and the KGE module are bridged by specially designed cross&amp;compress units. The proposed unit can automatically learn high-order feature interactions of items in recommender systems and entities in the KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross&amp;compress Unit</head><p>To model feature interactions between items and entities, we design a cross&amp;compress unit in MKR framework. As shown in <ref type="figure" target="#fig_1">Figure 1b</ref>  for item v and one of its associated entities e, we first construct d ×d pairwise interactions of their latent feature v l ∈ R d and e l ∈ R d from layer l:</p><formula xml:id="formula_0">C l = v l e ⊤ l =        v (1) l e (1) l · · · v (1) l e (d ) l · · · · · · v (d ) l e (1) l · · · v (d ) l e (d ) l        ,<label>(1)</label></formula><p>where C l ∈ R d ×d is the cross feature matrix of layer l, and d is the dimension of hidden layers. This is called the cross operation, since each possible feature interaction v</p><formula xml:id="formula_1">(i) l e (j)</formula><p>l , ∀(i, j) ∈ {1, ..., d } 2 between item v and its associated entity e is modeled explicitly in the cross feature matrix. We then output the feature vectors of items and entities for the next layer by projecting the cross feature matrix into their latent representation spaces:</p><formula xml:id="formula_2">v l +1 =C l w V V l + C ⊤ l w EV l + b V l = v l e ⊤ l w V V l + e l v ⊤ l w EV l + b V l , e l +1 =C l w V E l + C ⊤ l w EE l + b E l = v l e ⊤ l w V E l + e l v ⊤ l w EE l + b E l ,<label>(2)</label></formula><p>where w ·· l ∈ R d and b · l ∈ R d are trainable weight and bias vectors. This is called the compress operation, since the weight vectors project the cross feature matrix from R d ×d space back to the feature spaces R d . Note that in Eq. (2), the cross feature matrix is compressed along both horizontal and vertical directions (by operating on C l and C ⊤ l ) for the sake of symmetry, but we will provide more insights of the design in Section 3.2. For simplicity, the cross&amp;compress unit is denoted as:</p><p>[</p><formula xml:id="formula_3">v l +1 , e l +1 ] = C(v l , e l ),<label>(3)</label></formula><p>and we use a suffix <ref type="bibr">[v]</ref> or [e] to distinguish its two outputs in the following of this paper. Through cross&amp;compress units, MKR can adaptively adjust the weights of knowledge transfer and learn the relevance between the two tasks. It should be noted that cross&amp;compress units should only exist in low-level layers of MKR, as shown in <ref type="figure" target="#fig_1">Figure 1a</ref>. This is because:</p><p>(1) In deep architectures, features usually transform from general to specific along the network, and feature transferability drops significantly in higher layers with increasing task dissimilarity <ref type="bibr" target="#b38">[38]</ref>. Therefore, sharing high-level layers risks to possible negative transfer, especially for the heterogeneous tasks in MKR. <ref type="bibr" target="#b1">(2)</ref> In highlevel layers of MKR, item features are mixed with user features, and entity features are mixed with relation features. The mixed features are not suitable for sharing since they have no explicit association.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Recommendation Module</head><p>The input of the recommendation module in MKR consists of two raw feature vectors u and v that describe user u and item v, respectively. u and v can be customized as one-hot ID <ref type="bibr" target="#b8">[8]</ref>, attributes <ref type="bibr" target="#b30">[30]</ref>, bag-of-words <ref type="bibr" target="#b29">[29]</ref>, or their combinations, based on the application scenario. Given user u's raw feature vector u, we use an L-layer MLP to extract his latent condensed feature <ref type="bibr" target="#b6">6</ref> :</p><formula xml:id="formula_4">u L = M(M(· · · M(u))) = M L (u),<label>(4)</label></formula><p>where M(x) = σ (Wx+b) is a fully-connected neural network layer 7 with weight W, bias b, and nonlinear activation function σ (·). For item v, we use L cross&amp;compress units to extract its feature:</p><formula xml:id="formula_5">v L = E e∼S(v) C L (v, e)[v] ,<label>(5)</label></formula><p>where S(v) is the set of associated entities of item v.</p><p>After having user u's latent feature u L and item v's latent feature v L , we combine the two pathways by a predicting function f RS , for example, inner product or an H -layer MLP. The final predicted probability of user u engaging item v is:</p><formula xml:id="formula_6">y uv = σ f RS (u L , v L ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Knowledge Graph Embedding Module</head><p>Knowledge graph embedding is to embed entities and relations into continuous vector spaces while preserving their structure. Recently, researchers have proposed a great many KGE methods, including translational distance models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">13]</ref> and semantic matching models <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b19">19]</ref>. In MKR, we propose a deep semantic matching architecture for KGE module. Similar to the recommendation module, for a given knowledge triple (h, r , t), we first utilize multiple cross&amp;compress units and nonlinear layers to process the raw feature vectors of head h and relation r (including ID <ref type="bibr" target="#b13">[13]</ref>, types <ref type="bibr" target="#b36">[36]</ref>, textual description <ref type="bibr" target="#b35">[35]</ref>, etc.), respectively. Their latent features are then concatenated together, followed by a K-layer MLP for predicting tail t:</p><formula xml:id="formula_7">h L = E v∼S(h) C L (v, h)[e] , r L = M L (r), t = M K h L r L ,<label>(7)</label></formula><p>where S(h) is the set of associated items of entity h, andt is the predicted vector of tail t. Finally, the score of the triple (h, r, t) is calculated using a score (similarity) function f KG :</p><formula xml:id="formula_8">score(h, r , t) = f KG (t,t),<label>(8)</label></formula><p>where t is the real feature vector of t. In this paper, we use the normalized inner product f KG (t,t) = σ (t ⊤t ) as the choice of score function <ref type="bibr" target="#b18">[18]</ref>, but other forms of (dis)similarity metrics can also be applied here such as KullbackâĂŞLeibler divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Learning Algorithm</head><p>The complete loss function of MKR is as follows:</p><formula xml:id="formula_9">L =L RS + L KG + L REG = u ∈U,v ∈V J (ŷ uv , y uv ) − λ 1 (h,r,t )∈ G score(h, r, t) − (h ′ ,r,t ′ ) G score(h ′ , r , t ′ ) + λ 2 ∥W∥ 2 2 .<label>(9)</label></formula><p>In Eq. (9), the first term measures loss in the recommendation module, where u and v traverse the set of users and the items, respectively, and J is the cross-entropy function. The second term calculates the loss in the KGE module, in which we aim to increase the score for all true triples while reducing the score for all false triples. The last item is the regularization term for preventing overfitting, λ 1 and λ 2 are the balancing parameters. <ref type="bibr" target="#b8">8</ref> Note that the loss function in Eq. (9) traverses all possible useritem pairs and knowledge triples. To make computation more efficient, following <ref type="bibr" target="#b17">[17]</ref>, we use a negative sampling strategy during training. The learning algorithm of MKR is presented in Algorithm 1, in which a training epoch consists of two stages: recommendation task (line 3-7) and KGE task (line 8-10). In each iteration, we repeat training on recommendation task for t times (t is a hyperparameter and normally t &gt; 1) before training on KGE task once in 8 λ 1 can be seen as the ratio of two learning rates for the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Multi-Task Training for MKR</head><formula xml:id="formula_10">Input: Interaction matrix Y, knowledge graph G Output: Prediction function F (u, v |Θ, Y, G)</formula><p>1: Initialize all parameters <ref type="bibr">2:</ref> for number of training iteration do // recommendation task <ref type="bibr">3:</ref> for t steps do 4:</p><p>Sample minibatch of positive and negative interactions from Y; <ref type="bibr">5:</ref> Sample e ∼ S(v) for each item v in the minibatch; <ref type="bibr">6:</ref> Update parameters of F by gradient descent on Eq. (1)-(6), (9); <ref type="bibr">7:</ref> end for // knowledge graph embedding task <ref type="bibr">8:</ref> Sample minibatch of true and false triples from G; <ref type="bibr">9:</ref> Sample v ∼ S(h) for each head h in the minibatch; <ref type="bibr">10:</ref> Update parameters of F by gradient descent on Eq. (1)-(3), (7)-(9); 11: end for each epoch, since we are more focused on improving recommendation performance. We will discuss the choice of t in the experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL ANALYSIS</head><p>In this section, we prove that cross&amp;compress units have sufficient capability of polynomial approximation. We also show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Polynomial Approximation</head><p>According to the Weierstrass approximation theorem <ref type="bibr" target="#b25">[25]</ref>, any function under certain smoothness assumption can be approximated by a polynomial to an arbitrary accuracy. Therefore, we examine the ability of high-order interaction approximation of the cross&amp;compress unit. We show that cross&amp;compress units can model the order of item-entity feature interaction up to exponential degree: Theorem 1. Denote the input of item and entity in MKR network as v = [v 1 · · · v d ] ⊤ and e = [e 1 · · · e d ] ⊤ , respectively. Then the cross terms about v and e in ∥v L ∥ 1 and</p><formula xml:id="formula_11">∥e L ∥ 1 (the L1-norm of v L and e L ) with maximal degree is k α , β v α 1 1 · · · v α d d e β 1 1 · · · e β d d , where k α , β ∈ R, α i , β i ∈ N for i ∈ {1, · · · , d}, α 1 + · · · + α d = 2 L−1 , and β 1 + · · · + β d = 2 L−1 (L ≥ 1, v 0 = v, e 0 = e). In recommender systems, d i=1 v α i i e β i</formula><p>i is also called combinatorial feature, as it measures the interactions of multiple original features. Theorem 1 states that cross&amp;compress units can automatically model the combinatorial features of items and entities for sufficiently high order, which demonstrates the superior approximation capacity of MKR as compared with existing work such as Wide&amp;Deep <ref type="bibr" target="#b2">[3]</ref>, factorization machines <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> and DCN <ref type="bibr" target="#b34">[34]</ref>. The proof of Theorem 1 is provided in the Appendix. Note that Theorem 1 gives a theoretical view of the polynomial approximation ability of the cross&amp;compress unit rather than providing guarantees on its actual performance. We will empirically evaluate the cross&amp;compress unit in the experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unified View of Representative Methods</head><p>In the following we provide a unified view of several representative models in recommender systems and multi-task learning, by showing that they are restricted versions of or theoretically related to MKR. This justifies the design of cross&amp;compress unit and conceptually explains its strong empirical performance as compared to baselines. <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> are a generic method for recommender systems. Given an input feature vector, FMs model all interactions between variables in the input vector using factorized parameters, thus being able to estimate interactions in problems with huge sparsity such as recommender systems. The model equation for a 2-degree factorization machine is defined aŝ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Factorization machines. Factorization machines</head><formula xml:id="formula_12">y(x) = w 0 + d i=1 w i x i + d i=1 d j=i+1 ⟨v i , v j ⟩x i x j ,<label>(10)</label></formula><p>where x i is the i-th unit of input vector x, w · is weight scalar, v · is weight vector, and ⟨·, ·⟩ is dot product of two vectors. We show that the essence of FM is conceptually similar to an 1-layer cross&amp;compress unit:</p><formula xml:id="formula_13">Proposition 1.</formula><p>The L1-norm of v 1 and e 1 can be written as the following form:</p><formula xml:id="formula_14">∥v 1 ∥ 1 (or ∥e 1 ∥ 1 ) = b + d i=1 d j=1 ⟨w i , w j ⟩v i e j ,<label>(11)</label></formula><p>where ⟨w i , w j ⟩ = w i + w j is the sum of two scalars.</p><p>It is interesting to notice that, instead of factorizing the weight parameter of x i x j into the dot product of two vectors as in FM, the weight of term v i e j is factorized into the sum of two scalars in cross&amp;compress unit to reduce the number of parameters and increase robustness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Deep&amp;Cross</head><p>Network. DCN <ref type="bibr" target="#b34">[34]</ref> learns explicit and highorder cross features by introducing the layers:</p><formula xml:id="formula_15">x l +1 = x 0 x ⊤ l w l + x l + b l ,<label>(12)</label></formula><p>where x l , w l , and b l are representation, weight, and bias of the l-th layer. We demonstrate the link between DCN and MKR by the following proposition:</p><formula xml:id="formula_16">Proposition 2. In the formula of v l +1 in Eq. (2), if we restrict w V V l</formula><p>in the first term to satisfy e ⊤ l w V V l = 1 and restrict e l in the second term to be e 0 (and impose similar restrictions on e l +1 ), the cross&amp;compress unit is then conceptually equivalent to DCN layer in the sense of multi-task learning:</p><formula xml:id="formula_17">v l +1 = e 0 v ⊤ l w EV l + v l + b V l , e l +1 = v 0 e ⊤ l w V E l + e l + b E l .<label>(13)</label></formula><p>It can be proven that the polynomial approximation ability of the above DCN-equivalent version (i.e., the maximal degree of cross terms in v l and e l ) is O(l), which is weaker than original cross&amp;compress units with O(2 l ) approximation ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Cross-stitch Networks.</head><p>Cross-stitch networks <ref type="bibr" target="#b18">[18]</ref> is a multi-task learning model in convolutional networks, in which the designed cross-stitch unit can learn a combination of shared and task-specific representations between two tasks. Specifically, given two activation maps x A and x B from layer l for both the tasks, cross-stitch networks learn linear combinationsx A andx B of both the input activations and feed these combinations as input to the next layers' filters. The formula at location (i, j) in the activation map is</p><formula xml:id="formula_18">     x i j Ã x i j B       = α AA α AB α BA α BB       x i j A x i j B       ,<label>(14)</label></formula><p>where α's are trainable transfer weights of representations between task A and task B. We show that the cross-stitch unit in Eq. <ref type="formula" target="#formula_0">(14)</ref> is a simplified version of our cross&amp;compress unit by the following proposition:</p><p>Proposition 3. If we omit all biases in Eq. <ref type="formula" target="#formula_2">(2)</ref>, the cross&amp;compress unit can be written as</p><formula xml:id="formula_19">v l +1 e l +1 = e ⊤ l w V V l v ⊤ l w EV l e ⊤ l w V E l v ⊤ l w EE l v l e l .<label>(15)</label></formula><p>The transfer matrix in Eq. (15) serves as the cross-stitch unit [α AA α AB ; α BA α BB ] in Eq. <ref type="bibr" target="#b14">(14)</ref>. Like cross-stitch networks, MKR network can decide to make certain layers task specific by setting</p><formula xml:id="formula_20">v ⊤ l w EV l (α AB ) or e ⊤ l w V E l (α BA )</formula><p>to zero, or choose a more shared representation by assigning a higher value to them. But the transfer matrix is more fine-grained in cross&amp;compress unit, because the transfer weights are replaced from scalars to dot products of two vectors. It is rather interesting to notice that Eq. (15) can also be regarded as an attention mechanism <ref type="bibr" target="#b0">[1]</ref>, as the computation of transfer weights involves the feature vectors v l and e l themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the performance of MKR in four realworld recommendation scenarios: movie, book, music, and news 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We utilize the following four datasets in our experiments:</p><p>• MovieLens-1M 10 is a widely used benchmark dataset in movie recommendations, which consists of approximately 1 million explicit ratings (ranging from 1 to 5) on the Movie-Lens website. • Book-Crossing 11 dataset contains 1,149,780 explicit ratings (ranging from 0 to 10) of books in the Book-Crossing community. • Last.FM 12 dataset contains musician listening information from a set of 2 thousand users from Last.fm online music system. • Bing-News dataset contains 1,025,192 pieces of implicit feedback collected from the server logs of Bing News 13 from Since MovieLens-1M, Book-Crossing, and Last.FM are explicit feedback data (Last.FM provides the listening count as weight for each user-item interaction), we transform them into implicit feedback where each entry is marked with 1 indicating that the user has rated the item positively, and sample an unwatched set marked as 0 for each user. The threshold of positive rating is 4 for MovieLens-1M, while no threshold is set for Book-Crossing and Last.FM due to their sparsity.</p><p>We use Microsoft Satori to construct the KG for each dataset. We first select a subset of triples from the whole KG with a confidence level greater than 0.9. For MovieLens-1M and Book-Crossing, we additionally select a subset of triples from the sub-KG whose relation name contains "film" or "book" respectively to further reduce KG size.</p><p>Given the sub-KGs, for MovieLens-1M, Book-Crossing, and Last.FM, we collect IDs of all valid movies, books, or musicians by matching their names with tail of triples (head, film.film.name, tail), (head, book.book.title, tail), or (head, type.object.name, tail), respectively. For simplicity, items with no matched or multiple matched entities are excluded. We then match the IDs with the head and tail of all KG triples and select all well-matched triples from the sub-KG. The constructing process is similar for Bing-News except that: (1) we use entity linking tools to extract entities in news titles; (2) we do not impose restrictions on the names of relations since the entities in news titles are not within one particular domain. The basic statistics of the four datasets are presented in <ref type="table" target="#tab_0">Table 1</ref>. Note that the number of users, items, and interactions are smaller than original datasets since we filtered out items with no corresponding entity in the KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our proposed MKR with the following baselines. Unless otherwise specified, the hyper-parameter settings of baselines are the same as reported in their original papers or as default in their codes.</p><p>• PER <ref type="bibr" target="#b39">[39]</ref> treats the KG as heterogeneous information networks and extracts meta-path based features to represent the connectivity between users and items. In this paper, we use manually designed user-item-attribute-item paths as features, i.e., "user-movie-director-movie", "user-moviegenre-movie", and "user-movie-star-movie" for MovieLens-20M; "user-book-author-book" and "user-book-genre-book" for Book-Crossing; "user-musician-genre-musician", "usermusician-country-musician", and "user-musician-age-musician" (age is discretized) for Last.FM. Note that PER cannot be applied to news recommendation because it's hard to pre-define meta-paths for entities in news. • CKE <ref type="bibr" target="#b40">[40]</ref> combines CF with structural, textual, and visual knowledge in a unified framework for recommendation. We implement CKE as CF plus structural knowledge module in this paper.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments setup</head><p>In MKR, we set the number of high-level layers K = 1, f RS as inner product, and λ 2 = 10 −6 for all three datasets, and other hyperparameter are given in <ref type="table" target="#tab_0">Table 1</ref>. The settings of hyper-parameters are determined by optimizing AUC on a validation set. For each dataset, the ratio of training, validation, and test set is 6 : 2 : 2. Each experiment is repeated 3 times, and the average performance is reported. We evaluate our method in two experiment scenarios: <ref type="bibr" target="#b0">(1)</ref> In click-through rate (CTR) prediction, we apply the trained model to each piece of interactions in the test set and output the predicted click probability. We use AUC and Accuracy to evaluate the performance of CTR prediction. (2) In top-K recommendation, we use the trained model to select K items with highest predicted click probability for each user in the test set, and choose Precision@K and Recall@K to evaluate the recommended sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Empirical study</head><p>We conduct an empirical study to investigate the correlation of items in RS and their corresponding entities in KG. Specifically, we aim to reveal how the number of common neighbors of an item pair in KG changes with their number of common raters in RS. To this end, we first randomly sample 1 million item pairs from MovieLens-1M. We then classify each pair into 5 categories based on the number of their common raters in RS, and count their average number of common neighbors in KG for each category. The result is presented in <ref type="figure" target="#fig_2">Figure 2a</ref>, which clearly shows that if two items have more common raters in RS, they are likely to share more common neighbors in KG. <ref type="figure" target="#fig_2">Figure 2b</ref> shows the positive correlation from an opposite direction. The above findings empirically demonstrate that items share the similar structure of proximity in KG and RS, thus the cross knowledge transfer of items benefits both recommendation and KGE tasks in MKR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Comparison with baselines.</head><p>The results of all methods in CTR prediction and top-K recommendation are presented in <ref type="table" target="#tab_2">Table  2</ref> and <ref type="figure">Figure 3</ref>, 4, respectively. We have the following observations:</p><p>• PER performs poor on movie, book, and music recommendation because the user-defined meta-paths can hardly be optimal in reality. Moreover, PER cannot be applied to news recommendation. • CKE performs better in movie, book, and music recommendation than news. This may be because MovieLens-1M, Book-Crossing, and Last.FM are much denser than Bing-News, which is more favorable for the collaborative filtering part in CKE. • DKN performs best in news recommendation compared with other baselines, but performs worst in other scenarios. This is because movie, book, and musician names are too short and ambiguous to provide useful information. • RippleNet performs best among all baselines, and even outperforms MKR on MovieLens-1M. This demonstrates that RippleNet can precisely capture user interests, especially in the case where user-item interactions are dense. However, RippleNet is more sensitive to the density of datasets, as it performs worse than MKR in Book-Crossing, Last.FM, and Bing-News. We will further study their performance in sparse scenarios in Section 4.5.3. • In general, our MKR performs best among all methods on the four datasets. Specifically, MKR achieves average Accuracy gains of 11.6%, 11.5%, 12.7%, and 8.7% in movie, book, music, and news recommendation, respectively, which demonstrates the efficacy of the multi-task learning framework in MKR. Note that the top-K metrics are much lower for Bing-News because the number of news is significantly larger than movies, books, and musicians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Comparison with MKR variants.</head><p>We further compare MKR with its three variants to demonstrate the efficacy of cross&amp;compress unit:</p><p>• MKR-1L is MKR with one layer of cross&amp;compress unit, which corresponds to FM model according to Proposition 1. Note that MKR-1L is actually MKR in the experiments for MovieLens-1M.  From <ref type="table" target="#tab_2">Table 2</ref> we observe that MKR outperforms MKR-1L and MKR-DCN, which shows that modeling high-order interactions between item and entity features is helpful for maintaining decent performance. MKR also achieves better scores than MKR-stitch. This validates the efficacy of fine-grained control on knowledge transfer in MKR compared with the simple cross-stitch units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.3</head><p>Results in sparse scenarios. One major goal of using knowledge graph in MKR is to alleviate the sparsity and the cold start problem of recommender systems. To investigate the efficacy of the KGE module in sparse scenarios, we vary the ratio of training set of MovieLens-1M from 100% to 10% (while the validation and test set are kept fixed), and report the results of AU C in CTR prediction for all methods. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. We observe that the performance of all methods deteriorates with the reduce of the training set. When r = 10%, the AUC score decreases by 15.8%, 15.9%, 11.6%, 8.4%, 10.2%, 12.2% for PER, CKE, DKN, RippleNet, LibFM, and Wide&amp;Deep, respectively, compared with the case when full training set is used (r = 100%). In contrast, the  AU C score of MKR only decreases by 5.3%, which demonstrates that MKR can still maintain a decent performance even when the user-item interaction is sparse. We also notice that MKR performs better than RippleNet in sparse scenarios, which is accordance with our observation in Section 4.5.1 that RippleNet is more sensitive to the density of user-item interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.5.4</head><p>Results on KGE side. Although the goal of MKR is to utilize KG to assist with recommendation, it is still interesting to investigate whether the RS task benefits the KGE task, since the principle of multi-task learning is to leverage shared information to help improve the performance of all tasks <ref type="bibr" target="#b42">[42]</ref>. We present the result of RMSE (rooted mean square error) between predicted and real vectors of tails in the KGE task in <ref type="table" target="#tab_4">Table 4</ref>. Fortunately, we find that the existence of RS module can indeed reduce the prediction error by 1.9% ∼ 6.4%. The results show that the cross&amp;compress units are able to learn general and shared features that mutually benefit both sides of MKR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Parameter Sensitivity</head><p>4.6.1 Impact of KG size. We vary the size of KG to further investigate the efficacy of usage of KG. The results of AUC on Bing-News are plotted in <ref type="figure" target="#fig_5">Figure 5a</ref>. Specifically, the AUC and Accuracy is enhanced by 13.6% and 11.8% with the KG ratio increasing from 0.1 to 1.0 in three scenarios, respectively. This is because the Bing-News dataset is extremely sparse, making the effect of KG usage rather obvious. 4.6.2 Impact of RS training frequency. We investigate the influence of parameters t in MKR by varying t from 1 to 10, while keeping other parameters fixed. The results are presented in <ref type="figure" target="#fig_5">Figure 5b</ref>. We observe that MKR achieves the best performance when t = 5. This is because a high training frequency of the KGE module will mislead the objective function of MKR, while too small of a training frequency of KGE cannot make full use of the transferred knowledge from the KG. 4.6.3 Impact of embedding dimension. We also show how the dimension of users, items, and entities affects the performance of MKR in <ref type="figure" target="#fig_5">Figure 5c</ref>. We find that the performance is initially improved with the increase of dimension, because more bits in embedding layer can encode more useful information. However, the performance drops when the dimension further increases, as too large number of dimensions may introduce noises which mislead the subsequent prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK 5.1 Knowledge Graph Embedding</head><p>The KGE module in MKR connects to a large body of work in KGE methods. KGE is used to embed entities and relations in a knowledge into low-dimensional vector spaces while still preserving the structural information <ref type="bibr" target="#b33">[33]</ref>. KGE methods can be classified into the following two categories: (1) Translational distance models exploit distance-based scoring functions when learning representations of entities and relations, such as TransE <ref type="bibr" target="#b1">[2]</ref>, TransH <ref type="bibr" target="#b35">[35]</ref>, and TransR <ref type="bibr" target="#b13">[13]</ref>; (2) Semantic matching models measure plausibility of knowledge triples by matching latent semantics of entities and relations, such as RESCAL <ref type="bibr" target="#b20">[20]</ref>, ANALOGY <ref type="bibr" target="#b19">[19]</ref>, and HolE <ref type="bibr" target="#b14">[14]</ref>. Recently, researchers also propose incorporating auxiliary information, such as entity types <ref type="bibr" target="#b36">[36]</ref>, logic rules <ref type="bibr" target="#b24">[24]</ref>, and textual descriptions <ref type="bibr" target="#b46">[46]</ref> to assist KGE. The above KGE methods can also be incorporated into MKR as the implementation of the KGE module, but note that the cross&amp;compress unit in MKR needs to be redesigned accordingly. Exploring other designs of KGE module as well as the corresponding bridging unit is also an important direction of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-Task Learning</head><p>Multi-task learning is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks <ref type="bibr" target="#b42">[42]</ref>. All of the learning tasks are assumed to be related to each other, and it is found that learning these tasks jointly can lead to performance improvement compared with learning them individually. In general, MTL algorithms can be classified into several categories, including feature learning approach <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b41">41]</ref>, low-rank approach <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>, task clustering approach <ref type="bibr" target="#b47">[47]</ref>, task relation learning approach <ref type="bibr" target="#b12">[12]</ref>, and decomposition approach <ref type="bibr" target="#b6">[6]</ref>. For example, the cross-stitch network <ref type="bibr" target="#b41">[41]</ref> determines the inputs of hidden layers in different tasks by a knowledge transfer matrix; Zhou et. al <ref type="bibr" target="#b47">[47]</ref> aims to cluster tasks by identifying representative tasks which are a subset of the given m tasks, i.e., if task T i is selected by task T j as a representative task, then it is expected that model parameters for T j are similar to those of T i . MTL can also be combined with other learning paradigms to improve the performance of learning tasks further, including semi-supervised learning, active learning, unsupervised learning,and reinforcement learning.</p><p>Our work can be seen as an asymmetric multi-task learning framework <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref>, in which we aim to utilize the connection between RS and KG to help improve their performance, and the two tasks are trained with different frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Deep Recommender Systems</head><p>Recently, deep learning has been revolutionizing recommender systems and achieves better performance in many recommendation scenarios. Roughly speaking, deep recommender systems can be classified into two categories: (1) Using deep neural networks to process the raw features of users or items <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b40">40]</ref>; For example, Collaborative Deep Learning <ref type="bibr" target="#b29">[29]</ref> designs autoencoders to extract short and dense features from textual input and feeds the features into a collaborative filtering module; DeepFM <ref type="bibr" target="#b5">[5]</ref> combines factorization machines for recommendation and deep learning for feature learning in a neural network architecture. (2) Using deep neural networks to model the interaction among users and items <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>. For example, Neural Collaborative Filtering <ref type="bibr" target="#b8">[8]</ref> replaces the inner product with a neural architecture to model the user-item interaction. The major difference between these methods and ours is that MKR deploys a multi-task learning framework that utilizes the knowledge from a KG to assist recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>This paper proposes MKR, a multi-task learning approach for knowledge graph enhanced recommendation. MKR is a deep and endto-end framework that consists of two parts: the recommendation module and the KGE module. Both modules adopt multiple nonlinear layers to extract latent features from inputs and fit the complicated interactions of user-item and head-relation pairs. Since the two tasks are not independent but connected by items and entities, we design a cross&amp;compress unit in MKR to associate the two tasks, which can automatically learn high-order interactions of item and entity features and transfer knowledge between the two tasks. We conduct extensive experiments in four recommendation scenarios. The results demonstrate the significant superiority of MKR over strong baselines and the efficacy of the usage of KG.</p><p>For future work, we plan to investigate other types of neural networks (such as CNN) in MKR framework. We will also incorporate other KGE methods as the implementation of KGE module in MKR by redesigning the cross&amp;compress unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A Proof of Theorem 1</head><p>Proof. We prove the theorem by induction: Base case: When l = 1,</p><formula xml:id="formula_21">v 1 =ve ⊤ w V V 0 + ev ⊤ w EV 0 + b V 0 = v 1 d i=1 e i w V V (i) 0 · · · v d d i=1 e i w V V (i) 0 ⊤ + e 1 d i=1 v i w EV (i) 0 · · · e d d i=1 v i w EV (i) 0 ⊤ + b V (0) 0 · · · b V (d ) 0 ⊤ .</formula><p>Therefore, we have</p><formula xml:id="formula_22">∥v 1 ∥ 1 = d j=1 v j d i=1 e i w V V (i) 0 + d j=1 e j d i=1 v i w EV (i) 0 + d i=1 b V (d ) 0 = d i=1 d j=1 (w EV (i) 0 + w V V (j) 0 )v i e j + d i=1 b V (d ) 0</formula><p>.</p><p>It is clear that the cross terms about v and e with maximal degree is k α , β v i e j , so we have α 1 + · · · +α d = 1 = 2 1−1 , and β 1 + · · · + β d = 1 = 2 1−1 for v 1 . The proof for e 1 is similar.</p><p>Induction step: Suppose α 1 + · · · + α d = 2 l −1 and β 1 + · · · + β d = 2 l −1 hold for the maximal-degree term x and y in ∥v l ∥ 1 and ∥e l ∥ 1 . Since ∥v l ∥ 1 = d i=1 v l , respectively. Then for l + 1, we have</p><formula xml:id="formula_23">∥v l +1 ∥ 1 = d i=1 d j=1 (w EV (i) l + w V V (j) l )v (i) l e (j) l + d i=1 b V (d ) l .</formula><p>Obviously, the maximal-degree term in ∥v l +1 ∥ 1 is the cross term xy in v (a) l e (b) l . Since we have α 1 +· · ·+α d = 2 l −1 and β 1 +· · ·+β d = 2 l −1 for both x and y, the degree of cross term xy therefore satisfies α 1 + · · · + α d = 2 (l +1)−1 and β 1 + · · · + β d = 2 (l +1)−1 . The proof for ∥e l +1 ∥ 1 is similar. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Proposition 1</head><p>Proof. In the proof of Theorem 1 in Appendix A, we have shown that</p><formula xml:id="formula_24">∥v 1 ∥ 1 = d i=1 d j=1 (w EV (i) 0 + w V V (j) 0 )v i e j + d i=1 b V (d ) 0 .</formula><p>It is easy to see that w i = w</p><formula xml:id="formula_25">EV (i) 0 , w j = w V V (j) 0 , and b = d i=1 b V (d ) 0</formula><p>. The proof is similar for ∥e 1 ∥ 1 . □</p><p>We omit the proofs for Proposition 2 and Proposition 3 as they are straightforward.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) The framework of MKR. The left and right part illustrate the recommendation module and the KGE module, respectively, which are bridged by the cross&amp;compress units. (b) Illustration of a cross&amp;compress unit. The cross&amp;compress unit generates a cross feature matrix from item and entity vectors by cross operation, and outputs their vectors for the next layer by compress operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The correlation between the number of common neighbors of an item pair in KG and their number of common raters in RS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>PER 0 .</head><label>0</label><figDesc>710 (-22.6%) 0.664 (-21.2%) 0.623 (-15.1%) 0.588 (-16.7%) 0.633 (-20.6%) 0.596 (-20.7%) --CKE 0.801 (-12.6%) 0.742 (-12.0%) 0.671 (-8.6%) 0.633 (-10.3%) 0.744 (-6.6%) 0.673 (-10.5%) 0.553 (-19.7%) 0.516 (-20.0%) DKN 0.655 (-28.6%) 0.589 (-30.1%) 0.622 (-15.3%) 0.598 (-15.3%) 0.602 (-24.5%) 0.581 (-22.7%) 0.667 (-3.2%) 0.610 (-5.4%) RippleNet 0.920 (+0.3%) 0.842 (-0.1%) 0.729 (-0.7%) 0.662 (-6.2%) 0.768 (-3.6%) 0.691 (-8.1%) 0.678 (-1.6%) 0.630 (-2.3%) LibFM 0.892 (-2.7%) 0.812 (-3.7%) 0.685 (-6.7%) 0.640 (-9.3%) 0.777 (-2.5%) 0.709 (-5.7%) 0.640 (-7.1%) 0.591 (-8.4%) Wide&amp;Deep 0.898 (-2.1%) 0.820 (-2.7%) 0.712 (-3.0%) 0.624 (-11.6%) 0.756 (-5.1%) 0.688 (-8.5%) 0.651 (-5.5%) 0.597 (-7(-0.3%) 0.749 (-0.4%) 0.680 (-1.3%) 0.631 (-2.2%) MKR-DCN 0.883 (-3.7%) 0.802 (-4.9%) 0.705 (-4.3%) 0.676 (-4.2%) 0.778 (-2.4%) 0.730 (-2.9%) 0.671 (-2.6%) 0.614 (-4.8%) MKR-stitch 0.905 (-1.3%) 0.830 (-1.5%) 0.721 (-2.2%) 0.682 (-3.4%) 0.772 (-3.1%) 0.725 (-3.6%) 0.674 (-2.2%) 0.621 (-3.7%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>• MKR-DCN is a variant of MKR based on Eq. (13), which corresponds to DCN model. • MKR-stitch is another variant of MKR corresponding to the cross-stitch network, in which the transfer weights in Eq. (15) are replaced by four trainable scalars. The results of Precision@K in top-K recommendation. The results of Recall@K in top-K recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Parameter sensitivity of MKR on Bing-News w.r.t. (a) the size of the knowledge graph; (b) training frequency of the RS module t; and (c) dimension of embeddings d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>loss of generosity, we assume that x and y exist in v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics and hyper-parameter settings for the four datasets.</figDesc><table><row><cell>Dataset</cell><cell># users</cell><cell># items</cell><cell># interactions</cell><cell># KG triples</cell><cell>Hyper-parameters</cell></row><row><cell>MovieLens-1M</cell><cell>6,036</cell><cell>2,347</cell><cell>753,772</cell><cell>20,195</cell><cell>L = 1, d = 8, t = 3, λ 1 = 0.5</cell></row><row><cell>Book-Crossing</cell><cell>17,860</cell><cell>14,910</cell><cell>139,746</cell><cell>19,793</cell><cell>L = 1, d = 8, t = 2, λ 1 = 0.1</cell></row><row><cell>Last.FM</cell><cell>1,872</cell><cell>3,846</cell><cell>42,346</cell><cell>15,518</cell><cell>L = 2, d = 4, t = 2, λ 1 = 0.1</cell></row><row><cell>Bing-News</cell><cell>141,487</cell><cell>535,145</cell><cell>1,025,192</cell><cell>1,545,217</cell><cell>L = 3, d = 16, t = 5, λ 1 = 0.2</cell></row><row><cell cols="4">October 16, 2016 to August 11, 2017. Each piece of news has</cell><cell></cell><cell></cell></row><row><cell>a title and a snippet.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The dimension of user and item embeddings for the four datasets are set as 64, 128, 32, 64, respectively. The dimension of entity embeddings is 32.• DKN<ref type="bibr" target="#b32">[32]</ref> treats entity embedding and word embedding as multiple channels and combines them together in CNN for CTR prediction. In this paper, we use movie/book names and news titles as textual input for DKN. The dimension of word embedding and entity embedding is 64, and the number of filters is 128 for each window size 1, 2, 3. • RippleNet [31] is a memory-network-like approach that propagates usersâĂŹ preferences on the knowledge graph for recommendation. The hyper-parameter settings for Last.FM are d = 8, H = 2, λ 1 = 10 −6 , λ 2 = 0.01, η = 0.02. • LibFM [23] is a widely used feature-based factorization model. We concatenate the raw features of users and items as well as the corresponding averaged entity embeddings learned from TransR<ref type="bibr" target="#b13">[13]</ref> as input for LibFM. The dimension is {1, 1, 8} and the number of training epochs is 50. The dimension of TransR is 32.</figDesc><table><row><cell>• Wide&amp;Deep [3] is a deep recommendation model combin-</cell></row><row><cell>ing a (wide) linear channel with a (deep) nonlinear channel.</cell></row></table><note>The input for Wide&amp;Deep is the same as in LibFM. The di- mension of user, item, and entity is 64, and we use a two-layer deep channel with dimension of 100 and 50 as well as a wide channel.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The results of AUC and Accuracy in CTR prediction.</figDesc><table><row><cell></cell><cell cols="2">MovieLens-1M</cell><cell cols="2">Book-Crossing</cell><cell>Last.FM</cell><cell></cell><cell></cell><cell>Bing-News</cell></row><row><cell>Model</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell><cell>AUC</cell><cell>ACC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of AUC on MovieLens-1M in CTR prediction with different ratios of training set r .</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>r</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell>PER</cell><cell>0.598</cell><cell>0.607</cell><cell>0.621</cell><cell>0.638</cell><cell>0.647</cell><cell>0.662</cell><cell>0.675</cell><cell>0.688</cell><cell>0.697</cell><cell>0.710</cell></row><row><cell>CKE</cell><cell>0.674</cell><cell>0.692</cell><cell>0.705</cell><cell>0.716</cell><cell>0.739</cell><cell>0.754</cell><cell>0.768</cell><cell>0.775</cell><cell>0.797</cell><cell>0.801</cell></row><row><cell>DKN</cell><cell>0.579</cell><cell>0.582</cell><cell>0.589</cell><cell>0.601</cell><cell>0.612</cell><cell>0.620</cell><cell>0.631</cell><cell>0.638</cell><cell>0.646</cell><cell>0.655</cell></row><row><cell>RippleNet</cell><cell>0.843</cell><cell>0.851</cell><cell>0.859</cell><cell>0.862</cell><cell>0.870</cell><cell>0.878</cell><cell>0.890</cell><cell>0.901</cell><cell>0.912</cell><cell>0.920</cell></row><row><cell>LibFM</cell><cell>0.801</cell><cell>0.810</cell><cell>0.816</cell><cell>0.829</cell><cell>0.837</cell><cell>0.850</cell><cell>0.864</cell><cell>0.875</cell><cell>0.886</cell><cell>0.892</cell></row><row><cell>Wide&amp;Deep</cell><cell>0.788</cell><cell>0.802</cell><cell>0.809</cell><cell>0.815</cell><cell>0.821</cell><cell>0.840</cell><cell>0.858</cell><cell>0.876</cell><cell>0.884</cell><cell>0.898</cell></row><row><cell>MKR</cell><cell>0.868</cell><cell>0.874</cell><cell>0.881</cell><cell>0.882</cell><cell>0.889</cell><cell>0.897</cell><cell>0.903</cell><cell>0.908</cell><cell>0.913</cell><cell>0.917</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The results of RMSE on the KGE module for the three datasets. "KGE" means only KGE module is trained, while "KGE + RS" means KGE module and RS module are trained together.</figDesc><table><row><cell>dataset</cell><cell>KGE</cell><cell>KGE + RS</cell></row><row><cell>MovieLens-1M</cell><cell>0.319</cell><cell>0.302</cell></row><row><cell>Book-Crossing</cell><cell>0.596</cell><cell>0.558</cell></row><row><cell>Last.FM</cell><cell>0.480</cell><cell>0.471</cell></row><row><cell>Bing-News</cell><cell>0.488</cell><cell>0.459</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">KGE task can also benefit from recommendation task empirically as shown in the experiments section.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use the exponent notation L in Eq. (4) and following equations in the rest of this paper for simplicity, but note that the parameters of L layers are actually different.<ref type="bibr" target="#b7">7</ref> Exploring a more elaborate design of layers in the recommendation module is an important direction of future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">The source code is available at https://github.com/hwwang55/MKR. 10 https://grouplens.org/datasets/movielens/1m/ 11 http://www2.informatik.uni-freiburg.de/~cziegler/BX/ 12 https://grouplens.org/datasets/hetrec-2011/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning tree structure in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-Stage Multi-Task Learning with Reduced Rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1638" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A matrix factorization technique with trust propagation for recommendation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM conference on Recommender systems</title>
		<meeting>the 4th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asymmetric multi-task learning based on task relatedness and loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 29th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analogical Inference for Multi-Relational Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Multiple Tasks with Multilinear Relationship Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1593" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spectral k-support norm regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stamos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3644" to="3652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Holographic Embeddings of Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 30th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on Data Mining</title>
		<meeting>the 10th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Injecting logical background knowledge into embeddings for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Principles of mathematical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-domain collaboration recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1285" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Graphgan: Graph representation learning with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2508" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint Topic-Semantic-aware Social Recommendation for Online Voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shine: Signed heterogeneous information network embedding for sentiment link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DKN: Deep Knowledge-Aware Network for News Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1835" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep &amp; Cross Network for Ad Click Predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation Learning of Knowledge Graphs with Hierarchical Types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multitask learning for classification with dirichlet process priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnapuram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="35" to="63" />
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: A heterogeneous information network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 7th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep model based transfer and multi-task learning for biological image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhir</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<title level="m">A survey on multi-task learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A convex formulation for learning task relationships in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.3536</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A regularization approach to learning task relationships in multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Metagraph based recommendation fusion over heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dik Lun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aligning knowledge and text embeddings by entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Flexible Clustered Multi-Task Learning by Learning Representative Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="266" to="278" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
