<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of New Media and Communication</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
							<email>ghfu@hotmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>minzhang@suda.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Syntax-Enhanced Neural Machine Translation with Syntax-Aware Word Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Syntax has been demonstrated highly effective in neural machine translation (NMT). Previous NMT models integrate syntax by representing 1-best tree outputs from a welltrained parsing system, e.g., the representative Tree-RNN and Tree-Linearization methods, which may suffer from error propagation. In this work, we propose a novel method to integrate source-side syntax implicitly for NMT. The basic idea is to use the intermediate hidden representations of a well-trained end-to-end dependency parser, which are referred to as syntax-aware word representations (SAWRs). Then, we simply concatenate such SAWRs with ordinary word embeddings to enhance basic NMT models. The method can be straightforwardly integrated into the widelyused sequence-to-sequence (Seq2Seq) NMT models. We start with a representative RNNbased Seq2Seq baseline system, and test the effectiveness of our proposed method on two benchmark datasets of the Chinese-English and English-Vietnamese translation tasks, respectively. Experimental results show that the proposed approach is able to bring significant BLEU score improvements on the two datasets compared with the baseline, 1.74 points for Chinese-English translation and 0.80 point for English-Vietnamese translation, respectively. In addition, the approach also outperforms the explicit Tree-RNN and Tree-Linearization methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance <ref type="bibr" target="#b5">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b17">Jean et al., 2015;</ref><ref type="bibr" target="#b36">Shen et al., 2016;</ref><ref type="bibr" target="#b42">Vaswani et al., 2017)</ref>. The widely used * Corresponding author. • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. <ref type="figure">Figure 1</ref>: An example to illustrate our method of encoding source dependency syntax, where the English translation is "Education is the cornerstone of modern civilization" for the source Chinese input. sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English <ref type="bibr" target="#b29">Mi et al., 2016;</ref><ref type="bibr" target="#b42">Vaswani et al., 2017;</ref><ref type="bibr" target="#b9">Cheng et al., 2018)</ref>. Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence <ref type="bibr" target="#b10">(Cho et al., 2014a)</ref>.</p><p>Recently, inspired by the success of syntaxbased SMT , researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences <ref type="bibr" target="#b45">Wu et al., 2017b;</ref><ref type="bibr" target="#b6">Bastings et al., 2017;</ref><ref type="bibr" target="#b16">Hashimoto and Tsuruoka, 2017)</ref>.</p><p>As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. <ref type="bibr" target="#b15">Eriguchi et al. (2016)</ref>,  and  show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models.</p><p>Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of , our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classical Seq2Seq system.</p><p>To solve the problem, Tree-Linearization is a good alternative for syntax encoding. The main idea is to linearize syntax trees into sequential symbols, and then exploit the resulting sequences as inputs for NMT.  propose a depth-first method to traverse a constituent tree, converting it into a sequence of symbols mixed with sentential words and syntax labels. Similarly, <ref type="bibr" target="#b45">Wu et al. (2017b)</ref> combine several strategies of tree traversing for dependency syntax integration.</p><p>In this work, we present an implicit syntax encoding method for NMT, enhancing NMT models by syntax-aware word representations (SAWRs). <ref type="figure">Figure 1</ref> illustrates the basic idea, where trees are modeled indirectly by sequential vectors extracted from an encoder-decoder dependency parser. On the one hand, the method avoids the structural heterogeneity and thus can be integrated efficiently, and on the other hand, it does not require discrete 1-best tree outputs, alleviating the error propagation problem induced from syntax parsers. Concretely, the vector outputs are extracted from the encoding outputs of the encoder-decoder dependency parser. As shown in <ref type="figure">Figure 1</ref>, the encoding outputs, denoted as o = o 1 · · · o 6 , are then integrated into Seq2Seq NMT models by directly concatenated with the source input word embeddings after a linear projection.</p><p>We start with a Seq2Seq baseline with attention mechanism <ref type="bibr" target="#b5">(Bahdanau et al., 2014)</ref> for study, following previous studies of the same research line, and then integrate source dependency syntax by SAWRs. We conduct experiments on Chinese-English and English-Vietnamese translation tasks, respectively. The results show that our method is very effective in source syntax integration. With source dependency syntax, the performances of Chinese-English and English-Vietnamese translation can be significantly boosted by 1.74 BLEU points and 0.80 BLEU points, respectively. We also compare the method with the representative Tree-RNN and Tree-Linearization approaches of syntax integration, finding that our method is able to achieve larger improvements than the two approaches for both tasks. All the codes are released publicly available at https://github.com/zhangmeishan/SYN4NMT under Apache License 2.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline</head><p>We take the simple yet effective Seq2Seq model with attention mechanism proposed by  as our baseline. Under the standard encoder-decoder architecture, an encoder first maps the source-language input sentence into a sequence of hidden vectors, and a decoder then incrementally predicts the target output sentence. In particular, we should notice that several recent models <ref type="bibr" target="#b42">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b50">Zheng et al., 2017;</ref><ref type="bibr" target="#b9">Cheng et al., 2018)</ref> which have been shown to be more powerful can also serve as our baseline, since these models focus on very different aspects of NMT, which could be potentially complementary with our focus of syntax integration. We will demonstrate it by experimental analysis as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder</head><p>In the encoder part, a single-layer bi-directional recurrent neural network (Bi-RNN) is employed to encode the sentence in order to capture features from the current word and the unbounded left and right contextual words. Given a sourcelanguage input sentence x = x 1 · · · x n and its embedding sequence e x 1 · · · e xn , the Bi-RNN produces an encoding sequence of dense vectors h = h 1 · · · h i · · · h n :</p><formula xml:id="formula_0">h i = − → h i ⊕ ← − h i , − → h i = rnn L (e x i , − → h i−1 ) ← − h i = rnn R (e x i , ← − h i+1 )<label>(1)</label></formula><p>where rnn L/R can be either GRU <ref type="bibr" target="#b11">(Cho et al., 2014b)</ref> or LSTM. We use GRU all through this paper for efficiency following .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoder</head><p>The decoder part incrementally predicts the target word sequence y = y 1 · · · y m , whose translation probability is defined as follows:</p><formula xml:id="formula_1">p(y|x) = m j=1 p(y j |y 1 · · · y j−1 , h).<label>(2)</label></formula><p>The training objective is to maximize the probability of the reference translation. During evaluation, we aim to search for a target sentence with the highest probability for a given source sentence.</p><p>The probability of the j-th target word is computed by a two-layer feed-forward neural network:</p><formula xml:id="formula_2">p(y j |y 1 · · · y j−1 , h) = g(s j−1 , c j ),<label>(3)</label></formula><p>where s j−1 = rnn tgt (e y j−1 ⊕ c j−1 , s j−2 ) is the output of a left-to-right RNN over the predicted words, and the c j /c j−1 is the weighted sum over the encoding sequence h of the source sentence via the attention mechanism, which is computed as follows:</p><formula xml:id="formula_3">c j = n k=1 α j,k h k α j,k = exp(β j,k ) n l=1 exp(β j,l ) β j,l = s T j−1 W a h l<label>(4)</label></formula><p>where W a is the model parameter in attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>Syntax information has been demonstrated to be valuable for NMT. Previously, there were two representative approaches to encode syntax into an NMT model. The first approach directly represents an input syntax tree by Tree-RNN, and then uses the Tree-RNN outputs as additional encoder inputs for NMT. The second approach models source syntax trees indirectly by first converting a hierarchical tree into a sequence of symbols, and then use the symbols as inputs for NMT. The second method is referred to as Tree-Linearization here.</p><p>Tree-RNN is able to represent the syntax structures fully and comprehensively. However, because of the heterogeneity of different syntax trees, this approach suffers serious inefficiency problem as the increased difficulty of batch computation for GPU neural computation. The second approach exploits an alternative sequence to substitute the original trees, which solves the inefficiency problem. But it may bring loss of syntax information because the hierarchical tree structure is no longer maintained in the new representation, which could be potentially useful for NMT.</p><p>Both the two syntax integration approaches are based on discrete 1-best outputs of a supervised dependency parser, which may suffer from the error propagation problem. Incorrect syntax trees as inputs for NMT may produce erroneous outputs, leading to inappropriate translation results. In order to alleviate the problem, we present a novel method not using the discrete parsing outputs.</p><p>We focus on supervised dependency parsing models which can be formalized as an encoderdecoder architecture, and exploit the encoder outputs as the inputs for our Seq2Seq NMT model. The encoder outputs are sequences of dense vectors aligning with the source sentential words, as shown in <ref type="figure">Figure 1</ref>, and thus they could be easily combined with the encoder part of our NMT model. We refer to this method as SAWR for brief. Our approach takes the implicit hidden outputs from a supervised parser as inputs for NMT, which greatly reduces the direct influence brought from discrete 1-best parser outputs. <ref type="figure" target="#fig_0">Figure 2</ref> shows the framework of SAWR. Concretely, we first project the encoder outputs of a dependency parsing model into a sequence of vectors by a feed-forward linear layer, as shown by the projection module in <ref type="figure" target="#fig_0">Figure 2</ref>:</p><formula xml:id="formula_4">s i = W o i + b (5)</formula><p>where o=o 1 · · · o n is the encoder output of a parsing model, W and b are model parameters.</p><p>Then we concatenate the resulting vectors with the source embeddings as inputs for the baseline Bi-RNN Encoder. Thus the encoder process can be formalized as follows:</p><formula xml:id="formula_5">h = Bi-RNN e x 1 ⊕ s 1 , · · · , e xn ⊕ s n . (6)</formula><p>Noticeably, the SAWR method can be regarded as an adaption of joint learning as well. We can train both dependency parsing and machine translation model parameters concurrently. In this work, we focus on the machine translation task and do not involve the training objective of dependency parsing. However, we can still finetune model parameters of the encoder part of dependency parsing by back-propagating the training losses of NMT into this part as well.</p><p>Actually, SAWRs are also similar to the ELMO embeddings <ref type="bibr" target="#b33">(Peters et al., 2018)</ref>. ELMO learns context word representations by using language model as objective, while SAWRs learn syntaxaware word representations by using dependency parsing as objective. On the other hand, compared with the Tree-RNN and Tree-Linearization methods which encode syntax trees by neural networks directly, SAWRs are less sensitive to the output syntax trees. Thus the SAWR method can alleviate the error propagation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>Data. We conduct experiments on the Chinese-English and English-Vietnamese translation tasks, respectively. For Chinese-English, we use the parallel training data from the publicly available LDC corpora, 1 with 28.3M Chinese words and 34.5M English words, respectively, consisting of 1.25M sentence pairs, and test model performances on the NIST datasets, using NIST MT02 as the development data, and MT03-06 as test datasets. For English-Vietnamese, we use the standard IWSLT 2015 dataset, 2 which consists of about 133K sentence pairs, and evaluate our models by exploiting the TED tst2012 and tst2013 as the development and test datasets, respectively.</p><p>For the source side sentences, we construct vocabularies of the most frequent 50K words, while for the target side sentences, we apply byte-pair encodings (BPE)  with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively.</p><p>Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics <ref type="bibr" target="#b32">(Papineni et al., 2002)</ref>, and adopt the script multi-bleu.perl in the Mose toolkit. 3 Significance tests are conducted based on the best-BLEU results for each approach by using bootstrap resampling <ref type="bibr" target="#b19">(Koehn, 2004)</ref>.</p><p>Alternatively, in order to compare the effectiveness of our model with other syntax integration methods, we implement a Tree-RNN approach and a Tree-Linearization approach, respectively:</p><p>• Tree-RNN: We build a one-layer bidirectional Tree-RNN with GRU over input word embeddings, producing syntaxenhanced word representations, which are then fed into the encoder of NMT as basic inputs. The method is similar to the model proposed by .</p><p>• Tree-Linearization: We first convert dependency trees into constituent trees <ref type="bibr" target="#b38">(Sun and Wan, 2013)</ref>, and then feed it into the NMT model proposed by .</p><p>Hyperparameters. We set the dimension sizes of all hidden neural layers to 1024, except the input layers for RNNs (i.e. input word embeddings and the projection layer of SAWR), which are set to 512. We initialize all model parameters by random uniform distribution between [−0.1, 0.1]. We apply dropout on the output layer of word translation with a ratio of 0.5.</p><p>We adopt the Adam algorithm <ref type="bibr" target="#b18">(Kingma and Ba, 2014)</ref> for parameter optimization, with the initial learning rate of 5 × 10 −4 , the gradient clipping threshold of 5, and the mini-batch size of 80. During translation, we employ beam search for decoding with the beam size of 5. Source-Side Parsing. We employ the state-ofthe-art BiAffine dependency parser recently proposed by <ref type="bibr" target="#b14">Dozat and Manning (2016)</ref> to obtain the source-side dependency syntax information. The BiAffine parser can also be understood as an encoder-decoder model, where the encoder part is a three-layer bi-directional LSTM over the input words, and the decoder uses BiAffine operations to score all candidate dependency arcs and finds the highest-scoring trees via dynamic programming.</p><p>For Chinese-English translation, we train the dependency parser on Chinese Treebank 7.0 with Stanford dependencies, 4 using 50K random sentences as the training data and the remaining as the test data. The parser achieves 81.02% parsing accuracy (labeled attached score, LAS) on the test dataset. For English-Vietnamese translation, we train the dependency parser on English WSJ corpus, following the same data split as <ref type="bibr" target="#b14">Dozat and Manning (2016)</ref>, and obtaining a LAS of 93.84% on the test dataset. 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Speed Comparison</head><p>All our experiments are run on a single GPU NVIDIA TITAN Xp. We report the averaged one-epoch training time on the Chinese-English translation dataset (consuming all 125M sentence pairs) as follows:</p><p>Baseline 105 min SAWR 142 min Tree-RNN 498 min Tree-Linearization 137 min 4 https://nlp.stanford.edu/software/stanforddependencies.shtml 5 For simplicity, we use only words as inputs for both Chinese and English dependency parsing, avoiding the influences brought by other inputs, such as automatic POS tags.</p><p>The SAWR system spends averaged 142 minutes, 6 37 minutes slower than the baseline model. The Tree-Linearization spends averaged 137 minutes per epoch, which is the fastest syntax integration method. Our SAWR approach spends 5 more minutes than Tree-Linearization, appropriate 3.5% of the total spend time per epoch, which could be negligible. The Tree-RNN model spends 498 minutes per epoch, nearly four times slower than the baseline model. 7 According to the results, we can conclude that the Tree-RNN model is highly inefficient for encoding dependency syntax, whereas the SAWR and Tree-Linearization are almost as efficient as the baseline Seq2Seq system. <ref type="table" target="#tab_1">Table 1</ref> shows the main results of all approaches on Chinese-English datasets. Considering the effect of random initialization, we train three individual models for each approach, and use the averaged BLEU scores for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Chinese-English Translation</head><p>According to the results, we can see that all syntax-integrated approaches can bring significant improvements over the baseline system, which denotes that syntax is highly effective for Chinese-English machine translation. In addition, the proposed SAWR approach obtains the largest BLEU improvements, averaged ∆ = 1.74 BLEU points better than the baseline system. The Tree-RNN and Tree-Linearization approaches bring improve- <ref type="bibr">6</ref> We exclude the time consumed by the encoder part of the dependency parsing model for fair comparisons, as other methods require to perform parsing in an offline way. <ref type="bibr">7</ref> The Tree-RNN model is implemented with deliberate batching motivated by , without which the model is intolerably slow, reaching about 1,900 minutes per epoch.  ments of averaged ∆ = 1.32 and ∆ = 1.23 BLEU points, respectively. The results show that our implicit syntax-aware encoding method is better than Tree-RNN and Tree-Linearization.</p><p>We compare our NMT models with other stateof-the-art methods as well. The results are just for reference since experimental details could be very different. In particular, we list the relative improvements over the corresponding baseline models by integrating syntax structures, which are calculated according to their papers. All these studies exploit lower baselines compared with our models. The Tree-RNN and Tree-Linearization are essentially similar to  and , respectively. As shown, our approaches can still obtain large improvements based on a stronger baseline. <ref type="table" target="#tab_3">Table 2</ref> shows the final results on the IWSLT 2015 English-Vietnamese translation task. The overall tendency is similar to that of Chinese-English translation. The syntax information can boost the translation performances by using any of the three approaches. The SAWR approach gives the best translation performance, significantly outperform the baseline system by ∆ = 0.80 BLEU points. While although the other two approaches bring better performances, the improvements are not significant. The results demonstrate the advantage of the proposed implicit SAWR approach. By not using the 1-best parser outputs, our approach can reduce the error propagation problem, thus bring larger improvements with syntax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">English-Vietnamese Translation</head><p>In particular, we find that the increases of BLEU scores are smaller than that of Chinese-English translation by integrating syntactic features. The averaged BLEU increases are 0.55 for English-Vietnamese and 1.43 for Chinese-English. The possible reason may be due to that the source English sentences are more grammatically rigorous Parser MT03 MT04 MT05 MT06 Average no <ref type="bibr">Tune 38.42 40.60 38.27 38.04 38.83</ref> Tune 37.33 39.45 36.93 37.03 37.69 <ref type="table">Table 3</ref>: The influence of fine-tuning parser parameters in the SAWR system. than Chinese sentences. For example, the English functional words such as "of" and "'s" which indicate the possessive relationship, should be always kept in sentences by standard, while their Chinese correspondence "的" may be omitted in sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>In this section, we conduct analysis on Chinese-English translation from different aspects to better understand the SAWR approach of integrating source-side dependency syntax for NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Fine-Tuning Syntax-Oriented Inputs</head><p>The SAWR approach directly uses the encoder outputs of a dependency parser as extra inputs for NMT. In the above experiments, we keep the parser model parameters fixed, letting them uninfluenced from NMT optimization. Actually, this part can be further fine tuned along with the NMT learning, by treating them as one kind model parameters. Thus there arises a question that whether fine-tuning the parser model parameters can bring better performance.</p><p>As an interesting attempt, we can simultaneously fine tune the parameters of both the parser and the Seq2Seq NMT model during training. <ref type="figure">Figure 3</ref> shows the results. We can see that fine-tuning decreases the average BLEU score by 38.83 − 37.69 = 1.14 significantly. This may be because that fine-tuning disorders the representation ability of the parser and makes its function more overlapping with other network components. This further demonstrates that pretrained syntax-aware word representations are helpful for NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Alignment Study</head><p>Alignment quality is an important metric to illustrate and evaluate machine translation outputs. Here we study how syntax features influence the alignment results for NMT. We approximate the alignment scores by the attention probabilities as shown in Equation <ref type="formula" target="#formula_3">4</ref>   <ref type="figure">Figure 3</ref>: Alignments for the baseline and syntaxintegrated systems, where the same example in <ref type="figure">Figure  1</ref> is analyzed and the target English word is "of".</p><p>the effectiveness of syntax, we choose the targetside English word "of" for comparison, which is a grammatical functional word. <ref type="figure">Figure 3</ref> shows the alignment probability distributions returned by different approaches. Intuitively, this word should be aligned with the Chinese word "的(de)". But according to the results, we can see that only the SAWR model distributes a high attention score to it, which is consistent with our intuition. The other three models are all aligned to the source word "现代 (modern)" with high confidence over 85%. The possible reason for "of" being aligned to "现代 (modern)" could be due to that "of modern" is a high-frequency collocation in the training corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Ensemble Study</head><p>Here we perform model ensembles to examine the divergences of the three syntax-integration approaches <ref type="bibr" target="#b13">Denkowski and Neubig, 2017)</ref>.</p><p>Intuitively, the heteroapproach ensemble which combines three NMT models of different methods should obtain better performances than homo-approach ensembles which combine three NMT models of the same method, since NMT models of different syntaxintegrations approaches have larger divergences. <ref type="table" target="#tab_5">Table 4</ref> shows the results. First, we can see that ensemble is one effective technique to improve the translation performances. More impor- tantly, the results show that the heterogeneous ensemble achieves averaged BLEU improvements by 43.10 − 41.24 = 1.86 points, better than the gains achieved by all three homo-approach ensembles, denoting that the three approaches could be mutually complementary in representing dependency syntax, and the resulting models of the three approaches are highly diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Analysis by Source Sentence Length</head><p>Intuitively, by introducing the source syntax into the NMT model, relations between long-distance words are explicitly modeled by dependency trees, thus we can expect that models enhanced by source syntax are able to bring better translations for longer sentences. <ref type="figure" target="#fig_1">Figure 4</ref> shows the performances of the baseline and all syntax-enriched models in terms of source sentence lengths, where we bin all the MT03-MT06 sentences by their lengths into six intervals. The results show that the BLEU scores are improved significantly when source sentential lengths are over 10, which confirms our intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Effect of Parsing Performance</head><p>Finally, we examine how the performance of the dependency parser influences the final translation quality. While the full dependency parser is   trained on 50K sentences, we retrain three weaker dependency parsers on 30K, 10K and 5K sentences, respectively. <ref type="figure" target="#fig_2">Figure 5</ref> shows the NMT BLEU scores and the parsing accuracies. It is clear that the parsing accuracy directly influences the translation quality, indicating the effectiveness and importance of exploiting syntactic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.6">Transformer as Baseline</head><p>Here we conduct experiments based on the transformer NMT model <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>, which is a stronger baseline, to further verify the effectiveness of our proposed method. This also demonstrates that the proposed SAWR method does not limit to a certain NMT baseline. Concretely, we extend the bottom word representations by incorporating syntactic encodings s=s 1 · · · s n (shown in Equation 5) into them, and then feed them into the transformer encoder by a linear projection layer to align with the input dimension. We implement Tree-RNN and Tree-Linearization for Transformer in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. <ref type="table" target="#tab_7">Table 5</ref> shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give positive influences based on the transformer. The SAWR approach can also outperform the baseline system significantly. Particularly, we find that our SAWR approach is much more effective than the Tree-RNN and Tree-Linearization approaches. The results further demonstrate the effectiveness of SAWRs in syntax integration for NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>tactic tags. The method is as effective as Tree-RNN approaches yet more effective. Noticeably, all these studies focus on constituent trees.</p><p>There have been several studies for NMT using dependency syntax. <ref type="bibr" target="#b16">Hashimoto and Tsuruoka (2017)</ref> propose to combine the head information with sequential words together as source encoder inputs, where their input trees are latent dependency graphs. Recently, there are several studies by using convolutional neural structures to represent source dependency trees, where tree nodes are modeled individually <ref type="bibr" target="#b8">(Chen et al., 2017b;</ref><ref type="bibr" target="#b6">Bastings et al., 2017)</ref>. <ref type="bibr" target="#b45">Wu et al. (2017b)</ref> build a syntax enhanced encoder by multiple Bi-RNNs over several different word sequences based on different traversing orders over dependency trees, i.e., the original sequential order and several tree-based orders. All these methods require certain extra efforts to encode the source dependency syntax over a baseline Seq2Seq NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a novel syntax integration method, SAWR, to incorporate source dependency-based syntax for NMT. It encodes dependency syntax implicitly, not requiring discrete syntax trees as inputs. Experiments showed that the method can bring significantly better performances for both Chinese-English and English-Vietnamese translation tasks. In addition, we compared the method with two approaches based on Tree-RNN and Tree-Linearization, which has been previously exploited for syntax integration, finding that our method is more effective and meanwhile very efficient. We conducted several experimental analyses to study our proposed methods deeper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The framework of the SAWR approach, where the left part shows the encoder-decoder of a supervised dependency parsing model and the right part shows the NMT encoder-decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>(0,10] (11,20] (21,30] (31,40] (41,50] The effect of source input length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The effect of dependency parsing performances on our proposed approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>An example of input dependency tree.</figDesc><table><row><cell>root</cell><cell></cell><cell></cell></row><row><cell></cell><cell>attr</cell><cell></cell></row><row><cell></cell><cell></cell><cell>assmod</cell></row><row><cell>top</cell><cell>amod</cell><cell>assm</cell></row><row><cell cols="3">教育 Education is modern civilization 's cornerstone 是 现代 文明 的 基石</cell></row><row><cell>• Input</cell><cell>Encoder</cell><cell>Decoder</cell></row><row><cell>教育</cell><cell>o 1</cell><cell>head=1, top</cell></row><row><cell>是</cell><cell>o 2</cell><cell>head=0, root</cell></row><row><cell>现代</cell><cell>o 3</cell><cell>head=4, amod</cell></row><row><cell>文明</cell><cell>o 4</cell><cell>head=6, assmod</cell></row><row><cell>的</cell><cell>o 5</cell><cell>head=4, assm</cell></row><row><cell>基石</cell><cell>o 6</cell><cell>head=2, attr</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>39.35 36.26 36.32 37.09 SAWR 38.42 40.60 38.27 38.04 38.83/+1.74 Tree-RNN 38.12 40.35 37.86 37.32 38.41/+1.32 Tree-Linearization 37.95 40.24 37.64 37.44 38.32/+1.23 Previous Work Chen et al. (2017a) 35.64 36.63 34.35 30.57 34.30/+2.59Chen et al. (2017b) 35.91 38.73 34.18 33.76 35.65/+1.52    Final results of Chinese-English translation. All syntax-integrated approaches are significantly better than the baseline system (p &lt; 0.05).</figDesc><table><row><cell>System</cell><cell cols="4">MT03 MT04 MT05 MT06 Average/∆</cell></row><row><cell cols="2">Baseline 36.44 Li et al. (2017) 34.9</cell><cell>38.6</cell><cell>35.5</cell><cell>35.6 36.15/+1.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Final Results on the IWSLT 2015 English-Vietnamese translation task. Only SAWR is significantly better than the baseline system (p &lt; 0.05).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>SAWR×3 41.94 44.59 41.91 41.97 42.60/+1.36 Tree-RNN×3 42.03 44.15 41.50 41.41 42.27/+1.03 Tree-Linearization×3 41.74 44.23 41.32 41.44 42.18/+0.94 Hybrid 42.72 45.14 42.38 42.15 43.10/+1.86</figDesc><table><row><cell>System</cell><cell cols="2">MT03 MT04 MT05 MT06 Average/∆</cell></row><row><cell>Baseline×3</cell><cell>40.90 43.25 40.64 40.16</cell><cell>41.24</cell></row><row><cell></cell><cell></cell><cell>. 8 For better understanding</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ensemble performances, where the Hybrid model denotes SAWR + Tree-RNN + Tree-Linearization.</figDesc><table><row><cell>System</cell><cell>...</cell><cell>现 现 现代 代 代 (modern)</cell><cell>...</cell><cell>的 的 的 ('s)</cell><cell>...</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAWR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tree-RNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tree-Linearization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>SAWR 41.63 43.60 41.68 40.21 41.78/+1.04 Tree-RNN 41.24 43.38 41.04 40.02 41.42/+0.68 Tree-Linearization 41.12 43.02 41.04 39.86 41.26/+0.52</figDesc><table><row><cell>System</cell><cell cols="2">MT03 MT04 MT05 MT06 Average/∆</cell></row><row><cell>Transformer</cell><cell>40.45 42.76 40.09 39.67</cell><cell>40.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Final results based on the transformer. Only the SAWR results are significantly better (p &lt; 0.05).</figDesc><table><row><cell></cell><cell>50K(81.02)</cell><cell>30K(79.43)</cell><cell>10K(73.69)</cell><cell>5K(70.61)</cell></row><row><cell></cell><cell>39</cell><cell></cell><cell></cell></row><row><cell>BLEU</cell><cell>38</cell><cell></cell><cell></cell></row><row><cell></cell><cell>37</cell><cell></cell><cell></cell></row><row><cell></cell><cell>36</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SAWR</cell><cell>Tree-RNN</cell><cell cols="2">Tree-Linearization Baseline</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 2 https://nlp.stanford.edu/projects/nmt/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.statmt.org/moses</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We aim to offer an intuitive interpretation by a carefullyselected example. In fact, the alignment computation method here may be problematic<ref type="bibr" target="#b20">(Koehn and Knowles, 2017)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all anonymous reviewers for their valuable comments. We thank Huadong Chen, Haoran Wei and Zaixiang Zheng for their help in implementing baseline neural machine translation models. This work is supported by National Natural Science Foundation of China (NSFC) grants 61525205, U1836222, and 61672211.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">and phrases, syntax trees been demonstrated helpful in SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Marton and Resnik</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cowan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees</title>
		<imprint>
			<publisher>Teng and Zhang</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Potamianos ;</forename><surname>Bansal ; Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>which are capable of representing the entire trees globally</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words and syn-References Roee Aharoni and Yoav Goldberg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Lem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Since Tree-RNN suffers serious inefficiency prob</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="132" to="140" />
		</imprint>
	</monogr>
	<note>Proceedings of the 55th ACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with a syntax-aware encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1936" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural machine translation with source dependency representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2846" to="2852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards robust neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8</title>
		<meeting>SSST-8</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A discriminative model for tree-to-tree translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kucerová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stronger baselines for trustable results in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First Workshop on Neural Machine Translation (NMT)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural machine translation with source-side latent graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for wmt&apos;15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SMT</title>
		<meeting>SMT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<meeting>EMNLP 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structural attention neural networks for improved sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippos</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling syntactic and semantic structures in hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="540" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT 2015</title>
		<meeting>IWSLT 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="76" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Soft syntactic constraints for hierarchical phrased-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Baskaran Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On-the-fly operation batching in dynamic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Does string-based neural mt learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Data-driven, pcfg-based and pseudo-pcfg-based models for chinese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="301" to="314" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bidirectional tree-structured lstm with head lexicalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Syntax-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="208" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence-to-dependency neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with source syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2017</title>
		<meeting>IJCAI 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4179" to="4185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A novel dependency-to-string model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="216" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards bidirectional hierarchical representations for attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on EMNLP</title>
		<meeting>the 2017 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1432" to="1441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Top-down tree long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="310" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Modeling past and future for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09502</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Chunk-based bi-scale decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th ACL</title>
		<meeting>the 55th ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="580" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural system combination for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="378" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
