<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scene Text Detection via Holistic, Multi-Channel Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology (HUST)</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology (HUST)</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
							<email>nsang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology (HUST)</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scene Text Detection via Holistic, Multi-Channel Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene text detection</term>
					<term>fully convolutional network</term>
					<term>holistic prediction</term>
					<term>natural images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, scene text detection has become an active research topic in computer vision and document analysis, because of its great importance and significant challenge. However, vast majority of the existing methods detect text within local regions, typically through extracting character, word or line level candidates followed by candidate aggregation and false positive elimination, which potentially exclude the effect of wide-scope and long-range contextual cues in the scene. To take full advantage of the rich information available in the whole natural image, we propose to localize text in a holistic manner, by casting scene text detection as a semantic segmentation problem. The proposed algorithm directly runs on full images and produces global, pixel-wise prediction maps, in which detections are subsequently formed. To better make use of the properties of text, three types of information regarding text region, individual characters and their relationship are estimated, with a single Fully Convolutional Network (FCN) model. With such predictions of text properties, the proposed algorithm can simultaneously handle horizontal, multi-oriented and curved text in real-world natural images. The experiments on standard benchmarks, including ICDAR 2013, ICDAR 2015 and MSRA-TD500, demonstrate that the proposed algorithm substantially outperforms previous state-ofthe-art approaches. Moreover, we report the first baseline result on the recently-released, large-scale dataset COCO-Text.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Textual information in natural scenes can be very valuable and beneficial in a variety of real-world applications, ranging from image search <ref type="bibr" target="#b45">[46]</ref>, human-computer interaction <ref type="bibr" target="#b23">[24]</ref>, to criminal investigation <ref type="bibr" target="#b0">[1]</ref> and assistance for the blind <ref type="bibr" target="#b56">[57]</ref>. In the past few years, scene text detection and recognition have received a lot of attention from both the computer vision community and document analysis community, and numerous inspiring ideas and methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b14">[15]</ref> have been proposed to tackle these problems.</p><p>However, localizing and reading text in uncontrolled environments (i.e., in the wild) are still overwhelmingly challenging, due to a number of factors, such as variabilities in text appearance, layout, font, language and style, as well as interferences from background clutter, noise, blur, occlusion, and non-uniform illumination. In this paper, we focus on the problem of scene text detection, which aims at predicting the presence of text, and if any, estimating the position and extent of each instance.</p><p>Previous methods mainly seek text instances (characters, words or text lines) in local regions, with sliding-window <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b15">[16]</ref> or connected component extraction <ref type="bibr" target="#b8">[9]</ref>, <ref type="figure">Fig. 1</ref>. Text regions predicted by the proposed text detection algorithm. In this work, scene text detection is casted as a semantic segmentation problem, which is conceptionally and functionally different from previous sliding-window or connected component based approaches. <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b35">[36]</ref> techniques. These algorithms have brought novel ideas into this field and constantly advanced the stateof-the-art. However, most of the existing algorithms spot text within local regions (up to text line level), making it nearly impossible to exploit context in wider scope, which can be critical for dealing with challenging situations. Consequently, they would struggle in hunting weak text instances and suppressing difficult false positives.</p><p>Moreover, almost all the previous methods (except for <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b58">[59]</ref>) have focused on detecting horizontal or nearhorizontal texts, overlooking non-horizontal ones. This largely limits the practicability and adaptability of these methods, since crucial information in regard to the scene might be embodied in such non-horizontal texts.</p><p>We propose in this work a novel algorithm for scene text detection, which treats text detection as semantic segmentation problem <ref type="bibr" target="#b42">[43]</ref>. This algorithm performs holistic, per-pixel estimation and produces dense maps, in which the properties of scene text are implied, as shown in <ref type="figure">Fig. 1</ref>. Nevertheless, simple two-class (text vs. non-text) semantic segmentation is not sufficient for scene text detection, since multiple text instances that are very close to each other may make it hard to separate each instance (see <ref type="figure" target="#fig_0">Fig. 2</ref>). We tackle this issue by taking into account the center and scale of individual characters as well as the linking orientation of nearby characters, in addition to the location of text regions.</p><p>When multiple text lines flock together and their orientations are unknown in advance, it is not trivial to identify and group each text line. Previous methods either simply assume that text lines are horizontal or near-horizontal, or use heuristics to perform text line grouping. In this paper, we propose to build a graph using predicted properties of characters (location, scale, linking orientation, etc) and form  <ref type="table" target="#tab_0">An issue caused by simple two-class semantic segmentation. When  several text lines are very close, the estimated text regions may stick together,  making it difficult to identify each individual text line.</ref> text lines with graph partition <ref type="bibr" target="#b2">[3]</ref>.</p><p>The proposed strategy is realized using the FCN framework <ref type="bibr" target="#b27">[28]</ref>, which was originally designed for semantic segmentation. This framework is chosen because it applies multiscale learning and prediction, conforming to the multi-scale nature of text in natural scenes, and includes rich prior information of natural images, by pretraining on large volume of real-world data (ImageNet <ref type="bibr" target="#b7">[8]</ref>). Building upon FCN, the proposed method is able to effectively detect text instances with high variability while coping with hard false alarms in real-world scenarios.</p><p>To evaluate the effectiveness and advantages of the proposed algorithm, we have conducted experiments on public datasets in this area, including ICDAR 2013 <ref type="bibr" target="#b21">[22]</ref>, ICDAR 2015 <ref type="bibr" target="#b20">[21]</ref> and MSRA-TD500 <ref type="bibr" target="#b53">[54]</ref>. The quantitative results demonstrate that the proposed method significantly outperforms previous state-of-the-art algorithms. Specifically, this is the first work 1 that reports quantitative performance on the large-scale benchmark COCO-Text <ref type="bibr" target="#b47">[48]</ref>, which is much larger (63,686 natural images) and exhibits far more variability and complexity than previous datasets.</p><p>In summary, the contributions of this paper are four-folds:</p><p>(1) We cast scene text detection as a semantic segmentation problem and make holistic prediction in the detection procedure, in contrast to previous approaches which mainly make decision locally and thus cannot make full use of the contextual information in the whole image.</p><p>(2) This work simultaneously predicts the probability of text regions, characters and the relationship among adjacent characters in a unified framework, excavating more properties of scene text and endowing the system with the ability to detect multi-oriented and curved text.</p><p>(3) The algorithm substantially outperforms the prior arts on standard benchmarks in this field.</p><p>The rest of the paper is structured as follows: We review previous ideas and approaches in Sec. II. The main idea and details of the proposed algorithm are explained in Sec III and the experiments and comparisons are presented Sec. IV. Conclusion remarks are given in Sec V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Scene text detection and recognition have been extensively studied for a few years in the computer vision community and document analysis community, and plenty of excellent works and effective strategies have been proposed <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Comprehensive and detailed reviews can be found in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b54">[55]</ref>. In this section, we will concentrate on text detection approaches that are most related to the proposed method.</p><p>Stroke Width Transform (SWT) <ref type="bibr" target="#b8">[9]</ref>, Maximally Stable Extremal Regions (MSER) <ref type="bibr" target="#b31">[32]</ref> as well as their variants <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b59">[60]</ref> have been the mainstream in scene text detection. These methods generally hunt character candidates via edge detection or extreme region extraction. Different from such component-based approaches, Neumann and Matas <ref type="bibr" target="#b34">[35]</ref> proposed to seek character strokes in a multi-scale slidingwindow manner. Zhang et al. <ref type="bibr" target="#b61">[62]</ref> presented a text detector that makes use of the local symmetry property of character groups. The work of Jaderberg et al. <ref type="bibr" target="#b14">[15]</ref> adopted object proposal and regression techniques to spot words in natural images, drawing inspiration from R-CNN <ref type="bibr" target="#b9">[10]</ref>. However, a common issue with these methods is that they all only use cues from local regions (up to text line level) in text detection. In contrast, the proposed algorithm makes decision in a much wider scope (up to whole image), and thus is able to take advantage of both short-range and long-range information in the images, which can be very useful in suppressing false alarms in complex scenes.</p><p>Most of the previous methods have focused on horizontal or near-horizontal text, with very few exceptions <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b58">[59]</ref>. However, text in real-world situations can be in any orientation. The ignorance of non-horizontal text can be a severe drawback, since important information regarding the scene may be embodied in non-horizontal text. The proposed algorithm, directly inferring the orientation property of each text instance, can naturally and effortlessly deal with text of arbitrary directions.</p><p>In recent years, deep learning based methods for scene text detection <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref> have been very popular, due to their advantage in performance over conventional strategies <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b10">[11]</ref>. The approach proposed in this paper also utilizes deep convolutional neural networks to detect text with high variability and to eliminate false positives caused by complex background. The main difference lies in that the proposed approach works in a holistic fashion and produces global, pixel-wise prediction maps, while the other deep learning based methods essentially perform classification on local regions (image patches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref> or proposals generated by other techniques <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>).</p><p>This work is mainly inspired by the Holistically-Nested Edge Detection (HED) method proposed by Xie and Tu <ref type="bibr" target="#b51">[52]</ref>. HED is an edge detection algorithm that adopts multi-scale and multi-level feature learning and performs holistic prediction. HED leverages the power of both Fully Convolutional Networks (FCN) <ref type="bibr" target="#b27">[28]</ref> and Deeply-Supervised Nets (DSN), and obtains significantly enhanced performance on edge detection in natural images. There are mainly three reasons that we adopt HED as the base model of our text detection algorithm: (1) Text is highly correlated with edge, as pointed out in previous works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b4">[5]</ref>. (2) HED makes holistic, imageto-image prediction, which allows for the use of wide-scope and long-range contextual information to effectively suppress false positives. (3) HED can directly handle edges of different scales and orientations, which fit well the multi-scale and multi-orientation nature of text in scene images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we will present the main idea, network architecture and details of the proposed method. Specifically, the pipeline for scene text detection is described in Sec. III-B, after an overview of the whole system (Sec III-A). The network architecture as well as the notation and formulation of the system are given in Sec. III-C and Sec. III-D, respectively. s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Generally, the proposed algorithm follows the paradigm of the FCN <ref type="bibr" target="#b27">[28]</ref> and HED framework <ref type="bibr" target="#b51">[52]</ref>, i.e., it infers properties of scene text in a holistic fashion by producing image-level, pixel-wise prediction maps. In this paper, we consider text regions (words or text lines), individual characters and their relationships (linking orientation between characters) as the three properties to be estimated at runtime, since these properties are effective for scene text detection and. Moreover, the ground truth of these properties can be easily obtained from standard benchmark datasets. After the pixel-wise prediction maps are generated, detections of scene text are formed by aggregating cues in these maps. As this idea is general, other useful properties regarding scene text (for instance, binary mask of character strokes and script type of text) can be readily introduced into this framework, which are expected to further improve the accuracy of scene text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Text Detection Pipeline</head><p>Unlike previous methods for scene text detection, which usually start from extracting connected components <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b10">[11]</ref> or scanning multi-scale windows <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b15">[16]</ref> within images, the proposed algorithm operates in an alternative way. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the pipeline is quite straightforward: The original image is fed into the trained model and three prediction maps, corresponding to text regions, characters and linking orientations of adjacent characters, are produced. Detections are formed by performing segmentation, aggregation and partition on the three maps. If required, word partition, the process of splitting text lines into individual words, is applied to the previously formed detections. </p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g) (h) (i)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Ground Truth Preparation:</head><p>Since the problem of scene text detection has been converted to semantic segmentation and the base framework is FCN/HED, the ground truth should be converted to label maps that are compatible with FCN/HED. As depicted in <ref type="figure" target="#fig_1">Fig. 4</ref>, for each image three label maps are produced from the ground truth annotations. The label map for text regions is a binary map, in which foreground pixels (i.e., those within text regions) are marked as '1' and background pixels '0'. The label map for characters is a binary map, in which foreground pixels (i.e., those within character regions) are marked as '1' and background pixels '0'. To avoid the situation that the characters in the prediction map stuck together at runtime, the binary masks are shrunk to half of its original size in both dimensions.</p><p>The label map for linking orientations is a soft map, in which each foreground pixel is assigned with to a value within the range of [0, 1]. The orientations of the foreground pixels is assigned as the orientation of the corresponding ground truth polygons of text regions. In this work, we consider linking orientation θ in the range of [−π/2, π/2]. Linking orientations beyond this range is converted to it. All linking orientations are mapped to [0, 1] by shifting and normalization.</p><p>2) Model Training: The prediction model is trained by feeding the training images and the corresponding ground truth maps into the network, which will be described in detail in Sec. III-C. The training procedure generally follows that of the HED method <ref type="bibr" target="#b51">[52]</ref> and the differences will be explained in Sec. III-D1.</p><p>3) Prediction Map Generation: With the trained model, maps that represent the information of text regions, characters and linking orientations are generated by inputting the original image into it. For details, see Sec. III-D2. Exemplar prediction maps generated by the trained model are demonstrated in <ref type="figure" target="#fig_3">Fig. 3</ref> (b) and <ref type="figure" target="#fig_2">Fig. 5 (b)</ref>, (c) and (d). Note that only the linking orientations within foreground regions are valid, so those in background regions (probability of being text region less than 0.5) are not shown. </p><formula xml:id="formula_1">(b) (c) (a)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Detection Formation:</head><p>Candidates of text regions and characters are hunted by segmenting the corresponding prediction map with adaptive thresholding (see <ref type="figure" target="#fig_2">Fig. 5</ref> (e) and (f)). Note that since the scales of characters are shrunk to half of their original scales in the training phase, the estimated radii of the character candidates are multiplied with a factor of 2.</p><p>Then, character candidates within the same text regions are grouped into cliques and represented as graphs. Given a clique of characters U = u i , i = 1, · · · , m, where m is the character count, Delaunay triangulation <ref type="bibr" target="#b19">[20]</ref> is applied to these characters. Delaunay triangulation provides an effective way to eliminate unnecessary linkings of distant characters. The triangulation T is used to construct a graph G = (U, E), in which the vertexes are the characters U and the edges E model the similarities between pairs of characters.</p><p>The weight w of edge e = (u i , u j ) is defined as:</p><formula xml:id="formula_2">w = s(i, j), if e ∈ T 0, otherwise ,<label>(1)</label></formula><p>where s(i, j) is the similarity between the pair of characters u i and u j . According to Eqn. 1, the weights of the linkings that not belonging to the triangulation are set to zero.</p><p>The similarity s(i, j) between u i and u j is the harmonic average of the spatial similarity and orientation similarity:</p><formula xml:id="formula_3">s(i, j) = 2a(i, j)o(i, j) a(i, j) + o(i, j) ,<label>(2)</label></formula><p>where a(i, j) and o(i, j) denote the spatial similarity and orientation similarity between u i and u j .</p><p>The spatial similarity a(i, j) defined as:</p><formula xml:id="formula_4">a(i, j) = exp(− d 2 (i, j) 2D 2 ),<label>(3)</label></formula><p>where d(i, j) is the distance of the centers of u i and u j , while D is the average length of all the edges in the Delaunay triangulation T .</p><p>The orientation similarity o(i, j) defined as:</p><formula xml:id="formula_5">o(i, j) = cos(Λ(φ(i, j) − ψ(i, j))),<label>(4)</label></formula><p>where φ(i, j) is the orientation of the line between the centers of u i and u j , while ψ(i, j) is the average value in the area between u i and u j in the linking orientation map (see 5 (d)). Λ is an operator for computing the included angle (acute angle) of two orientations. This definition of orientation similarity rewards pairs that are in accordance with the predicted linking orientations while punishes those that violate such prediction.</p><p>To split groups of character candidates into text lines, a simple yet effective strategy proposed by Yin et al. <ref type="bibr" target="#b57">[58]</ref> is adopted. Based on the graph G = (U, E), a Maximum Spanning Tree <ref type="bibr" target="#b38">[39]</ref>, M , is constructed. Selecting and eliminating edges in M is actually performing graph partition on G, leading to segmentation of text lines. For example, eliminating any edge in M will partition it into two parts, corresponding two text lines, while eliminating two edges will result in three text lines.</p><p>Since the number of text lines is unknown, it should be inferred in the procedure of text line segmentation. Under the assumption that text lines in real-world scenarios are in linear or near-linear form, a straightness measure function is defined:</p><formula xml:id="formula_6">S vm = K i=1 λ i1 λ i2 ,<label>(5)</label></formula><p>where K is the number of clusters (text lines), λ i1 and λ i2 are the largest and second largest eigenvalues of the covariance matrix C i . C i is computed using the coordinates of the centers of characters the in the ith cluster. The optimal segmentation of text lines is achieved, when the value of function S vm reaches its maximum.</p><p>However, this solution is not applicable to text lines with curved shapes (for example, those in <ref type="figure" target="#fig_6">Fig. 9</ref>), which they violate the linearity assumption. To tackle this problem, a threshold τ is introduced in this work. While performing text line segmentation, the edges with weight greater than τ will not be selected or eliminated. Since the proposed method is able to predict linking orientations of characters in straight text lines (see <ref type="figure" target="#fig_3">Fig. 3</ref>) as well as in curved text lines (see <ref type="figure" target="#fig_6">Fig. 9</ref>), this strategy works well on text lines with linear shapes and curved shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Post-Processing:</head><p>In certain tasks, such as ICDAR 2013 <ref type="bibr" target="#b21">[22]</ref> and ICDAR 2015 <ref type="bibr" target="#b20">[21]</ref>, word partition is required, since text in images from these datasets is labelled in word level. We adopted the word partition method in <ref type="bibr" target="#b8">[9]</ref>, as it has proven to be simple and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architecture</head><p>As shown in <ref type="figure">Fig. 6</ref>, the architecture used in this work is based on that of HED <ref type="bibr" target="#b51">[52]</ref>, which made surgery on the pretrained 5-stage VGG-16 Net model <ref type="bibr" target="#b43">[44]</ref>. Necessary modifications are applied to the architecture of HED, to accomplish the task of text detection. Specifically, three side-output layers, corresponding to text region, character and linking orientation predictions, are connected to the last convolutional layer of  <ref type="figure">Fig. 6</ref>. Network architecture of the proposed algorithm. The base network is inherited from HED <ref type="bibr" target="#b51">[52]</ref>, which made surgery on the pretrained VGG-16 Net model <ref type="bibr" target="#b43">[44]</ref>. each stage. This means that conv1 2, conv2 2, conv3 3, conv4 3 and conv5 3 in the network have side-output layers. The side-output layer for text region prediction, character prediction and that for linking orientation prediction are learned independently. Note that from stage 2 to 5, upsampling operation, which is realized by in-network deconvolution, is needed to rescale the prediction maps to the same size of the original image. The outputs of the side layers from the 5 stages are fused to form overall predictions, which is accomplished by another convolutional layer, the same as in <ref type="bibr" target="#b51">[52]</ref>. The loss of each stage is computed as described in Sec. III-D1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Formulation</head><p>The notation and formulation of the proposed method follow those of <ref type="bibr" target="#b51">[52]</ref>. Here we will keep consistent with <ref type="bibr" target="#b51">[52]</ref> and emphasis on the modifications and differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Training Phase:</head><p>Assume the training set is S = {(X n , Y n ), n = 1, · · · , N }, where N is the size of the training set, X n is the nth original image and Y n is the corresponding ground truth. Different from HED, in which the ground truth Y n is a binary edge map, Y n in this work is composed of three maps, i.e., Y n = {R n , C n , Θ n }, where R n = {r (n) j ∈ {0, 1}, j = 1, · · · , |R n |} is a binary map that indicates the presence of text regions at each pixel in the original image X n , C n = {c (n) j ∈ {0, 1}, j = 1, · · · , |C n |} is a binary map indicates the presence of characters (shrunk version are used in this work), and Θ n = {θ (n) j ∈ [0, 1], j = 1, · · · , |Θ n |} is a soft map that represents the linking orientations between adjacent characters in text regions. Note that the value of θj (n) is valid only if r (n) j = 1, since linking orientation of adjacent characters is undefined in background. For simplicity, the index n will be omitted hereafter, because the model treats each image independently.</p><p>The objective function is defined as the loss in the fused outputs, since the losses in lower side-output layers make little difference according to experiments:</p><formula xml:id="formula_7">L = L fuse (W, w),<label>(6)</label></formula><p>where L fuse (W, w, h) is the loss function of the fused outputs. W and w are the collection of parameters of all standard layers and those with the side-output layers and fuse layer, respectively. For more details, refer to <ref type="bibr" target="#b51">[52]</ref>.</p><p>In HED <ref type="bibr" target="#b51">[52]</ref>, each stage produces only one prediction map for edges. In this paper, however, each stage is used to generate three prediction maps: one for text regions, one for characters and one for linking orientations. Therefore, each stage is connected to two side-output layers, instead of one. Accordingly, the definition of the loss function is different. L fuse (W, w) is a weighted sum of the corresponding channels for the three types of targets:</p><formula xml:id="formula_8">L fuse (W, w) = λ 1 ∆ r (W, w)+ λ 2 ∆ c (W, w) + λ 3 ∆ o (W, w),<label>(7)</label></formula><p>where ∆ r (W, w), ∆ c (W, w) and ∆ o (W, w) are the losses in predicting text regions, characters and linking orientations, respectively. λ 1 , λ 2 and λ 3 are parameters to control the relative contributions of these three types of loss functions and λ 1 +λ 2 +λ 3 = 1.</p><p>For an image, assume the ground truth is Y = {R, C, Θ} and the prediction maps produced by the fuse layer areR,Ĉ andΘ, the loss function for text regions ∆ r (W, w) is similar with Eqn. (2) in <ref type="bibr" target="#b51">[52]</ref>:</p><formula xml:id="formula_9">∆ r (R, R; W, w) = − β |R| j=1 R j log P r(R j = 1; W, w) − (1 − β) |R| j=1 (1 − R j ) log P r(R j = 0; W, w). (8)</formula><p>β is a class-balancing parameter and it is defined as β = |R−| |R| . |R − | denotes the count of pixels in non-text regions and |R| denotes the total count of pixels. The definition of ∆ c is similar with ∆ r and thus is skipped here.</p><p>∆ o (W, w) is defined as:</p><formula xml:id="formula_10">∆ o (Θ, Θ; R, W, w) = |R| j=1 R j (sin(π|Θ j − Θ j |)). (9)</formula><p>This loss function has a double-peak shape. When the difference (included angle) between the estimated orientation and true orientation is small, the loss is close to 0; when it is near π/2 (or −π/2), the loss approaches to 1; when it goes beyond π/2 (or below −π/2), the loss deceases since the difference turns around. According to the definition in Eqn. 9, orientation errors at background pixels will not be taken into account.</p><p>2) Testing Phase: In the testing phase, the test image I is mean-subtracted and fed to the trained model. The prediction maps for text regions, characters and linking orientations are obtained by taking the output of the fusion layer:</p><formula xml:id="formula_11">(R,Ĉ,Θ) =Ŷ fuse<label>(10)</label></formula><p>Different from HED, which used the average of the outputs of all the output-layers and the fusion layers, in this work we only employ the responses of the fusion layer. In practice, we found that the outputs of the side-output layers are likely to introduce noises and unimportant details into the prediction maps, which may be harmful to the task of scene text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND DISCUSSIONS</head><p>We implemented the proposed algorithm using the code 2 released by the authors of HED <ref type="bibr" target="#b51">[52]</ref>, which is based on the Caffe framework <ref type="bibr" target="#b17">[18]</ref>. The proposed algorithm was evaluated on three standard benchmarks in this field and compared with other scene text detection methods. All the experiments were conducted on a regular server (2.6GHz 8-core CPU, 128G RAM, Tesla K40m GPU and Linux 64-bit OS) and the routine ran on a single GPU in each time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The datasets used in the experiments will be introduced briefly:</p><p>ICDAR 2013. The ICDAR 2013 dataset 3 is from the ICDAR 2013 Robust Reading Competition <ref type="bibr" target="#b21">[22]</ref>. There are 229 natural images for training and 233 natural images for testing. All the text instances in this dataset are in English and are horizontally placed.</p><p>ICDAR 2015. The ICDAR 2015 dataset 4 is from the Challenge 4 (Incidental Scene Text challenge) of the ICDAR 2015 Robust Reading Competition <ref type="bibr" target="#b20">[21]</ref>. The dataset includes 1500 natural images in total, which are acquired using Google Glass. Different from the images from the previous ICDAR competitions <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b21">[22]</ref>, in which the text instances are well positioned and focused, the images from ICDAR 2015 are taken without user's prior preference or intention, so the text instances are usually skewed or blurred.</p><p>MSRA-TD500. The MSRA Text Detection 500 Database (MSRA-TD500) 5 is a benchmark dataset for assessing detection algorithms for text of different orientations, which is originally proposed in <ref type="bibr" target="#b53">[54]</ref>. This dataset has 500 highresolution natural scene images, in which text may be in varying directions and the language types include both Chinese and English. The training set consists of 300 images and the test set contains 200 images. This dataset is challenging due to the variability of text as well as the complexity of backgrounds.</p><p>COCO-Text. The COCO-Text 6 is a newly released, large scale dataset for text detection and recognition in natural images. The original images are from the Microsoft COCO <ref type="bibr" target="#b26">[27]</ref> dataset. In COCO-Text, 173,589 text instances from 63,686 images are annotated and each instance has 3 fine-grained text attributes. 43,686 images were chosen by the authors as training set, while 20,000 as validation/test set. This database is the largest benchmark in this area to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We directly made surgery on the network architecture of HED <ref type="bibr" target="#b51">[52]</ref> to construct our model for scene text detection, and most parameters were inherited from it. In our model, the learning rate is adjusted to 1e-8 to avoid gradient explosion. Following <ref type="bibr" target="#b51">[52]</ref>, we also used VGG-16 Net <ref type="bibr" target="#b43">[44]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>. λ 1 = λ 2 = λ 3 = 1 3 and τ = 0.8 are used in all the experiments.</p><p>The training data are the union of the training images from the three datasets ICDAR 2013 (229 training images), ICDAR 2015 (1000 training images) and MSRA-TD500 (300 training images). These 1529 training images were first rescaled to have maximum dimension of 960 pixels with aspect ratio kept unchanged. Then we evenly rotated the images to 36 different angles (with 10 degree angle interval). The corresponding ground truth maps were generated as described in Sec. III and undergone the same augmentations as the original images. We found that the framework is insensitive to image scale, in accordance with <ref type="bibr" target="#b51">[52]</ref>. Therefore, we did not resize the images (as well as the ground truth maps) to multiple scales. In testing, text detection is performed at multiple scales and the detections of different scales are fused to form the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments and Discussions</head><p>1) Qualitative Results: <ref type="figure" target="#fig_5">Fig. 7</ref> illustrates a group of detection examples of the proposed algorithm on the four benchmark datasets. As can be seen, the proposed algorithm is able to handle text instances of different orientations, languages, fonts, colors and scales in diverse scenarios. Moreover, it is insensitive to non-uniform illumination, blur, local distractor and connected strokes, to some extent. These examples show the adaptability and robustness of the proposed method.</p><p>2) Quantitative Results on ICDAR 2013: The text detection performance of the proposed algorithm as well as other methods evaluated on the ICDAR 2013 dataset are shown in Tab. I. The proposed method achieves the highest recall (0.8022) among all the methods. Specifically, the F-measure of the proposed algorithm is slightly better than that of previous state-of-the-art methods <ref type="bibr" target="#b12">[13]</ref>.</p><p>Note that on the ICDAR 2013 dataset the performance improvement of our algorithm over previous methods is not   <ref type="bibr" target="#b12">[13]</ref> 0.9218 0.7732 0.8410 Zhang et al. <ref type="bibr" target="#b62">[63]</ref> 0.88 0.78 0.83 Zhang et al. <ref type="bibr" target="#b61">[62]</ref> 0.88 0.74 0.80 Tian et al. <ref type="bibr" target="#b44">[45]</ref> 0.85 0.76 0.80 Lu et al. <ref type="bibr" target="#b28">[29]</ref> 0.89 0.70 0.78 iwrr2014 <ref type="bibr" target="#b60">[61]</ref> 0.86 0.70 0.77 USTB TexStar <ref type="bibr" target="#b59">[60]</ref> 0.88 0.66 0.76 Text Spotter <ref type="bibr" target="#b33">[34]</ref> 0.88 0.65 0.74 CASIA NLPR <ref type="bibr" target="#b21">[22]</ref> 0.79 0.68 0.73 Text Detector CASIA <ref type="bibr" target="#b41">[42]</ref> 0.85 0.63 0.72 I2R NUS FAR <ref type="bibr" target="#b21">[22]</ref> 0.75 0.69 0.72 I2R NUS <ref type="bibr" target="#b21">[22]</ref> 0.73 0.66 0.69 TH-TextLoc <ref type="bibr" target="#b21">[22]</ref> 0.70 0.65 0.67 as obvious as on the ICDAR 2015 dataset (see Tab. II) and MSRA-TD500 dataset (see Tab. III). There are two main reasons: <ref type="bibr" target="#b0">(1)</ref> The ICDAR 2013 dataset is much more time-honored (most images were from previous ICDAR competitions that can date back to 2003) and most text detection algorithms have saturated on it.</p><formula xml:id="formula_12">(a) (b) (c) (d)</formula><p>(2) The main advantage of our algorithm lies in the capacity of handling multi-oriented text, but vast majority of the text instances in ICDAR 2013 are horizontal. So the advantage of our algorithm cannot be reflected when using ICDAR 2013 as benchmark.</p><p>3) Quantitative Results on ICDAR 2015: The text detection performance of the proposed method as well as other competing methods on the ICDAR 2015 dataset are shown in Tab. II. The proposed method achieves the highest recall (0.5869) and the second highest precision (0.7226). Specifically, the  <ref type="bibr" target="#b62">[63]</ref> 0.71 0.43 0.54 Stradvision-2 <ref type="bibr" target="#b20">[21]</ref> 0.7746 0.3674 0.4984 Stradvision-1 <ref type="bibr" target="#b20">[21]</ref> 0.5339 0.4627 0.4957 NJU <ref type="bibr" target="#b20">[21]</ref> 0.7044 0.3625 0.4787 AJOU <ref type="bibr" target="#b24">[25]</ref> 0.4726 0.4694 0.471 HUST-MCLAB <ref type="bibr" target="#b20">[21]</ref> 0.44 0.3779 0.4066 Deep2Text-MO <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b58">[59]</ref> 0.4959 0.3211 0.3898 CNN MSER <ref type="bibr" target="#b20">[21]</ref> 0.3471 0.3442 0.3457 TextCatcher-2 <ref type="bibr" target="#b20">[21]</ref> 0.2491 0.3481 0.2904 F-measure of the proposed algorithm is significantly better than that of previous state-of-the-art <ref type="bibr" target="#b62">[63]</ref> (0.6477 vs. 0.54). This confirms the effectiveness and advantage of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Quantitative</head><p>Results on MSRA-TD500: The performances of different text detection methods on the MSRA-TD500 dataset <ref type="bibr" target="#b53">[54]</ref> are depicted in Tab. III. As can be seen, the proposed algorithm achieves the highest recall and F-measure and the second highest precision. Specially, the F-measure of the proposed algorithm significantly outperforms that of the prior art <ref type="bibr" target="#b62">[63]</ref> (0.7591 vs. 0.74). The improvement in recall is even more obvious (0.7531 vs. 0.63).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Quantitative Results on COCO-Text:</head><p>The performance of the proposed method on the COCO-Text dataset <ref type="bibr" target="#b47">[48]</ref> is depicted in Tab. IV 7 . On this large, challenging benchmark, the proposed algorithm achieves 0.4323, 0.271 and 0.3331 in precision, recall and F-measure, respectively. As reference, the performances of the baseline methods from <ref type="bibr" target="#b47">[48]</ref> are included. However, it is not possible to directly compare our method with these baselines, since they were used for labelling the ground truth in the data annotation procedure and they were evaluated on the full dataset (63686 image) instead of the validation set (20,000 images).</p><p>The performance of the proposed method on different subcategories in the COCO-Text dataset is depicted in Tab. V 8 . The proposed algorithm performs much better on legible, machine printed text than illegible, handwritten text. Likewise, the performances of the baseline methods from <ref type="bibr" target="#b47">[48]</ref> are also included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Running time of Proposed Method</head><p>On 640x480 images, it take the proposed method about 0.42s on average to produce the prediction maps, when running on a K40m GPU. The subsequent processing consumes about 0.2s on CPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Generalization Ability of Proposed Algorithm</head><p>As can be seen from <ref type="figure">Fig. 8</ref> and <ref type="figure" target="#fig_6">Fig. 9</ref>, although the proposed method was not trained with examples of various languages nor curved instances, it generalize well to such scenarios. This verifies the strong generalization ability of the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Limitations of Proposed Algorithm</head><p>The proposed method shows excellent capability in most scenarios, but it may fail in certain conditions. For example, it is found sensitive to serious blur and highlight. We believe if more examples and augmentation are introduced, these issues can be alleviated. Moreover, the model size of the proposed method (about 56M) is still not compact enough and the speed without GPU (more than 14s for 640x480 images) is not sufficient, if we port the system to low-end PCs or mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we have presented a novel algorithm for text detection in natural scene images, which is based on the HED model <ref type="bibr" target="#b51">[52]</ref>. The proposed algorithm is fundamentally different from majority of the previous methods in that: (1) It approaches scene text detection via whole image semantic segmentation, instead of local component extraction <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b35">[36]</ref> or window-based classification. <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and thus is able to utilize more contextual information from larger scope. (2) It concurrently predicts the probability of text regions, characters and the linking orientation of nearby characters in a unified framework, while other methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b58">[59]</ref> estimate such properties in separate stages. (3) It can directly spot multi-oriented and curved text from images, while most previous approaches <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref> have focused on horizontal or near-horizontal text. Moreover, the experiments on standard benchmark datasets demonstrate that the proposed method achieves superior performance than other competing systems.</p><p>Future directions worthy of further investigation and exploration might include: (1) Architecture. The 5-stage architecture of HED leads to breakthrough in edge detection, but this architecture is not necessarily the best for scene text detection. We would modify the network architecture and seek better ones that are suitable for this task. (2) Capacity. Training with more detailed labels (e.g., binary masks of character shapes) will endow the system with the ability to explicitly segment out character shapes from original images, which may potentially facilitate the subsequent text recognition procedure. (3) Efficiency. Adopting acceleration techniques <ref type="bibr" target="#b16">[17]</ref> could largely speed up the proposed method, making it more practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An issue caused by simple two-class semantic segmentation. When several text lines are very close, the estimated text regions may stick together, making it difficult to identify each individual text line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Ground truth preparation. (a) Original image. (b) Ground truth polygons of text regions. (c) Ground truth polygons of characters. (d) Ground truth map for text regions. (e) Ground truth map for characters. (f) Ground truth map for linking orientations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Detection formation. (a) Original image. (b) Prediction map of text regions. (c) Prediction map of characters. (d) Prediction map of linking orientations. (e) Text region (red rectangle). (f) Characters (green circles). The center and radius of the circles represent the location and scale of the corresponding characters. (g) Delaunay triangulation. (h) Graph partition. Blue lines: linkings retained. Black lines: linkings eliminated. (i) Detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Pipeline of the proposed algorithm. (a) Original image. (b) Prediction maps. From left to right: text region map, character map and linking orientation map. For better visualization, linking orientations are represented with color-coded lines and only those within text regions are shown. (c) Detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Detection examples of the proposed algorithm. (a) ICDAR 2013. (b) ICDAR 2015. (c) MSRA-TD500. (d) COCO-Text</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Generalization to curved text. Curved examples are rarely seen in the training set, but the trained model can easily handle curved text. Original images are from [40].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>PERFORMANCES OF DIFFERENT TEXT DETECTION METHODS EVALUATED ON ICDAR 2013.</figDesc><table><row><cell>Algorithm</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>Proposed</cell><cell>0.8888</cell><cell>0.8022</cell><cell>0.8433</cell></row><row><cell>VGGMaxNet</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II .</head><label>II</label><figDesc>PERFORMANCES OF DIFFERENT TEXT DETECTION METHODS EVALUATED ON ICDAR 2015.</figDesc><table><row><cell>Algorithm</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>Proposed</cell><cell>0.7226</cell><cell>0.5869</cell><cell>0.6477</cell></row><row><cell>Zhang et al.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III .</head><label>III</label><figDesc>PERFORMANCES OF DIFFERENT TEXT DETECTION METHODS EVALUATED ON MSRA-TD500.</figDesc><table><row><cell cols="2">Algorithm</cell><cell>Precision</cell><cell cols="2">Recall</cell><cell>F-measure</cell></row><row><cell cols="2">Proposed</cell><cell>0.7651</cell><cell cols="2">0.7531</cell><cell>0.7591</cell></row><row><cell cols="2">Zhang et al. [63]</cell><cell>0.83</cell><cell>0.67</cell><cell>0.74</cell></row><row><cell cols="2">Yin et al. [59]</cell><cell>0.81</cell><cell>0.63</cell><cell>0.71</cell></row><row><cell cols="2">Kang et al. [20]</cell><cell>0.71</cell><cell>0.62</cell><cell>0.66</cell></row><row><cell cols="2">Yin et al. [60]</cell><cell>0.71</cell><cell>0.61</cell><cell>0.66</cell></row><row><cell cols="2">Unified [53]</cell><cell>0.64</cell><cell>0.62</cell><cell>0.61</cell></row><row><cell cols="2">TD-Mixture [54]</cell><cell>0.63</cell><cell>0.63</cell><cell>0.60</cell></row><row><cell cols="2">TD-ICDAR [54]</cell><cell>0.53</cell><cell>0.52</cell><cell>0.50</cell></row><row><cell cols="2">Epshtein et al. [9]</cell><cell>0.25</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell cols="2">Chen et al. [6]</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>TABLE IV.</cell><cell cols="4">PERFORMANCES OF DIFFERENT TEXT DETECTION</cell></row><row><cell cols="5">METHODS EVALUATED ON COCO-TEXT.</cell></row><row><cell cols="3">Algorithm Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell cols="2">Proposed</cell><cell>0.4323</cell><cell>0.271</cell><cell>0.3331</cell></row><row><cell></cell><cell></cell><cell cols="2">Baselines from [48]</cell></row><row><cell>A</cell><cell></cell><cell>0.8378</cell><cell>0.233</cell><cell>0.3648</cell></row><row><cell>B</cell><cell></cell><cell>0.8973</cell><cell>0.107</cell><cell>0.1914</cell></row><row><cell>C</cell><cell></cell><cell>0.1856</cell><cell>0.047</cell><cell>0.0747</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V .</head><label>V</label><figDesc>PERFORMANCES ON SUB-CATEGORIES IN COCO-TEXT. Generalization to text of different scripts. Even though the algorithm is only trained on examples of English and Chinese, it can generalize well to different types of scripts, such as Arabic, Hebrew, Korean and Russian. Original image are harvested from the Internet.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fig. 8.</cell></row><row><cell>Algorithm</cell><cell></cell><cell cols="2">Recall</cell><cell></cell></row><row><cell></cell><cell cols="2">Legible</cell><cell cols="2">Illegible</cell></row><row><cell></cell><cell>Machine</cell><cell>Hand</cell><cell>Machine</cell><cell>Hand</cell></row><row><cell>Proposed</cell><cell>0.3563</cell><cell>0.3109</cell><cell>0.0297</cell><cell>0.0567</cell></row><row><cell></cell><cell cols="3">Baselines from [48]</cell><cell></cell></row><row><cell>A</cell><cell>0.3401</cell><cell>0.1511</cell><cell>0.0409</cell><cell>0.0231</cell></row><row><cell>B</cell><cell>0.1616</cell><cell>0.1096</cell><cell>0.0069</cell><cell>0.0022</cell></row><row><cell>C</cell><cell>0.0709</cell><cell>0.0463</cell><cell>0.0028</cell><cell>0.0022</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The algorithms evaluated in<ref type="bibr" target="#b47">[48]</ref> were used in the process of data annotation, thus they cannot be considered as valid baselines. We learn about this via correspondence with the authors of<ref type="bibr" target="#b47">[48]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/s9xie/hed 3 http://rrc.cvc.uab.es/?ch=2&amp;com=downloads 4 http://rrc.cvc.uab.es/?ch=4&amp;com=downloads 5 http://www.iapr-tc11.org/mediawiki/index.php/MSRA Text Detection 500 Database (MSRA-TD500)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://vision.cornell.edu/se3/coco-text/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The numbers in this table have been modified, since the evaluation script for COCO-Text (https://github.com/andreasveit/coco-text) has been updated.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The numbers in this table have been modified, since the evaluation script for COCO-Text (https://github.com/andreasveit/coco-text) has been updated.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.nist.gov/itl/iad/ig/trait-2016.cfm" />
		<title level="m">Text Recognition Algorithm Independent Evaluation (TRAIT)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PhotoOCR: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meyerhenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Safro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schulz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Recent advances in graph partitioning. In arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of text detection and recognition in images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shearer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IDIAP</title>
		<imprint>
			<date type="published" when="2000-08" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust text detection in natural images with edge-enhanced maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICIP</title>
		<meeting>of ICIP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text detection and character recognition in scene images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced mser trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep structured output learning for unconstrained text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of BMVC</title>
		<meeting>of BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM International Conference on Multimedia</title>
		<meeting>of ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Text information extraction in images and video: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PR</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="977" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc. of ICDAR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Texture-based approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1631" to="1639" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Real-Time Vision for Human-Computer Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisacanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene text detection via connected component clustering and nontext filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2296" to="2305" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene text extraction with edge constraint and text collinearity link</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J M S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICPR</title>
		<meeting>of ICPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context. In arXiv</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scene text extraction based on edges and support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="135" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icdar 2005 text locating competition results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Top-down and bottom-up cues for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV</title>
		<meeting>of ACCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Text localization in real-world images using efficiently pruned exhaustive search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scene text localization and recognition with oriented stroke detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient scene text localization and recognition with local character refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-lexicon attribute-consistent text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A hybrid approach to detect and localize texts in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="800" to="813" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pemmaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<title level="m">Computational Discrete Mathematics: Combinatorics and Graph Theory in Mathematica. Cambridge University Press</title>
		<meeting><address><addrLine>Cambridge, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ESWA</title>
		<meeting>of ESWA</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ICDAR 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scene text detection using graph model built upon maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mobile visual search on printed documents using text and low bit-rate features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICIP</title>
		<meeting>of ICIP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Text localization and recognition in images and video. Handbook of Document Image Processing and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="843" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Toward integrated scene text reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Weinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="375" to="387" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A unified framework for multi-oriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Text string detection from natural scenes by structure-based partition and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2594" to="2605" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Assistive text reading from complex background for blind persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CBDAR</title>
		<meeting>of CBDAR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Handwritten text line extraction based on minimum spanning tree clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Wavelet Analysis and Pattern Recognition</title>
		<meeting>of International Conference on Wavelet Analysis and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-orientation scene text detection with adaptive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1930" to="1937" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="970" to="983" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Text localization based on fast feature pyramids and multi-resolution maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV workshop</title>
		<meeting>of ACCV workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
