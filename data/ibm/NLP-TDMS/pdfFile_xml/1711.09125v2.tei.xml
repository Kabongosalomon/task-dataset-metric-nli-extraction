<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Appearance-and-Relation Networks for Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Appearance-and-Relation Networks for Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatiotemporal feature learning in videos is a fundamental problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Network (ARTNet), to learn video representation in an end-toend manner. ARTNets are constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from RGB input in a separate and explicit manner. Specifically, SMART blocks decouple the spatiotemporal learning module into an appearance branch for spatial modeling and a relation branch for temporal modeling. The appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while the relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART blocks obtain an evident improvement over 3D convolutions for spatiotemporal feature learning. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appearance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenate Reduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMART Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion 3D Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D</head><p>Convolution FC (a) two-stream CNNs (b) 3D CNNs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Frames A RGB Frame</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D</head><p>Convolution FC</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Convolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optical Flow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Convolution</head><p>FC</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning has witnessed a series of remarkable successes in computer vision. In particular, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b24">[25]</ref> have turned out to be effective for visual tasks in image domain, such as image classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>, object detection <ref type="bibr" target="#b11">[12]</ref>, and semantic segmentation <ref type="bibr" target="#b26">[27]</ref>. Deep models have been also introduced into video domain for action recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b50">51]</ref>, and obtain comparable or better recognition accuracy to those traditional methods with hand-crafted representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>. However, the progress of architecture design and representation learning in video domain is much slower, partially due to its inherent complexity and <ref type="bibr" target="#b0">1</ref> The code is at https://github.com/wanglimin/ARTNet. higher dimension. Video could be viewed as the temporal evolution of a sequence of static images. It is generally assumed that two visual cues are crucial for video classification and understanding: <ref type="bibr" target="#b0">(1)</ref> static appearance in each frame, and (2) temporal relation across multiple frames. Therefore, an effective deep architecture should be able to capture both information to achieve excellent classification accuracy.</p><p>There are three kinds of successful architectures or frameworks for video classification <ref type="bibr" target="#b1">[2]</ref>: <ref type="bibr" target="#b0">(1)</ref> two-stream CNNs <ref type="bibr" target="#b34">[35]</ref>, (2) 3D CNNs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41]</ref>, and (3) 2D CNNs with temporal models on top such as LSTM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>, temporal convolution <ref type="bibr" target="#b32">[33]</ref>, sparse sampling and aggregation <ref type="bibr" target="#b48">[49]</ref>, and attention modeling <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9]</ref>. Two-stream CNNs capture appearance and motion information with different streams, which turn out to be effective for video classification. Yet, it is time-consuming to train two networks and calculate optical flow in advance. To overcome this limitation, 3D CNNs employ 3D convolutions and 3D pooling operations to directly learn spatiotemporal features from stacked RGB volumes. However, the performance of 3D CNNs is still worse than two-stream CNNs, and it is still unclear whether this straightforward 3D extension over 2D convolution could efficiently model static appearance and temporal relation. 2D CNNs with temporal models usually focus on capturing coarser and long-term temporal structure, but lack capacity of representing finer temporal relation in a local spatiotemporal window.</p><p>In this paper, we address the problem of capturing appearance and relation in video domain, by proposing a new architecture unit termed as SMART block. Our SMART block aims to Simultaneously Model Appearance and Re-laTion from RGB input in a separate and explicit way with a two-branch unit, in contrast to modeling them with twostream inputs <ref type="bibr" target="#b34">[35]</ref> or jointly and implicitly with a 3D convolution <ref type="bibr" target="#b40">[41]</ref>. As shown in <ref type="figure">Figure 1</ref>, our SMART block is a multi-branch architecture, which is composed of appearance branch and relation branch, and fuses them with a concatenation and reduction operation. The appearance branch is based on the linear combination of pixels or filter responses in each frame to model spatial structure, while the <ref type="bibr">Figure 1</ref>. Video architecture comparison: Our Appearance-and-Relation Networks (ARTNets) are constructed based on the SMART building block, which aims to simultaneously model appearance and relation from RGB in a separate and explicit way. In contrast, twostream CNNs model them with two inputs and 3D CNNs model them jointly and implicitly with a single 3D convolution. relation branch is based on the multiplicative interactions between pixels or filter responses across multiple frames to capture temporal dynamics. Specifically, the appearance branch is implemented with a standard 2D convolution and the relation branch is implemented with a square-pooling structure. The responses from two branches are further concatenated and reduced to a more compact representation.</p><p>A SMART block is a basic and generic building module for video architecture design. For video classification, we present an appearance-and-relation network (ARTNet) by stacking a collection of SMART blocks. Essentially, the appearance and relation information in video domain exhibit multi-scale spatiotemporal structure. The ARTNet is able to capture this visual structure in a hierarchical manner, where SMART units in the early layers focus on describe local structure in a short term, while the ones in the later layers can capture increasingly coarser and longer-range visual structure. An ARTNet is a simple and general architecture which offers flexible implementations. In the current implementation of this paper, the ARTNet is instantiated with the network of C3D-ResNet18 <ref type="bibr" target="#b41">[42]</ref> for an engineering compromise between accuracy and computation consumption. Moreover, our ARTNet is complementary to those longterm temporal models, which means any of them could be employed to enhance its modeling capacity. As an example, we use the framework of temporal segment network (TSN) <ref type="bibr" target="#b48">[49]</ref> to jointly train ARTNets from a set of sparsely sampled snippets and further improve the recognition accuracy.</p><p>We test the ARTNet on the task of action recognition in video classification. Particularly, we first study the performance of the ARTNet on the Kinetics dataset <ref type="bibr" target="#b19">[20]</ref>. We observe that our ARTNet obtains an evident improvement over C3D, and superior performance to the exiting state-of-theart methods on this challenging benchmark under the setting of training from scratch with only RGB input. To further demonstrate the generality of ARTNet, we also transfer its learned video representation to other action recognition benchmarks including HMDB51 <ref type="bibr" target="#b21">[22]</ref> and UCF101 <ref type="bibr" target="#b36">[37]</ref>, where performance improvement is also achieved.</p><p>The main contribution of this paper is three-fold: (1) A SMART block is designed to simultaneously capture appearance and relation in a separate and explicit way. (2) An ARTNet is proposed by stacking multiple SMART blocks to model appearance and relation information from different scales, which also allows for optimizing the parameters of SMART blocks in an end-to-end way. (3) ARTNets are empirically investigated on the large-scale Kinetics benchmark and state-of-the-art performance on this dataset is obtained under the setting of using only RGB input and training from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep learning for video classification. Since the breakthrough of Convolutional Neural Networks (CNN) <ref type="bibr" target="#b24">[25]</ref> in image classification <ref type="bibr" target="#b20">[21]</ref>, several works have tried to design effective architectures for video classification and action recognition <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>. Karpathy et al. <ref type="bibr" target="#b18">[19]</ref> first tested deep networks with different temporal fusion strategies on a large-scale and noisily-labeled dataset (Sports-1M) and achieved lower performance than traditional features <ref type="bibr" target="#b43">[44]</ref>. Simonyan et al. <ref type="bibr" target="#b34">[35]</ref> designed a two-stream architecture containing spatial and temporal nets by explicitly exploiting pre-trained models and optical flow calculation. Tran et al. <ref type="bibr" target="#b40">[41]</ref> investigated 3D CNNs <ref type="bibr" target="#b17">[18]</ref> on realistic and large-scale video datasets and further studied deep ResNet with 3D convolution <ref type="bibr" target="#b41">[42]</ref>. Carreira et al. proposed a new Two-Stream Inflated 3D CNNs based on 2D CNN inflation, which allows for pre-training with Ima-geNet models. Meanwhile, several papers <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b48">49]</ref> tried to model long-term temporal information for action understanding. Ng et al. <ref type="bibr" target="#b32">[33]</ref> and Donahue et al. <ref type="bibr" target="#b4">[5]</ref> utilized the LSTM <ref type="bibr" target="#b14">[15]</ref> to capture the long range dynamics for action recognition. Wang et al. <ref type="bibr" target="#b48">[49]</ref> designed a temporal segment network (TSN) to perform sparse sampling and temporal fusion, which aims to learn from the entire video.</p><p>Our work focuses on short-term temporal modeling and is most related with 3D CNNs. Our ARTNet mainly differs to 3D CNNs in that we design a new SMART block to model appearance and relation separately and explicitly with a two-branch architecture, while 3D CNNs employ the 3D convolutions to capture appearance and relation jointly and implicitly.</p><p>Models based on multiplicative interactions. Modeling or learning correspondence is an important task in computer vision. Typically, these methods are fundamentally based on the multiplicative interactions between pixels or between filter responses <ref type="bibr" target="#b29">[30]</ref>. Mapping units <ref type="bibr" target="#b13">[14]</ref> first introduced the idea of multiplicative interactions to model relation between different views. Gated Boltzmann machines <ref type="bibr" target="#b30">[31]</ref> were proposed to learn image transformation in unsupervised manner. Energy models <ref type="bibr" target="#b0">[1]</ref>, which may be viewed as a way to emulate multiplicative interactions by computing squares, were proposed to model motion information in videos. Independent Subspace Analysis (ISA) <ref type="bibr" target="#b15">[16]</ref> was designed for invariant feature learning by computing sums over squared. ISA is similar to Energy model but its weights are trained from data. Highorder neural networks <ref type="bibr" target="#b10">[11]</ref> were proposed to learn invariance based on polynomial expansions of input. Recently, some action recognition methods are based on energy models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref> and feature learning with Gate Boltzmann machines <ref type="bibr" target="#b39">[40]</ref> and ISA <ref type="bibr" target="#b23">[24]</ref>. Meanwhile, these multiplicative interactions or correlation models were integrated into the CNN architecture for optical flow estimation <ref type="bibr" target="#b5">[6]</ref> and person re-identification <ref type="bibr" target="#b25">[26]</ref>.</p><p>Our proposed relation branch is inspired by these early works with multiplicative interactions and in particular it shares a similar square-pooling architecture with ISA. Our work differs from them in three important aspects: <ref type="bibr" target="#b0">(1)</ref> The weights of relation branch are learned in a supervised manner with standard back propagation, while the previous work manually set model weights or learn them in an unsupervised manner. (2) The relation branch is integrated with an appearance branch to form the SMART block to capture spatiotemporal information, while previous works only has a module focusing on modeling relation. (3) We construct ARTNets by stacking multiple SMART blocks to learn hierarchical spatiotemporal features, while previous work usually has a single layer based on multiplicative interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatiotemporal Feature Learning</head><p>In this section we describe our method for spatiotemporal feature learning. First, we discuss the role of multiplicative interaction in modeling relation across multiple frames. Next, we introduce the design of a SMART block. Finally, we propose the ART-Net by stacking multiple SMART blocks in the architecture of C3D-ResNet18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multiplicative interactions</head><p>Suppose we have two patches x and y from consecutive frames, we aim to learn the transformation (relation) z between them. A natural solution to this problem is to perform standard feature learning on the concatenation of these two patches, just like a 3D convolution <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41]</ref>, as follows:</p><formula xml:id="formula_0">z k = i w x ik x i + j w y jk y j ,<label>(1)</label></formula><p>where the transformation code z k is defined as a linear combination of [x, y] by the parameters w = [w x k , w y k ]. However, in this case, the activation would be dependent on the appearance (content) of patches not just the transformation between them <ref type="bibr" target="#b29">[30]</ref>. In this sense, if both patches change but not transformation between them, the activation value would also change. Thus, this solution couples the information of appearance and relation together, adding the modeling difficulty and increasing the over-fitting risk.</p><p>Assuming the independence between appearance and relation, it is reasonable to decouple these two kinds of information when designing learning modules. It is easy to propose an appearance-independent relation detector by using multiplicative interactions between patches x and y <ref type="bibr" target="#b29">[30]</ref>. Specifically, the transformation code z k could be defined as follows:</p><formula xml:id="formula_1">z k = ij w ijk x i y j ,<label>(2)</label></formula><p>where the transformation code z k is defined as a linear combination of quadratic form from x and y by the weight tensor w ..k . Essentially, this transformation code z k pools over the outer product of x and y, each element of which represents the evidence for a specific type of transformation. Thus, the activation value of z k is less likely dependent on the patch appearance than the transformation between them.</p><p>Factorization and energy models. The major obstacle to directly deploy Equation <ref type="formula" target="#formula_1">(2)</ref> is that the number of parameters is roughly cubic in the number of pixels. Factorizing the parameter tensor W into three matrices would be an efficient way to reduce model parameters <ref type="bibr" target="#b29">[30]</ref>, namely:</p><formula xml:id="formula_2">w ijk = F f =1 w x if w y jf w z kf .</formula><p>Thus, the transformation code z k between patch x and patch y in Equation (2) would be rewritten as follows:</p><formula xml:id="formula_3">z k = f w z kf i w x if x i j w y jf y j = f w z kf (w xT f x)(w yT f y).</formula><p>(</p><p>This factorization formulation is closely related to energy model <ref type="bibr" target="#b0">[1]</ref> and could be implemented with it. Specifically, a hidden unit z k in the energy model is calculated as follows:</p><formula xml:id="formula_5">z k = f w z kf (w xT f x + w yT f y) 2 = f w z kf [2(w xT f x)(w yT f y) + (w xT f x) 2 + (w yT f y) 2 ],</formula><p>where hidden units are the same with Equation (3) except the quadratic terms (w xT f x) <ref type="bibr" target="#b1">2</ref> and (w yT f y) 2 , which do not have a significant effect on the hidden unit <ref type="bibr" target="#b28">[29]</ref>. This energy model could be efficiently implemented with standard operations in 3D CNNs and easily stacked layer-by-layer as introduced in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SMART blocks</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, a SMART block is a basic computational unit operating on an input volume V ∈ R W ×H×T ×C and producing an output volume H ∈ R W ×H ×T ×C . The motivation of the SMART block is to simultaneously model appearance and relation in a separate and explicit manner. Specifically, it learns spatiotemporal features from volume input with a two-branch architecture:</p><p>(1) appearance branch for spatial feature learning, and <ref type="formula" target="#formula_1">(2)</ref> relation branch for temporal feature learning.</p><p>Appearance branch operates on individual frames and aims at capturing useful static information for action recognition. Static cues are sometimes important for action recognition as some action classes are strongly associated with certain object or scene categories. Specifically, we employ a 2D convolution to operate on the video volume V to capture the spatial structure in each frame. The output of 2D convolution is a volume F ∈ R Ws×Hs×Ts×Cs . The response values F of 2D convolution usually goes into another Batch Normalization (BN) <ref type="bibr" target="#b16">[17]</ref> layer and Rectified Linear Unit (ReLU) <ref type="bibr" target="#b31">[32]</ref> for non-linearity.</p><p>Relation branch operates on stacked consecutive frames and aims to capture the relation among these frames for action recognition. This relation (transformation) information is crucial for action understanding as it contains motion cues. According to the discussion on multiplication interactions in the previous subsection, we design a square-pooling architecture to model temporal relation on this volume input. Specifically, we first apply a 3D convolution to this volume input V, which further goes through a square function to obtain hidden units U ∈ R Wt×Ht×Tt×Ct . Then, we apply a cross-channel pooling to aggregate multiple hidden units in U into the transformation codes Z ∈ R Wt×Ht×Tt×C t . This crosschannel is implemented with a 1 × 1 × 1 convolution. In practice, the transformation code Z would also go through a BN layer and ReLU non-linearity to be consistent with the output of appearance branch. Meanwhile, we also add a BN layer between the 3D convolution and the square nonlinearity to improve its stability.</p><p>A SMART block combine the output of the appearance and relation branches with a concatenation and reduction operation. Intuitively, the spatial and temporal features are complementary for action recognition and this fusion step aims to compress them into a more compact representation. In particular, we employ a 1 × 1 × 1 convolution on the concatenation volume [F, Z] ∈ R W ×H ×T ×(Cs+C t ) to obtain the compressed feature volumes H ∈ R W ×H ×T ×C f . As a common practice, this compressed feature volume H further goes through a BN layer and ReLU activation function.</p><p>Implementation details. For the design simplicity of SMART block, some default setting is fixed as follows. First, the spatial and temporal dimension of output of two branches are ensured to be the same for concatenation operation, i.e., W s = W t = W , H s = H t = H , and T s = T t = T . In this sense, we let stride of 2D convolution in appearance branch and 3D convolution in relation branch be the same. Meanwhile, the spatial dimension of convolution kernels from two branches are the same as well. Second, the number of 2D convolution kernels in appearance branch is the same with that of 3D convolution kernels in relation branch, i.e., C s = C t . In cross-channel pooling layer, each transformation code z k in relation branch is locally connected a group of hidden units instead of using full connectivity. The number of transformation code is set to be half of that of hidden unit u, i.e., C t = 2C t , and thereby the group size is set to be 2. The weights in cross-channel pooling are fixed as 0.5. Finally, for the output of SMART block, we set its output number to be equal to that of appearance branch, i.e., C f = C s . Therefore, the design parameter of a SMART block is the same with that a normal 3D convolution, including kernel size k × k × t, convolutional stride s s and s t , the output number c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Exemplars: ARTNet-ResNet18</head><p>After introducing the SMART block, we are ready to plug it into the existing network architecture to build the appearance-and-relation network (ARTNet). The flexibility layer name  <ref type="table">Table 1</ref>. Architectures for the Kinetics dataset: we study three different networks for spatiotemporal feature learning by stacking two types of building blocks from <ref type="figure" target="#fig_0">Figure 2:</ref> (1) 3D convolutions and (2) SMART blocks. Building blocks are shown in brackets, with the numbers of stacked blocks. The input to these networks is volume of 112 × 112 × 16 and downsample is performed conv3 1, conv4 1, and conv5 1 with a stride of 2 × 2 × 2.</p><formula xml:id="formula_6">output size C3D-ResNet18 ARTNet-ResNet18 (s) ARTNet-ResNet18 (d) conv1 56 × 56 × 8 3D conv 7 × 7 × 3, stride 2 × 2 × 2 SMART 7 × 7 × 3, stride 2 × 2 × 2 conv2 x 56 × 56 × 8 3D conv 3 × 3 × 3 64 3D conv 3 × 3 × 3 64 × 2 3D conv 3 × 3 × 3 64 3D conv 3 × 3 × 3 64 × 2 3D conv 3 × 3 × 3 64 SMART 3 × 3 × 3 64 × 2 conv3 x 28 × 28 × 4 3D conv 3 × 3 × 3 128 3D conv 3 × 3 × 3 128 × 2 3D conv 3 × 3 × 3 128 3D conv 3 × 3 × 3 128 × 2 3D conv 3 × 3 × 3 128 SMART 3 × 3 × 3 128 × 2 conv4 x 14 × 14 × 2 3D conv 3 × 3 × 3 256 3D conv 3 × 3 × 3 256 × 2 3D conv 3 × 3 × 3 256 3D conv 3 × 3 × 3 256 × 2 3D conv 3 × 3 × 3 256 SMART 3 × 3 × 3 256 × 2 conv5 x 7 × 7 × 1 3D conv 3 × 3 × 3 512 3D conv 3 × 3 × 3 512 × 2 3D conv 3 × 3 × 3 512 3D conv 3 × 3 × 3 512 × 2 3D conv 3 × 3 × 3 512 3D conv 3 × 3 × 3 512 × 2 1 × 1 × 1 average pool,</formula><p>of the SMART block allows it to replace the role of a 3D convolution in learning spatiotemporal feature. In current implementation, we develop an ARTNet by integrating the SMART block into the C3D-ResNet18 architecture <ref type="bibr" target="#b41">[42]</ref>, and thereby the resulted architecture is coined as ARTNet-ResNet18.</p><p>We choose the C3D-ResNet18 to instantiate the ART-Net and the architecture details are shown <ref type="table">Table 1</ref>. These networks take an 112 × 112 × 16 input to keep a balance between model capacity and processing efficiency.To well evaluate the effectiveness of SMART block, we implement two kinds of ARTNet-ResNet18: (1) we only replace the first 3D convolution in C3D-ResNet18 with the SMART block while keep the remaining layers unchanged, denoted as ARTNet-ResNet18 (s). (2) we stack multiple SMART blocks and totally replace seven 3D convolutions, denoted as ARTNet-ResNet18 (d). Stacking multiple SMART blocks allows us to capture appearance and relation information from different scales and further enhance the modeling capacity of ARTNet-ResNet18 (s).</p><p>Implementation details. We test these networks on the recently introduced Kinetics dataset <ref type="bibr" target="#b19">[20]</ref>. All these models are trained on the train set of Kinetics dataset from scratch. We train the C3D-ResNet18 and ARTNet-ResNet18 by following the common practice in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20]</ref>. The network parameters are initialized randomly. We use the mini-batch stochastic gradient descent algorithm to learn network parameters, where the batch size is set to 256 and momentum is set to 0.9. The frames are resized to 128 × 170 and then a volume of 112×112×16 is randomly trimmed and cropped from each training video. This volume also undergoes a random horizontal flip, with the per-pixel mean subtracted.</p><p>The learning rate is initialized as 0.1 and divided by a factor of 10 when validation loss saturates. The total number of iteration is 250, 000 on the Kinetics dataset. To reduce the risk of over-fitting, we add a dropout layer before the final classification layer, where the dropout ratio is set to 0.2.</p><p>For testing network, we follow the common evaluation scheme <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b34">35]</ref>, where we sample 250 volumes of 112 × 112 × 16 from the whole video. Specifically, we first uniformly trim 25 clips of 128 × 170 × 16 and then generate 10 crops of 112 × 112 × 16 from each clip (4 corners, 1 center, and their horizontal flipping). The final prediction result is obtained by taking an average over these 250 volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we describe the experimental results of our method. First, we introduce the action recognition datasets and the evaluation settings. Then, we study different aspects of our proposed ARTNets on the Kinetics dataset and compare with the state-of-the-art methods. Finally, we transfer the learned spatiotemporal representations in ARTNets to the datasets of UCF101 and HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate the performance of ARTNets on three action recognition benchmarks: (1) Kinetics <ref type="bibr" target="#b19">[20]</ref>, (2) UCF101 <ref type="bibr" target="#b36">[37]</ref>, and (3) HMDB51 <ref type="bibr" target="#b21">[22]</ref>. The Kinetics dataset is the largest well-labeled action recognition dataset. Its current version contains 400 action classes and each category has at least 400 videos. In total, there are around 240, 000 training videos, 20, 000 validation videos, and 40, 000 testing videos. The evaluation metric on the Kinetics dataset is the average of top-1 and top-5 error. As Kinet-  <ref type="table">Table 2</ref>. Comparison of ARTNet and C3D on the validation set of Kinetics dataset. We investigate the performance of basic blocks, including: 2D convolution, 3D convolution, relation branch, and SMART. We also study the effect of the stacking depth of the ART-Net. The performance is measured by Top-1 and Top-5 accuracy.</p><p>ics is the largest available dataset, we mainly study different aspects of ARTNets on this dataset with only RGB input under the setting of training from scratch.</p><p>UCF101 and HMDB51 are another two popular action recognition datasets, whose sizes are relatively small and the performance on them is already very high. The UCF101 has 101 action classes and 13, 320 video clips. We follow the official evaluation scheme and report average accuracy over three training/testing splits. The HMDB51 dataset is a collection of realistic videos from various sources, including movies and web videos. This dataset has 6, 766 videos from 51 action categories. Our experiment follows the original evaluation scheme using three training/testing splits and reports the average accuracy. As these two datasets are relatively small, we cannot train ARTNets from scratch and thereby transfer the video representations learned from the Kinetics dataset to them by fine tuning. The fine-tuning process follows the good practice presented in the temporal segment networks (TSN) <ref type="bibr" target="#b48">[49]</ref>. The goal of experiment on UCF101 and HMDB51 is to test the generalization ability of learned spatiotemporal features by the ARTNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on the Kinetics dataset</head><p>Study on building block. We begin our experiment by studying the performance of four building blocks for spatiotemporal feature learning in videos. These building blocks include: (1) 2D convolution, (2) 3D convolution, (3) Relation branch, and (4) SMART block. We conduct experiments on Kinetics with the ResNet18 architecture as shown in <ref type="table">Table 1</ref>. For C2D-ResNet18, we replace the 3D convolutions in C3D-ResNet18 with 2D convolutions, and for Relation-ResNet18, we replace the SMART blocks in ARTNet-ResNet18 with relation branch. The results are shown in <ref type="table">Table 2</ref>. We see that 3D convolutions outperforms 2D convolutions for learning video representations (75.7% vs. 71.9%). Our newly designed relation branch and SMART block both outperform the original 3D convolutions (77.2% vs. 75.7% and 77.4% vs. 75.7%). SMART block obtains the best performance among these four building blocks, demonstrating the effectiveness of modeling ap- pearance and relation separately and explicitly. Study on block stacking. We also investigate the effectiveness of stacking multiple Relation branches and SMART blocks. As shown in <ref type="table">Table 2</ref>, we observe that stacking multiple SMART blocks is able to further boost error rate from 77.4% to 78.7%. This improvement indicates the effectiveness of capturing spatiotemporal features in a hierarchical manner. However, stacking multiple relation branch causes a small performance drop, indicating the importance of modeling spatial structure in higher layers. Remarkably, as stacking SMART blocks would increase the network depth, we also compare the performance with C3D-ResNet34 in <ref type="table">Table 2</ref>, where ARTNet-ResNet18 even outperforms the deeper C3D-ResNet34 (78.7% vs. 77.0%). This result demonstrates that the performance improvement is brought by the effectiveness of SMART block instead of the increased network depth. In the remaining experiments, we will use the ARTNet-ResNet18 (d) by default.</p><p>Study on two-stream inputs. Two stream CNN is a strong baseline for action recognition and its input has two modalities, i.e., RGB and Optical Flow. To further illustrate the effectiveness of SMART block over 3D convolution, we perform experiments with two-stream inputs for both ARTNet-ResNet18 and C3D-ResNet18. The numerical results are reported in <ref type="table">Table 3</ref>. First, we find that two-stream inputs are able to improve the performance of C3D-ResNet18 from 75.7% to 78.2%. This improvement indicates that although 3D convolution aims to directly learn spatiotemporal features from RGB, flow stream is still able to provide complementary information. Second, comparing two-stream C3D-ResNet18 with RGB-stream ARTNet-ResNet18, we notice that our proposed ARTNet is still able to yield a slightly better performance (78.7% vs. 78.2%). This better result demonstrates the superiority of SMART block over two stream inputs. Finally, we also experiment ARTNet-ResNet18 with two-stream inputs. In flow stream, similar improvement over C3D-ResNet18 is also observed with ARTNet-ResNet18. The two-stream ARTNet-ResNet18 can boost performance to 80.4%. But it is worth noting that the high computational cost of optical flow makes it extremely difficult to apply at large-scale datasets and deploy in real-world applications. Therefore, in the remaining experiment, we mainly compare the performance of only using RGB input. Study on long-term modeling. The proposed SMART block and ARTNet focus on short-term spatiotemporal feature learning and is complementary to the exiting long-term modeling architectures <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b32">33]</ref>. Temporal segment network (TSN) is a general and flexible video-level framework for learning action models in videos <ref type="bibr" target="#b48">[49]</ref>. The simplicity nature of TSN allows us to replace the original 2D CNNs with our proposed ARTNet-ResNet18. Specifically, to keep a balance between modeling capacity and training time, we set the segment number as 2. The experimental results are summarized in <ref type="table">Table 4</ref>. We see that TSN modeling is helpful to improve the performance of ARTNet-ResNet18. For example, ARTNet-ResNet18 with TSN training can yield the performance of 80.0% with RGB input and 81.4% with two-stream inputs, which is better than the original performance by 1.3% and 1.0%. This improvement demonstrates the complementarity of ARTNet to the TSN framework. In principle, ARTNet is a general short-term video model, that could be explored in any long-term learning framework, such as LSTM <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5]</ref> and attention modeling <ref type="bibr" target="#b47">[48]</ref>.</p><p>Comparison to the state of the art. We compare the performance of ARTNet-ResNet18 with the state-of-the-art approaches on the validation set and test set of Kinetics. The results are summarized in <ref type="table" target="#tab_4">Table 5</ref>. For fair comparison, we consider methods that only use RGB input and learned from scratch on the train set of Kinetics. We also list other important factors such as spatial resolution and backbone architectures.</p><p>We first compare with three baseline methods: (1) CNN+LSTM <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5]</ref>, (2) Spatial Stream <ref type="bibr" target="#b34">[35]</ref>, and (3) C3D <ref type="bibr" target="#b40">[41]</ref>. Our proposed ARTNets significantly outperform these baselines by around 10%. We then compare with deeper C3D architecture <ref type="bibr" target="#b41">[42]</ref> such as C3D-ResNet18 and C3D-ResNet34. Our ARTNet is able to yield a better performance (around 3%) than these fairly-comparable models. Finally, we compare with the recent state-of-theart methods, namely temporal segment network (TSN) <ref type="bibr" target="#b48">[49]</ref> and Inflated 3D CNN (I3D) <ref type="bibr" target="#b1">[2]</ref>. These two architectures employ a deeper backbone architecture (Inception <ref type="bibr" target="#b38">[39]</ref>) and larger spatial resolution (224 × 224). Besides, I3D is also equipped with long-term modeling <ref type="bibr" target="#b42">[43]</ref> by stacking 64 frames. Therefore, it is fair for us to use TSN to increase the temporal duration of ARTNet. Our ARTNet with TSN training obtains a slightly better performance than these two very competitive methods (80.0% vs. 77.8% on validation set, and 78.7% vs. 78.2% on test set).</p><p>It is worth noting that the current published state-of-theart performance is 82.7%, that is obtained by two-stream I3D <ref type="bibr" target="#b1">[2]</ref> with optical flow input and pre-training on Ima-geNet. Two-stream I3D is more computational expensive than ARTNet as it uses larger spatial resolution, deeper structure, and two-stream inputs. The winner solution <ref type="bibr" target="#b27">[28]</ref> at ActivityNet challenge <ref type="bibr" target="#b9">[10]</ref> gets a performance of 87.6% by using more modalities, multi-stage training, and model ensemble. These results are not directly comparable to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on the UCF101 and HMDB51 datasets</head><p>In this subsection we study the generalization ability of learned spatiotemporal representations on the Kinetics dataset <ref type="bibr" target="#b19">[20]</ref>. Specifically, we transfer the learned models to two popular action recognition benchmarks: UCF101 <ref type="bibr" target="#b36">[37]</ref> and HMDB51 <ref type="bibr" target="#b21">[22]</ref>. We consider fine tuning three models trained on the Kinetics dataset: C3D-ResNet18, ARTNet-ResNet18 without TSN, ARTNet-ResNet18 with TSN. The fine-tuning process is conducted with the TSN framework and follows the common practice proposed in the original TSN framework <ref type="bibr" target="#b48">[49]</ref>, where the segment number is set to 2.</p><p>The results are summarized in <ref type="table">Table 6</ref>. First, we compare the performance of C3D-ResNet18 and ARTNet-ResNet18 pre-trained on the Kinetics dataset and see that our ARTNet outperform C3D by 3.7% on the UCF101 dataset and by 5.5% on the HMDB51 dataset. This superior performance demonstrates that the spatiotemporal representation learned in ARTNet is more effective than C3D for transfer learning. Then, we investigate the ARTNet-ResNet18 models learned under the TSN framework on the Kinetics dataset and these models can yield a slightly better performance (94.3% on UCF101 and 70.9% on HMDB51). This better transfer learning performance on UCF101 and HMDB51 agrees with the original performance improvement on the Kinetics dataset as shown <ref type="table" target="#tab_4">Table 5</ref>, which indicates the importance of long-term modeling. Finally, we compare with other state-of-the-art methods that pre-train on different datasets. We see that the methods pre-trained on the Kinetics dataset significantly outperform those pretrained on ImageNet <ref type="bibr" target="#b2">[3]</ref> and Sports-1M <ref type="bibr" target="#b18">[19]</ref>, which might be explained by the better quality of Kinetics. Our ARTNet obtains a comparable performance to the best performer of RGB-3D that is trained at a larger spatial resolution and pretrained on two datasets (ImageNet and Kinetics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Spatial resolution Backbone architecture Kinetics val set Kinetics test set ConvNet+LSTM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref> 299 × 299 ResNet-50 -68.0% Two Stream Spatial Networks <ref type="bibr" target="#b34">[35]</ref> 299 × 299 ResNet-50 -66.6% C3D <ref type="bibr" target="#b40">[41]</ref> 112 × 112 VGGNet-11 -67.8% C3D <ref type="bibr" target="#b41">[42]</ref> 112 × 112 ResNet-18 75.7% 74.4% C3D <ref type="bibr" target="#b41">[42]</ref> 112 × 112 ResNet-34 77.0% 75.3% TSN Spatial Networks <ref type="bibr" target="#b48">[49]</ref> 224 × 224 Inception V2 77.8% -RGB-I3D <ref type="bibr" target="#b1">[2]</ref> 224  <ref type="table">Table 6</ref>. Comparison with state-of-the-art methods on the UCF101 and HMDB51 datasets. The accuracy is reported as average over three splits. For fair comparison, we consider methods that use only RGB input and pre-train on different datasets. The performance is grouped according to its pre-training dataset. Our ARTNet obtains the best performance under the setting of pre-training only on the Kinetics dataset, and a comparable performance to the RGB-I3D pre-trained on the datasets of ImageNet+Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper we have presented a new architecture, coined as ARTNet, for spatiotemporal feature learning in videos. The construction of ARTNet is based on a generic building block, termed as SMART, which aims to model appearance and relation separately and explicitly with a two-branch unit. As demonstrated on the Kinetics dataset, SMART block is able to yield better performance than the 3D convolution, and ARTNet with a single RGB input even outperforms the C3D with two-stream inputs. For representation transfer from Kinetics to datasets of UCF101 and HMDB51, ARTNet also achieves superior performance to the original C3D. This performance improvement may be ascribed to the fact that we separately model appearance and relation, by using the linear combination of filter responses in each frame and the multiplicative interactions between filter responses across frames, respectively.</p><p>For ARTNet, augmenting RGB input with optical flow also helps to improve performance. This improvement indicates optical flow modality is still able to provide complementary information. However, the high computational cost of optical flow prohibits its application in real-world systems. In the future we plan to further improve the ART-Net architecture to overcome the performance gap between single-stream and two-stream inputs. Meanwhile, we will try to instantiate the ARTNets with more deeper structures such as ResNet101 and train them on more larger spatial resolutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Building blocks: (a) the 3D convolution operation learns spatiotemporal features jointly and implicitly. (b) we first propose a square-pooling architecture to learn appearance-independent relation between frames. (c) we further construct a SMART block to learn spatiotemporal features separately and explicitly, where appearance branch uses a 2D convolution to capture static structure and relation branch employs a squaring-pooling architecture to model temporal relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison with state-of-the-art methods on the validation and test set of Kinetics. The performance is measured by the average of Top-1 and Top-5 accuracy. For fair comparison, we consider methods that use only RGB input and train from scratch on Kinetics. Our ARTNets are trained from the spatial resolution of 112 × 112 and can still yield better performance than those trained from the spatial resolution of 224 × 224 or 229 × 229.</figDesc><table><row><cell></cell><cell>× 224</cell><cell>Inception V1</cell><cell>-</cell><cell></cell><cell>78.2%</cell></row><row><cell>ARTNet w/o TSN</cell><cell>112 × 112</cell><cell>ResNet-18</cell><cell>78.7%</cell><cell></cell><cell>77.3%</cell></row><row><cell>ARTNet with TSN</cell><cell>112 × 112</cell><cell>ResNet-18</cell><cell>80.0%</cell><cell></cell><cell>78.7%</cell></row><row><cell>Method</cell><cell>Pre-train dataset</cell><cell cols="2">Spatial resolution Backbone architecture</cell><cell>UCF101</cell><cell>HMDB51</cell></row><row><cell>HOG [44]</cell><cell>None</cell><cell>240 × 320</cell><cell>None</cell><cell>72.4%</cell><cell>40.2%</cell></row><row><cell>ConvNet+LSTM [5]</cell><cell>ImageNet</cell><cell>224 × 224</cell><cell>AlexNet</cell><cell>68.2%</cell><cell>-</cell></row><row><cell>Two Stream Spatial Network [35]</cell><cell>ImageNet</cell><cell>224 × 224</cell><cell>VGG-M</cell><cell>73.0%</cell><cell>40.5%</cell></row><row><cell>Conv Pooling Spatial Network [8]</cell><cell>ImageNet</cell><cell>224 × 224</cell><cell>VGGNet-16</cell><cell>82.6%</cell><cell>-</cell></row><row><cell>Spatial Stream ResNet [7]</cell><cell>ImageNet</cell><cell>224 × 224</cell><cell>ResNet-50</cell><cell>82.3%</cell><cell>43.4%</cell></row><row><cell>Spatial TDD [46]</cell><cell>ImageNet</cell><cell>224 × 224</cell><cell>VGG-M</cell><cell>82.8%</cell><cell>50.0%</cell></row><row><cell>RGB-I3D [2]</cell><cell>ImageNet</cell><cell>224 × 224</cell><cell>Inception V1</cell><cell>84.5%</cell><cell>49.8%</cell></row><row><cell>TSN Spatial Network [49]</cell><cell>ImageNet</cell><cell>224 × 224</cell><cell>Inception V2</cell><cell>86.4%</cell><cell>53.7%</cell></row><row><cell>Slow Fusion [19]</cell><cell>Sports-1M</cell><cell>170 × 170</cell><cell>AlexNet</cell><cell>65.4%</cell><cell>-</cell></row><row><cell>C3D [41]</cell><cell>Sports-1M</cell><cell>112 × 112</cell><cell>VGGNet-11</cell><cell>82.3%</cell><cell>51.6%</cell></row><row><cell>LTC [43]</cell><cell>Sports-1M</cell><cell>71 × 71</cell><cell>VGGNet-11</cell><cell>82.4%</cell><cell>48.7%</cell></row><row><cell>C3D [42]</cell><cell>Sports-1M</cell><cell>112 × 112</cell><cell>ResNet-18</cell><cell>85.8%</cell><cell>54.9%</cell></row><row><cell>TSN Spatial Network [49]</cell><cell>ImageNet+Kinetics</cell><cell>224 × 224</cell><cell>Inception V2</cell><cell>91.1%</cell><cell>-</cell></row><row><cell>TSN Spatial Network [49]</cell><cell>ImageNet+Kinetics</cell><cell>229 × 229</cell><cell>Inception V3</cell><cell>93.2%</cell><cell>-</cell></row><row><cell>RGB-I3D [2]</cell><cell>ImageNet+Kinetics</cell><cell>224 × 224</cell><cell>Inception V1</cell><cell>95.6%</cell><cell>74.8%</cell></row><row><cell>C3D</cell><cell>Kinetics</cell><cell>112 × 112</cell><cell>ResNet-18</cell><cell>89.8%</cell><cell>62.1%</cell></row><row><cell>ARTNet w/o TSN</cell><cell>Kinetics</cell><cell>112 × 112</cell><cell>ResNet-18</cell><cell>93.5%</cell><cell>67.6%</cell></row><row><cell>ARTNet with TSN</cell><cell>Kinetics</cell><cell>112 × 112</cell><cell>ResNet-18</cell><cell>94.3%</cell><cell>70.9%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatiotemporal energy models for the perception of motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient action spotting based on a spacetime oriented structure representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sizintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Cannons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1990" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DevNet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<idno>abs/1710.08011</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning, invariance, and generalization in high-order neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4972" to="4978" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A parallel computation that assigns canonical object-based frames of reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="683" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1705" to="1720" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3361" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On multi-view feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to relate images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1829" to="1846" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of image transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4597" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Con-vNet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1604.04494</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Motionlets: Mid-level 3D parts for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2674" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MoFAP: A multi-level representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="271" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Untrimmed-Nets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4325" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Actions˜transforma-tions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2658" to="2667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
