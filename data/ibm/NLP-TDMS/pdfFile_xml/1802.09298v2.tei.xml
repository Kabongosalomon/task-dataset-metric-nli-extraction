<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Research Center</orgName>
								<orgName type="institution" key="instit1">KCIS</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Junaid</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ansari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Research Center</orgName>
								<orgName type="institution" key="instit1">KCIS</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Krishna</forename><surname>Murthy</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universite de Montreal</orgName>
								<address>
									<settlement>Mila</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mahdava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics Research Center</orgName>
								<orgName type="institution" key="instit1">KCIS</orgName>
								<orgName type="institution" key="instit2">IIIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename></persName>
						</author>
						<title level="a" type="main">Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>denotes equal contribution Fig. 1. An illustration of the proposed method. The first two rows show objects tracks in frames t and t + 1. The bottom row depicts how 3D position and orientation information is propagated from frame t to frame t + 1. This information is used to specify search areas for each object in the subsequent frame, and this greatly reduces the number of pairwise costs that are to be computed.</p><p>Abstract-This paper introduces geometry and novel object shape and pose costs for multi-object tracking in road scenes. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes. More qualitative and quantitative results can be found at https://junaidcs032.github.io/Geometry_ ObjectShape_MOT/. Code and data to reproduce our experiments and results are now available at https://github. com/JunaidCS032/MOTBeyondPixels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. An illustration of the proposed method. The first two rows show objects tracks in frames t and t + 1. The bottom row depicts how 3D position and orientation information is propagated from frame t to frame t + 1. This information is used to specify search areas for each object in the subsequent frame, and this greatly reduces the number of pairwise costs that are to be computed.</p><p>Abstract-This paper introduces geometry and novel object shape and pose costs for multi-object tracking in road scenes. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes. More qualitative and quantitative results can be found at https://junaidcs032.github.io/Geometry_ ObjectShape_MOT/. Code and data to reproduce our experiments and results are now available at https://github. com/JunaidCS032/MOTBeyondPixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Object tracking in road scenes is an important component of urban scene understanding. With the advent and subsequent surge in autonomous driving technologies, accurate multi-object trackers are desirable in several tasks such as navigation and planning, localization, and traffic behavior analysis.</p><p>In this paper, we focus on designing a simple and fast, yet accurate and robust solution to the Multi-Object Tracking (MOT) problem in an urban road scenario. The dominant approach to multi-object tracking is tracking-by-detection, where the entire process is divided into two phases. The first phase comprises object detection, where bounding-boxes of objects of interests are obtained in each frame of the video sequence. The second phase is the data association phase, which is often the hardest step in the trackingby-detection paradigm. Several factors such as spurious or missing detections, repeat detections, or occlusions and target interactions are confounding factors in this data association phase.</p><p>Although several approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> exist for accurate online tracking of moving vehicles from a moving camera, most of them <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b1">[2]</ref> use handcrafted cost functions that are either based on primitive features such as bounding box position in the image and color histograms, or are highly sophisticated and non-intuitive in design (eg. ALFD <ref type="bibr" target="#b5">[6]</ref>). On the other hand, we propose costs that are intuitive, easy to compute and implement, and provide complementary cues about the target.</p><p>We exploit the fact that road scenes have a unique geometry and use this prior information to design costs. The proposed costs capture 3D cues arising from this scene geometry, as well as appearance based information. Further, we introduce a novel cost that captures similarity of 3D shapes and poses of target hypotheses. To this end we leverage recent work on shape-priors for object detection and localization from monocular sequences <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. To the best of our knowledge, such pairwise costs have not been incorporated in multi-object tracking frameworks.</p><p>The efficacy of the monocular 3D cues is best portrayed in <ref type="figure">Fig.1</ref>. In this figure the first two rows illustrate the objects with their bounding boxes in two successive frames at t and t+ 1. Upon lifting the objects at t to 3D and ballooning their locations to account for large uncertainties in ego motion, we project them into the image observed at t + 1. This gated/overlapping area shown in their respective colors in the last row of <ref type="figure">Fig.1</ref> reduces the search area for each such object significantly thereby reducing the pairwise costs. By backprojecting that lie only within this gated area into 3D and ascertaining data association costs based on 3D volume overlaps significantly improves tracking accuracy even with a straight forward Hungarian data association scheme.</p><p>The proposed costs are not too dependent on the choice of data association framework. We demonstrate the superiority of the proposed costs over monocular video sequences of urban road scenes that capture a wide range of camera and target motions, and also consistent improvement over other costs regardless of the choice of the object detector. We perform an extensive evaluation of various modes of the proposed costs on the KITTI Tracking benchmark <ref type="bibr" target="#b8">[9]</ref> and obtain state-of-the-art performance, beating previous approaches by using a simple two-frame Hungarian association scheme. The approach is tested on KITTI online evaluation sever and outperforms the previous published approaches significantly. Naturally, more complex data association schemes, such as network flow based algorithms <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> can result in much better performance boosts upon incorporation of the proposed pairwise costs.</p><p>The paper contributes as follows.</p><p>1) It introduces novel data association cues based on single view reconstruction of objects that results in best tracking performance reported thus far in KITTI training datasets. It outperforms the nearest reported values in training data <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, by at-least 12% . The approach is tested on the KITTI Tracking online evaluation server where it outperforms the published approaches by a margin of over 6%.</p><p>2) It showcases that such improvements are sufficiently detector agnostic and repeatable over baseline appearance tracking based on object detectors such as <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> through ablation studies 3) Finally it also identifies a role for 3D pose and shape cues where they play a role in improving tracking performance. Monocular 3D cues especially based on single view geometry can often be unreliable. However when computed effectively they can be used reliably and repeatably even in challenging sequences such as KITTI. This constitutes the central theme of this effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we review relevant work on multi-object tracking, and compare and contrast it with the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Global Tracking</head><p>Many approaches to tackle the association problem are global <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, in the sense that they assume detections from all frames are available for processing. Most global methods operate by mapping the tracking problem to a min-cost network flow problem. The original idea was proposed in <ref type="bibr" target="#b9">[10]</ref> and also provides for a method for explicit occlusion reasoning. An efficient variant is an approach based on generalized minimum clique graphs <ref type="bibr" target="#b11">[12]</ref>, where associations are solved for one object at a time while other objects are implicitly incorporated. Another section of global methods attempts to construct small chunks of trajectories (called tracklets), and compose them hierarchically to form longer trajectories, rather than solving for a min-cost flow over a densely connected graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Online MOT</head><p>In contrast to this, online trackers <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref> do not assume any knowledge of future frames and operate greedily, only with the data available upto the current instant. Such trackers often formulate the association problem as that of bipartite matching, and solve it via the Hungarian algorithm. A recent variant proposes near-online trackers <ref type="bibr" target="#b5">[6]</ref>, in an attempt to provide the best of both worlds, i.e., to combine the capability of global methods to handle longterm occlusions and still achieve very low output latencies. Gieger et al <ref type="bibr" target="#b12">[13]</ref> propose a memory and computation cost bound variant of network flow using dynamic programming.</p><p>Both these paradigms rely on handcrafted pairwise costs being fed into the association framework. Most of these are sophisticated in design and do not end up capturing 3D information that is easily available in road scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Learning Costs for MOT</head><p>Significant attention has also been devoted to the task of learning pairwise costs for target tracking problems. In <ref type="bibr" target="#b2">[3]</ref>, a structured SVM was used to learn pairwise costs for a bipartite matching data association framework. Other works have used graphical models, divide and conquer strategies and also learn unary costs. A more recent work <ref type="bibr" target="#b0">[1]</ref> learns all costs using a deep neural network. On the other hand, we show that our simple, yet clean and efficient cost function designs significantly improve performance without the need of extensive hyperparameter search or cost learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>We adopt the tracking-by-detection paradigm where we assume that we are provided with a monocular video se-</p><formula xml:id="formula_0">quence of F frames {I f } for f ∈ {1..F }, and a set of object detections D f for each frame I f . Each detection set consists of object detections {D i f }, where i ∈ {1..N f } (N f is the number of detections in frame f ).</formula><p>Note that D f can also be an empty set, in the case where no objects are detected in a frame. Each detection D i f is parametrized as and s i f is the detectors confidence in the bounding box (greater value indicates higher confidence). The multi-object tracking problem is to associate each bounding box to a target trajectory T k such that the following constraints are met.</p><formula xml:id="formula_1">D i f = (x i f , y i f , w i f , h i f , s i f ), where (x i f , y i f ) corresponds</formula><p>• Each target trajectory T k comprises of a set of bounding boxes (all from different frames) belonging to a unique target in the scene. • There are exactly as many trajectories K as there are targets to be tracked. • In all frames where a target is visible, it is detected and assigned to the corresponding unique trajectory for the object. • All spurious bounding box detections are unassigned to any target trajectory. The tracking problem formulated above is usually solved in a min-cost network flow framework (global tracking), a moving window dynamic programming framework (nearonline tracking) or a bipartite matching framework (online tracking). Note that these are not the only available frameworks, but a representative set of most tracking approaches. All these frameworks (and the others not mentioned here) use pairwise costs to define affinity across pairs of detections. The association framework then computes a Maximum A Posteriori (MAP) estimate of the target trajectory T k , given the detection hypotheses D = (D 1 , D 2 , ...D f ) and an affinity matrix that gives the likelihood of each detection in each frame corresponding to every detection in every other frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. GEOMETRY AND OBJECT SHAPE COSTS</head><p>The core contribution of this paper is to design intuitive pairwise costs that are efficient to compute, and are accurate for tracking. We focus on urban driving scenarios and demonstrate how the geometry of urban road scenes can be exploited to infer 3D cues for tracking.</p><p>Typical costs in tracking algorithms include bounding box locations, trajectory priors, optical flow, bounding box overlap, and appearance information (color histograms or path-based cross-correlation measures). These costs require careful handcrafting, finetuning, and hyperparameter estimation. We propose to use a set of simple complementary costs that are readily available from recent monocular 3D object localization systems <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. We also introduce a novel cost based on the 3D shape and pose of the target. We show that this cost, apart from improving data association performance, also assists in discarding false detections without incurring large computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Setup</head><p>We focus on autonomous driving scenarios, where the video sequence is from a monocular camera mounted on a car moving on the road plane, and the targets to be tracked are also moving on the road. Feature based odometry is run on a background thread (for rough frame-to-frame motion estimation). Also, we make use of a recent approach that goes beyond bounding boxes and estimates the 3D shape and pose of objects, given just a single image <ref type="bibr" target="#b6">[7]</ref>. This is done by lifting discriminative parts in 2D (keypoints) to 3D. These <ref type="figure">Fig. 2</ref>. Illustrating the concept of 3D-2D and 3D-3D costs Two subsequent frames, t and t+1, are shown in the left. For each detection in the frame t, we compute and propagate (with uncertainty) its 3D bounding box in the next frame t + 1. These boxes are projected to 2D in the frame t+1. The intersection between the detection boxes of t+1 and these projections constitutes the 3D-2D cost. The intersection of the 3D bounding boxes in 3D constitute the 3D-3D cost as shown in the right; propagated bounding boxes are colored with their respective 2D box in frame t and 3D bounding boxes of detections in frame t+1 are numbered respectively. keypoints are a set of points chosen so that they are common across all object instances (eg. for a car, we have centers of wheels, headlights, taillights, etc). The authors use a CNN architecture <ref type="bibr" target="#b7">[8]</ref> to localize these keypoints in 2D, given a detection.</p><p>The 3D shape of the object is parametrized as the sum of the mean shape (for the object category) and a linear combination of so-called basis shapes. Mathematically,</p><formula xml:id="formula_2">S =S + b b=1 λ b V b<label>(1)</label></formula><p>where S is the shape of a particular instance,S is the mean shape for the object category, and V b is the deformation basis (a set of eigenvectors) that characterizes deformation directions of the mean shape. We use the same model in <ref type="bibr" target="#b6">[7]</ref> and denote the shape vector of an object by Λ</p><formula xml:id="formula_3">= [λ 1 ..λ B ] T ,</formula><p>where B is the number of vectors in the deformation basis (typically, B = 5). The pipeline in <ref type="bibr" target="#b6">[7]</ref> also estimates the 3D pose of the object, which is parametrized as an axis-angle vector ω. Moreover, an estimate of object dimensions (height, width, and length) is also returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D-2D Cost</head><p>Given the height h cam of the camera above the ground, assuming that the bottom line of each bounding box detection d i f in frame f is on the road plane, a depth estimate of the car in the current camera coordinates can be obtained by back projection via the road plane as in <ref type="bibr" target="#b21">[22]</ref>, using</p><formula xml:id="formula_4">X f = π −1 G (x) = hK −1 x n T K −1 x<label>(2)</label></formula><p>where x is the bottom center of the detected bounding box, K is the camera intrinsic matrix and π −1 G is used as shorthand for backprojection via the ground plane. This backprojection equation is only accurate when x is known precisely, which is not usually the case. Hence, we estimate the uncertainty in 3D location of X f by using a linearized version of <ref type="bibr" target="#b1">(2)</ref> and assuming that the detector confidence is an isotropic 2D Gaussian, i.e., (x i f , y i f ) T ∼ N (0, σ 2 I 2×2 ). This region is expanded (anisotropically) by the estimates of the target dimensions returned by the system <ref type="bibr" target="#b6">[7]</ref>. Now, assume we have another detection d j f in frame f with which we wish to compute the pairwise affinity of d i f . We obtain a rough estimate of the camera motion from frame f to frame f using a feature-based odometry thread running in the background. Using this estimate of the camera motion, we transport X f to the camera coordinates of frame f , while duly accounting for the uncertainty in camera motion estimate, and in the backprojection via the road plane. The obtained coordinates X f are then projected down to the image frame f to obtain a 2D search area in which potential matches for X f are expected to be found, as shown in the frame t+1 of <ref type="figure">Fig.2</ref>. Mathematically, the 3D-2D cost for two detections d i f and d j f is defined as follows</p><formula xml:id="formula_5">C 3D2D (d i f , d j f ) = 1 − (π(g(ξ, φ(π −1 G (y i f ), s i f )) ∩ b j f )) b j f<label>(3)</label></formula><p>Intuitively, this cost measures a (weighted) overlap of the 2D region in which the target is expected in frame f and the detection d j f . π denotes the projection operator that projects a 3D point to image pixel coordinates. g(ξ, X) denotes a rigid-body motion ξ ∈ se(3) applied to a 3D point X ∈ R 3 . φ(X, s) denotes the function that estimates the uncertainty of the 3D point S according to a linearized form of (2) and the detector confidence s.</p><p>Most importantly, this cost is evaluated only for detections d j f that lie inside the expected target area (π(g(ξ, φ(π −1 G (y i f ), s i f )). This significantly reduces the number of comparisons needed to be made among target pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D-3D cost</head><p>Although useful in reducing the number of candidate detections to be evaluated, the 3D-2D cost has frequent confounding cases. This is because, we still measure overlap in the image space. To mitigate this drawback, we define a 3D-3D cost, which, instead of measuring 2D overlap, measures overlap in 3D,as shown in <ref type="figure">Fig.2 (right side)</ref>. Here, we backproject each candidate d j f via the road plane, and measure overlap with respect to the transformed 3D volume from frame f given by g(ξ, φ(π −1 G (y i f ), s i f ). The 3D-3D cost for two detections d i f and d j f is defined as</p><formula xml:id="formula_6">C 3D3D (d i f , d j f ) = 1 − g(ξ, φ(π −1 G (y i f ), s i f )) φ(π −1 G (b j f ), s j f )<label>(4)</label></formula><p>In order to speed up evaluation of 3D overlap, we exploit the inherent geometry of road scenes. Since all objects of interest are on the road plane (the XZ plane in our case), it is sufficient to measure overlap in the XZ plane. This is because all objects are at nearly constant heights above the ground and hence have similar overlap in the Y direction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Appearance Cost</head><p>In <ref type="bibr" target="#b7">[8]</ref>, the authors train a stacked-hourglass CNN architecture to localize a discriminative set of keypoints on an image. This deep CNN architecture captures various discriminative features for each detection, along with the keypoint evidence. We use weighted combination of activation maps from the output of the layers of the hourglass network as a feature descriptor for each detection, as shown in <ref type="figure" target="#fig_1">Fig.3</ref> and compute a similarity score between detections using the L2 Norm between descriptors from the image patch inside each of the bounding boxes. If ψ(.) denotes the feature descriptor of each detection, the appearance cost is defined as</p><formula xml:id="formula_7">C app (d i f , d j f ) = η app ψ(d i f ) − ψ(d j f ) 2 2 (5)</formula><p>where η app is a normalization constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Shape and Pose Cost</head><p>We use a novel shape and pose cost based on the single image shape and pose returned by the pipeline of <ref type="bibr" target="#b6">[7]</ref> . Shape is parameterized as a vector comprising of deformation coefficients Λ = [λ 1 ..λ B ] T , where B is the number of deformation basis vectors (usually 5). Each possible value of Λ denotes a unique class of object instances and hence carries useful information about the 3D shape of the target. For instance varying certain parameters of Λ may represent a shape that is more SUV-like than Sedan-like, and so on. Pose is parametrized as an axis-angle vector ω. For detections d i f and d j f , the shape and pose cost is specified as</p><formula xml:id="formula_8">C s (d i f , d j f ) = η s Λ(d i f ) − Λ(d j f ) 2 2 + η p ω(d i f ) − ω(d j f ) 2 2 (6)</formula><p>where η s and η p are normalization constants.</p><p>The overall pairwise cost term is a weighted linear combination of all the aforementioned cost. The weights of the linear combination are determined by four-fold cross validation on the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In this section, we present an account of the experiments we performed, and we report and analyze the findings thereof. In nutshell, we evaluate our tracking framework on a variety of challenging urban driving sequences and demonstrate a substantial performance boost over the stateof-the-art in multi-object tracking, by using the simplest of tracking frameworks, viz. bipartite matching using the Hungarian algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We evaluate the proposed multi-object tracking framework on the popular KITTI Tracking benchmark on both training as well as testing dataset. <ref type="bibr" target="#b8">[9]</ref>. As prescribed in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, we divide the training dataset, which contains 21 sequences, into four splits,for cross validation. The splits are chosen so that each split contains a similar distribution of number of vehicles per sequence, occlusion and truncation levels, and relative motion patterns between the camera and the target. The cross validation helps us to tune the weight for each of the proposed costs to compute the final cost matrix. The best performing combination of these weighted costs are used for reporting the result on the KITTI Tracking benchmark. Multiple vehicles moving with varying speeds, variance in the ego camera motion, and target objects appearing in non conforming locations in frames make the KITTI Tracking dataset <ref type="bibr" target="#b8">[9]</ref> a truly challenging one. We report results on the Car class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>To evaluate the performance of our approach, we adopt the widely used CLEAR MOT metrics <ref type="bibr" target="#b24">[25]</ref>. The overall performance of the tracker is summed up in two intuitive metrics, viz. Multi-Object Tracking Accuracy (MOTA) and Multi-Object Tracking Precision (MOTP). While MOTA is concerned with tracking accuracy, MOTP deals with object localization precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. System Overview</head><p>The proposed approach is a tracking-by-detection approach and hence assumes per-frame bounding box detections as input. We choose two recent object detectors -Recurrent Rolling Convolution (RRC) <ref type="bibr" target="#b14">[15]</ref> and SubCNN <ref type="bibr" target="#b15">[16]</ref>. Each of these detectors provides multiple detections per frame. A threshold is applied on the detection scores and those detections whose confidence scores are lower than the threshold are pruned. In addition to this, we run a non-maxima suppression (NMS) scheme to subdue multiple detections around the same object. These detections are used to compute pairwise costs as outlined in the previous section. These pairwise costs constitute a cost matrix that is used for a bipartite matching algorithm that associates detections across two frames. In practice, bipartite matching is performed using the O(n 3 ) Hungarian algorithm <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Approaches Considered</head><p>We evaluate the proposed framework against the current best competitors on the KITTI Tracking dataset. We consider approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance Evaluation</head><p>We evaluate the performance of our approach on the current best competitors on the KITTI Tracking Benchmark. While <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b12">[13]</ref> rely on complex handcrafted costs, <ref type="bibr" target="#b0">[1]</ref> learns all unary and pairwise costs that are input to a network flow based tracker. Moreover, the data association steps of <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b12">[13]</ref> rely on complex optimization routines. The proposed approach is also evaluated on the KITTI Tracking evaluation sever. <ref type="table" target="#tab_0">Table I</ref>,where we compare our two-frame based approach with the other competitors using the best performing object detector in the form of <ref type="bibr" target="#b14">[15]</ref> and a judicious combination of such appearance, 3D, pose and shape cues best possible results on KITTI training sequence are achieved in terms of MOTA (91.4%) and MOTP (89.84%). Although our method suffers from ID switches and fragmentations, this is typical of online trackers; more so of two-frame greedy trackers. Using the proposed pairwise costs in a slightly more sophisticated tracker such as <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref> will naturally reduce ID switches and fragmentations also. <ref type="table" target="#tab_0">Table II</ref>,where we compare our two-frame based approach with the other published approaches on the KITTI Tracking online server. We outperform the next best competitor by a margin of (6%) on the test set, achieving state of the art results in the form of MOTA (84.24%), MOTP (85.73%), MT (73.23%) and ML (2.77%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Study</head><p>We then perform a thorough ablation analysis of various cues used for computing pairwise costs across two distinct object detectors: RRC <ref type="bibr" target="#b14">[15]</ref> and SubCNN <ref type="bibr" target="#b15">[16]</ref>. Results are summarized in <ref type="table" target="#tab_0">Table III</ref>. This analysis captures the importance of each of the proposed cue and demonstrates that the combination of all these is crucial for overall performance. Notice how each cue improves the performance of our system in terms of MOTA ,ID switches and fragmentations. Even with underperforming detectors such as <ref type="bibr" target="#b15">[16]</ref>, there is a tangible performance boost by using a combination of monocular 3D cues. This is portrayed in ablation analysis of SubCNN detectors in <ref type="table" target="#tab_0">Table III</ref>. Furthermore the repeatability of performance gain using these novel cues over any baseline detection methods is also delineated.</p><p>There exist subsequences where the role played by shape and pose cues become relevant. While in a typical road scene involving lane driving the pose cues are not discriminatory (as the vehicles are aligned with the lane direction), they become discerning enough in areas such as intersections, round abouts where pose and viewpoint changes are heterogeneous. This is showcase in <ref type="table" target="#tab_0">Table IV</ref>. Here, we select particular frames from the KITTI Tracking dataset, which have images containing cars moving at intersections, which captures different viewpoints and shapes of cars. Using detections from a weak detector <ref type="bibr" target="#b15">[16]</ref> and a simplistic combination 2D-2D cues along with shape and pose cue of the car performs better than the stand alone 2D cue, for sequences which have cars with various viewpoints over the frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Qualitative Results</head><p>Finally, we present qualitative results from challenging sequences in <ref type="figure" target="#fig_2">Fig.4 and Fig.5</ref>. These results clearly indicate the ability of the proposed pairwise costs to disambiguate and track across viewpoint variations, clutter, and varying relative motion between the camera and the target.   For example the first column of <ref type="figure" target="#fig_2">Fig 4</ref> shows cars occluded on either sides of the road accurately tracked almost till the horizon. Whereas the second column shows efficient tracking of cars at varying depths and varying poses in an intersection while the third column shows precise tracking of occluding cars as well as a car that is being overtaken from the right by the ego car. In fact in the 4th frame a very small portion of the car is visible yet accurately tracked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Summary of Results</head><p>The cornerstone of this effort is that single view monocular 3D cues obtained though formalisms developed on the basis of single view geometry can be effectively exploited to track vehicles in challenging scenes. This gets illustrated in the various tabulations of this section.</p><p>Table I depicts significant improvements over many of the current state of the art methods with a tracking accuracy in excess of 90%. We test our approach on the KITTI Tracking online server. Table II depicts significant improvements over the published approaches, with tracking accuracy over 84%.</p><p>Whereas the ablation studies in <ref type="table" target="#tab_0">Table III</ref> does showcase the repeatability of 3D cues in improving the baseline appearance only tracking over detectors. While not as significant as in <ref type="bibr" target="#b14">[15]</ref> baseline improvement over SubNN object detector <ref type="bibr" target="#b15">[16]</ref> can be gleaned from <ref type="table" target="#tab_0">Table III</ref>. The improvement in ID switches and fragmentations can also be seen over both detector baselines as a consequence of the 3D cues. <ref type="table" target="#tab_0">Table IV</ref> shows the relevance of pose and shape cues over a subsequence where association costs due to such cues improves baseline performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>Most state of the art tracking formalisms have not explored the role of 3D cues and when they have done those cues have been due to immediately available stereo depth. This paper showcased for the first time monocular 3D cues obtained from single view geometry along with pose and shape cues results in the best tracking performance on popular object tracking training datasets. These cues result in a set of simple, intuitive pairwise costs for multi-object tracking in a tracking-by-detection setting. Despite being more difficult to compute than ready made 3D depth data, monocular 3D cues have a role to play in diverse on road applications including object and vehicle tracking. Apart from the quantitative, qualitative results too signify its advantage in challenging scenes that involve considerable occlusions, minimal appearance of the object in the scene and objects that are far enough that they appear on the horizon. Although we demonstrated results using a simple Hungarian method based tracker, incorporation of sophisticated trackers would result in even higher performance boosts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>to the top-left corner of the detection box in the image, w i f is the bounding box width, h i f is the bounding box height,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Weighted combination of the features captured by the hourglass network. Such a descriptor is able to capture the dissimilarity between different detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results on some challenging sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results on some challenging equences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RESULTS</head><label>I</label><figDesc>ON THE KITTI TRACKING TRAIN SET. TRACKING ACCURACY(MOTA) AND PRECISION(MOTP), MOSTLY TRACKED(MT),PARTLY TRACKED(PT), MOSTLY LOST(ML) , TRUE POSITIVES(TP) , FALSE POSITIVES(FP), ID-SWITCHES(IDS), FRAGMENTATION(FRAG)</figDesc><table><row><cell></cell><cell cols="3">MOTA MOTP Recall</cell><cell>Precision</cell><cell>MT</cell><cell>PT</cell><cell>ML</cell><cell>TP</cell><cell>FP</cell><cell>IDS</cell><cell>FRAG</cell></row><row><cell>CIWT [21]</cell><cell>74.38</cell><cell>82.85</cell><cell>-</cell><cell>-</cell><cell cols="3">49.59 40.68 9.80</cell><cell>-</cell><cell>-</cell><cell>26</cell><cell>131</cell></row><row><cell>Ours (On split of [21])</cell><cell>91.75</cell><cell>89.90</cell><cell>94.83</cell><cell>98.62</cell><cell>88.61</cell><cell>10.39</cell><cell>0.9</cell><cell>9814</cell><cell>137</cell><cell>93</cell><cell>151</cell></row><row><cell>Deep Network Flow [1]</cell><cell>74.11</cell><cell>-</cell><cell>84.74</cell><cell>92.05</cell><cell>61.73</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29</cell><cell>335</cell></row><row><cell>NOMT [6]</cell><cell>73.07</cell><cell>-</cell><cell>85.07</cell><cell>90.92</cell><cell>61.73</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43</cell><cell>386</cell></row><row><cell>SSP [13]</cell><cell>67</cell><cell>79</cell><cell>-</cell><cell>-</cell><cell>41</cell><cell>-</cell><cell>9</cell><cell>-</cell><cell>-</cell><cell>194</cell><cell>977</cell></row><row><cell>Ours</cell><cell>91.4</cell><cell>89.83</cell><cell>94.65</cell><cell>98.47</cell><cell>87.76</cell><cell>10.63</cell><cell>1.59</cell><cell cols="2">25309 392</cell><cell>232</cell><cell>423</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">RESULTS ON THE KITTI TRACKING TEST SET. FOR MORE DETAILS, VISIT</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">H T T P://W W W.C V L I B S.N E T/D A T A S E T S/K I T T I/E V A L_T R A C K I N G.P H P</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MOTA MOTP</cell><cell>MT</cell><cell>ML</cell><cell>IDS</cell><cell>FRAG</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">CIWT [21]</cell><cell>75.39</cell><cell>79.25</cell><cell cols="2">49.85 10.31</cell><cell>165</cell><cell>660</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">SCEA [23]</cell><cell>75.58</cell><cell>79.39</cell><cell cols="2">53.08 11.54</cell><cell>104</cell><cell>448</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MDP [24]</cell><cell>76.59</cell><cell>82.10</cell><cell cols="2">52.15 13.38</cell><cell>130</cell><cell>387</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">LP-SSVM [3]</cell><cell>77.63</cell><cell>77.80</cell><cell>56.31</cell><cell>8.46</cell><cell>62</cell><cell>539</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">NOMT [6]</cell><cell>78.15</cell><cell>79.46</cell><cell cols="2">57.23 13.23</cell><cell>31</cell><cell>207</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MCMOT-CPD [2]</cell><cell>78.90</cell><cell>82.13</cell><cell cols="2">52.31 11.69</cell><cell>228</cell><cell>536</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Ours(RRC-IIITH)</cell><cell>84.24</cell><cell>85.73</cell><cell>73.23</cell><cell>2.77</cell><cell>468</cell><cell>944</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDY. COMPARISION ACROSS VARIOUS CUES USED FOR PAIRWISE COST COMPUTATION AND CHOICE OF OBJECT DETECTOR. (APP -</figDesc><table><row><cell></cell><cell></cell><cell cols="3">APPEARANCE COST)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cue(s)</cell><cell>MOTA</cell><cell cols="3">MOTP Recall Precision</cell><cell>MT</cell><cell>PT</cell><cell>ML</cell><cell>TP</cell><cell>FP</cell><cell>IDS</cell><cell>FRAG</cell></row><row><cell>SubCNN + App</cell><cell>66.18</cell><cell>82.60</cell><cell>86.45</cell><cell>89.29</cell><cell cols="5">71.63 24.46 3.90 23563 2825</cell><cell>708</cell><cell>1100</cell></row><row><cell>SubCNN + 3D-2D</cell><cell>68.24</cell><cell>82.60</cell><cell>86.45</cell><cell>89.29</cell><cell cols="5">71.63 24.46 3.90 23563 2825</cell><cell>429</cell><cell>829</cell></row><row><cell>SubCNN + 3D-3D</cell><cell>69.36</cell><cell>82.60</cell><cell>86.45</cell><cell>89.29</cell><cell cols="5">71.63 24.46 3.90 23563 2825</cell><cell>377</cell><cell>778</cell></row><row><cell>SubCNN + 3D-2D + App</cell><cell>70.96</cell><cell>82.60</cell><cell>86.45</cell><cell>89.29</cell><cell cols="5">71.63 24.46 3.90 23563 2825</cell><cell>472</cell><cell>870</cell></row><row><cell>SubCNN + 3D-3D + 3D-2D + App + Shape-Pose</cell><cell>71.52</cell><cell>82.60</cell><cell>86.45</cell><cell>89.29</cell><cell>71.63</cell><cell>24.46</cell><cell>3.90</cell><cell>23563</cell><cell>2825</cell><cell>338</cell><cell>740</cell></row><row><cell>RRC + App</cell><cell>80.53</cell><cell>89.83</cell><cell>94.62</cell><cell>98.47</cell><cell cols="4">87.76 10.63 1.59 25309</cell><cell>392</cell><cell>2863</cell><cell>3022</cell></row><row><cell>RRC + 3D-2D</cell><cell>86.91</cell><cell>89.83</cell><cell>94.65</cell><cell>98.47</cell><cell cols="4">87.76 10.63 1.59 25309</cell><cell>392</cell><cell>1328</cell><cell>1507</cell></row><row><cell>RRC + 3D-3D</cell><cell>87.56</cell><cell>89.83</cell><cell>94.65</cell><cell>98.47</cell><cell cols="4">87.76 10.63 1.59 25309</cell><cell>392</cell><cell>1170</cell><cell>1333</cell></row><row><cell>RRC + 3D-2D + App</cell><cell>89.65</cell><cell>89.83</cell><cell>94.65</cell><cell>98.47</cell><cell cols="4">87.76 10.63 1.59 25309</cell><cell>392</cell><cell>668</cell><cell>849</cell></row><row><cell>RRC + 3D-3D + 3D-2D + App + Shape-Pose</cell><cell>91.4</cell><cell>89.83</cell><cell>94.65</cell><cell>98.47</cell><cell>87.76</cell><cell>10.63</cell><cell>1.59</cell><cell>25309</cell><cell>392</cell><cell>232</cell><cell>423</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>USING SHAPE AND POSE ALONG WITH OTHER CUES</figDesc><table><row><cell></cell><cell cols="2">MOTA MOTP</cell><cell>Recall</cell><cell>Precision</cell><cell>MT</cell><cell cols="2">PT ML</cell><cell>TP</cell><cell>FP</cell><cell>IDS</cell><cell>FRAG</cell></row><row><cell>w/o Shape and Pose</cell><cell>55.3</cell><cell>86.01</cell><cell>98.9648</cell><cell>84.75</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>478</cell><cell>86</cell><cell>5</cell><cell>7</cell></row><row><cell>with Shape and Pose</cell><cell>57.29</cell><cell>86.01</cell><cell>98.9648</cell><cell>84.75</cell><cell>100</cell><cell>0</cell><cell>0</cell><cell>478</cell><cell>86</cell><cell>1</cell><cell>5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep network flow for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-class multi-object tracking using changing point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdenee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning optimal parameters for multitarget tracking with contextual interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reconstructing vehicles from a single image: Shape priors for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Robotics and Automation</title>
		<meeting>the IEEE Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shape priors for realtime monocular object localization in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<publisher>In Press</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global data association for multiobject tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discrete-continuous optimization for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gmmcp tracker: Globally optimal generalized maximum multi clique problem for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Modiri</forename><surname>Assari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Followme: Efficient online min-cost flow tracking with bounded memory and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A general framework for tracking multiple people from a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X J W J</forename><surname>Qiongyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Lixu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Subcategory-aware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in Applications of Computer Vision (WACV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Everybody needs somebody: Modeling social and grouping behavior on a linear programming multiple people tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On pairwise costs for network flow multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust multiperson tracking from a mobile platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1831" to="1846" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combined image-and world-space tracking in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint sfm and detection cues for monocular 3d localization in road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online multiobject tracking via structural constraint event aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to track: Online multiobject tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval Research Logistics (NRL)</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
