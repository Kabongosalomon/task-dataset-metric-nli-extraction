<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointPillars: Fast Encoders for Object Detection from Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
							<email>sourabh@nutonomy.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
							<email>holger@nutonomy.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
							<email>lubing@nutonomy.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
							<email>jiong.yang@nutonomy.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
						</author>
						<title level="a" type="main">PointPillars: Fast Encoders for Object Detection from Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird's eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 -4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deploying autonomous vehicles (AVs) in urban environments poses a difficult technological challenge. Among other tasks, AVs need to detect and track moving objects such as vehicles, pedestrians, and cyclists in realtime. To achieve this, autonomous vehicles rely on several sensors out of which the lidar is arguably the most important. A lidar uses a laser scanner to measure the distance to the environment, thus generating a sparse point cloud representation. Traditionally, a lidar robotics pipeline interprets such point clouds as object detections through a bottomup pipeline involving background subtraction, followed by spatiotemporal clustering and classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref>.  <ref type="bibr" target="#b4">[5]</ref> test set. Lidar-only methods drawn as blue circles; lidar &amp; vision methods drawn as red squares. Also drawn are top methods from the KITTI leaderboard: M : MV3D <ref type="bibr" target="#b1">[2]</ref>, A AVOD <ref type="bibr" target="#b10">[11]</ref>, C : ContFuse <ref type="bibr" target="#b14">[15]</ref>, V : VoxelNet <ref type="bibr" target="#b30">[31]</ref>, F : Frustum PointNet <ref type="bibr" target="#b20">[21]</ref>, S : SECOND <ref type="bibr" target="#b27">[28]</ref>, P+ PIXOR++ <ref type="bibr" target="#b28">[29]</ref>. PointPillars outperforms all other lidar-only methods in terms of both speed and accuracy by a large margin. It also outperforms all fusion based method except on pedestrians. Similar performance is achieved on the 3D metric ( <ref type="table">Table 2</ref>).</p><p>Following the tremendous advances in deep learning methods for computer vision, a large body of literature has investigated to what extent this technology could be applied towards object detection from lidar point clouds <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref>. While there are many similarities between the modalities, there are two key differences: 1) the point cloud is a sparse representation, while an image is dense and 2) the point cloud is 3D, while the image is 2D. As a result object detection from point clouds does not trivially lend itself to standard image convolutional pipelines.</p><p>Some early works focus on either using 3D convolu-tions <ref type="bibr" target="#b2">[3]</ref> or a projection of the point cloud into the image <ref type="bibr" target="#b13">[14]</ref>. Recent methods tend to view the lidar point cloud from a bird's eye view <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>. This overhead perspective offers several advantages such as lack of scale ambiguity and the near lack of occlusion. However, the bird's eye view tends to be extremely sparse which makes direct application of convolutional neural networks impractical and inefficient. A common workaround to this problem is to partition the ground plane into a regular grid, for example 10 x 10 cm, and then perform a hand crafted feature encoding method on the points in each grid cell <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. However, such methods may be sub-optimal since the hard-coded feature extraction method may not generalize to new configurations without significant engineering efforts. To address these issues, and building on the PointNet design developed by Qi et al. <ref type="bibr" target="#b21">[22]</ref>, VoxelNet <ref type="bibr" target="#b30">[31]</ref> was one of the first methods to truly do endto-end learning in this domain. VoxelNet divides the space into voxels, applies a PointNet to each voxel, followed by a 3D convolutional middle layer to consolidate the vertical axis, after which a 2D convolutional detection architecture is applied. While the VoxelNet performance is strong, the inference time, at 4.4 Hz, is too slow to deploy in real time.</p><p>Recently SECOND <ref type="bibr" target="#b27">[28]</ref> improved the inference speed of VoxelNet but the 3D convolutions remain a bottleneck.</p><p>In this work we propose PointPillars: a method for object detection in 3D that enables end-to-end learning with only 2D convolutional layers. PointPillars uses a novel encoder that learn features on pillars (vertical columns) of the point cloud to predict 3D oriented boxes for objects. There are several advantages of this approach. First, by learning features instead of relying on fixed encoders, PointPillars can leverage the full information represented by the point cloud. Further, by operating on pillars instead of voxels there is no need to tune the binning of the vertical direction by hand. Finally, pillars are highly efficient because all key operations can be formulated as 2D convolutions which are extremely efficient to compute on a GPU. An additional benefit of learning features is that PointPillars requires no hand-tuning to use different point cloud configurations. For example, it can easily incorporate multiple lidar scans, or even radar point clouds.</p><p>We evaluated our PointPillars network on the public KITTI detection challenges which require detection of cars, pedestrians, and cyclists in either the bird's eye view (BEV) or 3D <ref type="bibr" target="#b4">[5]</ref>. While our PointPillars network is trained using only lidar point clouds, it dominates the current state of the art including methods that use lidar and images, thus establishing new standards for performance on both BEV and 3D detection ( <ref type="table">Table 1 and Table 2</ref>). At the same time PointPillars runs at 62 Hz, which is orders of magnitude faster than previous art. PointPillars further enables a trade off between speed and accuracy; in one setting we match state of the art performance at over 100 Hz ( <ref type="figure" target="#fig_4">Figure 5</ref>). We have also released code (https://github.com/nutonomy/second.pytorch) that can reproduce our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>We start by reviewing recent work in applying convolutional neural networks toward object detection in general, and then focus on methods specific to object detection from lidar point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Object detection using CNNs</head><p>Starting with the seminal work of Girshick et al. <ref type="bibr" target="#b5">[6]</ref> it was established that convolutional neural network (CNN) architectures are state of the art for detection in images. The series of papers that followed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref> advocate a two-stage approach to this problem, where in the first stage a region proposal network (RPN) suggests candidate proposals. Cropped and resized versions of these proposals are then classified by a second stage network. Two-stage methods dominated the important vision benchmark datasets such as COCO <ref type="bibr" target="#b16">[17]</ref> over single-stage architectures originally proposed by Liu et al. <ref type="bibr" target="#b17">[18]</ref>. In a single-stage architecture a dense set of anchor boxes is regressed and classified in a single stage into a set of predictions providing a fast and simple architecture. Recently Lin et al. <ref type="bibr" target="#b15">[16]</ref> convincingly argued that with their proposed focal loss function a single stage method is superior to two-stage methods, both in terms of accuracy and runtime. In this work, we use a single stage method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Object detection in lidar point clouds</head><p>Object detection in point clouds is an intrinsically three dimensional problem. As such, it is natural to deploy a 3D convolutional network for detection, which is the paradigm of several early works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. While providing a straightforward architecture, these methods are slow; e.g. Engelcke et al. <ref type="bibr" target="#b2">[3]</ref> require 0.5s for inference on a single point cloud. Most recent methods improve the runtime by projecting the 3D point cloud either onto the ground plane <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref> or the image plane <ref type="bibr" target="#b13">[14]</ref>. In the most common paradigm the point cloud is organized in voxels and the set of voxels in each vertical column is encoded into a fixed-length, handcrafted, feature encoding to form a pseudo-image which can be processed by a standard image detection architecture. Some notable works here include MV3D <ref type="bibr" target="#b1">[2]</ref>, AVOD <ref type="bibr" target="#b10">[11]</ref>, PIXOR <ref type="bibr" target="#b29">[30]</ref> and Complex YOLO <ref type="bibr" target="#b25">[26]</ref> which all use variations on the same fixed encoding paradigm as the first step of their architectures. The first two methods additionally fuse the lidar features with image features to create a multimodal detector. The fusion step used in MV3D and AVOD forces them to use two-stage detection pipelines, while PIXOR and Complex YOLO use single stage pipelines.  <ref type="figure">Figure 2</ref>. Network overview. The main components of the network are a Pillar Feature Network, Backbone, and SSD Detection Head. See Section 2 for more details. The raw point cloud is converted to a stacked pillar tensor and pillar index tensor. The encoder uses the stacked pillars to learn a set of features that can be scattered back to a 2D pseudo-image for a convolutional neural network. The features from the backbone are used by the detection head to predict 3D bounding boxes for objects. Note: here we show the backbone dimensions for the car network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point cloud</head><p>In their seminal work Qi et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> proposed a simple architecture, PointNet, for learning from unordered point sets, which offered a path to full end-to-end learning. Vox-elNet <ref type="bibr" target="#b30">[31]</ref> is one of the first methods to deploy PointNets for object detection in lidar point clouds. In their method, PointNets are applied to voxels which are then processed by a set of 3D convolutional layers followed by a 2D backbone and a detection head. This enables end-to-end learning, but like the earlier work that relied on 3D convolutions, Voxel-Net is slow, requiring 225ms inference time (4.4 Hz) for a single point cloud. Another recent method, Frustum Point-Net <ref type="bibr" target="#b20">[21]</ref>, uses PointNets to segment and classify the point cloud in a frustum generated from projecting a detection on an image into 3D. Frustum PointNet's achieved high benchmark performance compared to other fusion methods, but its multi-stage design makes end-to-end learning impractical. Very recently SECOND <ref type="bibr" target="#b27">[28]</ref> offered a series of improvements to VoxelNet resulting in stronger performance and a much improved speed of 20 Hz. However, they were unable to remove the expensive 3D convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>• We propose a novel point cloud encoder and network, PointPillars, that operates on the point cloud to enable end-to-end training of a 3D object detection network.</p><p>• We show how all computations on pillars can be posed as dense 2D convolutions which enables inference at 62 Hz; a factor of 2-4 times faster than other methods.</p><p>• We conduct experiments on the KITTI dataset and demonstrate state of the art results on cars, pedestrians, and cyclists on both BEV and 3D benchmarks.</p><p>• We conduct several ablation studies to examine the key factors that enable a strong detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PointPillars Network</head><p>PointPillars accepts point clouds as input and estimates oriented 3D boxes for cars, pedestrians and cyclists. It consists of three main stages ( <ref type="figure">Figure 2</ref>): (1) A feature encoder network that converts a point cloud to a sparse pseudoimage; (2) a 2D convolutional backbone to process the pseudo-image into high-level representation; and (3) a detection head that detects and regresses 3D boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pointcloud to Pseudo-Image</head><p>To apply a 2D convolutional architecture, we first convert the point cloud to a pseudo-image.</p><p>We denote by l a point in a point cloud with coordinates x, y, z and reflectance r. As a first step the point cloud is discretized into an evenly spaced grid in the x-y plane, creating a set of pillars P with |P| = B. Note that there is no need for a hyper parameter to control the binning in the z dimension. The points in each pillar are then augmented with x c , y c , z c , x p and y p where the c subscript denotes distance to the arithmetic mean of all points in the pillar and the p subscript denotes the offset from the pillar x, y center. The augmented lidar point l is now D = 9 dimensional.</p><p>The set of pillars will be mostly empty due to sparsity of the point cloud, and the non-empty pillars will in general have few points in them. For example, at 0.16 2 m 2 bins the point cloud from an HDL-64E Velodyne lidar has 6k-9k non-empty pillars in the range typically used in KITTI for ∼ 97% sparsity. This sparsity is exploited by imposing a limit both on the number of non-empty pillars per sample (P ) and on the number of points per pillar (N ) to create a dense tensor of size (D, P, N ). If a sample or pillar holds too much data to fit in this tensor the data is randomly sampled. Conversely, if a sample or pillar has too little data to populate the tensor, zero padding is applied.  Next, we use a simplified version of PointNet where, for each point, a linear layer is applied followed by Batch-Norm <ref type="bibr" target="#b9">[10]</ref> and ReLU <ref type="bibr" target="#b18">[19]</ref> to generate a (C, P, N ) sized tensor. This is followed by a max operation over the channels to create an output tensor of size (C, P ). Note that the linear layer can be formulated as a 1x1 convolution across the tensor resulting in very efficient computation.</p><p>Once encoded, the features are scattered back to the original pillar locations to create a pseudo-image of size (C, H, W ) where H and W indicate the height and width of the canvas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Backbone</head><p>We use a similar backbone as <ref type="bibr" target="#b30">[31]</ref> and the structure is shown in <ref type="figure">Figure 2</ref>. The backbone has two sub-networks: one top-down network that produces features at increasingly small spatial resolution and a second network that performs upsampling and concatenation of the top-down features. The top-down backbone can be characterized by a series of blocks Block(S, L, F ). Each block operates at stride S (measured relative to the original input pseudo-image). A block has L 3x3 2D conv-layers with F output channels, each followed by BatchNorm and a ReLU. The first convolution inside the layer has stride S Sin to ensure the block operates on stride S after receiving an input blob of stride S in . All subsequent convolutions in a block have stride 1. The final features from each top-down block are combined through upsampling and concatenation as follows. First, the features are upsampled, Up(S in , S out , F ) from an initial stride S in to a final stride S out (both again measured wrt. original pseudo-image) using a transposed 2D convolution with F final features. Next, BatchNorm and ReLU is applied to the upsampled features. The final output features are a concatenation of all features that originated from different strides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Detection Head</head><p>In this paper, we use the Single Shot Detector (SSD) <ref type="bibr" target="#b17">[18]</ref> setup to perform 3D object detection. Similar to SSD, we match the priorboxes to the ground truth using 2D Intersection over Union (IoU) <ref type="bibr" target="#b3">[4]</ref>. Bounding box height and elevation were not used for matching; instead given a 2D match, the height and elevation become additional regression targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation Details</head><p>In this section we describe our network parameters and the loss function that we optimize for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network</head><p>Instead of pre-training our networks, all weights were initialized randomly using a uniform distribution as in <ref type="bibr" target="#b7">[8]</ref>.</p><p>The encoder network has C = 64 output features. The car and pedestrian/cyclist backbones are the same except for the stride of the first block (S = 2 for car, S = 1 for pedestrian/cyclist). Both network consists of three blocks, Block1(S, 4, C), Block2(2S, 6, 2C), and Block3(4S, 6, 4C). Each block is upsampled by the following upsampling steps: Up1(S, S, 2C), Up2(2S, S, 2C) and Up3(4S, S, 2C). Then the features of Up1, Up2 and Up3 are concatenated together to create 6C features for the detection head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss</head><p>We use the same loss functions introduced in SEC-OND <ref type="bibr" target="#b27">[28]</ref>. Ground truth boxes and anchors are defined by (x, y, z, w, l, h, θ). The localization regression residuals between ground truth and anchors are defined by:</p><formula xml:id="formula_0">∆x = x gt − x a d a , ∆y = y gt − y a d a , ∆z = z gt − z a h a ∆w = log w gt w a , ∆l = log l gt l a , ∆h = log h gt h a ∆θ = sin θ gt − θ a ,</formula><p>where x gt and x a are respectively the ground truth and anchor boxes and d a = (w a ) 2 + (l a ) 2 . The total localization loss is:</p><formula xml:id="formula_1">L loc = b∈(x,y,z,w,l,h,θ) SmoothL1 (∆b)</formula><p>Since the angle localization loss cannot distinguish flipped boxes, we use a softmax classification loss on the discretized directions <ref type="bibr" target="#b27">[28]</ref>, L dir , which enables the network to learn the heading.</p><p>For the object classification loss, we use the focal loss <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_2">L cls = −α a (1 − p a ) γ log p a ,</formula><p>where p a is the class probability of an anchor. We use the original paper settings of α = 0.25 and γ = 2. The total loss is therefore:</p><formula xml:id="formula_3">L = 1 Npos (β loc L loc + β cls L cls + β dir L dir ) ,</formula><p>where N pos is the number of positive anchors and β loc = 2, β cls = 1, and β dir = 0.2.</p><p>To optimize the loss function we use the Adam optimizer with an initial learning rate of 2 * 10 −4 and decay the learning rate by a factor of 0.8 every 15 epochs and train for 160 epochs. We use a batch size of 2 for validation set and 4 for our test submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>In this section we present our experimental setup, including dataset, experimental settings and data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>All experiments use the KITTI object detection benchmark dataset <ref type="bibr" target="#b4">[5]</ref>, which consists of samples that have both lidar point clouds and images. We only train on lidar point clouds, but compare with fusion methods that use both lidar and images. The samples are originally divided into 7481 training and 7518 testing samples. For experimental studies we split the official training into 3712 training samples and 3769 validation samples <ref type="bibr" target="#b0">[1]</ref>, while for our test submission we created a minival set of 784 samples from the validation set and trained on the remaining 6733 samples. The KITTI benchmark requires detections of cars, pedestrians, and cyclists. Since the ground truth objects were only annotated if they are visible in the image, we follow the standard convention <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref> of only using lidar points that project into the image. Following the standard literature practice on KITTI <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28]</ref>, we train one network for cars and one network for both pedestrians and cyclists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Settings</head><p>Unless explicitly varied in an experimental study, we use an xy resolution: 0.16 m, max number of pillars (P ): 12000, and max number of points per pillar (N ): 100.</p><p>We use the same anchors and matching strategy as <ref type="bibr" target="#b30">[31]</ref>. Each class anchor is described by a width, length, height, and z center, and is applied at two orientations: 0 and 90 degrees. Anchors are matched to ground truth using the 2D IoU with the following rules. A positive match is either the highest with a ground truth box, or above the positive match threshold, while a negative match is below the negative threshold. All other anchors are ignored in the loss.</p><p>At inference time we apply axis aligned non maximum suppression (NMS) with an overlap threshold of 0.5 IoU. This provides similar performance compared to rotational NMS, but is much faster.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Augmentation</head><p>Data augmentation is critical for good performance on the KITTI benchmark <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>First, following SECOND <ref type="bibr" target="#b27">[28]</ref>, we create a lookup table of the ground truth 3D boxes for all classes and the associated point clouds that falls inside these 3D boxes. Then for each sample, we randomly select 15, 0, 8 ground truth samples for cars, pedestrians, and cyclists respectively and place them into the current point cloud. We found these settings to perform better than the proposed settings <ref type="bibr" target="#b27">[28]</ref>.</p><p>Next, all ground truth boxes are individually augmented. Each box is rotated (uniformly drawn from [−π/20, π/20]) and translated (x, y, and z independently drawn from N (0, 0.25)) to further enrich the training set.</p><p>Finally, we perform two sets of global augmentations that are jointly applied to the point cloud and all boxes. First, we apply random mirroring flip along the x axis <ref type="bibr" target="#b29">[30]</ref>, then a global rotation and scaling <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b27">28]</ref>. Finally, we apply a global translation with x, y, z drawn from N (0, 0.2) to simulate localization noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section we present results of our PointPillars method and compare to the literature. Quantitative Analysis. All detection results are measured using the official KITTI evaluation detection metrics which are: bird's eye view (BEV), 3D, 2D, and average orientation similarity (AOS). The 2D detection is done in the image plane and average orientation similarity assesses the average orientation (measured in BEV) similarity for 2D detections. The KITTI dataset is stratified into easy, moderate, and hard difficulties, and the official KITTI leaderboard is ranked by performance on moderate. <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref>, PointPillars outperforms all published methods with respect to mean average precision (mAP). Compared to lidar-only methods, Point-Pillars achieves better results across all classes and difficulty strata except for the easy car stratum. It also outperforms fusion based methods on cars and cyclists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>While PointPillars predicts 3D oriented boxes, the BEV and 3D metrics do not take orientation into account. Orientation is evaluated using AOS <ref type="bibr" target="#b4">[5]</ref>, which requires projecting the 3D box into the image, performing 2D detection matching, and then assessing the orientation of these matches. The performance of PointPillars on AOS significantly exceeds in all strata as compared to the only two 3D detection methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> that predict oriented boxes <ref type="table">(Table 3</ref>). In general, image only methods perform best on 2D detection since the 3D projection of boxes into the image can result in loose boxes depending on the 3D pose. Despite this, Point-Pillars moderate cyclist AOS of 68.16 outperforms the best image based method <ref type="bibr" target="#b26">[27]</ref>.</p><p>For comparison to other methods on val, we note that our network achieved BEV AP of (87.98, 63.55, 69.71) and 3D AP of (77.98, 57.86, 66.02) for the moderate strata of cars, pedestrians, and cyclists respectively.  <ref type="table">Table 3</ref>. Results on the KITTI test average orientation similarity (AOS) detection benchmark. SubCNN is the best performing image only method, while AVOD-FPN, SECOND, and PointPillars are the only 3D object detectors that predict orientation.</p><p>Qualitative Analysis. We provide qualitative results in <ref type="figure" target="#fig_1">Figure 3</ref> and 4. While we only train on lidar point clouds, for ease of interpretation we visualize the 3D bounding box predictions from the BEV and image perspective. <ref type="figure" target="#fig_1">Figure 3</ref> shows our detection results, with tight oriented 3D bounding boxes. The predictions for cars are particularly accurate and common failure modes include false negatives on difficult samples (partially occluded or faraway objects) or false positives on similar classes (vans or trams). Detecting pedestrians and cyclists is more challenging and leads to some interesting failure modes. Pedestrians and cyclists are commonly misclassified as each other (see <ref type="figure" target="#fig_2">Figure 4a</ref> for a standard example and <ref type="figure" target="#fig_2">Figure 4d</ref> for the combination of pedestrian and table classified as a cyclist). Additionally, pedestrians are easily confused with narrow vertical features of the environment such as poles or tree trunks (see <ref type="figure" target="#fig_2">Figure 4b</ref>). In some cases we correctly detect objects that are missing in the ground truth annotations (see <ref type="figure" target="#fig_2">Figure 4c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Realtime Inference</head><p>As indicated by our results <ref type="table">(Table 1</ref> and <ref type="figure" target="#fig_4">Figure 5</ref>), Point-Pillars represent a significant improvement in terms of inference runtime. In this section we break down our runtime and consider the different design choices that enabled this speedup. We focus on the car network, but the pedestrian and bicycle network runs at a similar speed since the smaller range cancels the effect of the backbone operating at lower strides. All runtimes are measured on a desktop with an Intel i7 CPU and a 1080ti GPU.</p><p>The main inference steps are as follows. First, the point cloud is loaded and filtered based on range and visibility in the images (1.4 ms). Then, the points are organized in pillars and decorated (2.7 ms). Next, the PointPillar tensor is uploaded to the GPU (2.9 ms), encoded (1.3 ms), scattered to the pseudo-image (0.1 ms), and processed by the backbone and detection heads (7.7 ms). Finally NMS is applied on the CPU (0.1 ms) for a total runtime of 16.2 ms. Encoding. The key design to enable this runtime is the PointPilar encoding. For example, at 1.3 ms it is 2 orders of magnitude faster than the VoxelNet encoder (190 ms) <ref type="bibr" target="#b30">[31]</ref>. Recently, SECOND proposed a faster sparse version of the VoxelNet encoder for a total network runtime of 50 ms. They did not provide a runtime analysis, but since the rest of their architecture is similar to ours, it suggests that the encoder is still significantly slower; in their open source implementation 1 the encoder requires 48 ms. Slimmer Design. We opt for a single PointNet in our encoder, compared to 2 sequential PointNets suggested by <ref type="bibr" target="#b30">[31]</ref>. This reduced our runtime by 2.5 ms in our PyTorch runtime. The number of dimensions of the first block were also lowered 64 to match the encoder output size, which reduced the runtime by 4.5 ms. Finally, we saved another 3.9 ms by cutting the output dimensions of the upsampled feature layers by half to 128. Neither of these changes affected detection performance.</p><p>TensorRT. While all our experiments were performed in PyTorch <ref type="bibr" target="#b19">[20]</ref>, the final GPU kernels for encoding, backbone and detection head were built using NVIDIA TensorRT, which is a library for optimized GPU inference. Switching to TensorRT gave a 45.5% speedup from the PyTorch pipeline which runs at 42.4 Hz. The Need for Speed. As seen in <ref type="figure" target="#fig_4">Figure 5</ref>, PointPillars can achieve 105 Hz with limited loss of accuracy. While it could be argued that such runtime is excessive since a lidar typically operates at 20 Hz, there are two key things to keep in mind. First, due to an artifact of the KITTI ground truth annotations, only lidar points which projected into the front image are utilized, which is only ∼ 10% of the entire point cloud. However, an operational AV needs to view the full environment and process the complete point cloud, significantly increasing all aspects of the runtime. Second, timing measurements in the literature are typically done on a high-power desktop GPU. However, an operational AV may instead use embedded GPUs or embedded compute which may not have the same throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Ablation Studies</head><p>In this section we provide ablation studies and discuss our design choices compared to the recent literature. the CNN backbone). To quantify this effect we performed a sweep across grid sizes. From <ref type="figure" target="#fig_4">Figure 5</ref> it is clear that the larger bin sizes lead to faster networks; at 0.28 2 we achieve 105 Hz at similar performance to previous methods. The decrease in performance was mainly due to the pedestrian and cyclist classes, while car performance was stable across the bin sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Spatial Resolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Per Box Data Augmentation</head><p>Both VoxelNet <ref type="bibr" target="#b30">[31]</ref> and SECOND <ref type="bibr" target="#b27">[28]</ref> recommend extensive per box augmentation. However, in our experiments, minimal box augmentation worked better. In particular, the detection performance for pedestrians degraded significantly with more box augmentation. Our hypothesis is that the introduction of ground truth sampling mitigates the need for extensive per box augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Point Decorations</head><p>During the lidar point decoration step, we perform the VoxelNet <ref type="bibr" target="#b30">[31]</ref> decorations plus two additional decorations: x p and y p which are the x and y offset from the pillar x, y center. These extra decorations added 0.5 mAP to final detection performance and provided more reproducible experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Encoding</head><p>To assess the impact of the proposed PointPillar encoding in isolation, we implemented several encoders in the official codebase of SECOND <ref type="bibr" target="#b27">[28]</ref>. For details on each encoding, we refer to the original papers.</p><p>As shown in <ref type="table">Table 4</ref>, learning the feature encoding is strictly superior to fixed encoders across all resolution. This is expected as most successful deep learning architectures are trained end-to-end. Further, the differences increase with larger bin sizes where the lack of expressive power of the fixed encoders are accentuated due to a larger point  <ref type="table">Table 4</ref>. Encoder performance evaluation. To fairly compare encoders, the same network architecture and training procedure was used and only the encoder and xy resolution were changed between experiments. Performance is measured as BEV mAP on KITTI val. Learned encoders clearly beat fixed encoders, especially at larger resolutions. cloud in each pillar. Among the learned encoders Voxel-Net is marginally stronger than PointPillars. However, this is not a fair comparison, since the VoxelNet encoder is orders of magnitude slower and has orders of magnitude more parameters. When the comparison is made for a similar inference time, it is clear that PointPillars offers a better operating point ( <ref type="figure" target="#fig_4">Figure 5</ref>).</p><p>There are a few curious aspects of <ref type="table">Table 4</ref>. First, despite notes in the original papers that their encoder only works on cars, we found that the MV3D <ref type="bibr" target="#b1">[2]</ref> and PIXOR <ref type="bibr" target="#b29">[30]</ref> encoders can learn pedestrians and cyclists quite well. Second, our implementations beat the respective published results by a large margin (1 − 10 mAP). While this is not an apples to apples comparison since we only used the respective encoders and not the full network architectures, the performance difference is noteworthy. We see several potential reasons. For VoxelNet and SECOND we suspect the boost in performance comes from improved data augmentation hyperparameters as discussed in Section 7.2. Among the fixed encoders, roughly half the performance increase can be explained by the introduction of ground truth database sampling <ref type="bibr" target="#b27">[28]</ref>, which we found to boost the mAP by around 3% mAP. The remaining differences are likely due to a combination of multiple hyperparameters including network design (number of layers, type of layers, whether to use a feature pyramid); anchor box design (or lack thereof <ref type="bibr" target="#b29">[30]</ref>); localization loss with respect to 3D and angle; classification loss; optimizer choices (SGD vs Adam, batch size); and more. However, a more careful study is needed to isolate each cause and effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we introduce PointPillars, a novel deep network and encoder that can be trained end-to-end on lidar point clouds. We demonstrate that on the KITTI challenge, PointPillars dominates all existing methods by offering higher detection performance (mAP on both BEV and 3D) at a faster speed. Our results suggests that PointPillars offers the best architecture so far for 3D object detection from lidar.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Bird's eye view performance vs speed for our proposed PointPillars, PP method on the KITTI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative analysis of KITTI results. We show a bird's-eye view of the lidar point cloud (top), as well as the 3D bounding boxes projected into the image for clearer visualization. Note that our method only uses lidar. We show predicted boxes for car (orange), cyclist (red) and pedestrian (blue). Ground truth boxes are shown in gray. The orientation of boxes is shown by a line connected the bottom center to the front of the box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Failure cases on KITTI. Same visualize setup from Figure 3 but focusing on several common failure modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Car.</head><label></label><figDesc>The x, y, z range is [(0, 70.4), (-40, 40), (-3, 1)] meters respectively. The car anchor has width, length, and height of (1.6, 3.9, 1.5) m with a z center of -1 m. Matching uses positive and negative thresholds of 0.6 and 0.45. Pedestrian &amp; Cyclist. The x, y, z range of [(0, 48), (-20, 20), (-2.5, 0.5)] meters respectively. The pedestrian anchor has width, length, and height of (0.6, 0.8, 1.73) meters with a z center of -0.6 meters, while the cyclist anchor has width, length, and height of (0.6, 1.76, 1.73) meters with a z center of -0.6 meters. Matching uses positive and negative thresholds of 0.5 and 0.35.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>AFigure 5 .</head><label>5</label><figDesc>trade-off between speed and accuracy can be achieved by varying the size of the spatial binning. Smaller pillars allow finer localization and lead to more features, while larger pillars are faster due to fewer non-empty pillars (speeding up the encoder) and a smaller pseudo-image (speeding up BEV detection performance (mAP) vs speed (Hz) on the KITTI [5] val set across pedestrians, bicycles and cars. Blue circles indicate lidar only methods, red squares indicate methods that use lidar &amp; vision. Different operating points were achieved by using pillar grid sizes in {0.12 2 , 0.16 2 , 0.2 2 , 0.24 2 , 0.28 2 } m 2 . The number of max-pillars was varied along with the resolution and set to 16000, 12000, 12000, 8000, 8000 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Predictions Pillar Feature Net Backbone (2D CNN) Detection Head (SSD)</head><label></label><figDesc></figDesc><table><row><cell>Point</cell><cell cols="2">Stacked</cell><cell></cell><cell>Learned</cell><cell></cell><cell>Pseudo</cell><cell>Conv</cell><cell></cell><cell></cell><cell></cell><cell>Deconv</cell><cell></cell><cell></cell></row><row><cell>cloud</cell><cell></cell><cell>Pillars</cell><cell></cell><cell>Features</cell><cell></cell><cell>image</cell><cell>C</cell><cell cols="2">H/2 W/2</cell><cell>Conv</cell><cell></cell><cell>2C</cell><cell cols="2">H/2 W/2</cell></row><row><cell></cell><cell>D</cell><cell>N P</cell><cell>C</cell><cell>P</cell><cell>C</cell><cell>H W</cell><cell cols="2">W/4 2C</cell><cell cols="2">Conv H/4 H/8</cell><cell>Deconv Deconv</cell><cell>2C</cell><cell cols="2">H/2 W/2</cell><cell>Concat</cell><cell>6C</cell><cell>H/2 W/2</cell></row><row><cell></cell><cell></cell><cell>Pillar Index</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4C</cell><cell>W/8</cell><cell></cell><cell cols="2">2C</cell><cell>H/2 W/2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Results on the KITTI test BEV detection benchmark. Results on the KITTI test 3D detection benchmark.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell>Speed (Hz)</cell><cell>mAP Mod.</cell><cell>Easy</cell><cell>Car Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Pedestrian Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Cyclist Mod.</cell><cell>Hard</cell></row><row><cell>MV3D [2]</cell><cell>Lidar &amp; Img.</cell><cell>2.8</cell><cell>N/A</cell><cell cols="3">86.02 76.90 68.49</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Cont-Fuse [15]</cell><cell>Lidar &amp; Img.</cell><cell>16.7</cell><cell>N/A</cell><cell cols="3">88.81 85.83 77.33</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Roarnet [25]</cell><cell>Lidar &amp; Img.</cell><cell>10</cell><cell>N/A</cell><cell cols="3">88.20 79.41 70.02</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">AVOD-FPN [11] Lidar &amp; Img.</cell><cell>10</cell><cell>64.11</cell><cell cols="3">88.53 83.79 77.90</cell><cell cols="3">58.75 51.05 47.54</cell><cell cols="3">68.09 57.48 50.77</cell></row><row><cell>F-PointNet [21]</cell><cell>Lidar &amp; Img.</cell><cell>5.9</cell><cell>65.39</cell><cell cols="3">88.70 84.00 75.33</cell><cell cols="3">58.09 50.22 47.20</cell><cell cols="3">75.38 61.96 54.68</cell></row><row><cell>HDNET [29]</cell><cell>Lidar &amp; Map</cell><cell>20</cell><cell>N/A</cell><cell cols="3">89.14 86.57 78.32</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>PIXOR++ [29]</cell><cell>Lidar</cell><cell>35</cell><cell>N/A</cell><cell cols="3">89.38 83.70 77.97</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>VoxelNet [31]</cell><cell>Lidar</cell><cell>4.4</cell><cell>58.25</cell><cell cols="3">89.35 79.26 77.39</cell><cell cols="3">46.13 40.74 38.11</cell><cell cols="3">66.70 54.76 50.55</cell></row><row><cell>SECOND [28]</cell><cell>Lidar</cell><cell>20</cell><cell>60.56</cell><cell cols="3">88.07 79.37 77.95</cell><cell cols="3">55.10 46.27 44.76</cell><cell cols="3">73.67 56.04 48.78</cell></row><row><cell>PointPillars</cell><cell>Lidar</cell><cell>62</cell><cell>66.19</cell><cell cols="3">88.35 86.10 79.83</cell><cell cols="3">58.66 50.23 47.19</cell><cell cols="3">79.14 62.25 56.00</cell></row><row><cell>Method</cell><cell>Modality</cell><cell>Speed (Hz)</cell><cell>mAP Mod.</cell><cell>Easy</cell><cell>Car Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Pedestrian Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Cyclist Mod.</cell><cell>Hard</cell></row><row><cell>MV3D [2]</cell><cell>Lidar &amp; Img.</cell><cell>2.8</cell><cell>N/A</cell><cell cols="3">71.09 62.35 55.12</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Cont-Fuse [15]</cell><cell>Lidar &amp; Img.</cell><cell>16.7</cell><cell>N/A</cell><cell cols="3">82.54 66.22 64.04</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Roarnet [25]</cell><cell>Lidar &amp; Img.</cell><cell>10</cell><cell>N/A</cell><cell cols="3">83.71 73.04 59.16</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">AVOD-FPN [11] Lidar &amp; Img.</cell><cell>10</cell><cell>55.62</cell><cell cols="3">81.94 71.88 66.38</cell><cell cols="3">50.80 42.81 40.88</cell><cell cols="3">64.00 52.18 46.61</cell></row><row><cell>F-PointNet [21]</cell><cell>Lidar &amp; Img.</cell><cell>5.9</cell><cell>57.35</cell><cell cols="3">81.20 70.39 62.19</cell><cell cols="3">51.21 44.89 40.23</cell><cell cols="3">71.96 56.77 50.39</cell></row><row><cell>VoxelNet [31]</cell><cell>Lidar</cell><cell>4.4</cell><cell>49.05</cell><cell cols="3">77.47 65.11 57.73</cell><cell cols="2">39.48 33.69</cell><cell>31.5</cell><cell cols="3">61.22 48.36 44.37</cell></row><row><cell>SECOND [28]</cell><cell>Lidar</cell><cell>20</cell><cell>56.69</cell><cell cols="3">83.13 73.66 66.20</cell><cell cols="3">51.07 42.56 37.29</cell><cell cols="3">70.51 53.85 46.90</cell></row><row><cell>PointPillars</cell><cell>Lidar</cell><cell>62</cell><cell>59.20</cell><cell cols="3">79.05 74.99 68.30</cell><cell cols="3">52.08 43.53 41.49</cell><cell cols="3">75.78 59.07 52.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>EncoderType 0.16 2 0.20 2 0.24 2 0.28 2</figDesc><table><row><cell>MV3D [2]</cell><cell>Fixed</cell><cell>72.8</cell><cell>71.0</cell><cell>70.8</cell><cell>67.6</cell></row><row><cell>C. Yolo [26]</cell><cell>Fixed</cell><cell>72.0</cell><cell>72.0</cell><cell>70.6</cell><cell>66.9</cell></row><row><cell>PIXOR [30]</cell><cell>Fixed</cell><cell>72.9</cell><cell>71.3</cell><cell>69.9</cell><cell>65.6</cell></row><row><cell cols="2">VoxelNet [31] Learned</cell><cell>74.4</cell><cell>74.0</cell><cell>72.9</cell><cell>71.9</cell></row><row><cell>PointPillars</cell><cell>Learned</cell><cell>73.7</cell><cell>72.6</cell><cell>72.9</cell><cell>72.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/traveller59/second.pytorch/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lidar-based 3d object perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Himmelsbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lüttel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Wünsche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st international workshop on cognition for technical systems</title>
		<meeting>1st international workshop on cognition for technical systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A perception-driven autonomous urban vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Field Robotics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Roarnet: A robust 3d object detection based on region approximation refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03818</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06199</idno>
		<title level="m">Complex-YOLO: Real-time 3d object detection on point clouds</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HDNET: Exploiting HD maps for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PIXOR: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
