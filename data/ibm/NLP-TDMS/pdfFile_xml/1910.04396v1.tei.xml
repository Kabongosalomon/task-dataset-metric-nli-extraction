<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeop</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Seong</roleName><forename type="first">Jeonghun</forename><surname>Baek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
							<email>hwalsuk.lee@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI Research</orgName>
								<orgName type="institution" key="instit2">NAVER/LINE Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text recognition (STR) is the task of recognizing character sequences in natural scenes. While there have been great advances in STR methods, current methods still fail to recognize texts in arbitrary shapes, such as heavily curved or rotated texts, which are abundant in daily life (e.g. restaurant signs, product labels, company logos, etc). This paper introduces a novel architecture to recognizing texts of arbitrary shapes, named Self-Attention Text Recognition Network (SATRN), which is inspired by the Transformer. SATRN utilizes the self-attention mechanism to describe two-dimensional (2D) spatial dependencies of characters in a scene text image. Exploiting the full-graph propagation of self-attention, SATRN can recognize texts with arbitrary arrangements and large inter-character spacing. As a result, SATRN outperforms existing STR models by a large margin of 5.7 pp on average in "irregular text" benchmarks. We provide empirical analyses that illustrate the inner mechanisms and the extent to which the model is applicable (e.g. rotated and multi-line text). We will open-source the code.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Scene text recognition (STR) addresses the following problem: given an image patch tightly containing text taken from natural scenes (e.g. license plates and posters on the street), what is the sequence of characters? <ref type="bibr" target="#b29">(Zhu, Yao, and Bai 2016;</ref><ref type="bibr" target="#b16">Long, He, and Ya 2018)</ref> Applications of deep neural networks have led to great improvements in the performance of STR models <ref type="bibr" target="#b20">(Shi et al. 2016;</ref><ref type="bibr" target="#b11">Lee and Osindero 2016;</ref><ref type="bibr" target="#b26">Yang et al. 2017;</ref><ref type="bibr" target="#b3">Cheng et al. 2017;</ref><ref type="bibr" target="#b15">Liu, Chen, and Wong 2018;</ref><ref type="bibr" target="#b2">Bai et al. 2018)</ref>. They typically combine a convolutional neural network (CNN) feature extractor, designed for abstracting the input patch, with a subsequent recurrent neural network (RNN) character sequence generator, responsible for character decoding and language modeling. The model is trained in an end-to-end manner.</p><p>While these methods have brought advances in the field, they are built upon the assumption that input texts are written horizontally. <ref type="bibr" target="#b3">Cheng et al. (Cheng et al. 2017</ref>) and <ref type="bibr" target="#b20">Shi et al. (Shi et al. 2016;</ref>, for example, have collapsed the height component of the 2D CNN feature maps into a 1D feature map. They are conceptually and empirically inept at  <ref type="figure">Figure 2</ref>: SATRN addresses the text images of difficult shapes (curved "BMW" logo) by adopting a self-attention mechanism, while keeping intermediate feature maps two dimensional. SATRN thus models long-range dependencies spanning 2D space, a feature necessary for recognizing texts of irregular geometry.</p><p>interpreting texts with arbitrary shapes, which are important challenges in realistic deployment scenarios.</p><p>Realizing the significance and difficulty of recognizing texts of arbitrary shapes, the STR community has put more emphasis on such image types. The introduction of "irregular shape" STR benchmarks <ref type="bibr" target="#b1">(Baek et al. 2019</ref>) is an evidence of such interest. On the method side, recent STR approaches are focusing more on addressing texts of irregular shapes. There are largely two lines of research: (1) input rectification and (2) usage of 2D feature maps. Input rectification <ref type="bibr" target="#b20">(Shi et al. 2016;</ref><ref type="bibr" target="#b15">Liu, Chen, and Wong 2018;</ref><ref type="bibr" target="#b14">Liu et al. 2016;</ref><ref type="bibr" target="#b6">Gao et al. 2018</ref>) uses spatial transformer networks (STN, <ref type="bibr" target="#b9">(Jaderberg et al. 2015)</ref>) to normalize text images into canonical shapes: horizontally aligned characters of uniform heights and widths. These methods, however, suffer from the limitation that the possible family of arXiv:1910.04396v1 [cs.CV] 10 Oct 2019 transformations have to be specified beforehand.</p><p>Methods using 2D feature maps <ref type="bibr" target="#b26">Yang et al. 2017;</ref><ref type="bibr" target="#b12">Li et al. 2019</ref>), on the other hand, take the original input image without any modification, learn 2D feature maps, and sequentially retrieve characters on the 2D space. While the usage of 2D feature maps certainly increases room for more complex modelling, specific designs of existing methods are still limited by either the assumption that input texts are written horizontally (SAR <ref type="bibr" target="#b12">(Li et al. 2019)</ref>), overly complicated model structure (AON ), or requirement of ground truth character bounding boxes (ATR <ref type="bibr" target="#b26">(Yang et al. 2017)</ref>). We believe the community has lacked a simple solution to nicely handle texts of arbitrary shapes. In this paper, we propose an STR model that adopts a 2D self-attention mechanism to resolve the remaining challenging case within STR. Our architecture is heavily inspired by the Transformer <ref type="bibr" target="#b23">(Vaswani et al. 2017)</ref>, which has made profound advances in the natural language processing <ref type="bibr" target="#b0">(Al-Rfou et al. 2018;</ref><ref type="bibr" target="#b5">Devlin et al. 2019</ref>) and vision <ref type="bibr" target="#b17">(Parmar et al. 2018)</ref> fields. Our solution, Self-Attention Text Recognition Network (SATRN), adopts the encoder-decoder construct of Transformer to address the cross-modality between the image input and the text output. The intermediate feature maps are two dimensional throughout the network. By never collapsing the height dimension, we better preserve the spatial information than prior approaches <ref type="bibr" target="#b12">(Li et al. 2019)</ref>. <ref type="figure">Figure 2</ref> describes how SATRN preserves spatial information throughout the forward pass, unlike prior approaches.</p><p>While SATRN is performant due to the decoder following original character-level Transformer, we have discovered that a few novel modifications on the Transformer encoder is necessary to fully realize the benefit of self-attention in a 2D feature map. Three new modules are introduced: (1) shallow CNN, (2) adaptive 2D positional encoding, and (3) locality-aware feedforward layer. We will explain them in greater detail in the main text.</p><p>The resulting model, SATRN, is architecturally simple, memory efficient, and accurate. We have evaluated SATRN for its superior accuracy on the seven benchmark datasets and our newly introduced rotated and multi-line texts, along with its edge on computational cost. We justify the design choices in the encoder through ablative experiments. We note that SATRN is the state of the art model in five out of seven benchmark datasets considered, with notable gain of 5.7 pp average boost on "irregular" benchmarks over the prior state of the art.</p><p>We contribute (1) SATRN, inspired by Transformer, to address remaining challenges for STR; (2) novel modules in SATRN encoder to make Transformer effective and efficient for STR; and (3) experimental analysis on the effect of proposed modules and verification that SATRN is particularly good at texts of extreme shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>In this section, we present prior works on scene text recognition, focusing on how they have attempted to address texts of arbitrary shapes. Then, we discuss previous works on using Transformer on visual tasks and compare how our approach differs from them.</p><p>Scene text recognition on arbitrary shapes Early STR models have assumed texts are horizontally aligned. These methods have extracted width-directional 1D features from an input image and have transformed them into sequences of characters <ref type="bibr" target="#b20">(Shi et al. 2016;</ref><ref type="bibr" target="#b11">Lee and Osindero 2016;</ref><ref type="bibr" target="#b26">Yang et al. 2017;</ref><ref type="bibr" target="#b3">Cheng et al. 2017;</ref><ref type="bibr" target="#b15">Liu, Chen, and Wong 2018;</ref><ref type="bibr" target="#b2">Bai et al. 2018;</ref><ref type="bibr" target="#b18">Sheng, Chen, and Xu 2018;</ref><ref type="bibr" target="#b1">Baek et al. 2019)</ref>. By design, such models fail to address curved or rotated text. To overcome this issue, spatial transformation networks (STN) have been applied to align text image into a canonical shape (horizontal alignment and uniform character widths and heights) <ref type="bibr" target="#b20">(Shi et al. 2016;</ref><ref type="bibr" target="#b15">Liu, Chen, and Wong 2018;</ref><ref type="bibr" target="#b14">Liu et al. 2016;</ref><ref type="bibr" target="#b6">Gao et al. 2018)</ref>. STN does handle non-canonical text shapes to some degree, but is limited by the hand-crafted design of transformation space and the loss in fine details due to image interpolation.</p><p>Instead of the input-level normalization, recent works have spread the normalization burden across multiple layers, by retaining two-dimensional feature maps up to certain layers in the network and information propagation across 2D space. <ref type="bibr" target="#b4">Cheng et al. (Cheng et al. 2018</ref>) have first computed four 1D features by projecting an intermediate 2D feature map in four directions. They have introduced a selection module to dynamically pick one of the four features. Their method is still confined to those four predefined directions. <ref type="bibr" target="#b26">Yang et al. (Yang et al. 2017</ref>), on the other hand, have developed a 2D attention model over 2D features. The key disadvantage of their method is the need for expensive character-level supervision. <ref type="bibr" target="#b12">Li et al. (Li et al. 2019</ref>) have directly applied attention mechanism on 2D feature maps to generate text. However, their method loses full spatial information due to height pooling and RNN, thus being inherently biased towards horizontally aligned texts. These previous works have utilized a sequence generator sequentially attending to certain regions on the 2D feature map following the character order in texts. In this work, we propose a simpler solution with the self-attention mechanism <ref type="bibr" target="#b23">(Vaswani et al. 2017</ref>) applied on 2D feature maps. This approach enables character features to be aware of their spatial order and supports the sequence generator to track the order without any additional supervision.</p><p>Transformer for visual tasks Transformer has been introduced in the natural language processing field <ref type="bibr" target="#b23">(Vaswani et al. 2017;</ref><ref type="bibr" target="#b5">Devlin et al. 2019;</ref><ref type="bibr" target="#b0">Al-Rfou et al. 2018)</ref>. By allowing long-range pairwise dependencies through selfattention, it has achieved breakthroughs in numerous benchmarks. The original Transformer is a sequence-to-sequence model consisting of an encoder and decoder pair, without relying on any recurrent module.</p><p>Transformer has been adopted by methods solving general vision tasks such as action recognition , object detection , semantic segmentation <ref type="bibr" target="#b8">Huang et al. 2019)</ref>, and image generation <ref type="bibr" target="#b17">Parmar et al. 2018</ref>). Self-attention mechanism has been extended to two dimensional feature maps to capture long-range spatial dependen-cies. Since naive extension to spatial features induces high computational cost, these works have considered reducing number of pairwise connections through convolution layers  or pair pruning <ref type="bibr" target="#b8">(Huang et al. 2019)</ref>. We have adopted the techniques to STR task in SATRN; details will be discussed later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SATRN Method</head><p>This section describes our scene text recognition (STR) model, self-attention text recognition network (SATRN), in full detail. Many of the modules and design choices have been inherited and inspired from the successful Transformer model <ref type="bibr" target="#b23">(Vaswani et al. 2017</ref>), but there are several novel modifications for successful adaptation of STR task. We will provide an overview of the SATRN architecture, and then focus on the newly introduced modules. <ref type="figure" target="#fig_1">Figure 3</ref> shows the overall architecture of SATRN. It consists of an encoder (left column), which embeds an image into a 2D feature map, and a decoder (right column), which then extracts a sequence of characters from the feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SATRN Overview</head><p>Encoder The encoder processes input image through a shallow CNN that captures local patterns and textures. The feature map is then passed to a stack of self-attention modules, together with an adaptive 2D positional encoding, a novel positional encoding methodology developed for STR task. The self-attention modules are modified version of the original Transformer self-attention modules, where the point-wise feed forward is replaced by our locality-aware feedforward layer. The self-attention block is repeated N e times (without sharing weights). In the next section, we will describe in detail the components of SATRN that are newly introduced in the encoder on top of the original Transformer.</p><p>Decoder The decoder retrieves the enriched twodimensional features from the encoder to generate a sequence of characters. The cross-modality between image input and text output happens at the second multi-head attention module. The module retrieves the next character's visual feature The feature of the current character is used to retrieve the next character's visual features upon the 2D feature map. Most of the decoder modules, such as multi-head attention and point-wise feedforward layers, are identical to the decoder of Transformer <ref type="bibr" target="#b23">(Vaswani et al. 2017)</ref>, as the decoder in our case also deals with sequence of characters (Al-Rfou et al. 2018). Our methodological contributions are focused on adapting the encoder to extract sequential information embedded in images along arbitrary shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Designing Encoder for STR</head><p>We explain how we have designed the encoder to effectively and efficiently extract sequential information from images. There are three main constructs that modify the original Transformer architecture. Each of them will be explained.  Shallow CNN block Input images are first processed through a shallow CNN. This stage extracts elementary patterns and textures in input images for further processing in the subsequent self-attention blocks. Unlike in natural language processing, visual inputs tend to require much more abstraction as there are many background features to suppress (e.g. background texture of menu plate). Therefore, directly applying the Transformer architecture will put great burden to the expensive self-attention computations. This shallow CNN block performs pooling operations to reduce such burden.</p><p>More specifically, the shallow CNN block consists of two convolution layers with 3×3 kernels, each followed by a max pooling layer with 2×2 kernel of stride 2. The resulting 1/4 reduction factor has provided a good balance in computation-performance trade-off in our preliminary studies. If spatial dimensions are further reduced, performance drops heavily; if reduced less, computation burden for later self-attention blocks increases a lot.</p><p>Adaptive 2D positional encoding The feature map produced by the shallow CNN is fed to self-attention blocks. The self-attention block, however, is agnostic to spatial arrangements of its input (just like a fully-connected layer). Therefore, the original Transformer has further fed positional encodings, an array containing modified index values, to the self-attention module to supply the lacking positional information.</p><p>Positional encoding (PE) has not been essential in vision tasks <ref type="bibr" target="#b8">Huang et al. 2019)</ref>; the focus in these cases has been to provide longrange dependencies not captured by convolutions. On the other hand, positional information plays an important role in recognizing text of arbitrary shape, since the self-attention itself is not supplied the absolute location information: given current character location exactly where in the image can we find the next character? Missing the positional information makes it hard for the model to sequentially track character positions. SATRN thus employs a 2D extension of the positional encoding.</p><p>However, naive application of positional encoding cannot handle the diversity of character arrangements. For example, 10 pixels along width dimension for horizontal text will contain less number of characters than for diagonal text on average. Therefore, different length elements should be used in the positional encoding depending on the type of input. We thus propose the adaptive 2D positional encoding (A2DPE) to dynamically determine the ratio between height and width element depending on the input.</p><p>We first describe the self-attention module without positional encoding. We write E for the 2D feature output of shallow CNN and e hw for its entry at position (h, w) ∈ [1, ..., H] × [1, ..., W ]. The self-attention is computed as</p><formula xml:id="formula_0">att-out hw = h w softmax(rel (h w )→(hw) )v h w ,<label>(1)</label></formula><p>where the value array v hw = e hw W v is a transformation of the input feature through linear weights W v and rel (h w )→(hw) is defined as</p><formula xml:id="formula_1">rel (h w )→(hw) ∝ e hw W q W k T e h w T ,<label>(2)</label></formula><p>where W q and W k are linear weights that map the input into queries q hw = e hw W q and keys k hw = e hw W k . Intuitively, rel (h w )→(hw) dictates how much feature at (h , w ) attends to feature at (h, w).</p><p>We now introduce our positional encoding A2DPE p hw in this framework as below:</p><formula xml:id="formula_2">rel (h w )→(hw) ∝ (e hw + p hw )W q W k T (e h w + p h w ) T .</formula><p>(3) Note that A2DPE are added on top of the input features. Now, A2DPE itself is defined as α and β.</p><formula xml:id="formula_3">p hw = α(E)p sinu h + β(E)p sinu w ,<label>(4)</label></formula><p>where p sinu h and p sinu w are sinusoidal positional encoding over height and width, respectively, as defined in <ref type="bibr" target="#b23">(Vaswani et al. 2017)</ref>.</p><formula xml:id="formula_4">p sinu p,2i = sin(p/10000 2i/D ),<label>(5)</label></formula><formula xml:id="formula_5">p sinu p,2i+1 = cos(p/10000 2i/D ),<label>(6)</label></formula><p>where p and i are indices along position and hidden dimensions, respectively. The scale factors, α(E) and β(E), are computed from the input feature map E with 2-layer perceptron applied on global average pooled input feature as the followings:  where W h 1 , W h 2 , W w 1 and W w 2 are linear weights. The g(E) indicates an average pooling over all features in E. The outputs go through a sigmoid operation. The identified α and β affects the height and width positional encoding directly to control the relative ratio between horizontal and vertical axes to express the spatial diversity. By learning to infer α and β from the input, A2DPE allows the model to adapt the length elements along height and width directions.</p><formula xml:id="formula_6">α(E) = sigmoid max(0, g(E)W h 1 )W h 2 ,<label>(7)</label></formula><formula xml:id="formula_7">β(E) = sigmoid (max(0, g(E)W w 1 )W w 2 ) ,<label>(8)</label></formula><p>Locality-aware feedforward layer For good STR performance, a model should not only utilize long-range dependencies but also local vicinity around single characters. Selfattention layer itself is good at modelling long-term dependencies, but is not equipped to give sufficient focus on local structures. We have thus improved the original pointwise feedforward layer <ref type="figure" target="#fig_2">(Figure 4a</ref>), consisting of two 1 × 1 convolutional layers, by utilizing 3 × 3 convolutions <ref type="figure" target="#fig_2">(Figures 4b, 4c</ref>). In the experiments, we will show that between the naive 3 × 3 convolution and the depth-wise variant, the latter gives a better performance-efficiency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We report experimental results on our model, SATRN. First, we evaluate the accuracy of our model against state of the art methods. We add an analysis on spatial dependencies shown by SATRN. Second, we assess SATRN in terms of computational efficiency, namely memory consumption and the number of FLOPs. Third, we conduct ablation studies to evaluate our design choices including the shallow CNN, adaptive 2D positional encoding, and the locality-aware feedforward layer. Finally, we evaluate SATRN on more challenging cases not covered by current benchmarks, namely rotated and multi-lined texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STR Benchmark Datasets</head><p>Seven widely used real-word STR benchmark datasets are used for evaluation <ref type="bibr" target="#b1">(Baek et al. 2019)</ref>. They are divided into two groups, "Regular" and "Irregular", according to the difficulty and geometric layout of texts.</p><p>Below are "regular" datasets that contain horizontally aligned texts. IIIT5K contains 2,000 for training and 3,000 for testing images collected from the web, with mostly horizontal texts. Street View Text (SVT) consists of 257 for training and 647 for testing images collected from the Google Street View. Many examples are severely corrupted by noise and blur. ICDAR2003 (IC03) contains 867 cropped text images taken in a mall. ICDAR2013 (IC13) consists of 1015 images inheriting most images from IC03. "Irregular" benchmarks contain more texts of arbitrary shapes. ICDAR2015 (IC15) contains 2077 examples more irregular than do IC03 and IC13. Street View Text Perspective (SVTP) consists of 645 images which text are typically captured in perspective views. CUTE80 (CT80) includes 288 heavily curved text images with high resolution. Samples are taken from the real world scenes in diverse domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Training set Two widely used training datasets for STR are Mjsynth and SynthText. Mjsynth is a 9-million synthetic dataset for text recognition, generated by Jaderberg et al. <ref type="bibr" target="#b9">(Jaderberg et al. 2014)</ref>. SynthText represents 8-million text boxes from 800K synthetic scene images, provided by <ref type="bibr" target="#b7">Gupta et al. (Gupta, Vedaldi, and Zisserman 2016)</ref>. Most previous works have used these two synthetic datasets to learn diverse styles of synthetic sets, each generated with different engines. SATRN is trained on the combined training set, SynthText+Mjsynth, as suggested in <ref type="bibr" target="#b1">Baek et al. (Baek et al. 2019)</ref> for fair comparison.</p><p>Architecture details Input images are resized to 32 × 100 both during training and testing following common practice. The number of hidden units for self-attention layers is 512, and the number of filter units for feedforward layers is 4times of the hidden unit. The number of self-attention layers in encoder and decoder are N e = 12 and N d = 6. The final output is a vector of 94 scores; 10 for digits, 52 for alphabets, 31 for special characters, and 1 for the end token.</p><p>Optimization Our model has been trained in an end-toend manner using the cross-entropy loss. We have applied image rotation augmentation, where the amount of rotation follows the normal distribution N (0, (34 • ) 2 ). SATRN is trained with Adam optimizer <ref type="bibr" target="#b10">(Kingma and Ba 2015)</ref> with the initial learning rate 3e-4. Cyclic learning rate <ref type="bibr" target="#b22">(Smith 2017)</ref> has been used, where the cycle step is 250,000. Batch size is 256, and the learning is finished after 4 epochs. In our ablation study, we applied this optimization method on our baseline models for fair comparison.</p><p>Evaluation We trained our model with spacial characters, adopting the suggestion by <ref type="bibr" target="#b1">(Baek et al. 2019</ref>). When we evaluate our model, we calculate the case-insensitive word accuracy <ref type="bibr" target="#b21">(Shi et al. 2018)</ref>. Such training and evaluation method has been conducted in recent STR papers <ref type="bibr" target="#b21">(Shi et al. 2018;</ref><ref type="bibr" target="#b12">Li et al. 2019;</ref><ref type="bibr" target="#b13">Liao et al. 2018)</ref>. In our ablation studies, we use the unified evaluation dataset of all benchmarks (8,539 images in total) as done in <ref type="bibr" target="#b1">(Baek et al. 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison against Prior STR Methods</head><p>We compare the SATRN performance against existing STR models in <ref type="table">Table 1</ref>. The accuracies for previous models are reported accuracies. Methods are grouped according to the dimensionality of feature maps, and whether the spatial transformer network (STN) has been used. The STN module and 2D feature maps have been designed to help recognizing texts of arbitrary shapes. We observe that SATRN outperforms other 2D approaches on all benchmarks and that it attains the best performance on five of them against all prior methods considered. In particular, on irregular benchmarks that we aim to solve, SATRN improves upon the second best method with a large margin of 4.7 pp on average.  <ref type="figure">Figure 5</ref>: Accuracy-efficiency trade-off plots for SAR and SATRN. We have made variations, small, middle, and big, to control over the number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing SATRN against SAR</head><p>Since SATRN shares many similarities with SAR <ref type="bibr" target="#b12">(Li et al. 2019)</ref>, where the difference is the choice of encoder (self-attention versus convolutions) and decoder (selfattention versus LSTM), we provide a more detailed analysis through a thorough comparison against SAR. We analyze the accuracy-efficiency trade-off as well as their qualitative differences.</p><p>Accuracy-efficiency trade-off We analyze the contributions of self-attention layers in SATRN encoder and decoder, focusing both on the accuracy and efficiency. See <ref type="table">Table 2</ref> for ablative analysis. The baseline model is SAR <ref type="bibr" target="#b12">(Li et al. 2019)</ref> given in the first row (ResNet encoder with 2D attention LSTM decoder), and one can partially update SAR by replacing either only the encoder or the decoder of SATRN. We observe that upgrading ResNet encoder to SATRN encoder improve the accuracy by 1.0 pp and 0.9 pp over LSTM and SATRN decoders, respectively, while actually improving the space and time efficiency (reduction of 12M parameters and 5.5B FLOPs in both cases). This is the result of inherent computational efficiency enjoyed by selfattention layers and careful design of SATRN encoder to reduce FLOPs by modeling long-term and short-term dependencies of the features efficiently. The SATRN decoder, which is nearly identical to the original Transformer decoder, does provide further gain of 0.3 pp accuracy boost, but at the cost of increased memory consumption (+11M) and FLOPs (+19.5B).</p><p>To provide a broader view on the computational efficiency due to self-attention layers, we have made variations over SAR <ref type="bibr" target="#b12">(Li et al. 2019</ref>) and SATRN with varying number of layers. The original SAR contains ResNet34 as an encoder (SAR-middle), and we consider replacing the encoder with ResNet18 (SAR-small) and ResNet101 (SAR-big). Our base construct SATRN is considered SATRN-big. We consider reducing the channel dimensions in all layers from 512 to 256 (SATRN-middle) and further reducing the number of encoder layers N e = 9 and that of decoder layers N d = 3 (SATRN-small). <ref type="figure">Figure 5</ref> compares the accuracy-cost trade-offs of SAR <ref type="bibr" target="#b12">(Li et al. 2019</ref>) and SATRN. We observe more clearly that SATRN design involving self-attention layers provides a better accuracy-efficiency trade-off than SAR approach. We conclude that for addressing STR problems, SATRN design is a favorable choice.   Qualitative comparison We provide a qualitative analysis of how the 2D self-attention layers in encoder extract informative features. <ref type="figure" target="#fig_3">Figure 6</ref> show the human-defined character region of interest (ROI) as well as the corresponding selfattention heatmaps (SA) at depth n, generated by propagating the character ROI from the last layer to n layers below through self-attention weights. It shows the supporting signals relations at n for recognizing the designated character. We observe that for character 'M' the last self-attention layer identifies the dependencies with the next character 'A'. SA at depth 2 already propagates the supporting signal globally, taking advantage of long-range connections in self-attention. By allowing long-range computations within small number of layers, SATRN achieves a good performance while removing redundancies created by accumulating local information too many times (convolutional encoder).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies on Proposed Modules</head><p>SATRN encoder is made of many design choices to adapt Transformer to the STR task. We report ablative studies on those factors in the following part, and experimentally analyze alternative design choices. The default model used hereafter is SATRN-small. the inherent aspect ratios incurred by overall text alignment <ref type="bibr">(horizontal, diagonal, or vertical)</ref>. As alternative options, we consider not doing any positional encoding at all ("None") , using 1D positional encoding over flattened feature map ("1D-Flatten"), using concatenation of height and width positional encodings ("2D-Concat") <ref type="bibr" target="#b17">(Parmar et al. 2018)</ref>, and the A2DPE that we propose. See <ref type="table" target="#tab_5">Table 3a</ref> for the results. We observe that A2DPE provides the best accuracy among four options considered. We visualize random input images from three groups with different predicted aspect ratios, as a by-product of A2DPE. <ref type="figure">Figure 7</ref> shows the examples according to the ratios ||α|| 1 /||β|| 1 . Low aspect ratio group, as expected, contains mostly horizontal samples, and high aspect ratio group contains mostly vertical samples. By dynamically adjusting the grid spacing, A2DPE reduces the representation burden for the other modules, leading to performance boost.</p><p>Locality-aware feedforward layer We have replaced the point-wise feedforward layers in the Transformer encoder <ref type="bibr" target="#b23">(Vaswani et al. 2017</ref>) with our novel locality-aware feedforward layers to seek performance boost at low extra cost. To analyze their effects, we consider the two alternatives described in <ref type="figure" target="#fig_2">Figure 4</ref>, each with different number of encoder layers (3, 6, or 9).</p><p>The resulting accuracy-performance trade-offs are visualized in <ref type="figure">Figure 8</ref>. Compared to the point-wise feedforward, naive convolution results in improved accuracy, but roughly with four times more parameters and FLOPs. We alleviate the computation cost with depth-wise convolutions (localityaware feedforward) and achieve a better accuracy at nearly identical computational costs.</p><p>Feature map height Finally, we study the impact of the spatial degree of freedom in the 2D feature map of SATRNsmall on its accuracy and computational costs. We interpolate between SATRN-small using full 2D feature map and the same model using 1D feature map by controlling the downsampling rate along height and width dimensions. SATRN-small is using the 1/4 downsampling factor for both height and width, and we consider further downsampling the height sequentially with 1/2 factor, until only 1 height dimension is left (1/32 height downsampling). To see the other extreme, we have considered downsampling less (downsample only 1/2 for both width and height).  FLOPs and accuracy as the feature map sizes are reduced. When height is downsampled with rate greater than 1/8, performances drop dramatically (more than 2.9 pp). The results re-emphasize the importance of maintaining the 2D feature maps throughout the computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Challenges: Rotated and Multi-Line Text</head><p>Irregular text recognition benchmarks (IC15, SVTP, and CT80) are attempts to shift the focus of STR research to more difficult challenges yet to be address by the field. While these datasets do contain texts of more difficult shapes, it is not easy to analyze the impact of the type and amount of shape distortions. We have thus prepared new synthetic test sets (transformed from IC13) that consists purely of single type and degree of perturbation. Specifically, we measure the performance against texts with varying degrees of rotations (0 • , 90 • , 180 • , and 270 • ) as well as multi-line texts. We compare against two representative baseline models, FAN <ref type="bibr" target="#b3">(Cheng et al. 2017</ref>) and SAR <ref type="bibr" target="#b12">(Li et al. 2019</ref>). Optimization and pre-processing details including training dataset and augmentation are unified for fair comparison.</p><p>Rotated text Most STR models based upon the horizontal text assumption cannot handle heavily rotated texts. SATRN on the other hand does not rely on any such inductive bias; its ability to recognize rotated texts purely depends upon the ratio of such cases shown during training. To empirically validate this, we have trained the models with wider range of rotations: Uniform(0 • , 360 • ). Input images are then resized to 64×64. Second column group in <ref type="table" target="#tab_7">Table 4</ref> shows the results of rotated text experiments. We confirm that SATRN outperforms FAN and SAR while retaining stable performances for all rotation levels.</p><p>Multi-line text We analyze the capability of models on recognizing multi-line texts, which would require the functionality to change line during inference. We have synthesized multi-line texts using SynthText and MJSynth for training the models. For evaluation we have utilized multiline text manually cropped from the scene images in IC13.</p><p>Last column in <ref type="table" target="#tab_7">Table 4</ref> shows the results. SATRN indeed performs better than the baselines, showing its capability to make a long-range jump to change line during inference. <ref type="figure">Figure 9</ref> shows the attention map of the SATRN decoder to retrieve 2D features. SATRN distinguishes the two lines and successes to track the next line. The results show that SATRN enables the 2D attention transition from the current region to a non-adjacent region on the image. <ref type="figure">Figure 9</ref>: The 2D attention maps on a multi-line example. The 2D attention follows the first text line and then moves to the next line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Scene text recognition (STR) field has seen great advances in the last couple of years. Models are now working well on texts of canonical shapes. We argue that the important remaining challenge for STR is the recognition of texts with arbitrary shapes. To address this problem, we have proposed the Self-Attention Text Recognition Network (SATRN). By allowing long-range dependencies through self-attention layer, SATRN is able to sequentially locate next characters even if they do not follow canonical arrangements. We have made several novel modifications on the Transformer architecture to adapt it to STR task. We have achieved the new state of the art performances on irregular text recognition benchmarks with great margin (5.7 pp boost on average). SATRN has shown particularly good performance on our more controlled experiments on rotated and multi-line texts, ones that constitute the future STR challenges. We will open source the code.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Texts of arbitrary shapes: remaining challenges for scene text recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>SATRN architecture overview. Left column is encoder and right column is decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Feedforward architecture options applied after the selfattention layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the self-attention maps. See text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Adaptive 2D positional encoding (A2DPE) This new positional encoding is necessary for dynamically adapting to(a) r ∈ (0, 0.6) (b) r ∈ (0.6, 0.8) (c) r ∈ (0.8, ∞)Examples in three groups separated by the range of the aspect ratios, r = ||α||1/||β||1. Performance comparison of the feedforward blocks according to the number of parameters and FLOPs. Numbers above data points denote the number of encoding layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Performance of SATRN-small with different positional en-</cell></row><row><cell>coding (PE) schemes and downsampling rates.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3b</head><label>3b</label><figDesc>shows the results. There is a consistent drop in</figDesc><table><row><cell>Model</cell><cell>0 •</cell><cell cols="2">Rotated (IC13) 90 • 180 • 270 •</cell><cell>Multi-line</cell></row><row><cell>FAN (1D)</cell><cell cols="2">87.0 81.9 86.8</cell><cell>84.1</cell><cell>44.7</cell></row><row><cell>SAR (2D)</cell><cell cols="2">88.5 88.4 89.1</cell><cell>88.8</cell><cell>46.7</cell></row><row><cell cols="3">SATRN (2D) 90.7 90.5 91.6</cell><cell>91.5</cell><cell>63.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The results on two challenging text datasets; heavily rotated text and mutli-line text.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04444</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5086" to="5094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AON: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07145</idno>
		<title level="m">Recurrent calibration network for irregular text recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
	<note>Workshop on Deep Learning, NIPS. Jaderberg, M</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive recurrent nets with attention modeling for ocr in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06508</idno>
		<title level="m">Scene text recognition from twodimensional perspective</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Star-net: A spatial attention residue network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Char-net: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scene text detection and recognition: The deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04256</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Image transformer</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">NRTR: A no-recurrence sequence-to-sequence model for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00926</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gated recurrent convolution neural network for ocr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR abs/1711.07971</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ESIR: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: Recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
