<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MemNet: A Persistent Memory Network for Image Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MemNet: A Persistent Memory Network for Image Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, very deep convolutional neural networks (CNNs) have been attracting considerable attention in image restoration. However, as the depth grows, the long-term dependency problem is rarely realized for these very deep models, which results in the prior states/layers having little influence on the subsequent ones. Motivated by the fact that human thoughts have persistency, we propose a very deep persistent memory network (MemNet) that introduces a memory block, consisting of a recursive unit and a gate unit, to explicitly mine persistent memory through an adaptive learning process. The recursive unit learns multi-level representations of the current state under different receptive fields. The representations and the outputs from the previous memory blocks are concatenated and sent to the gate unit, which adaptively controls how much of the previous states should be reserved, and decides how much of the current state should be stored. We apply MemNet to three image restoration tasks, i.e., image denosing, superresolution and JPEG deblocking. Comprehensive experiments demonstrate the necessity of the MemNet and its unanimous superiority on all three tasks over the state of the arts. Code is available at https://github.com/ tyshiwo/MemNet. the additive noise. With this mathematical model, extensive studies are conducted on many image restoration tasks, e.g., image denoising <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">37]</ref>, single-image super-resolution (SISR) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38]</ref> and JPEG deblocking <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>As three classical image restoration tasks, image denoising aims to recover a clean image from a noisy observation, which commonly assumes additive white Gaussian noise with a standard deviation Ïƒ; single-image superresolution recovers a high-resolution (HR) image from a low-resolution (LR) image; and JPEG deblocking removes the blocking artifact caused by JPEG compression <ref type="bibr" target="#b6">[7]</ref>.</p><p>Recently, due to the powerful learning ability, very deep convolutional neural network (CNN) is widely used to tackle the image restoration tasks. Kim et al. construct a 20-layer CNN structure named VDSR for SISR <ref type="bibr" target="#b19">[20]</ref>, and adopts residual learning to ease training difficulty. To control the parameter number of very deep models, the authors further introduce a recursive layer and propose a Deeply-Recursive Convolutional Network (DRCN) <ref type="bibr" target="#b20">[21]</ref>. To mitegate training difficulty, Mao et al. <ref type="bibr" target="#b26">[27]</ref> introduce symmetric skip connections into a 30-layer convolutional auto-encoder</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image restoration <ref type="bibr" target="#b28">[29]</ref> is a classical problem in low-level computer vision, which estimates an uncorrupted image from a noisy or blurry one. A corrupted low-quality image x can be represented as: x = D(x) + n, wherex is a highquality version of x, D is the degradation function and n is * This work was supported by the National Science Fund of China under Grant Nos. 91420201, 61472187, 61502235, 61233011, 61373063 and 61602244, the 973 Program No. 2014CB349303, Program for Changjiang Scholars, and partially sponsored by CCF-Tencent Open Research Fund. Jian Yang and Xiaoming Liu are corresponding authors.  The blue circles denote a recursive unit with an unfolded structure which generates the short-term memory. The green arrow denotes the long-term memory from the previous memory blocks that is directly passed to the gate unit. network named RED for image denoising and SISR. Moreover, <ref type="bibr">Zhang et al. [40]</ref> propose a denoising convolutional neural network (DnCNN) to tackle image denoising, SISR and JPEG deblocking simultaneously.</p><p>The conventional plain CNNs, e.g., VDSR <ref type="bibr" target="#b19">[20]</ref>, DRCN <ref type="bibr" target="#b20">[21]</ref> and DnCNN <ref type="bibr" target="#b39">[40]</ref>  <ref type="figure" target="#fig_1">(Fig. 1(a)</ref>), adopt the singlepath feed-forward architecture, where one state is mainly influenced by its direct former state, namely short-term memory. Some variants of CNNs, RED <ref type="bibr" target="#b26">[27]</ref> and ResNet <ref type="bibr" target="#b11">[12]</ref> ( <ref type="figure" target="#fig_1">Fig. 1(b)</ref>), have skip connections to pass information across several layers. In these networks, apart from the short-term memory, one state is also influenced by a specific prior state, namely restricted long-term memory. In essence, recent evidence suggests that mammalian brain may protect previously-acquired knowledge in neocortical circuits <ref type="bibr" target="#b3">[4]</ref>. However, none of above CNN models has such mechanism to achieve persistent memory. As the depth grows, they face the issue of lacking long-term memory.</p><p>To address this issue, we propose a very deep persistent memory network (MemNet), which introduces a memory block to explicitly mine persistent memory through an adaptive learning process. In MemNet, a Feature Extraction Net (FENet) first extracts features from the low-quality image. Then, several memory blocks are stacked with a densely connected structure to solve the image restoration task. Finally, a Reconstruction Net (ReconNet) is adopted to learn the residual, rather than the direct mapping, to ease the training difficulty.</p><p>As the key component of MemNet, a memory block contains a recursive unit and a gate unit. Inspired by neuroscience <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref> that recursive connections ubiquitously exist in the neocortex, the recursive unit learns multi-level representations of the current state under different receptive fields (blue circles in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>), which can be seen as the short-term memory. The short-term memory generated from the recursive unit, and the long-term memory generated from the previous memory blocks 1 (green arrow in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>) are concatenated and sent to the gate unit, which is a non-linear function to maintain persistent memory. Further, we present an extended multi-supervised MemNet, which fuses all intermediate predictions of memory blocks to boost the performance.</p><p>In summary, the main contributions of this work include: A memory block to accomplish the gating mechanism to help bridge the long-term dependencies. In each memory block, the gate unit adaptively learns different weights for different memories, which controls how much of the longterm memory should be reserved, and decides how much of the short-term memory should be stored.</p><p>A very deep end-to-end persistent memory network (80 convolutional layers) for image restoration. The densely connected structure helps compensate mid/high-frequency signals, and ensures maximum information flow between memory blocks as well. To the best of our knowledge, it is by far the deepest network for image restoration.</p><p>The same MemNet structure achieves the state-of-theart performance in image denoising, super-resolution and JPEG deblocking. Due to the strong learning ability, our MemNet can be trained to handle different levels of corruption even using a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The success of AlexNet <ref type="bibr" target="#b21">[22]</ref> in ImageNet <ref type="bibr" target="#b30">[31]</ref> starts the era of deep learning for vision, and the popular networks, GoogleNet <ref type="bibr" target="#b32">[33]</ref>, Highway network <ref type="bibr" target="#b31">[32]</ref>, ResNet <ref type="bibr" target="#b11">[12]</ref>, reveal that the network depth is of crucial importance.</p><p>As the early attempt, Jain et al. <ref type="bibr" target="#b16">[17]</ref> proposed a simple CNN to recover a clean natural image from a noisy observation and achieved comparable performance with the wavelet methods. As the pioneer CNN model for SISR, superresolution convolutional neural network (SRCNN) <ref type="bibr" target="#b7">[8]</ref> predicts the nonlinear LR-HR mapping via a fully deep convolutional network, which significantly outperforms classical shallow methods. The authors further proposed an extended CNN model, named Artifacts Reduction Convolutional Neural Networks (ARCNN) <ref type="bibr" target="#b6">[7]</ref>, to effectively handle JPEG compression artifacts.</p><p>To incorporate task-specific priors, Wang et al. adopted a cascaded sparse coding network to fully exploit the natural sparsity of images <ref type="bibr" target="#b35">[36]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, a deep dual-domain approach is proposed to combine both the prior knowledge in the JPEG compression scheme and the practice of dual-domain sparse coding. Guo et al. <ref type="bibr" target="#b9">[10]</ref> also proposed a dual-domain convolutional network that jointly learns a very deep network in both DCT and pixel domains.</p><p>Recently, very deep CNNs become popular for image restoration. Kim et al. <ref type="bibr" target="#b19">[20]</ref> stacked 20 convolutional layers to exploit large contextual information. Residual learning and adjustable gradient clipping are used to speed up the training. Zhang et al. <ref type="bibr" target="#b39">[40]</ref> introduced batch normalization into a DnCNN model to jointly handle several image restoration tasks. To reduce the model complexity, the DRCN model introduced recursive-supervision and skipconnection to mitigate the training difficulty <ref type="bibr" target="#b20">[21]</ref>. Using symmetric skip connections, Mao et al. <ref type="bibr" target="#b26">[27]</ref> proposed a very deep convolutional auto-encoder network for image denoising and SISR. Very Recently, Lai et al. <ref type="bibr" target="#b22">[23]</ref> proposed LapSRN to address the problems of speed and accuracy for SISR, which operates on LR images directly and progressively reconstruct the sub-band residuals of HR images. Tai et al. <ref type="bibr" target="#b33">[34]</ref> proposed deep recursive residual network (DRRN) to address the problems of model parameters and accuracy, which recursively learns the residual unit in a multi-path model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MemNet for Image Restoration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Basic Network Architecture</head><p>Our MemNet consists of three parts: a feature extraction net FENet, multiple stacked memory blocks and finally a reconstruction net ReconNet <ref type="figure" target="#fig_2">(Fig. 2</ref>). Let's denote x and y as the input and output of MemNet. Specifically, a convolutional layer is used in FENet to extract the features from the noisy or blurry input image,</p><formula xml:id="formula_0">B 0 = f ext (x),<label>(1)</label></formula><p>where f ext denotes the feature extraction function and B 0 is the extracted feature to be sent to the first memory block.</p><p>Supposing M memory blocks are stacked to act as the feature mapping, we have</p><formula xml:id="formula_1">B m = M m (B mâˆ’1 ) = M m (M mâˆ’1 (...(M 1 (B 0 ))...)),<label>(2)</label></formula><p>where M m denotes the m-th memory block function and B mâˆ’1 and B m are the input and output of the m-th memory block respectively. Finally, instead of learning the direct mapping from the low-quality image to the high-quality image, our model uses a convolutional layer in ReconNet to reconstruct the residual image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. Therefore, our basic MemNet can be formulated as,</p><formula xml:id="formula_2">y = D(x) = f rec (M M (M M âˆ’1 (...(M 1 (f ext (x)))...))) + x,<label>(3)</label></formula><p>where f rec denotes the reconstruction function and D denotes the function of our basic MemNet. Given a training set</p><formula xml:id="formula_3">{x (i) ,x (i) } N i=1 ,</formula><p>where N is the number of training patches andx (i) is the ground truth highquality patch of the low-quality patch x (i) , the loss function of our basic MemNet with the parameter set Î˜, is</p><formula xml:id="formula_4">L(Î˜) = 1 2N N i=1 x (i) âˆ’ D(x (i) ) 2 ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Block</head><p>We now present the details of our memory block. The memory block contains a recursive unit and a gate unit. Recursive Unit is used to model a non-linear function that acts like a recursive synapse in the brain <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. Here, we use a residual building block, which is introduced in ResNet <ref type="bibr" target="#b11">[12]</ref> and shows powerful learning ability for object recognition, as a recursion in the recursive unit. A residual building block in the m-th memory block is formulated as,</p><formula xml:id="formula_5">H r m = R m (H râˆ’1 m ) = F(H râˆ’1 m , W m ) + H râˆ’1 m ,<label>(5)</label></formula><p>where H râˆ’1 m , H r m are the input and output of the r-th residual building block respectively. When r = 1, H 0 m = B mâˆ’1 . F denotes the residual function, W m is the weight set to be learned and R denotes the function of residual building block. Specifically, each residual function contains two convolutional layers with the pre-activation structure <ref type="bibr" target="#b12">[13]</ref>,</p><formula xml:id="formula_6">F(H râˆ’1 m , W m ) = W 2 m Ï„ (W 1 m Ï„ (H râˆ’1 m )),<label>(6)</label></formula><p>where Ï„ denotes the activation function, including batch normalization <ref type="bibr" target="#b15">[16]</ref> followed by ReLU <ref type="bibr" target="#b29">[30]</ref>, and W i m , i = 1, 2 are the weights of the i-th convolutional layer. The bias terms are omitted for simplicity.</p><p>Then, several recursions are recursively learned to generate multi-level representations under different receptive fields. We call these representations as the short-term memory. Supposing there are R recursions in the recursive unit, the r-th recursion in recursive unit can be formulated as,</p><formula xml:id="formula_7">H r m = R (r) m (Bmâˆ’1) = Rm(Rm(...(Rm r (Bmâˆ’1)</formula><p>)...)), <ref type="bibr" target="#b6">(7)</ref> where r-fold operations of R m are performed and {H r m } R r=1 are the multi-level representations of the recursive unit. These representations are concatenated as the short-term memory:</p><formula xml:id="formula_8">B short m = [H 1 m , H 2 m , ..., H R m ].</formula><p>In addition, the long-term memory coming from the previous memory blocks can be constructed as:</p><formula xml:id="formula_9">B long m = [B 0 , B 1 , ..., B mâˆ’1 ].</formula><p>The two types of memories are then concatenated as the input to the gate unit,</p><formula xml:id="formula_10">B gate m = [B short m , B long m ].<label>(8)</label></formula><p>Gate Unit is used to achieve persistent memory through an adaptive learning process. In this paper, we adopt a 1 Ã— 1 convolutional layer to accomplish the gating mechanism that can learn adaptive weights for different memories,  where f gate m and B m denote the function of the 1 Ã— 1 convolutional layer (parameterized by W gate m ) and the output of the m-th memory block, respectively. As a result, the weights for the long-term memory controls how much of the previous states should be reserved, and the weights for the short-term memory decides how much of the current state should be stored. Therefore, the formulation of the m-th memory block can be written as,</p><formula xml:id="formula_11">B m = f gate m (B gate m ) = W gate m Ï„ (B gate m ),<label>(9)</label></formula><formula xml:id="formula_12">B m = M m (B mâˆ’1 ) = f gate ([R m (B mâˆ’1 ), ..., R (R) m (B mâˆ’1 ), B 0 , ..., B mâˆ’1 ]).<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Supervised MemNet</head><p>To further explore the features at different states, inspired by <ref type="bibr" target="#b20">[21]</ref>, we send the output of each memory block to the same reconstruction netf rec to generate</p><formula xml:id="formula_13">y m =f rec (x, B m ) = x + f rec (B m ),<label>(11)</label></formula><p>where {y m } M m=1 are the intermediate predictions. All of the predictions are supervised during training, and used to compute the final output via weighted averaging: y = M m=1 w m Â· y m <ref type="figure" target="#fig_3">(Fig. 3</ref>). The optimal weights {w m } M m=1 are automatically learned during training and the final output from the ensemble is also supervised. The loss function of our multi-supervised MemNet can be formulated as,</p><formula xml:id="formula_14">L(Î˜) = Î± 2N N i=1 x (i) âˆ’ M m=1 w m Â· y (i) m 2 + 1 âˆ’ Î± 2M N M m=1 N i=1 x (i) âˆ’ y (i) m 2 ,<label>(12)</label></formula><p>where Î± denotes the loss weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dense Connections for Image Restoration</head><p>Now we analyze why the long-term dense connections in MemNet may benefit the image restoration. In very deep networks, some of the mid/high-frequency information can get lost at latter layers during a typical feedforward CNN process, and dense connections from previous layers can compensate such loss and further enhance </p><formula xml:id="formula_15">(b) MemNet_4-MemNet_NL_4 MemNet_6-MemNet_NL_6 MemNet_4-MemNet_6 MemNet_NL_4-MemNet_NL_6 (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low frequency</head><p>High frequency high-frequency signals. To verify our intuition, we train a 80-layer MemNet without long-term connections, which is denoted as MemNet NL, and compare with the original MemNet. Both networks have 6 memory blocks leading to 6 intermediate outputs, and each memory block contains 6 recursions. <ref type="figure" target="#fig_4">Fig. 4(a)</ref> shows the 4th and 6th outputs of both networks. We compute their power spectrums, center them, estimate spectral densities for a continuous set of frequency ranges from low to high by placing concentric circles, and plot the densities of four outputs in <ref type="figure" target="#fig_4">Fig. 4(b)</ref>.</p><p>We further plot differences of these densities in <ref type="figure" target="#fig_4">Fig. 4(c)</ref>. From left to right, the first case indicates the earlier layer does contain some mid-frequency information that the latter layers lose. The 2nd case verifies that with dense connections, the latter layer absorbs the information carried from the previous layers, and even generate more mid-frequency information. The 3rd case suggests in earlier layers, the frequencies are similar between two models. The last case demonstrates the MemNet recovers more high frequency than the version without long-term connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussions</head><p>Difference to Highway Network First, we discuss how the memory block accomplishes the gating mechanism and present the difference between MemNet and Highway Network -a very deep CNN model using a gate unit to regulate information flow <ref type="bibr" target="#b31">[32]</ref>.</p><p>To avoid information attenuation in very deep plain networks, inspired by LSTM, Highway Network introduced the bypassing layers along with gate units, i.e.,</p><formula xml:id="formula_16">b = A(a) Â· T (a) + a Â· (1 âˆ’ T (a)),<label>(13)</label></formula><p>where a and b are the input and output, A and T are two non-linear transform functions. T is the transform gate to control how much information produced by A should be stored to the output; and 1 âˆ’ T is the carry gate to decide how much of the input should be reserved to the output. In MemNet, the short-term and long-term memories are concatenated. The 1 Ã— 1 convolutional layer adaptively learns the weights for different memories. Compared to Highway Network that learns specific weight for each pixel, our gate unit learns specific weight for each feature map, which has two advantages: (1) to reduce model parameters and complexity; (2) to be less prone to overfitting. Difference to DRCN There are three main differences between MemNet and DRCN <ref type="bibr" target="#b20">[21]</ref>. The first is the design of the basic module in network. In DRCN, the basic module is a convolutional layer; while in MemNet, the basic module is a memory block to achieve persistent memory. The second is in DRCN, the weights of the basic modules (i.e., the convolutional layers) are shared; while in MemNet, the weights of the memory blocks are different. The third is there are no dense connections among the basic modules in DRCN, which results in a chain structure; while in MemNet, there are long-term dense connections among the memory blocks leading to the multi-path structure, which not only helps information flow across the network, but also encourages gradient backpropagation during training. Benefited from the good information flow ability, MemNet could be easily trained without the multi-supervision strategy, which is imperative for training DRCN <ref type="bibr" target="#b20">[21]</ref>. Difference to DenseNet Another related work to MemNet is DenseNet <ref type="bibr" target="#b13">[14]</ref>, which also builds upon a densely connected principle. In general, DenseNet deals with object recognition, while MemNet is proposed for image restoration. In addition, DenseNet adopts the densely connected structure in a local way (i.e., inside a dense block), while MemNet adopts the densely connected structure in a global way (i.e., across the memory blocks). In Secs. 3.4 and 5.2, we analyze and demonstrate the long-term dense connections in MemNet indeed play an important role in image restoration tasks.   For the curve of the mth block, the left (m Ã— 64) elements denote the long-term memories and the rest (Lm âˆ’ m Ã— 64) elements denote the short-term memories. The bar diagrams illustrate the average norm of long-term memories, short-term memories from the first R âˆ’ 1 recursions and from the last recursion, respectively. E.g., each yellow bar is the average norm of the short-term memories from the last recursion in the recursive unit (i.e., the last 64 elements in each curve).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Datasets For image denoising, we follow <ref type="bibr" target="#b26">[27]</ref> to use 300 images from the Berkeley Segmentation Dataset (BSD) <ref type="bibr" target="#b27">[28]</ref>, known as the train and val sets, to generate image patches as the training set. Two popular benchmarks, a dataset with 14 common images and the BSD test set with 200 images, are used for evaluation. We generate the input noisy patch by adding Gaussian noise with one of the three noise levels (Ïƒ = 30, 50 and 70) to the clean patch.</p><p>For SISR, by following the experimental setting in <ref type="bibr" target="#b19">[20]</ref>, we use a training set of 291 images where 91 images are from Yang et al. <ref type="bibr" target="#b37">[38]</ref>   <ref type="table">Table 2</ref>. SISR comparisons with start-of-the-art networks for scale factor Ã—3 on Set5. Red indicates the fewest number or best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MemNet_M6</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MemNet_M5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MemNet_M4</head><p>MemNet_M3 VDSR DRCN (sec.) <ref type="figure">Figure 6</ref>. PSNR, complexity vs. speed. BSD100 <ref type="bibr" target="#b27">[28]</ref> and Urban100 <ref type="bibr" target="#b14">[15]</ref> are used. Three scale factors are evaluated, including Ã—2, Ã—3 and Ã—4. The input LR image is generated by first bicubic downsampling and then bicubic upsampling the HR image with a certain scale. For JPEG deblocking, the same training set for image denoising is used. As in <ref type="bibr" target="#b6">[7]</ref>, Classic5 and LIVE1 are adopted as the test datasets. Two JPEG quality factors are used, i.e., 10 and 20, and the JPEG deblocking input is generated by compressing the image with a certain quality factor using the MATLAB JPEG encoder. Training Setting Following the method <ref type="bibr" target="#b26">[27]</ref>, for image denoising, the grayscale image is used; while for SISR and JPEG deblocking, the luminance component is fed into the model. The input image size can be arbitrary due to the fully convolution architecture. Considering both the training time and storage complexities, training images are split into 31 Ã— 31 patches with a stride of 21. The output of MemNet is the estimated high-quality patch with the same resolution as the input low-quality patch. We follow <ref type="bibr" target="#b33">[34]</ref> to do data augmentation. For each task, we train a single model for all different levels of corruption. E.g., for image denoising, noise augmentation is used. Images with different noise levels are all included in the training set. Similarly, for super-resolution and JPEG deblocking, scale and quality augmentation are used, respectively.</p><p>We use Caffe <ref type="bibr" target="#b18">[19]</ref> to implement two 80-layer MemNet networks, the basic and the multi-supervised versions. In both architectures, 6 memory blocks, each contains 6 recursions, are constructed (i.e., M6R6). Specifically, in multisupervised MemNet, 6 predictions are generated and used to compute the final output. Î± balances different regularizations, and is empirically set as Î± = 1/(M + 1).</p><p>The objective functions in Eqn. 4 and Eqn. 12 are optimized via the mini-batch stochastic gradient descent (SGD) with backpropagation <ref type="bibr" target="#b23">[24]</ref>. We set the mini-batch size of SGD to 64, momentum parameter to 0.9, and weight decay to 10 âˆ’4 . All convolutional layer has 64 filters. Except the 1 Ã— 1 convolutional layers in the gate units, the kernel size of other convolutional layers is 3 Ã— 3. We use the method in <ref type="bibr" target="#b10">[11]</ref> for weight initialization. The initial learning rate is set to 0.1 and then divided 10 every 20 epochs. Training a 80-layer basic MemNet by 91 images <ref type="bibr" target="#b37">[38]</ref> for SISR roughly takes 5 days using 1 Tesla P40 GPU. Due to space constraint and more recent baselines, we focus on SISR in Sec. 5.2, 5.4 and 5.6, while all three tasks in Sec. 5.3 and 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>Tab. 1 presents the ablation study on the effects of longterm and short-term connections. Compared to MemNet, MemNet NL removes the long-term connections (green curves in <ref type="figure" target="#fig_3">Fig. 3)</ref> and MemNet NS removes the short-term connections (black curves from the first R âˆ’ 1 recursions to the gate unit in <ref type="figure" target="#fig_1">Fig. 1</ref>. Connection from the last recursion to the gate unit is reserved to avoid a broken interaction between recursive unit and gate unit). The three networks have the same depth (80) and filter number (64). We see that, long-term dense connections are very important since MemNet significantly outperforms MemNet NL. Further, MemNet achieves better performance than MemNet NS, which reveals the short-term connections are also useful for image restoration but less powerful than the long-term connections. The reason is that the long-term connections skip much more layers than the short-term ones, which can carry some mid/high frequency signals from very early layers to latter layers as described in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Gate Unit Analysis</head><p>We now illustrate how our gate unit affects different kinds of memories. Inspired by <ref type="bibr" target="#b13">[14]</ref>, we adopt a weight norm as an approximate for the dependency of the current layer on its preceding layers, which is calculated by the corresponding weights from all filters w.r.t. each feature map: v l m = 64 i=1 (W gate m (1, 1, l, i)) 2 , l = 1, 2, ..., L m , where L m is the number of the input feature maps for the m-th gate unit, l denotes the feature map index, W gate m stores the weights with the size of 1 Ã— 1 Ã— L m Ã— 64, and v l m is the weight norm of the l-th feature map for the m-th gate unit. Basically, the larger the norm is, the stronger dependency it has on this particular feature map. For better visualization, we normalize the norms to the range of 0 to 1. <ref type="figure" target="#fig_6">Fig. 5</ref> presents the norm of the filter weights {v l m } 6 m=1 vs. feature map index l. We have three observations: (1) Different tasks have different norm distributions. (2) The average and variance of the weight norms become smaller as the memory block number increases. (3) In general, the short-term memories from the last recursion in recursive unit (the last 64 elements in each curve) contribute most than the other two memories, and the long-term memories seem to play a more important role in late memory blocks to recover useful signals than the short-term memories from the first R âˆ’ 1 recursions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Noise BM3D <ref type="bibr" target="#b4">[5]</ref> EPLL <ref type="bibr" target="#b40">[41]</ref> PCLR <ref type="bibr" target="#b1">[2]</ref> PGPD <ref type="bibr" target="#b36">[37]</ref> WNNM <ref type="bibr" target="#b8">[9]</ref> RED <ref type="bibr">[</ref>    <ref type="table">Table 5</ref>. Benchmark JPEG deblocking results. Average PSNR/SSIMs for quality factor 10 and 20 on datasets Classic5 and LIVE1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparision with Non-Persistent CNN Models</head><p>In this subsection, we compare MemNet with three existing non-persistent CNN models, i.e., VDSR <ref type="bibr" target="#b19">[20]</ref>, DRCN <ref type="bibr" target="#b20">[21]</ref> and RED <ref type="bibr" target="#b26">[27]</ref>, to demonstrate the superiority of our persistent memory structure. VDSR and DRCN are two representative networks with the plain structure and RED is representative for skip connections. Tab. 2 presents the published results of these models along with their training details. Since the training details are different among different work, we choose DRCN as a baseline, which achieves good performance using the least training images. But, unlike DRCN that widens its network to increase the parameters (filter number: 256 vs. 64), we deepen our MemNet by stacking more memory blocks (depth: 20 vs. 80). It can be seen that, using the fewest training images (91), filter number (64) and relatively few model parameters (667K), our basic MemNet already achieves higher PSNR than the prior networks. Keeping the setting unchanged, our multi-supervised MemNet further improves the performance. With more training images (291), our MemNet significantly outperforms the state of the arts.</p><p>Since we aim to address the long-term dependency problem in networks, we intend to make our MemNet very deep. However, MemNet is also able to balance the model complexity and accuracy. <ref type="figure">Fig. 6</ref> presents the PSNR of different intermediate predictions in MemNet (e.g., MemNet M3 denotes the prediction of the 3rd memory block) for scale Ã—3 on Set5, in which the colorbar indicates the inference time (sec.) when processing a 288 Ã— 288 image on GPU P40. Results of VDSR <ref type="bibr" target="#b19">[20]</ref> and DRCN <ref type="bibr" target="#b20">[21]</ref> are cited from their papers. RED <ref type="bibr" target="#b26">[27]</ref> is skipped here since its high number of parameters may reduce the contrast among other methods. We see that our MemNet already achieve comparable result at the 3rd prediction using much fewer parameters, and significantly outperforms the state of the arts by slightly increasing model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparisons with State-of-the-Art Models</head><p>We compare multi-supervised 80-layer MemNet with the state of the arts in three restoration tasks, respectively. Image Denoising Tab. 3 presents quantitative results on two benchmarks, with results cited from <ref type="bibr" target="#b26">[27]</ref>. For BSD200 dataset, by following the setting in RED <ref type="bibr" target="#b26">[27]</ref>, the original image is resized to its half size. As we can see, our MemNet achieves the best performance on all cases. It should be noted that, for each test image, RED rotates and mirror flips the kernels, and performs inference multiple times. The outputs are then averaged to obtain the final result. They claimed this strategy can lead to better performance. However, in our MemNet, we do not perform any post-processing. For qualitative comparisons, we use public codes of PCLR <ref type="bibr" target="#b1">[2]</ref>, PGPD <ref type="bibr" target="#b36">[37]</ref> and WNNM <ref type="bibr" target="#b8">[9]</ref>. The results are shown in <ref type="figure">Fig. 7</ref> Since LapSRN doesn't report the results on scale Ã—3, we use the symbol 'âˆ’' instead. <ref type="figure">Fig. 8</ref> shows the visual comparisons for SISR. SRCNN <ref type="bibr" target="#b7">[8]</ref>, VDSR <ref type="bibr" target="#b19">[20]</ref> and DnCNN <ref type="bibr" target="#b39">[40]</ref> are compared using their public codes. MemNet recovers relatively sharper edges, while others have blurry results. JPEG Deblocking Tab. 5 shows the JPEG deblocking results on Classic5 and LIVE1, by citing the results from <ref type="bibr" target="#b39">[40]</ref>.</p><p>Our network significantly outperforms the other methods, and deeper networks do improve the performance compared to the shallow one, e.g., ARCNN. <ref type="figure">Fig. 9</ref> shows the JPEG deblocking results of these three methods, which are generated by their corresponding public codes. As it can be seen, MemNet effectively removes the blocking artifact and recovers higher quality images than the previous methods.  <ref type="figure">Figure 9</ref>. Qualitative comparisons of JPEG deblocking. The first row shows image "barbara" from Classic5 with quality factor 10.</p><p>MemNet recovers the lines, while others give blurry results. The second row shows image "lighthouse" from LIVE1 with quality factor 10. MemNet accurately removes the blocking artifact. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison on Different Network Depths</head><p>Finally, we present the comparison on different network depths, which is caused by stacking different numbers of memory blocks or recursions. Specifically, we test four network structures: M4R6, M6R6, M6R8 and M10R10, which have the depth 54, 80, 104 and 212, respectively. Tab. 6 shows the SISR performance of these networks on Set5 with scale factor Ã—3. It verifies deeper is still better and the proposed deepest network M10R10 achieves 34.23 dB, with the improvement of 0.14 dB compared to M6R6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, a very deep end-to-end persistent memory network (MemNet) is proposed for image restoration, where a memory block accomplishes the gating mechanism for tackling the long-term dependency problem in the previous CNN architectures. In each memory block, a recursive unit is adopted to learn multi-level representations as the short-term memory. Both the short-term memory from the recursive unit and the long-term memories from the previous memory blocks are sent to a gate unit, which adaptively learns different weights for different memories. We use the same MemNet structure to handle image denoising, superresolution and JPEG deblocking simultaneously. Comprehensive benchmark evaluations well demonstrate the superiority of our MemNet over the state of the arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Prior network structures (a,b) and our memory block (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Basic MemNet architecture. The red dashed box represents multiple stacked memory blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Multi-supervised MemNet architecture. The outputs with purple color are supervised.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>(a) Ã—4 super-resolved images and PSNR/SSIMs of different networks. (b) We convert 2-D power spectrums to 1-D spectral densities by integrating the spectrums along each concentric circle. (c) Differences of spectral densities of two networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>The norm of filter weights v l m vs. feature map index l.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>... Memory block m</figDesc><table><row><cell>FENet</cell><cell>B0</cell><cell>Memory block 1</cell><cell>B1</cell><cell>Bm</cell><cell>...</cell><cell>Memory block M</cell><cell>BM</cell><cell>ReconNet</cell></row><row><cell>f ext</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>f rec</cell></row><row><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>y</cell></row><row><cell></cell><cell cols="3">Skip connection from input to the ReconNet</cell><cell>Short path transmission</cell><cell></cell><cell cols="3">Long path transmission to the gate unit</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.9591 37.71/0.9592 37.78/0.9597 Ã—3 33.96/0.9235 34.00/0.9239 34.09/0.9248 Ã—4 31.60/0.8878 31.65/0.8880 31.74/0.8893Table 1. Ablation study on effects of long-term and short-term connections. Average PSNR/SSIMs for the scale factor Ã—2, Ã—3 and Ã—4 on dataset Set5. Red indicates the best performance.</figDesc><table><row><cell>Methods</cell><cell>MemNet NL</cell><cell>MemNet NS</cell><cell>MemNet</cell></row><row><cell>Ã—2</cell><cell>37.68/0</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and other 200 are from BSD train set.</figDesc><table><row><cell>Dataset</cell><cell cols="2">VDSR [20] DRCN [21]</cell><cell>RED [27]</cell><cell></cell><cell>MemNet</cell><cell></cell></row><row><cell>Depth</cell><cell>20</cell><cell>20</cell><cell>30</cell><cell></cell><cell>80</cell><cell></cell></row><row><cell>Filters</cell><cell>64</cell><cell>256</cell><cell>128</cell><cell></cell><cell>64</cell><cell></cell></row><row><cell>Parameters</cell><cell>665K</cell><cell>1, 774K</cell><cell>4, 131K</cell><cell></cell><cell>677K</cell><cell></cell></row><row><cell>Traing images</cell><cell>291</cell><cell>91</cell><cell>300</cell><cell>91</cell><cell>91</cell><cell>291</cell></row><row><cell>Multi-supervision</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>PSNR</cell><cell>33.66</cell><cell>33.82</cell><cell>33.82</cell><cell cols="3">33.92 33.98 34.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">For testing, four benchmark datasets, Set5 [1], Set14 [39],</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.8204 28.35/0.8200 28.68/0.8263 28.55/0.8199 28.74/0.8273 29.17/0.8423 29.22/0.8444 50 26.08/0.7427 25.97/0.7354 26.29/0.7538 26.19/0.7442 26.32/0.7517 26.81/0.7733 26.91/0.7775 70 24.65/0.6882 24.47/0.6712 24.79/0.6997 24.71/0.6913 24.80/0.6975 25.31/0.7206 25.43/0.7260 BSD200 30 27.31/0.7755 27.38/0.7825 27.54/0.7827 27.33/0.7717 27.48/0.7807 27.95/0.8019 28.04/0.8053 50 25.06/0.6831 25.17/0.6870 25.30/0.6947 25.18/0.6841 25.26/0.6928 25.75/0.7167 25.86/0.7202 70 23.82/0.6240 23.81/0.6168 23.94/0.6336 23.89/0.6245 23.95/0.6346 24.37/0.6551 24.53/0.6608</figDesc><table><row><cell></cell><cell>27]</cell><cell>MemNet</cell></row><row><cell>30</cell><cell>28.49/0</cell></row><row><cell>14 images</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Benchmark image denoising results. Average PSNR/SSIMs for noise level 30, 50 and 70 on 14 images and BSD200. Red color indicates the best performance and blue color indicates the second best performance.</figDesc><table><row><cell>Dataset</cell><cell>Scale</cell><cell>Bicubic</cell><cell>SRCNN [8]</cell><cell>VDSR [20]</cell><cell>DRCN [21]</cell><cell>DnCNN [40]</cell><cell>LapSRN [23]</cell><cell>DRRN [34]</cell><cell>MemNet</cell></row><row><cell></cell><cell>Ã—2</cell><cell cols="5">33.66/0.9299 36.66/0.9542 37.53/0.9587 37.63/0.9588 37.58/0.9590</cell><cell>37.52/0.959</cell><cell>37.74/0.9591</cell><cell>37.78/0.9597</cell></row><row><cell>Set5</cell><cell>Ã—3</cell><cell cols="5">30.39/0.8682 32.75/0.9090 33.66/0.9213 33.82/0.9226 33.75/0.9222</cell><cell>âˆ’/âˆ’</cell><cell>34.03/0.9244</cell><cell>34.09/0.9248</cell></row><row><cell></cell><cell>Ã—4</cell><cell cols="5">28.42/0.8104 30.48/0.8628 31.35/0.8838 31.53/0.8854 31.40/0.8845</cell><cell>31.54/0.885</cell><cell>31.68/0.8888</cell><cell>31.74/0.8893</cell></row><row><cell></cell><cell>Ã—2</cell><cell cols="5">30.24/0.8688 32.45/0.9067 33.03/0.9124 33.04/0.9118 33.03/0.9128</cell><cell>33.08/0.913</cell><cell>33.23/0.9136</cell><cell>33.28/0.9142</cell></row><row><cell>Set14</cell><cell>Ã—3</cell><cell cols="5">27.55/0.7742 29.30/0.8215 29.77/0.8314 29.76/0.8311 29.81/0.8321</cell><cell>âˆ’/âˆ’</cell><cell>29.96/0.8349</cell><cell>30.00/0.8350</cell></row><row><cell></cell><cell>Ã—4</cell><cell cols="5">26.00/0.7027 27.50/0.7513 28.01/0.7674 28.02/0.7670 28.04/0.7672</cell><cell>28.19/0.772</cell><cell>28.21/0.7721</cell><cell>28.26/0.7723</cell></row><row><cell></cell><cell>Ã—2</cell><cell cols="5">29.56/0.8431 31.36/0.8879 31.90/0.8960 31.85/0.8942 31.90/0.8961</cell><cell>31.80/0.895</cell><cell>32.05/0.8973</cell><cell>32.08/0.8978</cell></row><row><cell>BSD100</cell><cell>Ã—3</cell><cell cols="5">27.21/0.7385 28.41/0.7863 28.82/0.7976 28.80/0.7963 28.85/0.7981</cell><cell>âˆ’/âˆ’</cell><cell>28.95/0.8004</cell><cell>28.96/0.8001</cell></row><row><cell></cell><cell>Ã—4</cell><cell cols="5">25.96/0.6675 26.90/0.7101 27.29/0.7251 27.23/0.7233 27.29/0.7253</cell><cell>27.32/0.728</cell><cell>27.38/0.7284</cell><cell>27.40/0.7281</cell></row><row><cell></cell><cell>Ã—2</cell><cell cols="5">26.88/0.8403 29.50/0.8946 30.76/0.9140 30.75/0.9133 30.74/0.9139</cell><cell>30.41/0.910</cell><cell>31.23/0.9188</cell><cell>31.31/0.9195</cell></row><row><cell>Urban100</cell><cell>Ã—3</cell><cell cols="5">24.46/0.7349 26.24/0.7989 27.14/0.8279 27.15/0.8276 27.15/0.8276</cell><cell>âˆ’/âˆ’</cell><cell>27.53/0.8378</cell><cell>27.56/0.8376</cell></row><row><cell></cell><cell>Ã—4</cell><cell cols="5">23.14/0.6577 24.52/0.7221 25.18/0.7524 25.14/0.7510 25.20/0.7521</cell><cell>25.21/0.756</cell><cell>25.44/0.7638</cell><cell>25.50/0.7630</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Benchmark SISR results. Average PSNR/SSIMs for scale factor Ã—2, Ã—3 and Ã—4 on datasets Set5, Set14, BSD100 and Urban100.</figDesc><table><row><cell>Dataset</cell><cell>Quality</cell><cell>JPEG</cell><cell>ARCNN [7]</cell><cell>TNRD [3]</cell><cell>DnCNN [40]</cell><cell>MemNet</cell></row><row><cell>Classic5</cell><cell>10 20</cell><cell cols="5">27.82/0.7595 29.03/0.7929 29.28/0.7992 29.40/0.8026 29.69/0.8107 30.12/0.8344 31.15/0.8517 31.47/0.8576 31.63/0.8610 31.90/0.8658</cell></row><row><cell>LIVE1</cell><cell>10 20</cell><cell cols="5">27.77/0.7730 28.96/0.8076 29.15/0.8111 29.19/0.8123 29.45/0.8193 30.07/0.8512 31.29/0.8733 31.46/0.8769 31.59/0.8802 31.83/0.8846</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>. As we can see, our MemNet handles Gaussian noise better than the previous state of the arts. Super-Resolution Tab. 4 summarizes quantitative results on four benchmarks, by citing the results of prior methods. MemNet outperforms prior methods in almost all cases.Figure 7. Qualitative comparisons of image denoising. The first row shows image "10" from 14-image dataset with noise level 30. Only MemNet recovers the fold. The second row shows image "206062" from BSD200 with noise level 70. Only MemNet correctly recovers the pillar. Please zoom in to see the details.Figure 8. Qualitative comparisons of SISR. The first row shows image "108005" from BSD100 with scale factor Ã—3. Only MemNet correctly recovers the pattern. The second row shows image "img 002" from Urban100 with scale factor Ã—4. MemNet recovers sharper lines.</figDesc><table><row><cell>Ground Truth</cell><cell>Noisy</cell><cell>PCLR</cell><cell>PGPD</cell><cell>WNNM</cell><cell>MemNet (ours)</cell></row><row><cell cols="6">(PSNR/SSIM) (18.56/0.2953) (29.89/0.8678) (29.80/0.8652) (29.93/0.8702) (30.48/0.8791)</cell></row><row><cell cols="6">(PSNR/SSIM) (11.19/0.1082) (24.67/0.6691) (24.49/0.6559) (24.50/0.6632) (25.37/0.6964)</cell></row><row><cell>Ground Truth</cell><cell>Bicubic</cell><cell>SRCNN</cell><cell>VDSR</cell><cell>DnCNN</cell><cell>MemNet (ours)</cell></row><row><cell cols="6">(PSNR/SSIM) (26.43/0.7606) (27.74/0.8194) (28.18/0.8341) (28.19/0.8349) (28.35/0.8388)</cell></row><row><cell cols="6">(PSNR/SSIM) (21.68/0.6491) (22.85/0.7249) (23.91/0.7859) (23.89/0.7838) (24.62/0.8167)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>(PSNR/SSIM)(25.79/0.7621) (26.92/0.7971) (27.24/0.8104) (27.59/0.8161) (28.15/0.8353)</figDesc><table><row><cell>Ground Truth</cell><cell>JPEG</cell><cell>ARCNN</cell><cell>TNRD</cell><cell>DnCNN</cell><cell>MemNet (ours)</cell></row><row><cell cols="6">(PSNR/SSIM) (28.29/0.7636) (29.63/0.7977) (29.76/0.8018) (29.82/0.8008) (30.13/0.8088)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 .</head><label>6</label><figDesc>Comparison on different network depths.</figDesc><table><row><cell>Network</cell><cell cols="4">M4R6 M6R6 M6R8 M10R10</cell></row><row><cell>Depth</cell><cell>54</cell><cell>80</cell><cell>104</cell><cell>212</cell></row><row><cell cols="2">PSNR (dB) 34.05</cell><cell>34.09</cell><cell>34.16</cell><cell>34.23</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the first memory block, its long-term memory comes from the output of FENet.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">External patch prior guided internal clustering for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Branch-specific dendritic ca2+ spikes cause persistent synaptic plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cichon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">520</biblScope>
			<biblScope unit="issue">7546</biblScope>
			<biblScope unit="page" from="180" to="185" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on IP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Theoretical neuroscience. Cambridge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
			<pubPlace>MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data-driven sparsity-based restoration of jpeg-compressed images in dual transform-pixel domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tour of modern image filtering: new insights and methods, both practical and theoretical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="128" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">D 3 : Deep dual-domain based fast restoration of jpegcompressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Patch group based nonlocal self-similarity prior learning for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on IP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On single image scaleup using sparse-representations. Curves and Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on IP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
