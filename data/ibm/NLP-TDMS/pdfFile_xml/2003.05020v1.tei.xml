<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Video Object Segmentation from Unlabeled Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
							<email>wenguanwang.ai@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<affiliation key="aff2">
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Indiana University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Salesforce Research Asia</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Video Object Segmentation from Unlabeled Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new method for video object segmentation (VOS) that addresses object pattern learning from unlabeled videos, unlike most existing methods which rely heavily on extensive annotated data. We introduce a unified unsupervised/weakly supervised learning framework, called MuG, that comprehensively captures intrinsic properties of VOS at multiple granularities. Our approach can help advance understanding of visual patterns in VOS and significantly reduce annotation burden. With a carefully-designed architecture and strong representation learning ability, our learned model can be applied to diverse VOS settings, including object-level zero-shot VOS, instance-level zero-shot VOS, and one-shot VOS. Experiments demonstrate promising performance in these settings, as well as the potential of MuG in leveraging unlabeled data to further improve the segmentation accuracy. * Corresponding author: Wenguan Wang.</p><p>1 Some conventions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b58">59]</ref> also use 'unsupervised VOS' and 'semisupervised VOS' to name the Z-VOS and O-VOS settings <ref type="bibr" target="#b2">[3]</ref>. In this work, for notational clarity, the terms 'supervised', 'weakly supervised' and 'unsupervised' are only used to address the different learning paradigms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) has two common settings, zero-shot and one-shot. Zero-shot VOS (Z-VOS) <ref type="bibr" target="#b0">1</ref> is to automatically segment out the primary foreground objects, without any test-time human supervision; whereas one-shot VOS (O-VOS) focuses on extracting the human determined foreground objects, typically assuming the firstframe annotations are given ahead inference <ref type="bibr" target="#b0">1</ref> . Current leading methods for both Z-VOS and O-VOS are supervised deep learning models that require extensive amounts of elaborately annotated data to improve the performance and avoid over-fitting. However, obtaining pixel-wise segmentation labels is labor-intensive and expensive ( <ref type="figure" target="#fig_0">Fig.1(a)</ref>).</p><p>It is thus attractive to design VOS models that can learn  from unlabeled videos. With this aim in mind, we develop a unified, unsupervised/weakly supervised VOS method that mines multi-granularity cues to facilitate video object pattern learning ( <ref type="figure" target="#fig_0">Fig.1(b)</ref>). This allows us to take advantage of nearly infinite amounts of video data. Below we give a more formal description of our problem setup and main idea. Problem Setup and Main Idea. Let X and Y denote the input video space and output VOS space, respectively. Deep learning based VOS solutions seek to learn a differentiable, ideal video-to-segment mapping g * :X →Y. To approximate g * , recent leading VOS models typically work in a supervised learning manner, requiring N input samples and their desired outputs y n := g * (x n ), where {(x n , y n )} n ⊂X ×Y.</p><p>In contrast, we address the problem in settings with much less supervision: (1) the unsupervised case, when we only have samples drawn from X , {x n } n ⊂X , and want to approximate g * , and (2) the weakly supervised learning setting, in which we have annotations for K, which is a related output domain for which obtaining annotations is easier than Y, and we approximate g * using samples from X ×K. The standard way of evaluating learning outcomes follows an empirical risk/loss minimization formulation <ref type="bibr" target="#b42">[43]</ref>: g ∈ arg min g∈G 1 N n ε(g(xn), z(xn)), <ref type="bibr" target="#b0">(1)</ref> where G denotes the hypothesis (solution) space, and ε:X×Y →R is an error function that evaluates the estimate g(x n ) against VOS-related prior knowledge z(x n ) ∈Z.</p><p>To makeg a good approximation of g * , current supervised VOS methods directly use the desired output y n , i.e., z(x n ):=g * (x n ), as the prior knowledge, with the price of vast amounts of well-annotated data. In our method, the prior knowledge Z, in the unsupervised learning setting, is built upon several heuristics and intrinsic properties of VOS itself, while in the weakly supervised learning setting, it additionally considers a related, easily-annotated output domain K. For example, part of the fore-background knowledge could be from a saliency model <ref type="bibr" target="#b69">[70]</ref>  <ref type="figure" target="#fig_0">(Fig. 1 (b)</ref>), or in a form of CAM maps <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b75">76]</ref> from a pre-trained image classifier <ref type="bibr" target="#b13">[14]</ref> (i.e., a related image classification domain K) 2 . Exploring VOS in an unsupervised or weakly supervised setting is appealing not only because it alleviates the annotation burden of Y, but also because it inspires an in-depth understanding of the nature of VOS by exploring Z. Specifically, we analyze several different types of cues at multiple granularities, which are crucial for video object pattern modeling:</p><p>• At the frame granularity, we leverage information from an unsupervised saliency method <ref type="bibr" target="#b69">[70]</ref> or CAM <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b75">76]</ref> activation maps to enhance the foreground and background discriminability of our intra-frame representation. • At the short-term granularity, we impose the local consistency within the representations of short video clips, to describe the continuous and coherent visual patterns within a few seconds. • At the long-range granularity, we address semantic correspondence among distant frames, which makes the crossframe representations robust to local occlusions, appearance variations and shape deformations. • At the whole-video granularity, we encourage the video representation to capture global and compact video content, by learning to aggregate multi-frame information and be discriminative to other videos' representations.</p><p>All these constraints are formulated under a unified, multi-granularity VOS (MuG) framework, which is fully differentiable and allows unsupervised/weakly supervised video object pattern learning, from unlabeled videos. Our extensive experiments over various VOS settings, i.e., object-level Z-VOS, instance-level Z-VOS, and O-VOS, show that MuG outperforms other unsupervised and weakly supervised methods by a large margin, and continuously improves its performance with more unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><formula xml:id="formula_0">2.1. Video Object Segmentation Z-VOS.</formula><p>As there is no indication for objects to be segmented, conventional ZVOS methods resorted to certain heuristics, such as saliency <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b6">7]</ref>, object proposals <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24]</ref>, and discriminative motion patterns <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33]</ref>. Recent advances have been driven by deep learning techniques. Various effective networks have been explored, from some early, relatively simple architectures, such as recurrent network <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b62">63]</ref>, and two-stream network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b76">77]</ref>, to recent, more powerful designs, such as teacher-student adaption <ref type="bibr" target="#b43">[44]</ref>, neural co-attention <ref type="bibr" target="#b25">[26]</ref> and graph neural network <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b67">68]</ref>. O-VOS. As the annotations for the first frame are assumed available at the test phase, O-VOS focuses on how to accurately propagate the initial labels to subsequent frames. Traditional methods typically used optical flow based propagation strategy <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b27">28]</ref>. Now, deep learning based solutions become the main stream, which can be broadly classified into three categories, i.e., online learning, propagation and matching based methods. Online learning based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b34">35]</ref> fine-tune the segmentation network for each test video on the first-frame annotations. Propagation based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> rely on the segments of the previous frames and work in a frame-by-frame manner. Matching based methods <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b26">27]</ref> segment each frame according to its correspondence/matching relation to the first frame.</p><p>Typically, current deep learning based VOS solutions, under either Z-VOS or O-VOS setting, are trained using a large amount of elaborately annotated data for supervised learning. In contrast, the proposed method trains a VOS network from scratch using unlabeled videos. This is essential for understanding how visual recognition works in VOS and for narrowing down the annotation budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">VOS with Unlabeled Training Videos</head><p>Learning VOS from unlabeled videos is an essential, yet rarely touched avenue. Among a few efforts, <ref type="bibr" target="#b33">[34]</ref> represents an early attempt in this direction, which uses a modified, purely unsupervised version of <ref type="bibr" target="#b6">[7]</ref> to generate proxy masks as pseudo annotations. With a similar spirit, some methods use heuristic segmentation masks <ref type="bibr" target="#b16">[17]</ref> or weakly supervised location maps <ref type="bibr" target="#b22">[23]</ref> as supervisory signals. With a broader view, some works <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b73">74]</ref> capitalized on untrimmed videos tagged with semantic labels. In addition to increased annotation efforts, they are hard to handle such a class-agnostic VOS setting. Recently, self-supervised video learning has been applied for O-VOS <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b64">65]</ref>, which imposes the learned features to capture certain constraints on local coherence, such as cross-frame color consistency <ref type="bibr" target="#b55">[56]</ref> and temporal cycle-correspondence <ref type="bibr" target="#b64">[65]</ref>.</p><p>Our method is distinctive for two aspects. First, it explores various intrinsic properties of videos as well as classagnostic fore-background knowledge in a unified, multigranularity framework, bringing a more comprehensive understanding of visual patterns in VOS. Second, it shows strong video object representation learning ability and, for the first time, it is applied to diverse VOS settings after only being trained once. This gives a new glimpse into the connections between the two most influential VOS settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Granularity VOS Network</head><p>For a training video X ∈X containing T frames: X = {X t } T t=1 , its features are specified as {x t } T t=1 , obtained from a fully convolutional feature extractor ϕ: x t =ϕ(X t ) ∈ R W×H×C . Four-granularity characteristics are explored to guide the learning of ϕ ( <ref type="figure" target="#fig_1">Fig.2)</ref>, described as follows. Frame Granularity Analysis: Fore-background Knowledge Understanding. As ϕ is VOS-aware, basic forebackground knowledge is desired to be encoded. In our method, such knowledge ( <ref type="figure" target="#fig_0">Fig.1(b)</ref>) is initially from a background prior based saliency model <ref type="bibr" target="#b69">[70]</ref> (in an unsupervised learning setting), or in a form of CAM maps <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b75">76]</ref> (in a weakly supervised learning setting).</p><p>Formally, for each frame X t , let us denote its corresponding initial fore-background mask as Q t ∈ {0, 1} W×H (i.e., a binarized saliency or CAM activation map). In our frame granularity analysis, the learning of ϕ is guided by the supervision signals of {Q t } T t=1 , i.e., utilizing the intraframe information x t =ϕ(X t ) to regress Q t :</p><formula xml:id="formula_1">Lframe = LCE(Pt, Qt).<label>(2)</label></formula><p>Here L CE is the cross-entropy loss, and P t = ρ(x t ) where ρ: R W×H×C →[0, 1] W×H maps the input single-frame feature x t into a fore-background prediction map P t . ρ is implemented by a 1×1 convolutional layer with sigmoid activation. Short-Term Granularity Analysis: Intra-Clip Coherence Modeling. Short-term coherence is an essential property in videos, as temporally close frames typically exhibit continuous visual content changes <ref type="bibr" target="#b14">[15]</ref>. To capture this property, we apply a forward-backward patch tracking mechanism <ref type="bibr" target="#b56">[57]</ref>. It learns ϕ by tracking a sampled patch forwards in a few successive frames and then backwards until the start frame, and penalizing the distance between the initial and final backwards tracked positions of that patch. Formally, given two consecutive frames X t and X t+1 , we first crop a patch p from X t and apply ϕ on p and X t+1 , separately. Then we get two feature embeddings: ϕ(p) ∈ R w×h×C and x t+1 = ϕ(X t+1 ) ∈ R W×H×C . With a design similar to the classic Siamese tracker <ref type="bibr" target="#b1">[2]</ref>, we forward track the patch p on the next frame X t+1 by conducting a crosscorrelation operation ' ' on ϕ(p) and ϕ(X t+1 ):</p><formula xml:id="formula_2">S⇒ = ϕ(p) ϕ(Xt+1) ∈ [0, 1] W×H ,<label>(3)</label></formula><p>where S ⇒ is a sigmoid-normalized response map whose size is rescaled into (H, W ). The new location of p in X t+1 is then inferred according to the peak value on S ⇒ . After obtaining the forward tracked patch p in X t+1 , we backward track p to X t and get a backward tracking response map S ⇐ :</p><formula xml:id="formula_3">S⇐ = ϕ(p ) ϕ(Xt) ∈ [0, 1] W×H .<label>(4)</label></formula><p>Ideally, the peak of S ⇐ should correspond to the location of p in the initial frame X t . Thus we build a consistency loss that measures the alignment error between the initial and forward-backward tracked positions of p:</p><formula xml:id="formula_4">L short = S⇐ − Gp 2 2 ,<label>(5)</label></formula><p>where G p ∈ [0, 1] W×H is a (H, W )-dimensional Gaussianshape map with the same center of p and variance proportional to the size of p. As in <ref type="bibr" target="#b56">[57]</ref>, the above forwardbackward tracking mechanism is extended to a multi-frame setting ( <ref type="figure">Fig. 3</ref>). Specifically, after obtaining the forward  tracked patch p in X t+1 , p is further tracked to the next frame X t+2 , and a new tracked patch p is obtained. Then p is reversely tracked to X t+1 and further to the initial frame X t , and the local consistency loss in Eq. 5 is computed. Moreover, during training, we first random sample a short video clip consisting of six successive frames. Then we perform above forward-backward tracking based learning strategy over three frames random drawn from the six-frame video clip. With above designs, ϕ captures the spatiotemporally local correspondence and is content-discriminative (due to its cross-frame target re-identification nature). Long-Term Granularity Analysis: Cross-Frame Semantic Matching. In addition to the local consistency among adjacent frames, there also exist strong semantic correlations among distant frames, as frames from the same video typically contain similar content <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b68">69]</ref>. Capturing this property is essential for ϕ, as it makes ϕ robust to many challenges, such as appearance variants, shape deformations, object occlusions, etc. To address this issue, we conduct a long-term granularity analysis, which casts crossframe correspondence learning as a dual-frame semantic matching problem ( <ref type="figure">Fig. 4)</ref>. Specifically, given a training pair of two disordered frames (X i , X j ) randomly sampled from X , we compute a similarity affinity A i,j between their embeddings: (ϕ(X i ), ϕ(X j )) by a co-attention operation <ref type="bibr" target="#b51">[52]</ref>:</p><formula xml:id="formula_5">Instance-level Z-VOS Segmentation Results Conv LSTM Dynamic Attention UVOS-aware Attention Mechanism Z T A T G T X T Y T ResNet50 DeepLabV3 A t S t I t G t Y t S T (b) S t I t 6 0×6 0 6 0×6 0 3 0×3 0 3 0×3 0 119×119 237×237 DVAP AGOS DVAP AGOS Y t G T Y T Frame Short-term</formula><formula xml:id="formula_6">Ai,j = softmax(xi xj) ∈ [0, 1] (WH)×(WH) ,<label>(6)</label></formula><p>where x i ∈ R C×(WH) and x j ∈ R C×(WH) are flat matrix formats of ϕ(X i ) and ϕ(X j ), respectively. 'softmax' indicates column-wise softmax normalization. Given the normalized cross-correlation A i,j , in line with <ref type="bibr" target="#b40">[41]</ref>, we use a small neural network κ : R (W×H)×(W×H) → R 6 to regress the parameters of a geometric transformation τ i,j , i.e., six-degree of freedom (translation, rotation and scale). τ i,j : R 2 → R 2 gives the relations between the spatial coordinates in X i and X j considering the corresponding semantic similarity:</p><formula xml:id="formula_7">mi = τi,j(mj),<label>(7)</label></formula><p>where m i is a 2-D spatial coordinate of X i , and m j the corresponding sampling coordinates in X j . Using τ i,j , we can warp X i to X j . Similarly, we can also compute τ j,i , i.e., a 2-D warping from X j to X i . Let us consider two sampling coordinates m i and n j in X j and X i , respectively, we introduce a semantic matching loss <ref type="bibr" target="#b40">[41]</ref>:</p><formula xml:id="formula_8">Llong = − m i ∈Ω o j ∈Ω Ai,j(mi, oj)ι(mi, oj)+ n j ∈Ω o i ∈Ω Aj,i(nj, oi)ι(mi, oi) ,<label>(8)</label></formula><p>where Ω refers to the image lattice,</p><formula xml:id="formula_9">A i,j (m i , o j ) ∈ [0, 1]</formula><p>gives the similarity value between the positions m i and o j in X i and X j , and ι(m i , o j ) determines if the correspondence between m i and o j is geometrically consistent. If</p><formula xml:id="formula_10">||m i , τ i,j (o j )||≤1, ι=1; otherwise ι=0.</formula><p>Video Granularity Analysis: Global and Discriminative Representation Learning. So far, we have used the pairwise cross-frame information in local and long terms to boost the learning of ϕ. ϕ is also desired to learn a compact and globally discriminative video representation. To achieve this, with a global information aggregation module, we perform a video granularity analysis within an unsupervised video embedding learning framework <ref type="bibr" target="#b0">[1]</ref>, which leverages supervision signals from different videos. Starting with our global information aggregation module, we split X = {X t } T t=1 into K segments of equal durations: X= ∪ K k=1 X k . For each segment X k , we randomly sample a single frame, resulting in a K-frame abstract X ={X t k } K k=1 of X . X reduces the redundancy among successive frames while preserving global information.</p><p>With a similar spirit of key-value retrieval networks <ref type="bibr" target="#b45">[46]</ref>, for each X t k ∈ X , we set it as a query and the rest frames X /X t k as reference. Then we compute the normalized cross-correlation between the query and reference:</p><formula xml:id="formula_11">At k = softmax(xt k [{xt k }t k ]) ∈ [0, 1] (WH)×(WH(K−1)) ,<label>(9)</label></formula><p>where k ∈ {1, · · · , K}/k, and '[·]' denotes the concatenation operation. x t k ∈R C×(WH) and [{x t k } t k ∈{1,··· ,K}/k ] ∈ R C×(WH(K−1)) are flat feature matrices of the query and reference, respectively. Subsequently, A t k is used as a weight matrix for global information summarization:</p><formula xml:id="formula_12">x t k = [{xt k }t k ] A t k ∈ R (WH)×C , where k ∈ {1, · · ·, K}/k.<label>(10)</label></formula><p>Our global information aggregation module gathers information from the reference set by a correlation-based feature summarization procedure. For the query frame X t k , we obtain its global information augmented representation by:</p><formula xml:id="formula_13">rt k = [x t k , xt k ] ∈ R W×H×2C .<label>(11)</label></formula><p>During training, the video granularity analysis essentially discriminates between a set of surrogate video classes <ref type="bibr" target="#b0">[1]</ref>. Specifically, given N training videos, we randomly sample a single frame from each video, leading to N training instances: {X n } N n=1 . The core idea is that, for a query frame X n t k in the n-th video, its global feature embedding is close to the instance X n from the same n-th video, and far from other unrelated instances {X n } n =n (from the other N−1 videos). We solve this as a binary classification problem via maximum likelihood estimation (MLE). In particular, for X n t k , instance X n should be classified into n, while other instances {X n } n =n shouldn't be. The probability of X n being recognized as instance n is:</p><formula xml:id="formula_14">P (n|X n ) = exp(GAP(r n t k r n )) N i=1 exp(GAP(r n t k r i ))</formula><p>. <ref type="bibr" target="#b11">(12)</ref> where 'GAP' stands for global average pooling. Similarly, given X n t k , the probability of other instances X n be recognized as instance n is:</p><formula xml:id="formula_15">P (n|X n ) = exp(GAP(r n t k r n )) N i=1 exp(GAP(r n t k r i )) .<label>(13)</label></formula><p>Correspondingly, the probability of X n not being recognized as instance n is 1−P (n|X n ). The joint probability of X n being recognized as instance n and X n not being is: P (n|X n ) n =n (1−P (n|X n )), under the assumption that different instances being recognized as n are independent.</p><p>Then the loss function is defined as the negative log likelihood over N query frames from N videos:</p><formula xml:id="formula_16">L global = − n log P (n|X n )− n n =n log(1−P (n|X n )). (14)</formula><p>Next we will describe the network architecture during the training and inference phases. An appealing advantage of our multi-granularity VOS network is that, after being trained in a unified mode, it can be directly applied to both Z-VOS and O-VOS settings with only slight adaption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.One Training Phase for both Z-VOS and O-VOS</head><p>Network Architecture. Our whole module is end-to-end trainable. The video representation space ϕ is learned by a fully convolutional network, whose design is inspired by ResNet-50 <ref type="bibr" target="#b12">[13]</ref>. In particular, the first four groups of convolutional layers in ResNet are preserved and dilated convolutional layer <ref type="bibr" target="#b71">[72]</ref> is used to maintain enough spatial details as well as ensure a large receptive field, resulting in a 512channel feature representation x whose spatial dimensions are 1/4 of an input video frame X.</p><p>During training, we use a mini-batch of N = 16 videos and scale all the training frames into 256×256 pixels. For frame granularity analysis, all the frames access to the supervision signal from the loss L frame in Eq.2.</p><p>For short-term granularity analysis, six successive video frames are first randomly sampled from each training video, resulting a six-frame video clip. For each video clip, we further sample three video frames orderly and randomly crop a 64 × 64 patch as p. With the feature embedding ϕ(p) ∈ R 16×16×64 of p, we forward-backward track p and get its final backward tracking response map S ⇐ ∈ [0, 1] 64×64 via Eq. 4. For computing the loss in Eq. 5, the Gaussianshape map G p ∈ [0, 1] 64×64 is obtained by convolving the center position of p with a two-dimension Gaussian map with a kernel width proportional (0.1) to the patch size.</p><p>For long-term granularity analysis, after randomly sampling two disordered frames (X i , X j ) (|i − j| ≥ 6) from a training video X , we compute the correlation map A i,j ∈ [0, 1] (64×64)×(64×64) by the normalized inner production operation in Eq. 6. For the geometric transformation parameter estimator κ: R (64×64)×(64×64) → R 6 , it is achieved by two convolutional layers and one linear layer, as in <ref type="bibr" target="#b40">[41]</ref>. Then the semantic matching loss in Eq.8 is computed.</p><p>For video granularity analysis, we split each training video X into K = 8 segments, and get the global information augmented representation r t k ∈ R 64×64×256 for each query frame X t k by Eq. 11. Then, we compute the softmax embedding learning loss using Eq.14, which leverages supervision signals from the N training videos. Iterative Training by Bootstrapping. As seen in <ref type="figure" target="#fig_0">Fig.1(b)</ref>, the fore-background knowledge from the saliency <ref type="bibr" target="#b69">[70]</ref> or CAM <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b75">76]</ref> is ambiguous and noisy. Inspired by Bootstrapping <ref type="bibr" target="#b39">[40]</ref>, we apply an iterative training strategy: after training with the initial fore-background maps, we use our trained model to re-label the training data. With each iteration, the learner bootstraps itself by mining better forebackground knowledge and then leading a better model. Specifically, for each training frame X, given the initial fore-background mask Q ∈ {0, 1} 64×64 and current predic-tionP i ∈ {0, 1} 64×64 of the model in i-th training iteration, the loss in Eq. 2 in (i + 1)-th iteration is formulated in a bootstrapping format:</p><formula xml:id="formula_17">L (i+1) frame = m∈Ω [αQm +(1−α)P i m ] log(P i+1 m )+ [α(1−Qm)+(1−α)(1−P i m )]log(1−P i+1 m ),<label>(15)</label></formula><p>where α = 0.05 and Q m gives the value in position m. In such a design, the 'confident' fore-background knowledge is generated as a convex combination of the initial forebackground information Q and model prediction P .</p><p>In the i-th training iteration, the overall loss to optimize the whole network parameters is the combination of the losses in Eq.15,4,8 and14:</p><formula xml:id="formula_18">L (h) = L (h) frame +β1L short +β2L long +β3L global ,<label>(16)</label></formula><p>where βs are coefficients: β 1 = 0.1, β 2 = 0.02 and β 3 = 0.5. The above designs enable a unified un-/weakly supervised feature learning framework. Once the model is trained, the learned representations ϕ can be used for Z-VOS and O-VOS, with slight modifications. In practice, we find that our model can perform well after being trained with 2 iterations; please see §4.2 for related experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference Modes for Z-VOS and O-VOS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we detail our inference modes for object-level Z-VOS, instance-level Z-VOS, and O-VOS settings.</head><p>Object-Level Z-VOS Setting. For each test frame, objectlevel Z-VOS aims to predict a binary segmentation mask where the primary foreground objects are separated from the background while the identities of different foreground objects are not distinguished. In the classic VOS setting, since there is no any test-time human intervention, how to discover the primary video objects is the central problem. Considering the fact that interested objects frequently appear throughout the video sequence, we readout the segmentation results from the global information augmented feature r, instead of directly using intra-frame information to predict the fore-background mask (i.e., ρ(x)). This is achieved by an extra segmentation readout layer υ : R 64×64×256 → [0, 1] 64×64 , which takes the global frame embedding r as the input and produces the final object-level segmentation prediction. υ is also trained by the crossentropy loss, as in Eq.15. For notation clarity, we omit this term in the overall training loss in Eq. 16. Please note that υ is only used in Z-VOS setting; for O-VOS setting, the segmentation masks are generated with a different strategy. Instance-Level Z-VOS Setting. Our model can also be adapted for the instance-level Z-VOS setting, in which different object instances must be discriminated, in addition to separating the primary video objects from the background without test-time human supervision. For each test frame, we first apply mask-RCNN <ref type="bibr" target="#b11">[12]</ref> to produce a set of category agnostic object proposals.Then we apply our trained model for producing a binary foregroundbackground mask per frame. After combining object bounding-box proposals with binary object-level segmentation masks, we can filter out the background proposals and obtain pixel-wise, instance-level object candidates for each frame. Finally, to link those object candidates across different frames, similar to <ref type="bibr" target="#b26">[27]</ref>, we use overlap ratio and optical flow as the cross-frame candidate-association metric. Note that, mask-RCNN can be replaced with nonlearning Edgebox <ref type="bibr" target="#b77">[78]</ref> and GrabCut, resulting a purely unsupervised/weakly-supervised protocol. O-VOS Setting. In O-VOS, for each test video sequence, instance-level annotations regarding multiple general foreground objects in the first frame are given. In such a setting, our trained network works in a per-frame matching based mask propagation fashion. Concretely, assume there are a total of L object instances (including the background) in the first-frame annotation, each spatial position n ∈ Ω will be associated with a one-hot class vectorŷ n ∈ {0, 1} L , whose elementŷ l n indicates whether pixel n belong to l-th object instance. Starting from the second frame, we use both the last segmented frame X t−1 as well as current under-segmented frame X t to build an input pair for our model. Then we compute their similarity affinity A t−1,t ∈[0, 1] (64×64)×(64×64) in the feature space: A t−1,t =softmax(x t−1 x t ). After that, for each pixel m in X t , we compute its probability distribution v m ∈ [0, 1] L over the L object instances as: </p><p>where A t−1,t (n, m) ∈ [0, 1] is the affinity value between pixel n in X t−1 and m in X t . For m, it is assigned to l *th instance: Then we get its label vectorŷ m . In this way, from the segmented frame X t , we move to the next input frame pair (X t , X t+1 ) and get the segmentation result for X t+1 . As our method does not use any first-frame fine-tuning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref> or online learning <ref type="bibr" target="#b54">[55]</ref> technique, it is fast for inference.</p><formula xml:id="formula_20">l * = arg max l ({v l m } L l=1 ), where v m = [v l m ] L l=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Common Setup</head><p>Implementation Details. We train the whole network from scratch on the OxUvA <ref type="bibr" target="#b50">[51]</ref> tracking dataset, as in <ref type="bibr" target="#b21">[22]</ref>. Ox-UvA comprises 366 video sequences with more than 1.5 million frames in total. We train our model with SGD optimizer. For our bootstrapping based iterative training, two iterations are used and each takes about 8 hours.</p><p>Configuration and Reproducibility. MuG is implemented on PyTorch. All experiments are conducted on an Nvidia TITAN Xp GPU and an Intel (R) Xeon E5 CPU. All our implementations, trained models, and segmentation results will be released to provide the full details of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Diagnostic Experiments</head><p>A series of ablation studies are performed for assessing the effectiveness of each essential component of MuG. Initial Fore-Background Knowledge. Baselines Heuristic Saliency and CAM give the scores of initial forebackground knowledge, based on their CRF-binarized outputs. As seen, with the low-quality initial knowledge, our MuG gains huge performance improvements (+20.8% and +15.9% promotions), showing the significance of our multi-granularity video object pattern learning scheme. Multi-Granularity Analysis. Next we investigate the contributions of multi-granularity cues in depth. As shown in Table1, the intrinsic, multi-granularity properties are indeed meaningful, as disabling any granularity analysis component causes performance to erode. For instance, removing the frame granularity analysis during learning hurts performance (mean J : 58.0 → 40.2, 61.2 → 40.2), due to the lack of fore-/background information. Similarly, performance drops when excluding short-or long-term granularity analysis, suggesting the importance of capturing local consis-    <ref type="bibr" target="#b15">16</ref> , we report region similarity J , boundary accuracy F and time stability T . For Youtube-Objects, the performance is evaluated in terms of region similarity J . Post-processing. Following the common protocol in this area <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b5">6]</ref>, the final segmentation results are optimized by CRF <ref type="bibr" target="#b20">[21]</ref> (about 0.3s per frame). Quantitative Results.  MuG also outperforms classical weakly-supervised Z-VOS method COSEG <ref type="bibr" target="#b49">[50]</ref>, and all the previous heuristic methods.   PDB <ref type="bibr" target="#b44">[45]</ref>). Notably, it significantly outperforms recent RVOS <ref type="bibr" target="#b52">[53]</ref> (mean T &amp;F: +14.8% and +19.2% in unsupervised and weakly-supervised learning setting, respectively). Runtime Comparison. The processing time for each frame is about 0.7s which is comparable to AGS <ref type="bibr" target="#b62">[63]</ref> and PDB <ref type="bibr" target="#b44">[45]</ref>, and slightly slower than RVOS <ref type="bibr" target="#b52">[53]</ref> (0.3s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance for O-VOS</head><p>Datasets. DAVIS 16 <ref type="bibr" target="#b35">[36]</ref> and DAVIS 17 <ref type="bibr" target="#b37">[38]</ref> datasets are used for performance evaluation under the O-VOS setting. Evaluation Criteria. Three standard evaluation criteria are reported: region similarity J , boundary accuracy F and the average value of T &amp;F. For DAVIS <ref type="bibr" target="#b15">16</ref> dataset, we further report the time stability T . Quantitative Results. <ref type="table" target="#tab_9">Table 5</ref> and <ref type="table" target="#tab_10">Table 6</ref> give evaluation results on DAVIS <ref type="bibr" target="#b15">16</ref> and DAVIS 17 , respectively. <ref type="table" target="#tab_9">Table 5</ref> shows that our unsupervised method exceeds representative self-supervised methods (i.e., TimeCyle <ref type="bibr" target="#b64">[65]</ref> and CorrFlow <ref type="bibr" target="#b64">[65]</ref>) and the best non-learning method (i.e., BVS <ref type="bibr" target="#b27">[28]</ref>) across most metrics. In particular, with the learned CAM as supervision, our weakly supervised method further improves the performance, e.g., mean J of 65.7. <ref type="table" target="#tab_10">Table 6</ref> verifies again our method performs favorably against the current best unsupervised method, CorrFlow, according to mean T &amp;F (54.3 vs 50.3). Note that CorrFlow and our method use the same training data. This demonstrates our MuG is able to learn more powerful video object patterns, compared to previous self-learning counterparts. Runtime Comparison. In instance-level Z-VOS setting, MuG runs about 0.4s per frame. This is faster than matching based methods (e.g., SIFT Flow <ref type="bibr" target="#b24">[25]</ref> (5.1s) and mgPFF <ref type="bibr" target="#b19">[20]</ref> (1.3s)), and favorably against self-supervised learning methods, e.g., TimeCycle <ref type="bibr" target="#b64">[65]</ref> and CorrFlow <ref type="bibr" target="#b21">[22]</ref>. (bottom row). For blackswan in DAVIS 16 <ref type="bibr" target="#b35">[36]</ref>, the primary objects undergo view changes and background clutter, but our MuG still generates accurate foreground segments. The effectiveness of instance-level Z-VOS can be observed in tram of DAVIS 17 <ref type="bibr" target="#b3">[4]</ref>. In addition, MuG can produce highquality results with the given first-frame annotations in O-VOS setting (see the results on the last row for scooter-black in DAVIS 17 <ref type="bibr" target="#b37">[38]</ref>), although the different instances suffer from fast motion and scale variation. More results can be found in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed MuG -an end-to-end trainable, unsupervised/weakly supervised learning approach for segmenting objects from the videos. Different from current popular supervised VOS solutions requiring extensive amounts of elaborately annotated training samples, our MuG models video object patterns by comprehensively exploring supervision signals from different granularities of unlabeled videos. Our model sets new state-of-the-arts over diverse VOS settings, including object-level Z-VOS, instance-level Z-VOS, and O-VOS. Our model opens up the probability of learning VOS from nearly infinite amount of unlabeled videos and unifying different VOS settings from a single view of video object pattern understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Current leading VOS methods are learned in a supervised manner, requiring large-scale elaborately labeled data. (b) Our model, MuG, provides an unsupervised/weakly-supervised framework that learns video object patterns from unlabeled videos. (c) Once trained, MuG can be applied to diverse VOS settings, with strong modeling ability and high generability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our approach. Intrinsic properties over frame, short-term, long-term and whole video granularities are explored to guide the video object pattern learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Left: Main idea of short-term granularity analysis. Right: Training details for intra-clip coherence modeling. Illustration of our long-term granularity analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t(n, m)ŷ m ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 Figure 5 :</head><label>55</label><figDesc>presents some visual results for object-level ZVOS (top row), instance-level Z-VOS (middle row) and O-VOS Visual results on three videos (top: blackswan, middle: tram, bottom: scooter-black) under object-level Z-VOS, instancelevel Z-VOS and O-VOS setting, respectively (see §4.6).For scooter-black, its first-frame annotation is also depicted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on DAVIS16<ref type="bibr" target="#b35">[36]</ref> val set, under the object-level Z-VOS setting. Please see §4.2 for details.</figDesc><table><row><cell>Aspects</cell><cell>Module</cell><cell cols="4">Unsuper. Weakly-super. mean J ∆J mean J ∆J</cell></row><row><cell>Reference</cell><cell>Full model (2 iterations)</cell><cell>58.0</cell><cell>-</cell><cell>61.2</cell><cell>-</cell></row><row><cell>Initial Fore-/Background</cell><cell>Heuristic Saliency[70]</cell><cell cols="2">37.2 -20.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Knowledge</cell><cell>CAM[73]</cell><cell>-</cell><cell>-</cell><cell cols="2">45.3 -15.9</cell></row><row><cell></cell><cell>w/o. Frame Granularity</cell><cell cols="4">40.2 -17.8 40.2 -21.0</cell></row><row><cell>Multi-Granularity</cell><cell cols="5">w/o. Short-term Granularity 51.3 -6.7 57.1 -4.1</cell></row><row><cell>Analysis</cell><cell cols="5">w/o. Long-term Granularity 52.8 -5.2 56.0 -5.2</cell></row><row><cell></cell><cell>w/o. Video Granularity</cell><cell cols="4">56.4 -1.6 60.4 -0.8</cell></row><row><cell>Iterative Training via Bootstrapping</cell><cell>1 iteration 3 iterations 4 iterations</cell><cell cols="4">50.8 -7.2 54.9 -6.3 58.0 0.0 61.2 0.0 58.0 0.0 61.2 0.0</cell></row><row><cell>More Data</cell><cell>+ LaSOT dataset[8]</cell><cell cols="4">59.5 +1.5 62.3 +1.1</cell></row><row><cell>Post-Process</cell><cell>w/o. CRF</cell><cell cols="4">55.3 -2.7 58.7 -2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>CVOS[48] KEY[24] MSG[31] NLC[7] FST[33] Motion Masks[34] TSN[17] Ours COSEG[50] Ours</figDesc><table><row><cell cols="2">Supervision</cell><cell></cell><cell></cell><cell cols="2">Non Learning</cell><cell></cell><cell></cell><cell cols="2">Unsupervised Learning</cell><cell></cell><cell cols="2">Weakly-supervised</cell></row><row><cell cols="3">Method Mean ↑ TRC[10] J 47.3 Recall ↑ 49.3</cell><cell>48.2 54.0</cell><cell>49.8 59.1</cell><cell>53.3 61.6</cell><cell>55.1 55.8</cell><cell>55.8 64.7</cell><cell>48.9 44.7</cell><cell>31.2 18.7</cell><cell>58.0 65.3</cell><cell>52.8 50.0</cell><cell>61.2 65.9</cell></row><row><cell></cell><cell>Decay ↓</cell><cell>8.3</cell><cell>10.5</cell><cell>14.1</cell><cell>2.4</cell><cell>12.6</cell><cell>0.0</cell><cell>19.2</cell><cell>-0.4</cell><cell>2.0</cell><cell>10.7</cell><cell>11.6</cell></row><row><cell></cell><cell>Mean ↑</cell><cell>44.1</cell><cell>44.7</cell><cell>42.7</cell><cell>50.8</cell><cell>52.3</cell><cell>51.1</cell><cell>39.1</cell><cell>18.4</cell><cell>51.5</cell><cell>49.3</cell><cell>56.1</cell></row><row><cell>F</cell><cell>Recall ↑</cell><cell>43.6</cell><cell>52.6</cell><cell>37.5</cell><cell>60.0</cell><cell>51.9</cell><cell>51.6</cell><cell>28.6</cell><cell>5.6</cell><cell>53.2</cell><cell>52.7</cell><cell>54.6</cell></row><row><cell></cell><cell>Decay ↓</cell><cell>12.9</cell><cell>11.7</cell><cell>10.6</cell><cell>5.1</cell><cell>11.4</cell><cell>2.9</cell><cell>17.9</cell><cell>1.9</cell><cell>2.1</cell><cell>10.5</cell><cell>20.3</cell></row><row><cell>T</cell><cell>Mean ↓</cell><cell>39.1</cell><cell>25.0</cell><cell>26.9</cell><cell>30.1</cell><cell>42.5</cell><cell>36.6</cell><cell>36.4</cell><cell>37.5</cell><cell>30.1</cell><cell>28.2</cell><cell>23.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of object-level Z-VOS on DAVIS16 val set [36] ( §4.3), with region similarity J , boundary accuracy F and time stability T . (The best scores in each supervision setting are marked in bold. These notes are the same to other tables.)</figDesc><table><row><cell>Supervision</cell><cell></cell><cell cols="2">Non Learning</cell><cell></cell><cell cols="2">Unsupervised Learning</cell><cell></cell><cell></cell><cell cols="2">Weakly-supervised Learning</cell><cell></cell></row><row><cell>Method</cell><cell cols="11">CRANE[47] NLC[7] FST[33] ARP[19] Motion Masks[34] TSN[17] Ours SOSD[75] BBF[42] COSEG[50] Ours</cell></row><row><cell>J Mean ↑</cell><cell>23.9</cell><cell>27.7</cell><cell>53.8</cell><cell>46.2</cell><cell>32.1</cell><cell>52.2</cell><cell>57.7</cell><cell>54.1</cell><cell>53.3</cell><cell>58.1</cell><cell>62.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of object-level Z-VOS on Youtube-Objects[39] ( §4.3), with mean J . See the supplementary for more details. 10 categories (such as cat, dog, etc.) and has 25,673 frames in total. The val set of DAVIS 16 and whole Youtube-Objects are used for evaluation. Evaluation Criteria. For fair comparison, we follow the official evaluation protocols of each dataset. For DAVIS</figDesc><table><row><cell>tency and semantic correspondence. Moreover, consider-</cell></row><row><cell>ing video granularity information also improves the final</cell></row><row><cell>performance, proving the meaning of comprehensive video</cell></row><row><cell>content understanding in video object pattern modeling.</cell></row><row><cell>Iterative Training Strategy. From Table1, we can see that</cell></row><row><cell>with more iterations of our bootstrapping training strategy</cell></row><row><cell>(1 → 2), better performance can be obtained. However,</cell></row><row><cell>further iterations (2 → 4) give only marginal performance</cell></row><row><cell>change. We thus use two iterations in all the experiments.</cell></row><row><cell>More Training Data. To show the potential of our unsuper-</cell></row><row><cell>vised/weakly supervised VOS learning scheme, we probe</cell></row><row><cell>the upper bound by training on additional videos. With</cell></row><row><cell>more training data (1400 videos) from LaSOT dataset [8],</cell></row><row><cell>performance boosts can be observed in both two settings.</cell></row><row><cell>4.3. Performance for Object-Level Z-VOS</cell></row><row><cell>Datasets. Experiments are conducted on two famous Z-</cell></row><row><cell>VOS datasets: DAVIS[36] and Youtube-Objects[39], which</cell></row><row><cell>have pixel-wise, object-level annotations. DAVIS 16 has 50</cell></row><row><cell>videos (3,455 frames), covering a wide range of challenges,</cell></row><row><cell>such as fast motion, occlusion, dynamic background, etc.</cell></row><row><cell>It is split into a train set (30 videos) and a val set (20</cell></row><row><cell>videos). Youtube-Objects contains 126 video sequences</cell></row><row><cell>that belong to</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>presents the comparison</cell></row><row><cell>results with several non-learning, unsupervised or weakly</cell></row><row><cell>supervised learning competitors in DAVIS 16 dataset. In</cell></row><row><cell>particular, MuG exceeds current leading unsupervised</cell></row><row><cell>learning-based methods (i.e., Motion Masks [34] and</cell></row><row><cell>TSN[17] ) in large margins (58.0 vs 48.9 and 58.0 vs 31.2).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Evaluation of instance-level Z-VOS on DAVIS17 test-dev set[4] ( §4.4), * denotes purely unsupervised/weakly- supervised protocol with non-learning Edgebox [78] and GrabCut.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc>We test the performance for instance-level Z-VOS on DAVIS<ref type="bibr" target="#b16">17</ref> <ref type="bibr" target="#b3">[4]</ref> dataset, which has 120 videos and 8,502 frames in total. It has three subsets, namely, train, val, and test-dev, containing 60, 30, and 30 video sequences, respectively. We use the ground-truth masks provided by the newest DAVIS challenge<ref type="bibr" target="#b3">[4]</ref>, as the original annotations are biased towards the O-VOS scenario. Evaluation Criteria. Three standard evaluation metrics, provided by DAVIS 17 , are used, i.e., region similarity J , boundary accuracy F and the average value of T &amp;F. Quantitative Results. Three top-performing ZVOS methods from the DAVIS 17 benchmark are included. As shown inTable 4, our model achieves comparable performance with the fully supervised methods (i.e., AGS<ref type="bibr" target="#b62">[63]</ref> and JMP<ref type="bibr" target="#b8">[9]</ref> FCP<ref type="bibr" target="#b36">[37]</ref> SIFT Flow<ref type="bibr" target="#b24">[25]</ref> BVS<ref type="bibr" target="#b27">[28]</ref> Vondrick et al.<ref type="bibr" target="#b55">[56]</ref> mgPFF<ref type="bibr" target="#b19">[20]</ref> TimeCycle<ref type="bibr" target="#b64">[65]</ref> CorrFlow<ref type="bibr" target="#b21">[22]</ref> Ours FlowNet2<ref type="bibr" target="#b15">[16]</ref> Ours</figDesc><table><row><cell>summarizes comparison results on Youtube-</cell></row><row><cell>Objects dataset, showing again our superior performance in</cell></row><row><cell>both unsupervised and weakly supervised learning settings.</cell></row><row><cell>Runtime Comparison. The inference time of MuG is</cell></row><row><cell>about 0.6s per frame, which is faster than most deep</cell></row><row><cell>learning based competitors (e.g., MotionMask [34] (1.1s),</cell></row><row><cell>TSN [17] (0.9s)). This is because, except CRF [21], there</cell></row><row><cell>is no other pre-/post-processing step (e.g., superpixel [50],</cell></row><row><cell>optical flow[33], etc.) and online fine-tuning[19].</cell></row><row><cell>4.4. Performance for Instance-Level Z-VOS</cell></row><row><cell>Datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of O-VOS on DAVIS16 val set[36] ( §4.5), with region similarity J , boundary accuracy F and time stability T .</figDesc><table><row><cell cols="2">Supervision</cell><cell cols="2">Non Learning</cell><cell></cell><cell></cell><cell cols="2">Unsupervised Learning</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Weakly-supervised</cell></row><row><cell></cell><cell>Method</cell><cell cols="8">SIFT Flow BVS DeepCluster Transitive Inv Vondrick et al. mgPFF TimeCycle CorrFlow [25] [28] [5] [64] [56] [20] [65] [22]</cell><cell>Ours</cell><cell>FlowNet2 [16]</cell><cell>Ours</cell></row><row><cell cols="2">J &amp;F Mean ↑</cell><cell>34.0</cell><cell>37.3</cell><cell>35.4</cell><cell>29.4</cell><cell>34.0</cell><cell>44.6</cell><cell>42.8</cell><cell>50.3</cell><cell>54.3</cell><cell>26.0</cell><cell>56.1</cell></row><row><cell>J</cell><cell>Mean ↑ Recall ↑</cell><cell>33.0 -</cell><cell>32.9 31.8</cell><cell>37.5 -</cell><cell>32.0 -</cell><cell>34.6 34.1</cell><cell>42.2 41.8</cell><cell>43.0 43.7</cell><cell>48.4 53.2</cell><cell>52.6 57.4</cell><cell>26.7 23.9</cell><cell>54.0 60.7</cell></row><row><cell></cell><cell>Mean ↑</cell><cell>35.0</cell><cell>41.7</cell><cell>33.2</cell><cell>26.8</cell><cell>32.7</cell><cell>46.9</cell><cell>42.6</cell><cell>52.2</cell><cell>56.1</cell><cell>25.2</cell><cell>58.2</cell></row><row><cell>F</cell><cell>Recall ↑</cell><cell>-</cell><cell>41.4</cell><cell>-</cell><cell>-</cell><cell>26.8</cell><cell>44.4</cell><cell>41.3</cell><cell>56.0</cell><cell>58.1</cell><cell>24.6</cell><cell>62.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of O-VOS on DAVIS17 val set[38] ( §4.5), with region similarity J , boundary accuracy F and average of J &amp;F.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that any unsupervised or weakly supervised object segmentation/saliency model can be used; saliency<ref type="bibr" target="#b69">[70]</ref>, and CAM<ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b75">76]</ref> are just chosen due to their popularity and relatively high performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dosovitskiy</forename><surname>Alexey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Springenberg Jost</forename><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00737</idno>
		<title level="m">The 2019 davis challenge on vos: Unsupervised multi-object segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video segmentation by nonlocal consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jumpcut: non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<idno>195:1-195:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">James Rehg, and Rahul Sukthankar. Weakly supervised learning of object segmentations from web-scale video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simple-cell-like receptive fields maximize temporal coherence in natural video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarmo</forename><surname>Hurri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="663" to="691" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bogolin Simion-Vlad, and Leordeanu Marius. Unsupervised learning from video to detect foreground objects in single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Croitoru</forename><surname>Ioana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multigrid predictive filter flow for unsupervised learning on videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01693</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised learning for video correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihanng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Frame-to-frame aggregation of active regions in web videos for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Keysegments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Marki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grundmann</forename><surname>Matthias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwatra</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Essa</forename><surname>Irfan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collobert</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename><surname>Jason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object segmentation in video: A hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep rnn framework for visual sequential applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anestis</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-toend weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bringing background into the foreground: Making all classes equal in weakly-supervised video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatemeh Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards a mathematical understanding of the difficulty in learning with feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video segmentation using teacher-student adaptation in a human robot interaction (hri) setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mennatullah</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Petrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Discriminative segment annotation in weakly labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasiliy</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantic co-segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Long-term tracking in the wild: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Saliencyaware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Semi-supervised video object segmentation with super-trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="998" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Consistent video saliency using local gradient flow optimization and global refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4185" to="4196" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Video salient object detection via fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning context graph for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fine-grained video captioning via graph-based multi-granularity interaction learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multi-source weak supervision for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Spftn: A self-paced fine-tuning network for segmenting objects in weakly labelled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Semantic object segmentation via detection in weakly labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqun</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Motion-attentive transition for zero-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
