<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deformable GANs for Pose-based Human Image Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<address>
									<settlement>Italy</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<address>
									<settlement>Italy</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
							<email>stephane.lathuiliere@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria Grenoble Rhone-Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<email>niculae.sebe@unitn.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DISI</orgName>
								<orgName type="institution" key="instit2">University of Trento</orgName>
								<address>
									<settlement>Italy</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deformable GANs for Pose-based Human Image Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L 1 and L 2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we deal with the problem of generating images where the foreground object changes because of a viewpoint variation or a deformable motion, such as the articulated human body. Specifically, inspired by Ma et al. <ref type="bibr" target="#b11">[12]</ref>, our goal is to generate a human image conditioned on two different variables: (1) the appearance of a specific person in a given image and (2) the pose of the same person in another image. The task our networks need to solve is to preserve the appearance details (e.g., the texture) contained in the first variable while performing a deformation on the structure of the foreground object according to the second variable. We focus on the human body which is an articulated "object", important for many applications (e.g., computer-graphics based manipulations or re-identification dataset synthesis). However, our approach can be used with other deformable objects such as human faces or animal bodies, provided that a significant number of keypoints can be automatically extracted from the object of interest in order to represent its pose.</p><p>Pose-based human-being image generation is motivated (a) Aligned task (b) Unaligned task <ref type="figure">Figure 1</ref>: (a) A typical "rigid" scene generation task, where the conditioning and the output image local structure is well aligned. (b) In a deformable-object generation task, the input and output are not spatially aligned.</p><p>by the interest in synthesizing videos <ref type="bibr" target="#b17">[18]</ref> with non-trivial human movements or in generating rare poses for human pose estimation <ref type="bibr" target="#b0">[1]</ref> or re-identification <ref type="bibr" target="#b22">[23]</ref> training datasets. However, most of the recently proposed, deepnetwork based generative approaches, such as Generative Adversarial Networks (GANs) <ref type="bibr" target="#b2">[3]</ref> or Variational Autoencoders (VAEs) <ref type="bibr" target="#b6">[7]</ref> do not explicitly deal with the problem of articulated-object generation. Common conditional methods (e.g., conditional GANs or conditional VAEs) can synthesize images whose appearances depend on some conditioning variables (e.g., a label or another image). For instance, Isola et al. <ref type="bibr" target="#b3">[4]</ref> recently proposed an "image-toimage translation" framework, in which an input image x is transformed into a second image y represented in another "channel" (see <ref type="figure">Fig. 1a</ref>). However, most of these methods have problems when dealing with large spatial deformations between the conditioning and the target image. For instance, the U-Net architecture used by Isola et al. <ref type="bibr" target="#b3">[4]</ref> is based on skip connections which help preserving local information between x and y. Specifically, skip connections are used to copy and then concatenate the feature maps of the generator "encoder" (where information is downsam-pled using convolutional layers) to the generator "decoder" (containing the upconvolutional layers). However, the assumption used in <ref type="bibr" target="#b3">[4]</ref> is that x and y are roughly aligned with each other and they represent the same underlying structure. This assumption is violated when the foreground object in y undergoes to large spatial deformations with respect to x (see <ref type="figure">Fig. 1b</ref>). As shown in <ref type="bibr" target="#b11">[12]</ref>, skip connections cannot reliably cope with misalignments between the two poses. Ma et al. <ref type="bibr" target="#b11">[12]</ref> propose to alleviate this problem using a two-stage generation approach. In the first stage a U-Net generator is trained using a masked L 1 loss in order to produce an intermediate image conditioned on the target pose. In the second stage, a second U-Net based generator is trained using also an adversarial loss in order to generate an appearance difference map which brings the intermediate image closer to the appearance of the conditioning image. In contrast, the GAN-based method we propose in this paper is end-to-end trained by explicitly taking into account pose-related spatial deformations. More specifically, we propose deformable skip connections which "move" local information according to the structural deformations represented in the conditioning variables. These layers are used in our U-Net based generator. In order to move information according to a specific spatial deformation, we decompose the overall deformation by means of a set of local affine transformations involving subsets of joints, then we deform the convolutional feature maps of the encoder according to these transformations and we use common skip connections to transfer the transformed tensors to the decoder's fusion layers. Moreover, we also propose to use a nearest-neighbour loss as a replacement of common pixelto-pixel losses (such as, e.g., L 1 or L 2 losses) commonly used in conditional generative approaches. This loss proved to be helpful in generating local information (e.g., texture) similar to the target image which is not penalized because of small spatial misalignments.</p><p>We test our approach using the benchmarks and the evaluation protocols proposed in <ref type="bibr" target="#b11">[12]</ref> obtaining higher qualitative and quantitative results in all the datasets. Although tested on the specific human-body problem, our approach makes few human-related assumptions and can be easily extended to other domains involving the generation of highly deformable objects. Our code and our trained models are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Most common deep-network-based approaches for visual content generation can be categorized as either Variational Autoencoders (VAEs) <ref type="bibr" target="#b6">[7]</ref> or Generative Adversarial Networks (GANs) <ref type="bibr" target="#b2">[3]</ref>. VAEs are based on probabilistic graphical models and are trained by maximizing a lower 1 https://github.com/AliaksandrSiarohin/pose-gan bound of the corresponding data likelihood. GANs are based on two networks, a generator and a discriminator, which are trained simultaneously such that the generator tries to "fool" the discriminator and the discriminator learns how to distinguish between real and fake images.</p><p>Isola et al. <ref type="bibr" target="#b3">[4]</ref> propose a conditional GAN framework for image-to-image translation problems, where a given scene representation is "translated" into another representation. The main assumption behind this framework is that there exits a spatial correspondence between the low-level information of the conditioning and the output image. VAEs and GANs are combined in <ref type="bibr" target="#b19">[20]</ref> to generate realistic-looking multi-view clothes images from a single-view input image. The target view is filled to the model via a viewpoint label as front or left side and a two-stage approach is adopted: pose integration and image refinement. Adopting a similar pipeline, Lassner et al. <ref type="bibr" target="#b7">[8]</ref> generate images of people with different clothes in a given pose. This approach is based on a costly annotation (fine-grained segmentation with 18 clothing labels) and a complex 3D pose representation.</p><p>Ma et al. <ref type="bibr" target="#b11">[12]</ref> propose a more general approach which allows to synthesize person images in any arbitrary pose. Similarly to our proposal, the input of their model is a conditioning image of the person and a target new pose defined by 18 joint locations. The target pose is described by means of binary maps where small circles represent the joint locations. Similarly to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>, the generation process is split in two different stages: pose generation and texture refinement. In contrast, in this paper we show that a single-stage approach, trained end-to-end, can be used for the same task obtaining higher qualitative results.</p><p>Jaderberg et al. <ref type="bibr" target="#b4">[5]</ref> propose a spatial transformer layer, which learns how to transform a feature map in a "canonical" view, conditioned on the feature map itself. However only a global, parametric transformation can be learned (e.g., a global affine transformation), while in this paper we deal with non-parametric deformations of articulated objects which cannot be described by means of a unique global affine transformation.</p><p>Generally speaking, U-Net based architectures are frequently adopted for pose-based person-image generation tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>. However, common U-Net skip connections are not well-designed for large spatial deformations because local information in the input and in the output images is not aligned <ref type="figure">(Fig. 1)</ref>. In contrast, we propose deformable skip connections to deal with this misalignment problem and "shuttle" local information from the encoder to the decoder driven by the specific pose difference. In this way, differently from previous work, we are able to simultaneously generate the overall pose and the texture-level refinement.</p><p>Finally, our nearest-neighbour loss is similar to the perceptual loss proposed in <ref type="bibr" target="#b5">[6]</ref> and to the style-transfer spatial-analogy approach recently proposed in <ref type="bibr" target="#b8">[9]</ref>. However, the perceptual loss, based on an element-by-element difference computed in the feature map of an external classifier <ref type="bibr" target="#b5">[6]</ref>, does not take into account spatial misalignments. On the other hand, the patch-based similarity, adopted in <ref type="bibr" target="#b8">[9]</ref> to compute a dense feature correspondence, is very computationally expensive and it is not used as a loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The network architectures</head><p>In this section we describe the architectures of our generator (G) and discriminator (D) and the proposed deformable skip connections. We first introduce some notation. At testing time our task, similarly to <ref type="bibr" target="#b11">[12]</ref>, consists in generating an imagex showing a person whose appearance (e.g., clothes, etc.) is similar to an input, conditioning image x a but with a body pose similar to P (x b ), where x b is a different image of the same person and P (x) = (p 1 , ...p k ) is a sequence of k 2D points describing the locations of the human-body joints in x. In order to allow a fair comparison with <ref type="bibr" target="#b11">[12]</ref>, we use the same number of joints (k = 18) and we extract P () using the same Human Pose Estimator (HPE) <ref type="bibr" target="#b0">[1]</ref> used in <ref type="bibr" target="#b11">[12]</ref>. Note that this HPE is used both at testing and at training time, meaning that we do not use manually-annotated poses and the so extracted joint locations may have some localization errors or missing detections/false positives.</p><p>At training time we use a dataset</p><formula xml:id="formula_0">X = {(x (i) a , x (i) b )} i=1,.</formula><p>..,N containing pairs of conditioningtarget images of the same person in different poses. For each pair (x a , x b ), a conditioning and a target pose P (x a ) and P (x b ) is extracted from the corresponding image and represented using two tensors H a = H(P (x a )) and H b = H(P (x b )), each composed of k heat maps, where H j (1 ≤ j ≤ k) is a 2D matrix of the same dimension as the original image. If p j is the j-th joint location, then:</p><formula xml:id="formula_1">H j (p) = exp − p − p j σ 2 ,<label>(1)</label></formula><p>with σ = 6 pixels (chosen with cross-validation). Using blurring instead of a binary map is useful to provide widespread information about the location p j . The generator G is fed with: (1) a noise vector z, drawn from a noise distribution Z and implicitly provided using dropout <ref type="bibr" target="#b3">[4]</ref> and (2) the triplet (x a , H a , H b ). Note that, at testing time, the target pose is known, thus H(P (x b )) can be computed. Note also that the joint locations in x a and H a are spatially aligned (by construction), while in H b they are different. Hence, differently from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref>, H b is not concatenated with the other input tensors. Indeed the convolutional-layer units in the encoder part of G have a small receptive field which cannot capture large spatial displacements. For instance, a large movement of a body limb in x b with respect to x a , is represented in different locations in x a and H b which may be too far apart from each other to be captured by the receptive field of the convolutional units. This is emphasized in the first layers of the encoder, which represent low-level information. Therefore, the convolutional filters cannot simultaneously process texture-level information (from x a ) and the corresponding pose information (from H b ).</p><p>For this reason we independently process x a and H a from H b in the encoder. Specifically, x a and H a are concatenated and processed using a convolutional stream of the encoder while H b is processed by means of a second convolutional stream, without sharing the weights <ref type="figure" target="#fig_0">(Fig. 2)</ref>. The feature maps of the first stream are then fused with the layerspecific feature maps of the second stream in the decoder after a pose-driven spatial deformation performed by our deformable skip connections (see Sec. 3.1).</p><p>Our discriminator network is based on the conditional, fully-convolutional discriminator proposed by Isola et al. <ref type="bibr" target="#b3">[4]</ref>. In our case, D takes as input 4 tensors: <ref type="figure" target="#fig_0">Fig. 2</ref>). These four tensors are concatenated and then given as input to D. The discriminator's output is a scalar value indicating its confidence on the fact that y is a real image.</p><formula xml:id="formula_2">(x a , H a , y, H b ), where either y = x b or y =x = G(z, x a , H a , H b ) (see</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deformable skip connections</head><p>As mentioned above and similarly to <ref type="bibr" target="#b3">[4]</ref>, the goal of the deformable skip connections is to "shuttle" local information from the encoder to the decoder part of G. The local information to be transferred is, generally speaking, contained in a tensor F , which represents the feature map activations of a given convolutional layer of the encoder. However, differently from <ref type="bibr" target="#b3">[4]</ref>, we need to "pick" the information to shuttle taking into account the object-shape deformation which is described by the difference between P (x a ) and P (x b ). To do so, we decompose the global deformation in a set of local affine transformations, defined using subsets of joints in P (x a ) and P (x b ). Using these affine transformations and local masks constructed using the specific joints, we deform the content of F and then we use common skip connections to copy the transformed tensor and concatenate it with the corresponding tensor in the destination layer (see <ref type="figure" target="#fig_0">Fig. 2</ref>). Below we describe in more detail the whole pipeline.</p><p>Decomposing an articulated body in a set of rigid subparts. The human body is an articulated "object" which can be roughly decomposed into a set of rigid sub-parts. We chose 10 sub-parts: the head, the torso, the left/right upper/lower arm and the left/right upper/lower leg. Each of them corresponds to a subset of the 18 joints defined by the HPE <ref type="bibr" target="#b0">[1]</ref> we use for extracting P (). Using these joint locations we can define rectangular regions which enclose the specific body part. In case of the head, the region is simply chosen to be the axis-aligned enclosing rectangle of all the corresponding joints. For the torso, which is the largest area, we use a region which includes the whole image, in such a way to shuttle texture information for the background pixels. Concerning the body limbs, each limb corresponds to only 2 joints. In this case we define a region to be a rotated rectangle whose major axis (r 1 ) corresponds to the line between these two joints, while the minor axis (r 2 ) is orthogonal to r 1 and with a length equal to one third of the mean of the torso's diagonals (this value is used for all the limbs). In <ref type="figure">Fig. 3</ref> we show an example. Let R a h = {p 1 , ..., p 4 } be the set of the 4 rectangle corners in x a defining the h-th body region (1 ≤ h ≤ 10). Note that these 4 corner points are not joint locations. Using R a h we can compute a binary mask M h (p) which is zero everywhere except those points p lying inside R a h . Moreover, let R b h = {q 1 , ..., q 4 } be the corresponding rectangular region in x b . Matching the points in R a h with the corresponding points in R b h we can compute the parameters of a body-part specific affine transformation (see below). In either x a or x b , some of the body regions can be occluded, truncated by the image borders or simply miss-detected by the HPE. In this case we leave the corresponding region R h empty and the h-th affine transform is not computed (see below).</p><p>Note that our body-region definition is the only humanspecific part of the proposed approach. However, similar regions can be easily defined using the joints of other articulated objects such as those representing an animal body or a human face.</p><p>Computing a set of affine transformations. During the forward pass (i.e., both at training and at testing time) we decompose the global deformation of the conditioning pose with respect to the target pose by means of a set of local affine transformations, one per body region. Specifically, given R a h in x a and R b h in x b (see above), we compute the 6 parameters k h of an affine transformation f h (·; k h ) using Least Squares Error: <ref type="figure">Figure 3</ref>: For each specific body part, an affine transformation f h is computed. This transformation is used to "move" the feature-map content corresponding to that body part.</p><formula xml:id="formula_3">min k h pj ∈R a h ,qj ∈R b h ||q j − f h (p j ; k h )|| 2 2<label>(2)</label></formula><p>The parameter vector k h is computed using the original image resolution of x a and x b and then adapted to the specific resolution of each involved feature map F . Similarly, we compute scaled versions of each M h . In case either R a h or R b h is empty (i.e., when any of the specific body-region joints has not been detected using the HPE, see above), then we simply set M h to be a matrix with all elements equal to 0 (f h is not computed).</p><p>Note that (f h (), M h ) and their lower-resolution variants need to be computed only once per each pair of real images (x a , x b ) ∈ X and, in case of the training phase, this is can be done before starting training the networks (but in our current implementation this is done on the fly).</p><p>Combining affine transformations to approximate the object deformation.</p><formula xml:id="formula_4">Once (f h (), M h ), h = 1, ..., 10</formula><p>are computed for the specific spatial resolution of a given tensor F , the latter can be transformed in order to approximate the global pose-dependent deformation. Specifically, we first compute for each h:</p><formula xml:id="formula_5">F h = f h (F M h ),<label>(3)</label></formula><p>where is a point-wise multiplication and f h (F (p)) is used to "move" all the channel values of F corresponding to point p. Finally, we merge the resulting tensors using:</p><formula xml:id="formula_6">d(F (p, c)) = max h=1,...,10 F h (p, c),<label>(4)</label></formula><p>where c is a specific channel. The rationale behind Eq. 4 is that, when two body regions partially overlap each other, the final deformed tensor d(F ) is obtained by picking the maximum-activation values. Preliminary experiments performed using average pooling led to slightly worse results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training</head><p>D and G are trained using a combination of a standard conditional adversarial loss L cGAN with our proposed nearest-neighbour loss L N N . Specifically, in our case L cGAN is given by:</p><formula xml:id="formula_7">L cGAN (G, D) = E (xa,x b )∈X [log D(x a , H a , x b , H b )]+ E (xa,x b )∈X ,z∈Z [log(1 − D(x a , H a ,x, H b ))],<label>(5)</label></formula><formula xml:id="formula_8">wherex = G(z, x a , H a , H b ).</formula><p>Previous works on conditional GANs combine the adversarial loss with either an L 2 <ref type="bibr" target="#b12">[13]</ref> or an L 1 -based loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> which is used only for G. For instance, the L 1 distance computes a pixel-to-pixel difference between the generated and the real image, which, in our case, is:</p><formula xml:id="formula_9">L 1 (x, x b ) = ||x − x b || 1 .<label>(6)</label></formula><p>However, a well-known problem behind the use of L 1 and L 2 is the production of blurred images. We hypothesize that this is also due to the inability of these losses to tolerate small spatial misalignments betweenx and x b . For instance, suppose thatx, produced by G, is visually plausible and semantically similar to x b , but the texture details on the clothes of the person in the two compared images are not pixel-to-pixel aligned. Both the L 1 and the L 2 loss will penalize this inexact pixel-level alignment, although not semantically important from the human point of view. Note that these misalignments do not depend on the global deformation between x a and x b , becausex is supposed to have the same pose as x b . In order to alleviate this problem, we propose to use a nearest-neighbour loss L N N based on the following definition of image difference:</p><formula xml:id="formula_10">L N N (x, x b ) = p∈x min q∈N (p) ||g(x(p)) − g(x b (q))|| 1 ,<label>(7)</label></formula><p>where N (p) is a n × n local neighbourhood of point p (we use 5 × 5 and 3 × 3 neighbourhoods for the DeepFashion and the Market-1501 dataset, respectively, see Sec. 6). g(x(p)) is a vectorial representation of a patch around point p in image x, obtained using convolutional filters (see below for more details). Note that L N N () is not a metrics because it is not symmetric. In order to efficiently compute Eq. 7, we compare patches inx and x b using their representation (g()) in a convolutional map of an externally trained network. In more detail, we use VGG-19 <ref type="bibr" target="#b14">[15]</ref>, trained on ImageNet and, specifically, its second convolutional layer (called conv 1 2 ). The first two convolutional maps in VGG-19 (conv 1 1 and conv 1 2 ) are both obtained using a convolutional stride equal to 1. For this reason, the feature map (C x ) of an image x in conv 1 2 has the same resolution of the original image x. Exploiting this fact, we compute the nearest-neighbour field directly on conv 1 2 , without losing spatial precision. Hence, we define: g(x(p)) = C x (p), which corresponds to the vector of all the channel values of C x with respect to the spatial position p. C x (p) has a receptive field of 5 × 5 in x, thus effectively representing a patch of dimension 5 × 5 using a cascade of two convolutional filters. Using C x , Eq. 7 becomes:</p><formula xml:id="formula_11">L N N (x, x b ) = p∈x min q∈N (p) ||Cx(p) − C x b (q)|| 1 ,<label>(8)</label></formula><p>In Sec. A, we show how Eq. 8 can be efficiently implemented using GPU-based parallel computing. The final L N N -based loss is:</p><formula xml:id="formula_12">L N N (G) = E (xa,x b )∈X ,z∈Z L N N (x, x b ).<label>(9)</label></formula><p>Combining Eq. 5 and Eq. 9 we obtain our objective:</p><formula xml:id="formula_13">G * = arg min G max D L cGAN (G, D) + λL N N (G),<label>(10)</label></formula><p>with λ = 0.01 used in all our experiments. The value of λ is small because it acts as a normalization factor in Eq. 8 with respect to the number of channels in C x and the number of pixels inx (more details in Sec. A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation details</head><p>We train G and D for 90k iterations, with the Adam optimizer (learning rate: 2 * 10 −4 , β 1 = 0.5, β 2 = 0.999). Following <ref type="bibr" target="#b3">[4]</ref> we use instance normalization <ref type="bibr" target="#b16">[17]</ref>. In the following we denote with: (1) C s m a convolution-ReLU layer with m filters and stride s, (2) CN s m the same as C s m with instance normalization before ReLU and (3) CD s m the same as CN s m with the addition of dropout at rate 50%. Differently from <ref type="bibr" target="#b3">[4]</ref>, we use dropout only at training time. The encoder part of the generator is given by two streams <ref type="figure" target="#fig_0">(Fig. 2)</ref>, each of which is composed of the following sequence of layers:</p><formula xml:id="formula_14">CN 1 64 − CN 2 128 − CN 2 256 − CN 2 512 − CN 2 512 − CN 2 512</formula><p>. The decoder part of the generator is given by:</p><formula xml:id="formula_15">CD 2 512 − CD 2 512 − CD 2 512 − CN 2 256 − CN 2 128 − C 1 3 .</formula><p>In the last layer, ReLU is replaced with tanh.</p><p>The discriminator architecture is:</p><formula xml:id="formula_16">C 2 64 − C 2 128 − C 2 256 − C 2 512 − C 2 1 ,</formula><p>where the ReLU of the last layer is replaced with sigmoid.</p><p>The generator for the DeepFashion dataset has one additional convolution block (CN 2 512 ) both in the encoder and in the decoder, because images in this dataset have a higher resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Datasets The person re-identification Market-1501 dataset <ref type="bibr" target="#b20">[21]</ref> contains 32,668 images of 1,501 persons captured from 6 different surveillance cameras. This dataset is challenging because of the low-resolution images (128×64) and the high diversity in pose, illumination, background and viewpoint. To train our model, we need pairs of images of the same person in two different poses. As this dataset is relatively noisy, we first automatically remove those images in which no human body is detected using the HPE, leading to 263,631 training pairs. For testing, following <ref type="bibr" target="#b11">[12]</ref>, we randomly select 12,000 pairs. No person is in common between the training and the test split.</p><p>The DeepFashion dataset (In-shop Clothes Retrieval Benchmark) <ref type="bibr" target="#b10">[11]</ref> is composed of 52,712 clothes images, matched each other in order to form 200,000 pairs of identical clothes with two different poses and/or scales of the persons wearing these clothes. The images have a resolution of 256×256 pixels. Following the training/test split adopted in <ref type="bibr" target="#b11">[12]</ref>, we create pairs of images, each pair depicting the same person with identical clothes but in different poses. After removing those images in which the HPE does not detect any human body, we finally collect 89,262 pairs for training and 12,000 pairs for testing.</p><p>Metrics Evaluation in the context of generation tasks is a problem in itself. In our experiments we adopt a redundancy of metrics and a user study based on human judgments. Following <ref type="bibr" target="#b11">[12]</ref>, we use Structural Similarity (SSIM) <ref type="bibr" target="#b18">[19]</ref>, Inception Score (IS) <ref type="bibr" target="#b13">[14]</ref> and their corresponding masked versions mask-SSIM and mask-IS <ref type="bibr" target="#b11">[12]</ref>. The latter are obtained by masking-out the image background and the rationale behind this is that, since no background information of the target image is input to G, the network cannot guess what the target background looks like. Note that the evaluation masks we use to compute both the mask-IS and the mask-SSIM values do not correspond to the masks ({M h }) we use for training. The evaluation masks have been built following the procedure proposed in <ref type="bibr" target="#b11">[12]</ref> and adopted in that work for both training and evaluation. Consequently, the maskbased metrics may be biased in favor of their method. Moreover, we observe that the IS metrics <ref type="bibr" target="#b13">[14]</ref>, based on the entropy computed over the classification neurons of an external classifier <ref type="bibr" target="#b15">[16]</ref>, is not very suitable for domains with only one object class. For this reason we propose to use an additional metrics that we call Detection Score (DS). Similarly to the classification-based metrics (FCN-score) used in <ref type="bibr" target="#b3">[4]</ref>, DS is based on the detection outcome of the state-of-theart object detector SSD <ref type="bibr" target="#b9">[10]</ref>, trained on Pascal VOC 07 <ref type="bibr" target="#b1">[2]</ref> (and not fine-tuned on our datasets). At testing time, we use the person-class detection scores of SSD computed on each generated imagex. DS(x) corresponds to the maximumscore box of SSD onx and the final DS value is computed by averaging the scores of all the generated images. In other words, DS measures the confidence of a person detector in the presence of a person in the image. Given the high accuracy of SSD in the challenging Pascal VOC 07 dataset <ref type="bibr" target="#b9">[10]</ref>, we believe that it can be used as a good measure of how much realistic (person-like) is a generated image.</p><p>Finally, in our tables we also include the value of each metrics computed using the real images of the test set. Since these values are computed on real data, they can be considered as a sort of an upper-bound to the results a generator can obtain. However, these values are not actual upper bounds in the strict sense: for instance the DS metrics on the real datasets is not 1 because of SSD failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Comparison with previous work</head><p>In Tab. 1 we compare our method with <ref type="bibr" target="#b11">[12]</ref>. Note that there are no other works to compare with on this task yet. The mask-based metrics are not reported in <ref type="bibr" target="#b11">[12]</ref> for the DeepFashion dataset. Concerning the DS metrics, we used the publicly available code and network weights released by the authors of <ref type="bibr" target="#b11">[12]</ref> in order to generate new images according to the common testing protocol and ran the SSD detector to get the DS values.</p><p>On the Market-1501 dataset our method reports the highest performance with all but the IS metrics. Specifically, our DS values are much higher than those obtained by <ref type="bibr" target="#b11">[12]</ref>. Conversely, on the DeepFashion dataset, our approach significantly improves the IS value but returns a slightly lower SSIM value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">User study</head><p>In order to further compare our method with the state-ofthe-art approach <ref type="bibr" target="#b11">[12]</ref> we implement a user study following  <ref type="bibr" target="#b11">[12]</ref>. For each dataset, we show 55 real and 55 generated images in a random order to 30 users for one second. Differently from Ma et al. <ref type="bibr" target="#b11">[12]</ref>, who used Amazon Mechanical Turk (AMT), we used "expert" (voluntary) users: PhD students and Post-docs working in Computer Vision and belonging to two different departments. We believe that expert users, who are familiar with GANlike images, can more easily distinguish real from fake images, thus confusing our users is potentially a more difficult task for our GAN. The results 2 in Tab. 2 confirm the significant quality boost of our images with respect to the images produced in <ref type="bibr" target="#b11">[12]</ref>. For instance, on the Market-1501 dataset, the G2R human "confusion" is one order of magnitude higher than in <ref type="bibr" target="#b11">[12]</ref>. Finally, in Sec. D we show some example images, directly comparing with <ref type="bibr" target="#b11">[12]</ref>. We also show the results obtained by training different person re-identification systems after augmenting the training set with images generated by our method. These experiments indirectly confirm that the degree of realism and diversity of our images is very significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation study and qualitative analysis</head><p>In this section we present an ablation study to clarify the impact of each part of our proposal on the final performance. We first describe the compared methods, obtained by "amputating" important parts of the full-pipeline presented in Sec. 3-4. The discriminator architecture is the same for all the methods.</p><p>• Baseline: We use the standard U-Net architecture <ref type="bibr" target="#b3">[4]</ref> without deformable skip connections. The inputs of G and D and the way pose information is represented 2 R2G means #Real images rated as generated / #Real images; G2R means #Generated images rated as Real / #Generated images. <ref type="figure">Figure 4</ref>: Qualitative results on the Market-1501 dataset. Columns 1, 2 and 3 represent the input of our model. We plot P (·) as a skeleton for the sake of clarity, but actually no joint-connectivity relation is exploited in our approach. Column 4 corresponds to the ground truth. The last four columns show the output of our approach with respect to different baselines.</p><formula xml:id="formula_17">x a P (x a ) P (x b ) x b Baseline DSC PercLoss Full</formula><p>(see the definition of tensor H in Sec. 3) is the same as in the full-pipeline. However, in G, x a , H a and H b are concatenated at the input layer. Hence, the encoder of G is composed of only one stream, whose architecture is the same as the two streams described in Sec.5.</p><p>• DSC: G is implemented as described in Sec. 3, introducing our Deformable Skip Connections (DSC). Both  in DSC and in Baseline, training is performed using an L 1 loss together with the adversarial loss.</p><p>• PercLoss: This is DSC in which the L 1 loss is replaced with the Perceptual loss proposed in <ref type="bibr" target="#b5">[6]</ref>. This loss is computed using the layer conv 2 1 of <ref type="bibr" target="#b14">[15]</ref>, chosen to have a receptive field the closest possible to N (p) in Eq. 8, and computing the element-to-element difference in this layer without nearest neighbor search.</p><p>• Full: This is the full-pipeline whose results are reported in Tab. 1, and in which we use the proposed L N N loss (see Sec. 4).</p><p>In Tab. 3 we report a quantitative evaluation on the Market-1501 and on the DeepFashion dataset with respect to the four different versions of our approach. In most of the cases, there is a progressive improvement from Baseline to DSC to Full. Moreover, Full usually obtains better results than PercLoss. These improvements are particularly evident looking at the DS metrics, which we believe it is a strong evidence that the generated images are realistic. DS values on the DeepFashion dataset are omitted because they are all close to the value ∼ 0.96. In <ref type="figure">Fig. 4</ref> and <ref type="figure" target="#fig_1">Fig. 5</ref> we show some qualitative results. These figures show the progressive improvement through the four baselines which is quantitatively presented above.</p><p>In fact, while pose information is usually well generated by all the methods, the texture generated by Baseline often does not correspond to the texture in x a or is blurred. In same cases, the improvement of Full with respect to Baseline is quite drastic, such as the drawing on the shirt of the girl in the second row of <ref type="figure" target="#fig_1">Fig. 5</ref> or the stripes on the clothes of the persons in the third and in the fourth row of <ref type="figure">Fig. 4</ref>. Further examples are shown in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper we presented a GAN-based approach for image generation of persons conditioned on the appearance and the pose. We introduced two novelties: deformable skip connections and nearest-neighbour loss. The first is used to solve common problems in U-Net based generators when dealing with deformable objects. The second novelty is used to alleviate a different type of misalignment between the generated image and the ground-truth image.</p><p>Our experiments, based on both automatic evaluation metrics and human judgments, show that the proposed method is able to outperform previous work on this task. Despite the proposed method was tested on the specific task of human-generation, only few assumptions are used which refer to the human body and we believe that our proposal can be easily extended to address other deformable-object generation tasks.</p><p>In this Appendix we report some additional implementation details and we show other quantitative and qualitative results. Specifically, in Sec. A we explain how Eq. 8 can be efficiently implemented using GPU-based parallel computing, while in Sec. B we show how the human-body symmetry can be exploited in case of missed limb detections. In Sec. C we train state-of-the-art Person Re-IDentification (Re-ID) systems using a combination of real and generated data, which, on the one hand, shows how our images can be effectively used to boost the performance of discriminative methods and, on the other hand, indirectly shows that our generated images are realistic and diverse. In Sec. D we show a direct (qualitative) comparison of our method with the approach presented in <ref type="bibr" target="#b11">[12]</ref> and in Sec. E we show other images generated by our method, including some failure cases. Note that some of the images in the DeepFashion dataset have been manually cropped (after the automatic generation) to improve the overall visualization quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Nearest-neighbour loss implementation</head><p>Our proposed nearest-neighbour loss is based on the definition of L N N (x, x b ) given in Eq. 8. In that equation, for each point p inx, the "most similar" (in the C x -based feature space) point q in x b needs to be searched for in a n × n neighborhood of p. This operation may be quite time consuming if implemented using sequential computing (i.e., using a "for-loop"). We show here how this computation can be sped-up by exploiting GPU-based parallel computing in which different tensors are processed simultaneously.</p><p>Given C x b , we compute n 2 shifted versions of C x b : {C (i,j)</p><p>x b }, where (i, j) is a translation offset ranging in a relative n × n neighborhood (i, j ∈ {− n−1 2 , ..., + n−1 2 }) and C</p><formula xml:id="formula_18">(i,j) x b</formula><p>is filled with the value +∞ in the borders. Using this translated versions of C x b , we compute n 2 corresponding difference tensors {D (i,j) }, where:</p><formula xml:id="formula_19">D (i,j) = |Cx − C (i,j) x b |<label>(11)</label></formula><p>and the difference is computed element-wise. D (i,j) (p) contains the channel-by-channel absolute difference between Cx(p) and C x b (p + (i, j)). Then, for each D (i,j) , we sum all the channel-based differences obtaining:</p><formula xml:id="formula_20">S (i,j) = c D (i,j) (c),<label>(12)</label></formula><p>where c ranges over all the channels and the sum is performed pointwise. S (i,j) is a matrix of scalar values, each value representing the L 1 norm of the difference between a point p in Cx and a corresponding point p + (i, j) in C x b :</p><formula xml:id="formula_21">S (i,j) (p) = ||Cx(p) − C x b (p + (i, j))|| 1 .<label>(13)</label></formula><p>For each point p, we can now compute its best match in a local neighbourhood of C x b simply using:</p><formula xml:id="formula_22">M (p) = min (i,j) S (i,j) (p).<label>(14)</label></formula><p>Finally, Eq. 8 becomes:</p><formula xml:id="formula_23">L N N (x, x b ) = p M (p).<label>(15)</label></formula><p>Since we do not normalize Eq. 12 by the number of channels nor Eq. 15 by the number of pixels, the final value L N N (x, x b ) is usually very high. For this reason we use a small value λ = 0.01 in Eq. 10 when weighting L N N with respect to L cGAN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Exploiting the human-body symmetry</head><p>As mentioned in Sec. 3.1, we decompose the human body in 10 rigid sub-parts: the head, the torso and 8 limbs (left/right upper/lower arm, etc.). When one of the joints corresponding to one of these body-parts has not been detected by the HPE, the corresponding region and affine transformation are not computed and the region-mask is filled with 0. This can happen because of either that region is not visible in the input image or because of falsedetections of the HPE.</p><p>However, when the missing region involves a limb (e.g., the right-upper arm) whose symmetric body part has been detected (e.g., the left-upper arm), we can "copy" information from the "twin" part. In more detail, suppose for instance that the region corresponding to the right-upper arm in the conditioning image is R a rua and this region is empty because of one of the above reasons. Moreover, suppose that R b rua is the corresponding (non-empty) region in x b and that R a lua is the (non-empty) left-upper arm region in x a . We simply set: R a rua := R a lua and we compute f rua as usual, using the (now, no more empty) region R a rua together with R b rua .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Improving person Re-ID via dataaugmentation</head><p>The goal of this section is to show that the synthetic images generated with our proposed approach can be used to train discriminative methods. Specifically, we use Re-ID approaches whose task is to recognize a human person in different poses and viewpoints. The typical application of a Re-ID system is a video-surveillance scenario in which images of the same person, grabbed by cameras mounted in different locations, need to be matched to each other. Due to the low-resolution of the cameras, person re-identification is usually based on the colours and the texture of the clothes <ref type="bibr" target="#b21">[22]</ref>. This makes our method particularly suited to automatically populate a Re-ID training dataset by generating images of a given person with identical clothes but in different viewpoints/poses.</p><p>In our experiments we use Re-ID methods taken from <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref> and we refer the reader to those papers for details about the involved approaches. We employ the Market-1501 dataset that is designed for Re-ID method benchmarking. For each image of the Market-1501 training dataset (T ), we randomly select 10 target poses, generating 10 corresponding images using our approach. Note that: (1) Each generated image is labeled with the identity of the conditioning image, (2) The target pose can be extracted from an individual different from the person depicted in the conditioning image (this is different from the other experiments shown here and in the main paper). Adding the generated images to T we obtain an augmented training set A. In Tab. 4 we report the results obtained using either T (standard procedure) or A for training different Re-ID systems. The strong performance boost, orthogonal to different Re-ID methods, shows that our generative approach can be effectively used for synthesizing training samples. It also indirectly shows that the generated images are sufficiently realistic and different from the real images contained in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with previous work</head><p>In this section we directly compare our method with the results generated by Ma et al. <ref type="bibr" target="#b11">[12]</ref>. The comparison is based on the pairs conditioning image-target pose used in <ref type="bibr" target="#b11">[12]</ref>, for which we show both the results obtained by Ma et al. <ref type="bibr" target="#b11">[12]</ref> and ours. <ref type="figure">Figs. 6-7</ref> show the results on the Market-1501 dataset. Comparing the images generated by our full-pipeline with the corresponding images generated by the full-pipeline presented in <ref type="bibr" target="#b11">[12]</ref>, most of the times our results are more realistic, sharper and with local details (e.g., the clothes texture or the face characteristics) more similar to the details of the conditioning image. For instance, in the first and the last row of <ref type="figure">Fig. 6</ref> and in the last row of <ref type="figure">Fig. 7</ref>, our results show human-like images, while the method proposed in <ref type="bibr" target="#b11">[12]</ref> produced images which can hardly be recognized as humans. <ref type="figure">Figs. 8-9</ref> show the results on the DeepFashion dataset. Also in this case, comparing our results with <ref type="bibr" target="#b11">[12]</ref>, most of the times ours look more realistic or closer to the details of the conditioning image. For instance, the second row of <ref type="figure">Fig. 8</ref> shows a male face, while the approach proposed in <ref type="bibr" target="#b11">[12]</ref> produced a female face (note that the DeepFashion dataset is strongly biased toward female subjects <ref type="bibr" target="#b11">[12]</ref>). Most of the times, the clothes texture in our case is closer to that depicted in the conditioning image (e.g., see rows 1, 3, 4, 5 and 6 in <ref type="figure">Fig. 8</ref> and rows 1 and 6 in <ref type="figure">Fig. 9</ref>). In row 5 of <ref type="figure">Fig. 9</ref> the method proposed in <ref type="bibr" target="#b11">[12]</ref> produced an image with a pose closer to the target; however it wrongly generated pants while our approach correctly generated the appearance of the legs according to the appearance contained in the conditioning image.</p><p>We believe that this qualitative comparison using the pairs selected in <ref type="bibr" target="#b11">[12]</ref>, shows that the combination of the proposed deformable skip-connections and the nearestneighbour loss produced the desired effect to "capture" and transfer the correct local details from the conditioning image to the generated image. Transferring local information while simultaneously taking into account the global pose deformation is a difficult task which can more hardly be implemented using "standard" U-Net based generators as those adopted in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Other qualitative results</head><p>In this section we present other qualitative results. <ref type="figure">Fig. 10</ref> and <ref type="figure">Fig. 11</ref> show some images generated using the Market-1501 dataset and the DeepFashion dataset, respectively. The terminology is the same adopted in Sec. 6.2. Note that, for the sake of clarity, we used a skeleton-based visualization of P (·) but, as explained in the main paper, only the point-wise joint locations are used in our method to represent pose information (i.e., no joint-connectivity information is used).</p><p>Similarly to the results shown in Sec. 6.2, also these images show that, despite the pose-related general structure is sufficiently well generated by all the different versions of our method, most of the times there is a gradual quality improvement in the detail synthesis from Baseline to DSC to PercLoss to Full.</p><p>Finally, <ref type="figure" target="#fig_0">Fig. 12</ref> and <ref type="figure">Fig. 13</ref> show some failure cases (badly generated images) of our method on the Market-1501 dataset and the DeepFashion dataset, respectively. Some common failure causes are:</p><p>• Errors of the HPE <ref type="bibr" target="#b0">[1]</ref>. For instance, see rows 2, 3 and 4 of <ref type="figure" target="#fig_0">Fig. 12</ref> or the wrong right-arm localization in row 2 of <ref type="figure">Fig. 13</ref>.</p><p>• Ambiguity of the pose representation. For instance, in row 3 of <ref type="figure">Fig. 13</ref>, the left elbow has been detected in x b although it is actually hidden behind the body. Since P (x b ) contains only 2D information (no depth or occlusion-related information), there is no way for the system to understand whether the elbow is behind or in front of the body. In this case our model chose to generate an arm considering that the arm is in front of the body (which corresponds to the most frequent situation in the training dataset).</p><p>• Rare poses. For instance, row 1 of <ref type="figure">Fig. 13</ref> shows a girl in an unusual rear view with a sharp 90 degree profile face (x b ). The generator by mistake synthesized a neck where it should have "drawn" a shoulder. Note that rare poses are a difficult issue also for the method proposed in <ref type="bibr" target="#b11">[12]</ref>.  <ref type="bibr" target="#b21">[22]</ref> 73.9 48.8 78.5 55.9 IDE + XQDA <ref type="bibr" target="#b21">[22]</ref> 73.2 50.9 77.8 57.9 IDE + KISSME <ref type="bibr" target="#b21">[22]</ref> 75.1 51.5 79.5 58.1 Discriminative Embedding <ref type="bibr" target="#b23">[24]</ref> 78.3 55.5 80.6 61.3</p><p>• Rare object appearance. For instance, the backpack in row 1 of <ref type="figure" target="#fig_0">Fig. 12</ref> is light green, while most of the backpacks contained in the training images of the Market-1501 dataset are dark. Comparing this image with the one generated in the last row of <ref type="figure">Fig. 10</ref> (where the backpack is black), we see that in <ref type="figure">Fig. 10</ref> the colour of the shirt of the generated image is not blended with the backpack colour, while in <ref type="figure" target="#fig_0">Fig. 12</ref> it is. We presume that the generator "understands" that a dark backpack is an object whose texture should not be transferred to the clothes of the generated image, while it is not able to generalize this knowledge to other backpacks.</p><p>• Warping problems. This is an issue related to our specific approach (the deformable skip connections). The texture on the shirt of the conditioning image in row 2 of <ref type="figure">Fig. 13</ref> is warped in the generated image. We presume this is due to the fact that in this case the affine transformations need to largely warp the texture details of the narrow surface of the profile shirt (conditioning image) in order to fit the much wider area of the target frontal pose.</p><p>x a</p><p>x b Full (ours) Ma et al. <ref type="bibr" target="#b11">[12]</ref> Figure 6: A qualitative comparison on the Market-1501 dataset between our approach and the results obtained by Ma et al. <ref type="bibr" target="#b11">[12]</ref>. Columns 1 and 2 show the conditioning and the target image, respectively, which are used as reference by both models. Columns 3 and 4 respectively show the images generated by our full-pipeline and by the full-pipeline presented in <ref type="bibr" target="#b11">[12]</ref>.</p><p>x a</p><p>x b Full (ours) Ma et al. <ref type="bibr" target="#b11">[12]</ref> Figure 7: More qualitative comparison on the Market-1501 dataset between our approach and the results obtained by Ma et al. <ref type="bibr" target="#b11">[12]</ref>.</p><p>x a</p><p>x b</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full (ours)</head><p>Ma et al. <ref type="bibr" target="#b11">[12]</ref> Figure 8: A qualitative comparison on the DeepFashion dataset between our approach and the results obtained by Ma et al. <ref type="bibr" target="#b11">[12]</ref>.</p><p>x a</p><p>x b</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full (ours)</head><p>Ma et al. <ref type="bibr" target="#b11">[12]</ref> Figure 9: More qualitative comparison on the DeepFashion dataset between our approach and the results obtained by Ma et al. <ref type="bibr" target="#b11">[12]</ref>. Baseline (ours) DSC (ours) PercLoss (ours) Full (ours) <ref type="figure" target="#fig_0">Figure 12</ref>: Examples of badly generated images on the Market-1501 dataset. See the text for more details. Baseline (ours) DSC (ours) PercLoss (ours) Full (ours) <ref type="figure">Figure 13</ref>: Examples of badly generated images on the DeepFashion dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>A schematic representation of our network architectures. For the sake of clarity, in this figure we depict P (·) as a skeleton and each tensor H as the average of its component matrices H j (1 ≤ j ≤ k). The white rectangles in the decoder represent the feature maps directly obtained using up-convolutional filters applied to the previous-layer maps. The reddish rectangles represent the feature maps "shuttled" by the skip connections from the H b stream. Finally, blueish rectangles represent the deformed tensors d(F ) "shuttled" by the deformable skip connections from the (x a , H a ) stream.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on the DeepFashion dataset with respect to different baselines. Some images have been cropped for visualization purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>PFigure 10 :Figure 11 :</head><label>1011</label><figDesc>(x a ) P (x b ) x bBaseline (ours) DSC (ours) PercLoss (ours) Full (ours) Other qualitative results on the Market-1501 dataset.x a P (x a ) P (x b ) x bBaseline (ours) DSC (ours) PercLoss (ours) Full (ours) Other qualitative results on the DeepFashion dataset.x a P (x a ) P (x b ) x b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>P</head><label></label><figDesc>(x a ) P (x b ) x b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with the state of the art. ( * ) These values have been computed using the code and the network weights released by Ma et al.<ref type="bibr" target="#b11">[12]</ref> in order to generate new images.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Market-1501</cell><cell></cell><cell></cell><cell cols="2">DeepFashion</cell></row><row><cell>Model</cell><cell>SSIM</cell><cell>IS</cell><cell cols="2">mask-SSIM mask-IS</cell><cell>DS</cell><cell>SSIM</cell><cell>IS</cell><cell>DS</cell></row><row><cell cols="3">Ma et al. [12] 0.253 3.460</cell><cell>0.792</cell><cell>3.435</cell><cell cols="4">0.39  *  0.762 3.090 0.95  *</cell></row><row><cell>Ours</cell><cell cols="2">0.290 3.185</cell><cell>0.805</cell><cell>3.502</cell><cell cols="4">0.72 0.756 3.439 0.96</cell></row><row><cell>Real-Data</cell><cell>1.00</cell><cell>3.86</cell><cell>1.00</cell><cell>3.36</cell><cell>0.74</cell><cell cols="2">1.000 3.898</cell><cell>0.98</cell></row><row><cell>the protocol of Ma et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>User study (%). ( * ) These results are reported in<ref type="bibr" target="#b11">[12]</ref> and refer to a similar study with AMT users.</figDesc><table><row><cell></cell><cell cols="2">Market-1501</cell><cell cols="2">DeepFashion</cell></row><row><cell>Model</cell><cell>R2G</cell><cell>G2R</cell><cell>R2G</cell><cell>G2R</cell></row><row><cell cols="2">Ma et al. [12] ( * ) 11.2</cell><cell>5.5</cell><cell>9.2</cell><cell>14.9</cell></row><row><cell>Ours</cell><cell cols="4">22.67 50.24 12.42 24.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Quantitative ablation study on the Market-1501 and the DeepFashion dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Market-1501</cell><cell></cell><cell></cell><cell cols="2">DeepFashion</cell></row><row><cell>Model</cell><cell>SSIM</cell><cell>IS</cell><cell cols="2">mask-SSIM mask-IS</cell><cell>DS</cell><cell>SSIM</cell><cell>IS</cell></row><row><cell>Baseline</cell><cell cols="2">0.256 3.188</cell><cell>0.784</cell><cell>3.580</cell><cell>0.595</cell><cell cols="2">0.754 3.351</cell></row><row><cell>DSC</cell><cell cols="2">0.272 3.442</cell><cell>0.796</cell><cell>3.666</cell><cell>0.629</cell><cell cols="2">0.754 3.352</cell></row><row><cell>PercLoss</cell><cell cols="2">0.276 3.342</cell><cell>0.788</cell><cell>3.519</cell><cell>0.603</cell><cell cols="2">0.744 3.271</cell></row><row><cell>Full</cell><cell cols="2">0.290 3.185</cell><cell>0.805</cell><cell>3.502</cell><cell cols="3">0.720 0.756 3.439</cell></row><row><cell>Real-Data</cell><cell>1.00</cell><cell>3.86</cell><cell>1.00</cell><cell>3.36</cell><cell>0.74</cell><cell cols="2">1.000 3.898</cell></row><row><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>a P (x a ) P (x b ) x b Baseline DSC PercLoss Full</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy of Re-ID methods on the Market-1501 test set (%)</figDesc><table><row><cell></cell><cell cols="4">Standard training set (T ) Augmented training set (A)</cell></row><row><cell>Model</cell><cell>Rank 1</cell><cell>mAP</cell><cell>Rank 1</cell><cell>mAP</cell></row><row><cell>IDE + Euclidean</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We want to thank the NVIDIA Corporation for the donation of the GPUs used in this project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A generative model of people in clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual attribute transfer through deep image analogy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-view image generation from a single-view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04886</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by GAN improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A discriminatively learned CNN embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>13:1-13:20</idno>
	</analytic>
	<monogr>
		<title level="j">Communications, and Applications (TOMCCAP)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ACM Trans. on Multimedia Computing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
