<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a Single RGB Panorama</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Ta</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-En</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Han</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
							<email>pwonka@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">KAUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
							<email>sunmin@ee.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Kuo</forename><surname>Chu</surname></persName>
							<email>hkchu@cs.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DuLa-Net: A Dual-Projection Network for Estimating Room Layouts from a Single RGB Panorama</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a deep learning framework, called DuLa-Net, to predict Manhattan-world 3D room layouts from a single RGB panorama. To achieve better prediction accuracy, our method leverages two projections of the panorama at once, namely the equirectangular panorama-view and the perspective ceiling-view, that each contains different clues about the room layouts. Our network architecture consists of two encoder-decoder branches for analyzing each of the two views. In addition, a novel feature fusion structure is proposed to connect the two branches, which are then jointly trained to predict the 2D floor plans and layout heights. To learn more complex room layouts, we introduce the Realtor360 dataset that contains panoramas of Manhattan-world room layouts with different numbers of corners. Experimental results show that our work outperforms recent state-of-the-art in prediction accuracy and performance, especially in the rooms with non-cuboid layouts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Inferring high-quality 3D room layouts from indoor panoramic images plays a crucial role in indoor scene understanding and can be beneficial to various applications, including virtual/augmented reality and robotics. To that end, recent methods recover 3D room layouts by using deep learning to predict the room corners and boundaries on the input panorama. For example, LayoutNet <ref type="bibr" target="#b32">[33]</ref> achieved impressive reconstruction accuracy for Manhattan worldconstrained rooms. However, the clutter in the room, e.g. furniture, poses a challenge to extract critical corners and edges that are occluded in the input panorama. In addition, estimating 3D layouts from 2D corner and edge maps is an ill-posed problem and thus imposing extra constraints in the optimization. Therefore, it remains challenging to process complex room layouts.</p><p>In this work, we present a novel end-to-end framework to estimate a 3D room layout from a single RGB panorama. By the intuition that a neural network may extract different kinds of features given the same panorama but in different projections, we propose to predict the room layouts from two distinct views of the panoramas, namely the equirectangular panorama-view and the perspective ceilingview. The network architecture follows the encoder-decoder scheme and consists of two branches, the panorama-branch and the ceiling-branch, for respectively analyzing images of the panorama-view and the ceiling-view. The outputs of panorama-branch include a floor-ceiling probability map and a layout height, while the ceiling-branch outputs a floor plan probability map. To share information between branches, we employ a feature fusion scheme to connect the first few layers of decoders through a E2P conversion that transforms intermediate feature maps from equirectangular projection to perspective ceiling-view. We find that better prediction performance is achieved by jointly training the two connected branches. The final 2D floor plan is then obtained by fitting an axis-aligned polygon to a fused floor plan probability map (see <ref type="figure" target="#fig_1">Figure 3</ref> for details) and then extruded by the estimated layout height.</p><p>To learn from panoramas with complex layouts, we need a proper dataset for network training and testing. However, existing public datasets, such as PanoContext <ref type="bibr" target="#b29">[30]</ref> dataset, provide mostly labeled 3D layouts with simple cuboid shapes. To learn more complex layouts, we introduce a new dataset, Realtor360, which includes a subset of SUN360 <ref type="bibr" target="#b23">[24]</ref> dataset (593 living rooms and bedrooms) and 1980 panoramas collected from a real estate database. We annotated the whole dataset with a custom-made interactive tool to obtain the ground-truth 3D layouts.</p><p>A key feature of our dataset is that it contains rooms with more complex shapes in terms of the numbers of the corners. The experimental results demonstrate that our method outperforms the current state-of-the-art method ( <ref type="bibr" target="#b32">[33]</ref>) in prediction accuracy, especially with rooms with more than four corners. Our method also takes much less time to compute the final room layouts. <ref type="figure" target="#fig_0">Fig. 1</ref> shows some room layouts estimated by our method. Our contributions are summarized as follows:</p><p>• We propose a novel network architecture that contains two encoder-decoder branches to analyze the input panorama in two different projections. These two branches are further connected through a feature fusion scheme. This dual-projection architecture can infer room layouts with more complex shapes beyond cuboids and L-shapes.</p><p>• Our neural network is an important step towards building an end-to-end architecture. Our network directly outputs a probability map of the 2D floor plan. This output requires significantly less post-processing to obtain the final 3D room layout than the output of the current state of the art.</p><p>• We introduce a new data set, called Realtor360, that contains 2573 panoramas depicting rooms with 4 to 12 corners. To the best of our knowledge, this is largest data set of indoor images with room layout annotations currently available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are multiple papers that propose a solution to estimate room layouts from a single image taken in an indoor environment. They mainly differ in three aspects: 1) the assumptions of the room layouts, 2) the types of the input images, and 3) the methods. In terms of room layout assumptions, a popular choice is the "Manhattan world" assumption <ref type="bibr" target="#b3">[4]</ref>, meaning that all walls are aligned with a global coordinate system <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>. To make the problem easier to solve, a more restrictive assumption is that the room is a cuboid <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13]</ref>, i.e., there exist exactly four room corners. Our method adopts the Manhattan world assumption but allows for arbitrary numbers of corners.</p><p>In terms of types of input images, the images may differ in the FoV (field of view) -ranging from being monocular (i.e., taken from a standard camera) to 360 • panoramas, and whether depth information is provided. The methods then largely depend on the input image types. The problem is probably most difficult to solve when only a monocular RGB image is given. Typically, geometric (e.g., lines and corners) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref> and/or semantic (e.g., segmentation into different regions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and volumetric reasoning <ref type="bibr" target="#b6">[7]</ref>) "cues" are extracted from the input image, a set of room layout hypotheses is generated, and then an optimization or voting process is taken to rank and select one among the hypotheses. Recently, neural network-based methods took stride in tackling this problem. A trend is that the neural networks generate higher and higher levels of information -starting from line segments <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>, surface labels <ref type="bibr" target="#b4">[5]</ref>, to room types <ref type="bibr" target="#b12">[13]</ref> and room boundaries and corners <ref type="bibr" target="#b32">[33]</ref>, to make the final layout generation process increasingly easier to solve. Our method pushes this trend one step further by using neural networks to directly predict a 2D floor plan probability map that requires only a 2D polygon fitting process to produce the final 2D room layout. If depth information is provided, there exist methods that estimate scene annotations including room layouts <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>. A deeper discussion is beyond the scope of this paper.</p><p>Closely related problems include depth estimation from a given image <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21]</ref> and scene reconstructions from point clouds <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>. Note that neither estimated depths nor reconstructed 3D scenes necessarily equate a clean room layout as such inputs may contain clutters. 360 • panorama: The seminal work by Zhang et al. <ref type="bibr" target="#b29">[30]</ref> advocates the use of 360 • panoramas for indoor scene understanding for the reason that the FOV of 360 • panoramas is much more expansive. Work in this direction flourished, including methods based on optimization approaches over geometric <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref> and/or semantic cues <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27]</ref> and later based on neural networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. Except for Lay-outNet <ref type="bibr" target="#b32">[33]</ref>, most methods rely on leveraging existing techniques for single perspective images on samples taken from the input panorama. We believe that this is a major reason of LayoutNet's superior performance since it performs predictions on the panorama as a whole, thus extracting more global information that the input panorama might contain. <ref type="figure">Figure 2</ref>. Our network architecture follows the encoder-decoder scheme and consists of two branches. Given a panorama in equirectangular projection, we additionally create a perspective ceiling-view image through a equirectangular-to-perspective (E2P) conversion. The panorama and the ceiling-view images are then fed to the panorama-view (upper) and ceiling-view (lower) branches. A E2P-based feature fusion scheme is employed to connect the two branches, which are jointly trained by the network to predict: 1) probability maps of the floor and ceiling in panorama view, 2) a floor plan in ceiling view, and 3) a layout height. Then, our system estimates a 2D floor plan by fitting a Manhattan-world aligned polygon to a weighted average of the three floor plans, which is further extruded using the predicted layout height to obtain the final 3D room layout.</p><p>A further step in this direction can be found in <ref type="bibr" target="#b20">[21]</ref>, in which the input panorama is projected to a 2D "floor" view in which the camera position is mapped to the center of the image and the vertical lines in the panorama become radial lines emanated from the image center. An advantage of this approach is that the room layout becomes a 2D closed loop that can be extracted more easily. We derived our "ceiling" view idea here -instead of looking downward toward the floor in which all the clutter in the room is included, we look upward toward the ceiling and got a more clutter-free view of the room layout. <ref type="figure">Fig. 2</ref> illustrates the overview of our framework. Given the input as an equirectangular panoramic image, we follow the same pre-processing step used in PanoContext <ref type="bibr" target="#b29">[30]</ref> to align the panoramic image with a global coordinate system, i.e. we make a Manhattan world assumption. Then, we transform the panoramic image into a perspective ceilingview image through an equirectangular to perspective (E2P) conversion (Sec. 4). The panorama-view and ceiling-view images are then fed to a network consisting of two encoderdecoder branches. These two branches are connected via a E2P-based feature fusion scheme and jointly trained to predict a floor plan probability map, a floor-ceiling probability map, and the layout height (Sec. 5). Two intermediate probability maps are derived from the floor-ceiling probability map using E2P conversion and combined with floor plan probability map to obtain a fused floor plan probabil-ity map. The final 3D Manhattan layout is determined by extruding a 2D Manhattan floor plan estimated on the fused floor plan probability map using the predicted layout height (Sec. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">E2P conversion</head><p>In this section, we explain the formulation of E2P conversion that transforms an equirectangular panorama to a perspective image. We assume the perspective image is square with dimension w × w. For every pixel in the perspective image at position (p x , p y ), we derive the position of the corresponding pixel in the equirectangular panorama, (p x , p y ), −1 ≤ p x ≤ 1, −1 ≤ p y ≤ 1, as follows. First, we define the field of view of the pinhole camera of the perspective image as F oV . Then, the focal length can be derived as:</p><formula xml:id="formula_0">f = 0.5 * w * cot(0.5 * FoV) .</formula><p>(p x , p y , f ), the 3D position of the pixel in the perspective image in the camera space, is then rotated by 90 • or -90 • along the x-axis (counter-clockwise) if the camera is looking upward (looking at the ceiling) or downward (looking at the floor), respectively.</p><p>Next, we project the rotated 3D position to the equirectangular space. To do so, we first project it onto a unit sphere by vector normalization, (s x , s y , s z ), and apply the following formula:</p><formula xml:id="formula_1">(p x , p y ) = ( arctan 2 ( sx sz ) π , arcsin(s y ) 0.5π ),<label>(1)</label></formula><p>to project (s x , s y , s z ), the 3D position on the unit sphere, back to (p x , p y ), the corresponding 2D position in the equirectangular panorama. Finally, we use (p x , p y ) to interpolate a pixel value from the panorama. We note that this process is differentiable so it can be used in conjunction with backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Network architecture</head><p>Our network architecture is illustrated in <ref type="figure">Fig. 2</ref>. It consists of two encoder-decoder branches, for the panoramaview and the ceiling-view input images. We denote the panorama-view branch as B P and the ceiling-view branch as B C . The encoder and decoder of B P are denoted as E B P and D B P and for B C they are denoted as E B C and D B C . A key concept is that our network predicts the floor plan and the layout height. With these two predictions, we can reconstruct a 3D room layout in a post-process (Sec. 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Encoder</head><p>We use ResNet-18 as the architecture for both E B P and E B C . The input dimension of E B P is 512 × 1024 × 3 (the dimension of the input panorama) and the output dimension is 16×32×512. For E B C , the input and output dimensions are 512 × 512 × 3 and 16 × 16 × 512. Note that the input of E B C is a perspective ceiling-view image generated by applying E2P conversion to the input panorama with F oV set to 160 • and w set to 512. We also tried other more computationally expensive network architectures such as ResNet-50 for the encoders. However, we find no improvements in accuracy so we chose to work with ResNet-18 for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Decoder</head><p>Both D B P and D B C consist of six convolutional layers. The first five layers are 3 × 3 resize convolutions <ref type="bibr" target="#b0">[1]</ref> with ReLU activations. The last layer is a regular 3 × 3 convolution with sigmoid activation. The numbers of channels of the six layers are 256, 128, 64, 32, 16, and 1. To infer the layout height, we add three fully connected layers to the middlemost feature of B P . The dimensions of the three layers are 256, 64, and 1. To make the regression of the layout height more robust, we add dropout layers after the first two layers. To take the middlemost feature as input, we first apply global average pooling along both x and y dimensions, which produces an 1-D feature with 512 dimension, and take it as the input of the fully connected layers.</p><p>The output of B P is a probability map of the floor and the ceiling in the equirectangular projection, denoted as the floor-ceiling probability map (M F C ). For B C , the output is a probability map of the floor plan in the ceiling view, denoted as the floor plan probability map (M F P ). Note that B P also outputs a predicted layout height (H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Feature fusion</head><p>We find that applying fusion techniques to merge the features in both B P and B C increases the prediction accuracy. We conjecture a reason as follows. In a ceiling-view image, the areas near the image boundary (where some useful visual clues such as shadows and furniture arrangements exist) are more distorted, which can have a detrimental effect for the ceiling-view branch to infer room structures. By fusing features from the panorama-view branch (in which distortion is less severe), performance of the ceiling-view branch can be improved.</p><p>We apply fusions before each of the first five layers of D B P and D B C . For each fusion connection, a E2P conversion (Sec. 4) with F oV set to 160 • is taken to project the features in D B P , which are originally in the equirectangular view, to the perspective ceiling view. Each fusion works as follows:</p><formula xml:id="formula_2">f * B C = f B C + α β i × f B P , i ∈ {0, 1, 2, 3, 4},<label>(2)</label></formula><p>where f B C is the feature from B C and f B P is the feature from B P after applying the E2P conversion. α and β are the decay coefficients. i is the index of the layer. After each fusion, the merged feature, f * B C , is sent into the next layer of D B C . The performance improvement of this technique is discussed in Sec. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Loss function</head><p>For M F C and M F P , we apply binary cross entropy loss:</p><formula xml:id="formula_3">E b (x, x * ) = − i x * i log(x i ) + (1 − x * i ) log(1 − x i ).<label>(3)</label></formula><p>For H (layout height), we use L1-loss:</p><formula xml:id="formula_4">E L1 (x, x * ) = i |x i − x * i |.<label>(4)</label></formula><p>The overall loss function is:</p><formula xml:id="formula_5">L = E b (M F C , M * F C ) + E b (M F P , M * F P ) + γE L1 (H, H * ),<label>(5)</label></formula><p>where M * F C , M * F P and H * are the ground truth of M F C , M F P , and H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Training details</head><p>We implement our method with PyTorch <ref type="bibr" target="#b19">[20]</ref>. We use the Adam <ref type="bibr" target="#b10">[11]</ref> optimizer with β 1 = 0.9 and β 2 = 0.999. The learning rate is 0.0003 and batch size is 4. Our training loss converges after about 120 epochs. For each training iteration we augment the input panorama with random flipping and horizontal rotatations by 0 • , 90 • , 180 • , and 270 • . For fusion, we set α and β in Eqn. 2 to be 0.6 and 3. We set the γ in Eqn. 5 to be 0.5. Because we estimate the floor plan probability map in the ceiling view, we assume the distance between the camera and the ceiling to be 1.6 meters, and use this constant to normalize the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">3D layout estimation</head><p>Given the probability maps (M F C and M F P ) and the layout height (H) predicted by the network, we reconstruct the final 3D layout in the following two steps:</p><p>1. Estimating a 2D Manhattan floor plan shape using the probability maps.</p><p>2. Extruding the floor plan shape along its normal according to the layout height.</p><p>For step 1, two intermediate maps, denoted as M C F C and M F F C , are derived from ceiling pixels and floor pixels of the floor-ceiling probability map using the E2P conversion. We further use a scaling factor, 1.6/(H − 1.6), to register the M F F C with M C F C , where the constant 1.6 is the distance between the camera and the ceiling. Finally, a fused floor plan probability map is computed as follows: is binarized using a threshold of 0.5. A bounding rectangle of the largest connected component is computed for later use. Next, we convert the binary image to a densely sampled piece-wise linear closed loop and simplify it using the Douglas-Peucker algorithm (see <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>). We run a regression analysis on the edges and cluster them into sets of axis-aligned horizontal and vertical lines. These lines divide the bounding rectangle into several disjoint grid cells (see <ref type="figure" target="#fig_1">Fig. 3 (c)</ref>). We define the shape of the 2D floor plan as the union of grid cells where the ratio of floor plan area is greater than 0.5 (see <ref type="figure" target="#fig_1">Fig. 3 (d)</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Realtor360 dataset</head><p>A dataset that contains a sufficient number of 3D room layouts with different numbers of corners is crucial for training as well as testing our network. Unfortunately, existing public domain datasets, such as the PanoContext <ref type="bibr" target="#b29">[30]</ref> dataset and the Stanford 2D-3D dataset labeled by Zou et al. <ref type="bibr" target="#b32">[33]</ref>, contain mostly layouts with a simple cuboid shape. To prove that our framework is flexible enough to deal with rooms with an arbitrary number of corners, we introduce a new dataset, named Realtor360, that contains over 2500 indoor panoramas and annotated 3D room layouts. We classify each room according to its layout complexity measured by the number of corners in the floor plan. <ref type="table" target="#tab_0">Table 1</ref> shows the statistics of the dataset and a few visual examples can be found in <ref type="figure" target="#fig_3">Fig. 4</ref>. The source panoramic images in the Realtor360 dataset are collected from two sources. The first one is a subset of the SUN360 dataset <ref type="bibr" target="#b23">[24]</ref>, which contains 593 living rooms and bedrooms panoramas. The other source is a real estate database with 1980 indoor panoramas acquired from a real-estate company. We annotate the 3D layouts of these indoor panoramas using a custom-made interactive tool as explained below.</p><p>Annotation tool. To annotate the 2D indoor panoramas with high-quality 3D room layouts, we developed an interactive tool to facilitate the labeling process. The tool first leverages existing automatic methods to extract a depth map <ref type="bibr" target="#b11">[12]</ref> and line segments <ref type="bibr" target="#b29">[30]</ref> from the input panorama. Then, an initial 3D Manhattan-world layout is created by sampling the depth along the horizontal line in the middle of the panorama. The tool allows the users to refine the initial 3D layout through a set of intuitive operations, including (i) pushing/pulling a wall; (ii) merging multiple walls; and (iii) splitting a wall. It also offers a handy function to snap the layout edges to the estimated line segments during the interactive editing to improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiments</head><p>We compare our method to LayoutNet <ref type="bibr" target="#b32">[33]</ref>, a state-ofthe-art method in room layout estimation, through a series of quantitative and qualitative experiments on our Re-altor360 dataset and the PanoContext <ref type="bibr" target="#b29">[30]</ref> dataset. We also conduct ablation study with several alternative configurations of our method. We adopt 2D and 3D Intersection over Union (IoU) to evaluate the accuracy of the estimated 2D floor plans and 3D layouts, which is a standard metric in similar tasks <ref type="bibr" target="#b2">[3]</ref>. All the experiments used the same hyperparameter discussed in Sec. 5.5. <ref type="figure">Fig. 5</ref> shows a few 3D room layouts with different numbers of corners estimated using our method. Please refer to the supplementary materials for more results in the following experiments.</p><p>Evaluation on the Realtor360 dataset. To train both LayoutNet <ref type="bibr" target="#b32">[33]</ref> and our DuLa-Net on the Realtor360 dataset, we randomly selected 2169 panoramas for training and took the remaining 404 panoramas for testing. We further classify the testing panoramas according to their numbers of corners. We run LayoutNet using the codes and default hyper-parameter released by the authors. The quantitative comparison with LayoutNet is shown in <ref type="table">Table 2</ref>. We observe that LayoutNet delivers good performance on cuboid-shaped rooms (4 corners), similar to the numbers reported in their paper. However, the accuracy drops signif-icantly as the number of corners increases. In comparison, Our DuLa-Net not only outperforms LayoutNet on cuboidshaped rooms by a small margin (around 2%), but also performs well on rooms with larger numbers of corners. This leads to an overall performance gain of ∼ 14% in both 2D and 3D metrics when compared to LayoutNet.</p><p>Since the 3D layout optimization and the hyperparameter of LayoutNet were tuned on a dataset that contains mostly cuboid-shaped rooms, we conducted another experiment by training both networks on a revised training set that excludes rooms of non-cuboid layouts, while keeping the testing set untouched. <ref type="table">Table 3</ref> shows the quantitative results. Note that while the performance of LayoutNet improves, our method still outperforms on all kinds of rooms.</p><p>From the qualitative comparison shown in <ref type="figure">Fig. 6</ref>, we can observe a strong tendency of LayoutNet to predict the rooms to be cuboid-shaped, possibly due to the constraints imposed in their 3D layout optimization. In comparison, our method simplifies the problem by directly predicting a Manhattan-world floor plan without any assumptions about the numbers of corners. We conjecture that this is a main reason why our method outperforms LayoutNet, especially with rooms with more than four corners.</p><p>We also conducted an ablation study that evaluates the performance of our method in different configurations as follows: 1) ours(fc-only): only panorama-view branch, 2) ours(fp-only): only ceiling-view branch, and 3) ours(w/o fusion): our full model but without feature fusion. The quantitative results in <ref type="table">Table 2</ref> shows that jointly training both branches leads to better performance than training only one of them. In addition, adding feature fusion between the two branches further improves the performance.</p><p>Evaluation on the PanoContext and Stanford 2D/3D datasets. LayoutNet provided quantitative results on the PanoContext <ref type="bibr" target="#b29">[30]</ref> dataset with 414 panoramas for training <ref type="table">Table 4</ref>. Quantitative evaluation on the PanoContext <ref type="bibr" target="#b29">[30]</ref> and Stanford 2D/3D <ref type="bibr" target="#b1">[2]</ref> datasets in 3D IoU (%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>PanoContext Stanford 2D-3D</p><p>LayoutNet <ref type="bibr" target="#b32">[33]</ref> 74.48 76.33 Ours (full) 77.42 79.36</p><p>Timing. An end-to-end computation takes three main steps -1) an alignment process to align the input panorama with a global coordinate system, 2) floor plan probability map prediction by our neural network, and 3) 2D floor plan fitting. Step 1) is most time-consuming, which takes about 13.37s measured on a machine with a single NVIDIA 1080ti GPU and Intel i7-7700 3.6GHZ CPU.</p><p>Step 2) takes only 34.68ms and step 3) takes only 21.71ms. Compared to LayoutNet, they carry out the same alignment process and their neural network prediction is also very fast (39ms). However, they needed another very timeconsuming 3D layout optimization step in the end, which takes 30.5s. In summary, an end-to-end computation by LayoutNet takes about 43.9s while our method takes about 13.4s, a speed up of 3.28X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>We present an end-to-end deep learning framework, called DuLa-Net, for estimating 3D room layouts from a <ref type="figure">Figure 6</ref>. Qualitative comparison with LayoutNet <ref type="bibr" target="#b32">[33]</ref>. The 3D room layouts generated by LayoutNet <ref type="bibr" target="#b32">[33]</ref> (green lines) and our method (orange lines). Results are displayed on both the equirectangular panorama-view (left) and floor plan view (right), where the blue lines and yellow solid shapes represent the ground truth, respectively. single RGB panorama. We propose a new network architecture that consists of two encoder-decoder branches for analyzing features from two distinct views of the input panoramas, namely the equirectangular panorama-view and the perspective ceiling-view. The two branches are connected through a novel feature fusion scheme and jointly trained to achieve the best accuracy in the prediction of 2D floor plan and layout height. To learn from complex layouts, we introduce a new dataset, Realtor360, which contains 2573 indoor panoramas of Manhattan-world room layouts with various complexity. Both the quantitative and qualitative results demonstrate that our method outperforms the cur- rent state-of-the-art in prediction accuracy, especially with rooms with more than four corners, and take much less time to compute the final 3D room layouts.</p><p>Limitations and future work. Our method has the following limitations: i) without knowing the object semantics, our network might get confused with the rooms that contains mirrors or large occluding objects as shown in <ref type="figure" target="#fig_4">Fig. 7; and</ref> ii) our approach of 3D layout estimation involves heuristics and assumptions that might over-or underestimate the underlying floor plan probability map and also restrain the results to Manhattan world. We propose to explore the following directions in the near future. First, introducing the object semantics, i.e., segmentation and labels, to the network architecture could potentially improve the accuracy by ignoring those distracting and occluding objects from the floor plan prediction. Second, designing a principled algorithm for a more robust 3D layout estimation, e.g., no Manhattan-world assumption and support rooms with curve shapes. Last but not the least, we believe that even better results can be achieved by experimenting with a larger range of encoders for our network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparisons to LayoutNet [33]</head><p>A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-training LayoutNet</head><p>For the comparisons with LayoutNet, we used the neural network and the 3D optimization module provided by the authors of LayoutNet. According to our analysis, the part of the code that concerns the 3D optimization module is specially designed for cuboid-shape rooms and works very well. However, the part designed for non-cuboid shaped rooms has multiple issues and is not sufficiently robust. Our analysis indicates that the algorithm has the best performance when it only tries to fit rooms with 4 corners. To provide the best possible version of LayoutNet, we experimented with three different settings in network training and report the results here. 1) LayoutNet with the original weights pre-trained by the authors. 2) LayoutNet retrained with the subset of our Realtor360 dataset, which contains only cuboid layouts (4 corners). 3) LayoutNet retrained with our complete Realtor360 training dataset. The performance comparison is shown in <ref type="table" target="#tab_2">Table 5</ref>. We can observe that LayoutNet's performance improved significantly when it is re-trained on our Realtor360 dataset. It is further improved when it is re-trained with cuboid-shaped rooms only since the 3D optimization module works correctly only for such rooms. In the paper, we therefore reported results for the best version of LayoutNet that we could create, i.e. Lay-outNet re-trained on our Realtor360 dataset with rooms of cuboid layouts (4-corners) only, followed by their 3D opti-mization module for cuboid-shaped rooms. A.2. More Qualitative Comparisons <ref type="figure" target="#fig_5">Fig. 8</ref> and <ref type="figure" target="#fig_6">Fig. 9</ref> show more results of qualitative comparisons with LayoutNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Visual Results of DuLa-Net</head><p>As the complexity of the estimated room layout is best viewed in 3D, we show more 3D room layouts with different numbers of corners generated by our method in <ref type="figure" target="#fig_0">Fig. 10</ref> and <ref type="figure" target="#fig_0">Fig. 11</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>3D room layouts with different complexity are estimated from a single RGB panorama using our system. (Left to right) Room layout with a floor plan of 6 corners, 8 corners, and 10 corners. The checkerboard patterns on the walls indicate the missing textures due to occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>2D floor plan fitting. (a) The probability maps that our network outputs are fused to a floor plan probability map M f use F P . (b) We apply image thresholding to M f use F P and fit a polygon shape to the floor plan region. (c) The polygon edges are regressed and clustered into two sets of horizontal lines (red) and vertical lines (green). (d) The final floor plane shape is defined by grids in (c) where the ratio of floor plan area is greater than 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 *Fig. 3 (</head><label>53</label><figDesc>M F P + 0.25 * M C F C + 0.25 * M F F C . (6) a) illustrates the above process. The probability map M f use F P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Few example panoramas in Realtor360. The annotated 3D room layouts are drawn as blue wireframes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Limitations. Two failure cases generated by our method (orange lines) due to the lack of object semantics. (Top) Our method is misled by the reflection of mirror. (Bottom) The boundary of floor plan is occluded by the refrigerator. The ground truth layout is rendered in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative comparison with LayoutNet [33]. The esitmated 3D room layouts in equirectangular panorama-view. (Blue lines) Ground truth. (Green lines) LayoutNet's result. (Orange lines) Our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Qualitative comparison with LayoutNet [33]. The esitmated 3D room layouts in equirectangular panorama-view. (Blue lines) Ground truth. (Green lines) LayoutNet's result. (Orange lines) Our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Visual results. The 3D room layouts estimated by DuLa-Net are displayed in equirectangular panorama-view (top) and 3D rendering with texture mapping (bottom). Note that the checkerboard patterns on the walls indicate the missing textures due to occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 .</head><label>11</label><figDesc>Visual results. The 3D room layouts estimated by DuLa-Net are displayed in equirectangular panorama-view (top) and 3D rendering with texture mapping (bottom). Note that the checkerboard patterns on the walls indicate the missing textures due to occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the Realtor360 dataset.</figDesc><table><row><cell cols="5">4 corners 6 corners 8 corners 10+ corners Total</cell></row><row><cell>1246</cell><cell>950</cell><cell>316</cell><cell>61</cell><cell>2573</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Quantitative evaluation on the Realtor360 dataset. We compare our method with the LayoutNet<ref type="bibr" target="#b32">[33]</ref>, and conduct an ablation study using different configurations of our method. Bold numbers indicate the best performance. Quantitative evaluation on the subset of Realtor360 dataset. We compare with LayoutNet<ref type="bibr" target="#b32">[33]</ref> using a training set that contains only rooms with cuboid layout (4 corners). Bold numbers indicate the best performance.</figDesc><table><row><cell></cell><cell cols="2">Average</cell><cell cols="2">4 corners</cell><cell cols="2">6 corners</cell><cell cols="2">8 corners</cell><cell cols="2">10+ corners</cell></row><row><cell>Method</cell><cell>2D IoU (%)</cell><cell>3D IoU (%)</cell><cell>2D IoU (%)</cell><cell>3D IoU (%)</cell><cell>2D IoU (%)</cell><cell>3D IoU (%)</cell><cell>2D IoU (%)</cell><cell>3D IoU (%)</cell><cell>2D IoU (%)</cell><cell>3D IoU (%)</cell></row><row><cell>LayoutNet [33]</cell><cell>65.84</cell><cell>62.77</cell><cell>80.41</cell><cell>76.6</cell><cell>60.5</cell><cell>57.87</cell><cell>41.16</cell><cell>38.61</cell><cell>22.35</cell><cell>21.52</cell></row><row><cell>ours (fc-only)</cell><cell>75.2</cell><cell>72.02</cell><cell>76.75</cell><cell>73.27</cell><cell>76.04</cell><cell>73.06</cell><cell>70.8</cell><cell>67.89</cell><cell>56.42</cell><cell>54.2</cell></row><row><cell>ours (fp-only)</cell><cell>75.75</cell><cell>72.18</cell><cell>79.66</cell><cell>75.54</cell><cell>75.42</cell><cell>72.23</cell><cell>70.51</cell><cell>67.39</cell><cell>51.03</cell><cell>48.57</cell></row><row><cell>ours (w/o fusion)</cell><cell>78.52</cell><cell>74.8</cell><cell>81.77</cell><cell>77.57</cell><cell>78.5</cell><cell>75.1</cell><cell>73.61</cell><cell>70.37</cell><cell>57.01</cell><cell>54.12</cell></row><row><cell>ours (full)</cell><cell>80.53</cell><cell>77.2</cell><cell>82.63</cell><cell>78.91</cell><cell>80.72</cell><cell>77.79</cell><cell>78.12</cell><cell>74.86</cell><cell>63.1</cell><cell>59.72</cell></row><row><cell></cell><cell cols="2">Average</cell><cell cols="2">4 corners</cell><cell cols="2">6 corners</cell><cell cols="2">8 corners</cell><cell cols="2">10+ corners</cell></row><row><cell>Method</cell><cell cols="2">2D IoU (%) 3D IoU (%)</cell><cell cols="2">2D IoU (%) 3D IoU (%)</cell><cell cols="2">2D IoU (%) 3D IoU (%)</cell><cell cols="2">2D IoU (%) 3D IoU (%)</cell><cell cols="2">2D IoU (%) 3D IoU (%)</cell></row><row><cell>LayoutNet [33]</cell><cell>71.31</cell><cell>67.91</cell><cell>80.69</cell><cell>76.82</cell><cell>68.95</cell><cell>65.83</cell><cell>50.31</cell><cell>47.23</cell><cell>44.53</cell><cell>42.51</cell></row><row><cell>Ours (full)</cell><cell>77.87</cell><cell>74.16</cell><cell>82.42</cell><cell>78.3</cell><cell>77.19</cell><cell>73.74</cell><cell>70.81</cell><cell>67.55</cell><cell>54.05</cell><cell>50.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Quantitative evaluation of LayoutNet on different training sets.</figDesc><table><row><cell>Metric</cell><cell>Training set</cell><cell>Average</cell></row><row><cell></cell><cell>pre-trained</cell><cell>57.96</cell></row><row><cell>3D IoU (%)</cell><cell>Realtor360 (4-only)</cell><cell>67.91</cell></row><row><cell></cell><cell>Realtor360 (all)</cell><cell>62.77</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">We also evaluate our model on the Stanford 2D-3D<ref type="bibr" target="#b1">[2]</ref> dataset with annotations labeled by LayoutNet<ref type="bibr" target="#b32">[33]</ref>. The dataset includes 404 panoramas for training and 113 panoramas for testing. The last column inTable 4shows the quantitative result on the Stanford 2D-3D<ref type="bibr" target="#b1">[2]</ref> dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 5</ref><p>. Visual results. Given a single RGB panorama, our method automatically estimates the corresponding 3D room layout. Our method is flexible to handle more complex room layout beyond the simple cuboid room. The checkerboard patterns on the walls indicate the missing textures due to occlusion. and 53 panoramas for testing. All rooms are labeled as cuboid-shape. To compare, we trained our network on the same dataset. The quantitative comparison is shown in Table 4. Our model outperforms LayoutNet by a small margin.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1707.02937</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Manhattan world: Compass direction from a single image by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">941</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delay: Robust spatial layout estimation for cluttered indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Room reconstruction from a single spherical image by higher-order energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Fukano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Mochizuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1768" to="1773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1849" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>Vasileios Belagiannis, Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Roomnet: End-to-end room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1703.06241</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Layered scene decomposition via the occlusion-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Floornet: A unified framework for floorplan reconstruction from 3d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rapter: Rebuilding man-made scenes with regular arrangements of planes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Mellado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<idno>103:1-103:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Omnidirectional image capture on mobile devices for fast automatic generation of 2.5d indoor maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pintore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ganovelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lifting 3d manhattan lines from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Manhattan junction catalogue for spatial reasoning of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing scene viewpoint using panoramic place representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pano2cad: Room layout from a single panorama image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient 3d room shape recovery from a single panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="5422" to="5430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic 3d indoor scene modeling from single panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyang</forename><surname>Shi Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimating the 3d layout of indoor scenes and its clutter from depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="1273" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepcontext: Context-encoding neural pathways for 3d holistic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingru</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Panocontext: A whole-room 3d context model for panoramic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Physics inspired optimization on semantic transfer features: An alternative method for room layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Omnidepth: Dense depth estimation for indoors spherical panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Zioulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Antonis Karakottas, Dimitrios Zarpalas, and Petros Daras</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>The European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Layoutnet: Reconstructing the 3d room layout from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
