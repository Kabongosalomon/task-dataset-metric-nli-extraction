<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Early-Learning Regularization Prevents Memorization of Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
							<email>shengliu@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science, and Courant Inst. of Mathematical Sciences</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
							<email>narges.razavian@nyulangone.org</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Population Health, and Department of Radiology NYU School of Medicine</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Data Science, and Courant Inst. of Mathematical Sciences</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Early-Learning Regularization Prevents Memorization of Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel framework to perform classification via deep learning in the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to first fit the training data with clean labels during an "early learning" phase, before eventually memorizing the examples with false labels. We prove that early learning and memorization are fundamental phenomena in high-dimensional classification tasks, even in simple linear models, and give a theoretical explanation in this setting. Motivated by these findings, we develop a new technique for noisy classification tasks, which exploits the progress of the early learning phase. In contrast with existing approaches, which use the model output during early learning to detect the examples with clean labels, and either ignore or attempt to correct the false labels, we take a different route and instead capitalize on early learning via regularization. There are two key elements to our approach. First, we leverage semi-supervised learning techniques to produce target probabilities based on the model outputs. Second, we design a regularization term that steers the model towards these targets, implicitly preventing memorization of the false labels. The resulting framework is shown to provide robustness to noisy annotations on several standard benchmarks and real-world datasets, where it achieves results comparable to the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure" target="#fig_0">Figure 1</ref><p>: Results of training a ResNet-34 <ref type="bibr" target="#b14">[15]</ref> neural network with a traditional cross entropy loss (top row) and our proposed method (bottom row) to perform classification on the CIFAR-10 dataset where 40% of the labels are flipped at random. The left column shows the fraction of examples with clean labels that are predicted correctly (green) and incorrectly (blue). The right column shows the fraction of examples with wrong labels that are predicted correctly (green), memorized (the prediction equals the wrong label, shown in red), and incorrectly predicted as neither the true nor the labeled class (blue). The model trained with cross entropy begins by learning to predict the true labels, even for many of the examples with wrong label, but eventually memorizes the wrong labels. Our proposed method based on early-learning regularization prevents memorization, allowing the model to continue learning on the examples with clean labels to attain high accuracy on examples with both clean and wrong labels. false labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b53">54]</ref>. In this work we study this phenomenon and introduce a novel framework that exploits it to achieve robustness to noisy labels. Our main contributions are the following:</p><p>• In Section 3 we establish that early learning and memorization are fundamental phenomena in high dimensions, proving that they occur even for simple linear generative models. • In Section 4 we propose a technique that utilizes the early-learning phenomenon to counteract the influence of the noisy labels on the gradient of the cross entropy loss. This is achieved through a regularization term that incorporates target probabilities estimated from the model outputs using several semi-supervised learning techniques. • In Section 6 we show that the proposed methodology achieves results comparable to the state of the art on several standard benchmarks and real-world datasets. We also perform a systematic ablation study to evaluate the different alternatives to compute the target probabilities, and the effect of incorporating mixup data augmentation <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section we describe existing techniques to train deep-learning classification models using data with noisy annotations. We focus our discussion on methods that do not assume the availability of small subsets of training data with clean labels (as opposed, for example, to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>). We also assume that the correct classes are known (as opposed to <ref type="bibr" target="#b43">[44]</ref>).</p><p>Robust-loss methods propose cost functions specifically designed to be robust in the presence of noisy labels. These include Mean Absolute Error (MAE) <ref type="bibr" target="#b9">[10]</ref>, Improved MAE <ref type="bibr" target="#b42">[43]</ref>, which is a reweighted MAE, Generalized Cross Entropy <ref type="bibr" target="#b55">[56]</ref>, which can be interpreted as a generalization of MAE, Symmetric Cross Entropy <ref type="bibr" target="#b44">[45]</ref>, which adds a reverse cross-entropy term to the usual crossentropy loss, and L DIM <ref type="bibr" target="#b47">[48]</ref>, which is based on information-theoretic considerations. Loss-correction methods explicitly correct the loss function to take into account the noise distribution, represented by a transition matrix of mislabeling probabilities <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Robust-loss and loss-correction techniques do not exploit the early-learning phenomenon mentioned in the introduction. This phenomenon was described in <ref type="bibr" target="#b2">[3]</ref> (see also <ref type="bibr" target="#b53">[54]</ref>), and analyzed theoretically in <ref type="bibr" target="#b22">[23]</ref>. Our theoretical approach differs from theirs in two respects. First, Ref. <ref type="bibr" target="#b22">[23]</ref> focus on a least squares regression task, whereas we focus on the noisy label problem in classification. Second, and more importantly, we prove that early learning and memorization occur even in a linear model.</p><p>Early learning can be exploited through sample selection, where the model output during the earlylearning stage is used to predict which examples are mislabeled and which have been labeled correctly. The prediction is based on the observation that mislabeled examples tend to have higher loss values. Co-teaching <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b51">52]</ref> performs sample selection by using two networks, each trained on a subset of examples that have a small training loss for the other network (see <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> for related approaches). A limitation of this approach is that the examples that are selected tend to be easier, in the sense that the model output during early learning approaches the true label. As a result, the gradient of the cross-entropy with respect to these examples is small, which slows down learning <ref type="bibr" target="#b5">[6]</ref>. In addition, the subset of selected examples may not be rich enough to generalize effectively to held-out data <ref type="bibr" target="#b34">[35]</ref>.</p><p>An alternative to sample selection is label correction. During the early-learning stage the model predictions are accurate on a subset of the mislabeled examples (see the top row of <ref type="figure" target="#fig_0">Figure 1</ref>). This suggests correcting the corresponding labels. This can be achieved by computing new labels equal to the probabilities estimated by the model (known as soft labels) or to one-hot vectors representing the model predictions (hard labels) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b50">51]</ref>. Another option is to set the new labels to equal a convex combination of the noisy labels and the soft or hard labels <ref type="bibr" target="#b32">[33]</ref>. Label correction is usually combined with some form of iterative sample selection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22]</ref> or with additional regularization terms <ref type="bibr" target="#b37">[38]</ref>. SELFIE <ref type="bibr" target="#b34">[35]</ref> uses label replacement to correct a subset of the labels selected by considering past model outputs. Ref. <ref type="bibr" target="#b26">[27]</ref> computes a different convex combination with hard labels for each example based on a measure of model dimensionality. Ref. <ref type="bibr" target="#b1">[2]</ref> fits a two-component mixture model to carry out sample selection, and then corrects labels via convex combination as in <ref type="bibr" target="#b32">[33]</ref>. They also apply mixup data augmentation <ref type="bibr" target="#b54">[55]</ref> to enhance performance. In a similar spirit, DivideMix <ref type="bibr" target="#b21">[22]</ref> uses two networks to perform sample selection via a two-component mixture model, and applies the semi-supervised learning technique MixMatch <ref type="bibr" target="#b3">[4]</ref>.</p><p>Our proposed approach is somewhat related in spirit to label correction. We compute a probability estimate that is analogous to the soft labels mentioned above, and then exploit it to avoid memorization. However it is also fundamentally different: instead of modifying the labels, we propose a novel regularization term explicitly designed to correct the gradient of the cross-entropy cost function. This yields strong empirical performance, without needing to incorporate sample selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Early learning as a general phenomenon of high-dimensional classification</head><p>As the top row of <ref type="figure" target="#fig_0">Figure 1</ref> makes clear, deep neural networks trained with noisy labels make progress during the early learning stage before memorization occurs. In this section, we show that far from being a peculiar feature of deep neural networks, this phenomenon is intrinsic to high-dimensional classification tasks, even in the simplest setting. Our theoretical analysis is also the inspiration for the early-learning regularization procedure we propose in Section 4.</p><p>We exhibit a simple linear model with noisy labels which evinces the same behavior as described above: the early learning stage, when the classifier learns to correctly predict the true labels, even on noisy examples, and the memorization stage, when the classifier begins to make incorrect predictions because it memorizes the wrong labels. This is illustrated in <ref type="figure">Figure A</ref>.1, which demonstrates that empirically the linear model has the same qualitative behavior as the deep-learning model in <ref type="figure" target="#fig_0">Figure 1</ref>. We show that this behavior arises because, early in training, the gradients corresponding to the correctly labeled examples dominate the dynamics-leading to early progress towards the true optimum-but that the gradients corresponding to wrong labels soon become dominant-at which point the classifier simply learns to fit the noisy labels.</p><p>We consider data drawn from a mixture of two Gaussians in R p . The (clean) dataset consists of n i.i.d. copies of (x, y * ). The label y * ∈ {0, 1} 2 is a one-hot vector representing the cluster assignment, and</p><formula xml:id="formula_0">x ∼ N (+v, σ 2 I p×p ) if y * = (1, 0) x ∼ N (−v, σ 2 I p×p ) if y * = (0, 1) ,</formula><p>where v is an arbitrary unit vector in R p and σ 2 is a small constant. The optimal separator between the two classes is a hyperplane through the origin perpendicular to v. We focus on the setting where σ 2 is fixed while n, p → ∞. In this regime, the classification task is nontrivial, since the clusters are, approximately, two spheres whose centers are separated by 2 units with radii σ √ p 2.</p><p>We only observe a dataset with noisy labels (y <ref type="bibr" target="#b0">[1]</ref> , . . . , y [n] ),</p><formula xml:id="formula_1">y [i] = (y * ) [i] with probability 1 − ∆ y [i] with probability ∆,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">{ỹ [i] } n i=1 are i.i.d</formula><p>. random one-hot vectors which take values (1, 0) and (0, 1) with equal probability.</p><p>We train a linear classifier by gradient descent on the cross entropy:</p><formula xml:id="formula_3">min Θ∈R 2×p L CE (Θ) := − 1 n n i=1 2 c=1 y [i] c log(S(Θx [i] ) c ) ,</formula><p>where S : R 2 → [0, 1] 2 is a softmax function. In order to separate the true classes well (and not overfit to the noisy labels), the rows of Θ should be correlated with the vector v.</p><p>The gradient of this loss with respect to the model parameters Θ corresponding to class c reads</p><formula xml:id="formula_4">∇L CE (Θ) c = 1 n n i=1 x [i] S(Θx [i] ) c − y [i] c ,<label>(2)</label></formula><p>Each term in the gradient therefore corresponds to a weighted sum of the examples x <ref type="bibr">[i]</ref> , where the weighting depends on the agreement between S(Θx [i] ) c and y</p><formula xml:id="formula_5">[i]</formula><p>c . Our main theoretical result shows that this linear model possesses the properties described above. During the early-learning stage, the algorithm makes progress and the accuracy on wrongly labeled examples increases. However, during this initial stage, the relative importance of the wrongly labeled examples continues to grow; once the effect of the wrongly labeled examples begins to dominate, memorization occurs. Theorem 1 (Informal). Denote by {Θ t } the iterates of gradient descent with step size η. For any ∆ ∈ (0, 1), there exists a constant σ ∆ such that, if σ ≤ σ ∆ and p/n ∈ (1 − ∆/2, 1), then with probability 1 − o(1) as n, p → ∞ there exists a T = Ω(1/η) such that:</p><p>• Early learning succeeds: For t &lt; T , −∇L(Θ t ) is well correlated with the correct separator v, and at t = T the classifier has higher accuracy on the wrongly labeled examples than at initialization.</p><p>• Gradients from correct examples vanish: Between t = 0 and t = T , the magnitudes of the coefficients</p><formula xml:id="formula_6">S(Θ t x [i] ) c − y [i] c</formula><p>corresponding to examples with clean labels decreases while the magnitudes of the coefficients for examples with wrong labels increases.</p><p>• Memorization occurs: As t → ∞, the classifier Θ t memorizes all noisy labels.</p><p>Due to space constraints, we defer the formal statement of Theorem 1 and its proof to the supplementary material.</p><p>The proof of Theorem 1 is based on two observations. First, while Θ is still not well correlated with v, the coefficients S(</p><formula xml:id="formula_7">Θx [i] ) c − y [i]</formula><p>c are similar for all i, so that ∇L CE points approximately in the average direction of the examples. Since the majority of data points are correctly labeled, this means the gradient is still well correlated with the correct direction during the early learning stage. Second, once Θ becomes correlated with v, the gradient begins to point in directions orthogonal to the correct direction v; when the dimension is sufficiently large, there are enough of these orthogonal directions to allow the classifier to completely memorize the noisy labels.</p><p>This analysis suggests that in order to learn on the correct labels and avoid memorization it is necessary to (1) ensure that the contribution to the gradient from examples with clean labels remains large, and (2) neutralize the influence of the examples with wrong labels on the gradient. In Section 4 we propose a method designed to achieve this via regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gradient analysis of softmax classification from noisy labels</head><p>In this section we explain the connection between the linear model from Section 3 and deep neural networks. Recall the gradient of the cross-entropy loss with respect to Θ given in <ref type="bibr" target="#b1">(2)</ref>. Performing gradient descent modifies the parameters iteratively to push S(Θx <ref type="bibr">[i]</ref> ) closer to y <ref type="bibr">[i]</ref> . If c is the true class so that y [i] c = 1, the contribution of the ith example to ∇L CE (Θ) c is aligned with −x <ref type="bibr">[i]</ref> , and gradient descent moves in the direction of x <ref type="bibr">[i]</ref> . However, if the label is noisy and y</p><p>[i] c = 0, then gradient descent moves in the opposite direction, which eventually leads to memorization as established by Theorem 1.</p><p>We now show that for nonlinear models based on neural networks, the effect of label noise is analogous. We consider a classification problem with C classes, where the training set consists of n examples</p><formula xml:id="formula_8">{x [i] , y [i] } n i=1 , x [i] ∈ R d is the ith input and y [i] ∈ {0</formula><p>, 1} C is a one-hot label vector indicating the corresponding class. The classification model maps each input x [i] to a C-dimensional encoding using a deep neural network N x [i] (Θ) and then feeds the encoding into a softmax function S to produce an estimate p [i] of the conditional probability of each class given x <ref type="bibr">[i]</ref> ,</p><formula xml:id="formula_9">p [i] := S (N x [i] (Θ)) .<label>(3)</label></formula><p>Θ denotes the parameters of the neural network. The gradient of the cross-entropy loss,</p><formula xml:id="formula_10">L CE (Θ) := − 1 n n i=1 C c=1 y [i] c log p [i] c ,<label>(4)</label></formula><p>with respect to Θ equals</p><formula xml:id="formula_11">∇L CE (Θ) = 1 n n i=1 ∇N x [i] (Θ) p [i] − y [i] ,<label>(5)</label></formula><p>where ∇N x [i] (Θ) is the Jacobian matrix of the neural-network encoding for the ith input with respect to Θ. Here we see that label noise has the same effect as in the simple linear model. If c is the true class, but y</p><p>[i] c = 0 due to the noise, then the contribution of the ith example to ∇L CE (Θ) c is reversed. The entry corresponding to the impostor class c , is also reversed because y  <ref type="figure">Figure B</ref>.1). In Section 4.2 we describe how to counteract this influence by exploiting the early-learning phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Early-learning regularization</head><p>In this section we present a novel framework for learning from noisy labels called early-learning regularization (ELR). We assume that we have available a target 1 vector of probabilities t [i] for each example i, which is computed using past outputs of the model. Section 4.3 describes several techniques to compute the targets. Here we explain how to use them to avoid memorization.</p><p>Due to the early-learning phenomenon, we assume that at the beginning of the optimization process the targets do not overfit the noisy labels. ELR exploits this using a regularization term that seeks to</p><formula xml:id="formula_12">p [i] c * − y [i] c * + λg [i] c *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clean labels</head><p>Wrong labels <ref type="figure">Figure 2</ref>: Illustration of the effect of the regularization on the gradient of the ELR loss (see <ref type="bibr">Lemma 2)</ref> for the same deep-learning model as in <ref type="figure" target="#fig_0">Figure 1</ref>. On the left, we plot the entry of p ). However, the regularization term compensates for this, forcing the model to continue learning mainly on the examples with clean labels. On the right, we show the CE and the regularization term (dark and light red respectively) separately for the examples with wrong labels. The regularization cancels out the CE term, preventing memorization. In all plots the curves represent the mean value, and the shaded regions are within one standard deviation of the mean.</p><p>maximize the inner product between the model output and the targets,</p><formula xml:id="formula_13">L ELR (Θ) := L CE (Θ) + λ n n i=1 log 1 − p [i] , t [i] .<label>(6)</label></formula><p>The logarithm in the regularization term counteracts the exponential function implicit in the softmax function in p <ref type="bibr">[i]</ref> . A possible alternative to this approach would be to penalize the Kullback-Leibler divergence between the model outputs and the targets. However, this does not exploit the earlylearning phenomenon effectively, because it leads to overfitting the targets as demonstrated in Section C.</p><p>The key to understanding why ELR is effective lies in its gradient, derived in the following lemma, which is proved in Section E. Lemma 2 (Gradient of the ELR loss). The gradient of the loss defined in Eq. (6) is equal to</p><formula xml:id="formula_14">∇L ELR (Θ) = 1 n n i=1 ∇N x [i] (Θ) p [i] − y [i] + λg [i]<label>(7)</label></formula><p>where the entries of g [i] ∈ R C are given by</p><formula xml:id="formula_15">g [i] c := p [i] c 1 − p [i] , t [i] C k=1 (t [i] k − t [i] c )p [i] k , 1 ≤ c ≤ C.<label>(8)</label></formula><p>In words, the sign of g</p><formula xml:id="formula_16">[i]</formula><p>c is determined by a weighted combination of the difference between t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[i]</head><p>c and the rest of the entries in the target.</p><p>If c * is the true class, then the c * th entry of t [i] tends to be dominant during early-learning. In that case, the c * th entry of g <ref type="bibr">[i]</ref> is negative. This is useful both for examples with clean labels and for those with wrong labels. For examples with clean labels, the cross-entropy term p [i] − y [i] tends to vanish after the early-learning stage because p [i] is very close to y <ref type="bibr">[i]</ref> , allowing examples with wrong labels to dominate the gradient. Adding g <ref type="bibr">[i]</ref> counteracts this effect by ensuring that the magnitudes of the coefficients on examples with clean labels remains large. The center image of <ref type="figure">Figure 2</ref> shows this effect. For examples with wrong labels, the cross entropy term p</p><formula xml:id="formula_17">[i] c * − y [i] c * is positive because y [i] c * = 0. Adding the negative term g [i]</formula><p>c * therefore dampens the coefficients on these mislabeled examples, thereby diminishing their effect on the gradient (see right image in <ref type="figure">Figure 2</ref>). Thus, ELR fulfils the two desired properties outlined at the end of Section 3: boosting the gradient of examples with clean labels, and neutralizing the gradient of the examples with false labels. Results with cosine annealing learning rate. <ref type="table">Table 1</ref>: Comparison with state-of-the-art methods on CIFAR-10 and CIFAR-100 with symmetric and asymmetric label noise. The bootstrap and SL methods were reimplemented using publicly available code, the rest of results are taken from <ref type="bibr" target="#b55">[56]</ref>. The mean accuracy and its standard deviation are computed over five noise realizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Target estimation</head><p>ELR requires a target probability for each example in the training set. The target can be set equal to the model output, but using a running average is more effective. In semi-supervised learning, this technique is known as temporal ensembling <ref type="bibr" target="#b19">[20]</ref>. Let t [i] (k) and p [i] (k) denote the target and model output respectively for example i at iteration k of training. We set</p><formula xml:id="formula_18">t [i] (k) := βt [i] (k − 1) + (1 − β)p [i] (k),<label>(9)</label></formula><p>where 0 ≤ β &lt; 1 is the momentum. The basic version of our proposed method alternates between computing the targets and minimizing the cost function <ref type="formula" target="#formula_13">(6)</ref> via stochastic gradient descent.</p><p>Target estimation can be further improved in two ways. First, by using the output of a model obtained through a running average of the model weights during training. In semi-supervised learning, this weight averaging approach has been proposed to mitigate confirmation bias <ref type="bibr" target="#b39">[40]</ref>. Second, by using two separate neural networks, where the target of each network is computed from the output of the other network. The approach is inspired by Co-teaching and related methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b21">22]</ref>. The ablation results in Section 6 show that weight averaging, two networks, and mixup data augmentation <ref type="bibr" target="#b54">[55]</ref> all separately improve performance. We call the combination of all these elements ELR+. A detailed description of ELR and ELR+ is provided in Section F of the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the proposed methodology on two standard benchmarks with simulated label noise, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b17">[18]</ref>, and two real-world datasets, Clothing1M <ref type="bibr" target="#b46">[47]</ref> and WebVision <ref type="bibr" target="#b23">[24]</ref>. For CIFAR-10 and CIFAR-100 we simulate label noise by randomly flipping a certain fraction of the labels in the training set following a symmetric uniform distribution (as in Eq. <ref type="formula" target="#formula_1">(1)</ref>), as well as a more realistic asymmetric class-dependent distribution, following the scheme proposed in <ref type="bibr" target="#b30">[31]</ref>. Clothing1M consists of 1 million training images collected from online shopping websites with labels generated using surrounding text. Its noise level is estimated at 38.5% <ref type="bibr" target="#b35">[36]</ref>. For ease of comparison to previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7]</ref>, we consider the mini WebVision dataset which contains the top 50 classes from the Google image subset of WebVision, which results in approximately 66 thousand images. The noise level of WebVision is estimated at 20% <ref type="bibr" target="#b23">[24]</ref>. In our experiments, we prioritize making our results comparable to the existing literature. When possible we use the same preprocessing, and architectures as previous methods. The details are described in Section G of the supplementary material. We focus on two variants of the proposed approach: ELR with temporal ensembling, which we call ELR, and ELR with temporal ensembling, weight averaging, two networks, and mixup data augmentation, which we call ELR+ (see Section F). The choice of hyperparameters is performed on separate validation sets. Section H shows that the sensitivity to different hyperparameters is quite low. Finally, we also perform an ablation study on CIFAR-10 for two levels of symmetric noise (40% and 80%) in order to evaluate the contribution of  <ref type="table">Table 2</ref>: Comparison with state-of-the-art methods on CIFAR-10 and CIFAR-100 with symmetric and asymmetric noise. For ELR+, we use 10% of the training set for validation, and treat the validation set as a held-out test set. The result for DivideMix on CIFAR-100 with 40% asymmetric noise was obtained using publicly available code. The rest of the results are taken from <ref type="bibr" target="#b21">[22]</ref>, which reports the highest accuracy observed on the validation set during training. We also report the performance of ELR+ under this metric on the rightmost column (ELR+ * ).</p><p>the different elements in ELR+. Code to reproduce the experiments is publicly available online at https://github.com/shengliu66/ELR. <ref type="table">Table 1</ref> evaluates the performance of ELR on CIFAR-10 and CIFAR-100 with different levels of symmetric and asymmetric label noise. We compare to the best performing methods that only modify the training loss. All techniques use the same architecture (ResNet34), batch size, and training procedure. ELR consistently outperforms the rest by a significant margin. To illustrate the influence of the training procedure, we include results with a different learning-rate scheduler (cosine annealing <ref type="bibr" target="#b25">[26]</ref>), which further improves the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In <ref type="table">Table 2</ref>, we compare ELR+ to state-of-the-art methods, which also apply sample selection and data augmentation, on CIFAR-10 and CIFAR-100. All methods use the same architecture (PreAct ResNet-18     <ref type="bibr" target="#b21">[22]</ref>. All methods use an InceptionResNetV2 architecture.  <ref type="table">Table 5</ref>: Ablation study evaluating the influence of weight averaging, the use of two networks, and mixup data augmentation for the CIFAR-10 dataset with medium (40%) and high (80%) levels of symmetric noise. The mean accuracy and its standard deviation are computed over five noise realizations. <ref type="table">Table 5</ref> shows the results of an ablation study evaluating the influence of the different elements of ELR+ for the CIFAR-10 dataset with medium (40%) and high (80%) levels of symmetric noise. Each element seems to provide an independent performance boost. At the medium noise level the improvement is modest, but at the high noise level it is very significant. This is in line with recent works showing the effectiveness of semi-supervised learning techniques in such settings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Future Work</head><p>In this work we provide a theoretical characterization of the early-learning and memorization phenomena for a linear generative model, and build upon the resulting insights to propose a novel framework for learning from data with noisy annotations. Our proposed methodology yields strong results on standard benchmarks and real-world datasets for several different network architectures. However, there remain multiple open problems for future research. On the theoretical front, it would be interesting to bridge the gap between linear and nonlinear models (see <ref type="bibr" target="#b22">[23]</ref> for some work in this direction), and also to investigate the dynamics of the proposed regularization scheme. On the methodological front, we hope that our work will trigger interest in the design of new forms of regularization that provide robustness to label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Broader Impact</head><p>This work has the potential to advance the development of machine-learning methods that can be deployed in contexts where it is costly to gather accurate annotations. This is an important issue in applications such as medicine, where machine learning has great potential societal impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Theoretical analysis of early learning and memorization in a linear model</head><p>In this section, we formalize and substantiate the claims of Theorem 1.</p><p>Theorem 1 has three parts, which we address in the following sections. First, in Section A.2, we show that the classifier makes progress during the early-learning phase: over the first T iterations, the gradient is well correlated with v and the accuracy on mislabeled examples increases. However, as noted in the main text, this early progress halts because the gradient terms corresponding to correctly labeled examples begin to disappear. We prove this rigorously in Section A.3, which shows that the overall magnitude of the gradient terms corresponding to correctly labeled examples shrinks over the first T iterations. Finally, in Section A.4, we prove the claimed asymptotic behavior: as t → ∞, gradient descent perfectly memorizes the noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Notation and setup</head><p>We consider a softmax regression model parameterized by two weight vectors Θ1 and Θ2, which are the rows of the parameter matrix Θ ∈ R 2×p . In the linear case this is equivalent to a logistic regression model, because the cross-entropy loss on two classes depends only on the vector Θ1 − Θ2. If we reparametrize the labels as</p><formula xml:id="formula_19">ε [i] = 1 if y [i] 1 = 1 −1 if y [i] 2 = 1 ,</formula><p>and set θ := Θ1 − Θ2, we can then write the loss as</p><formula xml:id="formula_20">LCE(θ) = 1 n n i=1 log(1 + e −ε [i] θ x [i] ) .</formula><p>We write ε * for the true cluster assignments:</p><formula xml:id="formula_21">(ε * ) [i] = 1 if x [i]</formula><p>comes from the cluster with mean +v, and (ε * ) [i] = −1 otherwise. Note that, with this convention, we can always write</p><formula xml:id="formula_22">x [i] = (ε * ) [i] (v − σz [i] ), where z [i]</formula><p>is a standard Gaussian random vector independent of all other random variables.</p><p>In terms of θ and ε, the gradient (2) reads</p><formula xml:id="formula_23">∇LCE(θ) = 1 2n n i=1 x [i] tanh(θ x [i] ) − ε [i] ,<label>(10)</label></formula><p>As noted in the main text, the coefficient tanh(θ x [i] ) − ε [i] is the key quantity governing the properties of the gradient.</p><p>Let us write C for the set of indices for which the labels are correct, and W for the set of indices for which labels are wrong.</p><p>We assume that θ0 is initialized randomly on the sphere with radius 2, and then optimized to minimize L via gradient descent with fixed step size η &lt; 1. We denote the iterates by θt.</p><p>We consider the asymptotic regime where σ 1 and ∆ are constants and p, n → ∞, with p/n ∈ (1 − ∆/2, 1). We will let σ∆ denote a constant, whose precise value may change from proposition to proposition; however, in all cases the requirements on σ will be independent of p and n. For convenience, we assume that ∆ ≤ 1/2, though it is straightforward to extend the analysis below to any ∆ bounded away from 1. Note that when ∆ = 1, each observed label is independent of the data, so no learning is possible. We will use the phrase "with high probability" to denote an event which happens with probability 1 − o(1) as n, p → ∞, and we use oP (1) to denote a random quantity which converges to 0 in probability. We use the symbol c to refer to an unspecified positive constant whose value may change from line to line. We use subscripts to indicate when this constant depends on other parameters of the problem.</p><p>We let T be the smallest positive integer such that θ T v ≥ 1/10. By Lemmas 7 and 8 in Section A.5, T = Ω(1/η) with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Early-learning succeeds</head><p>We first show that, for the first T iterations, the negative gradient −∇LCE(θt) has constant correlation with v. (Note that, by contrast, a random vector in R p typically has negligible correlation with v.) Proposition 3. There exists a constant σ∆, depending only on ∆, such that if σ ≤ σ∆ then with high probability, for all t &lt; T , we have θt − θ0 ≤ 1 and −∇LCE(θt) v/ ∇LCE(θt) ≥ 1/6 .</p><p>Proof. We will prove the claim by induction. We write</p><formula xml:id="formula_24">−∇LCE(θt) = 1 2n n i=1 ε [i] x [i] − 1 2n n i=1 x [i] tanh(θ t x [i] ) . Since Ev (ε [i] x [i] ) = (1 − ∆), the law of large numbers implies v 1 2n n i=1 ε [i] x [i] = 1 2 (1 − ∆) + oP (1) .</formula><p>Moreover, by Lemma 9, there exists a positive constant c such that with high probability v 1 2n</p><formula xml:id="formula_25">n i=1 x [i] tanh(θ t x [i] ) ≤ 1 2 1 n n i=1 (v x [i] ) 2 1/2 1 n n i=1 tanh(θ t x [i] ) 2 1/2 ≤ 1 2 | tanh(θ t v)| + cσ(1 + θt − θ0 ) .</formula><p>Thus, applying Lemma 8 yields that with high probability</p><formula xml:id="formula_26">− ∇LCE(θt) v/ ∇LCE(θt) ≥ 1 2 ((1 − ∆) − | tanh(θ t v)|) − cσ(1 + θt − θ0 ) .<label>(11)</label></formula><p>When t = 0, the first term is 1 2 (1 − ∆) + oP (1) by Lemma 7, and the second term is cσ. Since we have assumed that ∆ ≤ 1/2, as long as σ ≤ σ∆ &lt; c −1 2 3 − ∆ 2 we will have that −∇LCE(θ0) v/ ∇LCE(θ0) ≥ 1/6 with high probability, as desired.</p><p>We proceed with the induction. We will show that θt − θ0 ≤ 1 with high probability for t &lt; T , and use <ref type="bibr" target="#b10">(11)</ref> to show that this implies the desired bound on the correlation of the gradient. If we assume the claim holds up to time t, then the definition of gradient descent implies</p><formula xml:id="formula_27">θt − θ0 = η t−1 s=0 gs ,</formula><p>where gs satisfies g s v/ gs ≥ 1/6. Since the set of vectors satisfying this requirement forms a convex cone, we obtain that</p><formula xml:id="formula_28">(θt − θ0) v/ θt − θ0 ≥ 1/6</formula><p>From this observation, we obtain two facts about θt. First, since t &lt; T , the definition of T implies that θ t v &lt; .1.</p><p>Since |θ 0 v| = oP (1) by Lemma 7, we obtain that θt − θ0 ≤ 1 with high probability. Second, θ t v ≥ θ 0 v, and since |θ 0 v| = oP (1) we have in particular that θ t v &gt; −.1. Therefore, with high probability, we also have |θt v| &lt; .1</p><p>Examining <ref type="formula" target="#formula_1">(11)</ref>, we therefore see that the quantity on the right side is at least</p><formula xml:id="formula_29">1 2 ((1 − ∆) − .1) − 2cσ .</formula><p>Again since we have assumed that ∆ ≤ 1/2, as long as σ ≤ σ∆ &lt; (2c) −1 2 3 − ∆ 2 − .1 , we obtain by (11) that −∇LCE(θt) v/ ∇LCE(θt) ≥ 1/6.</p><p>Given θt, we denote byÂ </p><formula xml:id="formula_30">(θt) = 1 |W | i∈W 1{sign(θ t x [i] ) = (ε * ) [i] } the</formula><formula xml:id="formula_31">E[1{sign(θ 0 x [i] ) = (ε * ) [i] }|θ0] = P[σθ 0 z [i] &lt; θ 0 v|θ0] ≤ 1/2 + O(|θ 0 v|/σ) .</formula><p>By the law of large numbers, we have that, conditioned on θ0,</p><formula xml:id="formula_32">A(θ0) ≤ 1/2 + O(|θ 0 v|/σ) + oP (1) ,</formula><p>and applying Lemma 7 yieldsÂ(θ0) ≤ 1/2 + oP (1).</p><p>In the other direction, we employ a method based on <ref type="bibr" target="#b29">[30]</ref>. The proof of Proposition 3 establishes that θt −θ0 ≤ 1 for all t &lt; T − 1 with high probability. Since η &lt; 1 and θ0 = 2, Lemma 8 implies that as long as σ &lt; 1/2, θT ≤ 5 with high probability. Since θ T v ≥ .1 by assumption, we obtain that θ T v/ θT ≥ 1/50 with high probability.</p><p>Note that W is a random subset of <ref type="bibr">[n]</ref>. For now, let us condition on this random variable. If we write Φ for the Gaussian CDF, then by the same reasoning as above, for any fixed θ ∈ R p ,</p><formula xml:id="formula_33">E[1{sign(θ x [i] ) = (ε * ) [i] }] = P[σθ z [i] &lt; θ v] = Φ(σ −1 θ v/ θ ) Therefore, if θ v/ θ ≥ τ , then for any δ &gt; 0, we havê A(θ) ≥ Φ(σ −1 τ − δ) − 1 |W | i∈W Φ(σ −1 τ − δ) − 1{θ z [i] / θ &lt; σ −1 τ } (14) Set φ(x) :=    1 if x &lt; σ −1 τ − δ 1 δ (σ −1 τ − x) if x ∈ [σ −1 τ − δ, σ −1 τ ] 0 if x &gt; σ −1 τ . By construction, φ is 1 δ -Lipschitz and satisfies 1{x &lt; σ −1 τ − δ} ≤ φ(x) ≤ 1{x &lt; σ −1 τ }</formula><p>for all x ∈ R. In particular, we have</p><formula xml:id="formula_34">Φ(σ −1 τ − δ) − 1{θ z [i] / θ &lt; σ −1 τ } ≤ E[φ(θ z [i] / θ )] − φ(θ z [i] / θ ) .</formula><p>Denote the set of θ ∈ R p satisfying θ v/ θ ≥ τ by Cτ . Combining the last display with <ref type="bibr" target="#b13">(14)</ref> yields</p><formula xml:id="formula_35">E inf θ∈CτÂ (θ) ≥ Φ(σ −1 τ − δ) − E sup θ∈Cτ 1 |W | i∈W E[φ(θ z [i] / θ )] − φ(θ z [i] / θ ) .</formula><p>To control the last term, we employ symmetrization and contraction (see <ref type="bibr" target="#b20">[21,</ref><ref type="bibr">Chapter 4]</ref>) to obtain</p><formula xml:id="formula_36">E sup θ∈Cτ 1 |W | i∈W E[φ(θ z [i] / θ )] − φ(θ z [i] / θ ) ≤ E sup θ∈Cτ 1 |W | i∈W iφ(θ z [i] / θ ) ≤ 1 δ E sup θ∈Cτ 1 |W | i∈W iθ z [i] / θ ≤ 1 δ E sup θ∈R p 1 |W | i∈W iθ z [i] / θ = 1 δ E 1 |W | i∈W iz [i] .</formula><p>where i are independent Rademacher random variables. The final quantity is easily seen to be at most 1</p><formula xml:id="formula_37">δ p/|W |. Therefore we have E inf θ∈CτÂ (θ) ≥ Φ(σ −1 τ − δ) − 1 δ p/|W | ,</formula><p>and a standard application of Azuma's inequality implies that this bound also holds with high probability. Since θ T v/ θT ≥ 1/50 and |W | ≥ ∆n/2 with high probability, there exists a positive constant c∆ such that</p><formula xml:id="formula_38">A(θT ) ≥ Φ((50σ) −1 − δ) − c∆/δ .</formula><p>If we choose δ = 10 −4 /2c∆, then there exists a σ∆ for which Φ((50σ∆) −1 − δ) &gt; 1 − 10 −4 /2. We obtain that for any σ ≤ σ∆,Â(θT ) ≤ 1 − 10 −4 , as claimed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Vanishing gradients</head><p>We now show that, over the first T iterations, the coefficients tanh(θ </p><formula xml:id="formula_39">1 |C| i∈C (κ [i] (θT )) 2 &lt; 1 |C| i∈C (κ [i] (θ0)) 2 − .05 1 |W | i∈W (κ [i] (θT )) 2 &gt; 1 |W | i∈W (κ [i] (θ0)) 2 + .05 .</formula><p>That is, during the first stage, the coefficients on correct examples decrease while the coefficients on wrongly labeled examples increase.</p><p>Proof. Let us first consider</p><formula xml:id="formula_40">1 |C| i∈C (tanh(θ 0 x [i] ) − ε [i] ) 2</formula><p>For fixed initialization θ0, the law of large numbers implies that this quantity is near</p><formula xml:id="formula_41">Ex,ε(ε tanh(θ 0 x) − 1) 2 ≥ Ex,εε tanh(θ 0 x) − 1 2 .</formula><p>Let us write x = ε * (v − σz), where z is a standard Gaussian vector. Then the fact that tanh is Lipschitz implies</p><formula xml:id="formula_42">Ex,εε tanh(θ 0 x) ≤ Ex,εε tanh(ε * σθ 0 z) + |θ 0 v| = |θ 0 v| ,</formula><p>where we have used that E[tanh(ε * σθ 0 z)|ε] = 0. By Lemma 7, |θ 0 v| = oP (1). Hence</p><formula xml:id="formula_43">1 |C| i∈C (tanh(θ 0 x [i] ) − ε [i] ) 2 ≥ 1 − oP (1) .</formula><p>At iteration T , we have that θ T v ≥ .1, by assumption, and θT − θ0 ≤ 3, by the proof of Proposition 3. We can therefore apply Lemma 9 to obtain</p><formula xml:id="formula_44">1 |C| i∈C (κ [i] ) 2 1/2 ≤ 1 |C| i∈C ((ε * ) [i] tanh(θ T v) − ε [i] ) 1/2 + σ(2 + 3c∆) + oP (1) = | tanh(θ T v) − 1| + +σ(2 + 3c∆) + oP (1) ≤ | tanh(.1) − 1| + σ(2 + 3c∆) + oP (1) ,</formula><p>where the equality uses the fact that (ε * ) [i] = ε [i] for all i ∈ C. As long as σ ≤ σ∆ &lt; .01/(2 + 3c∆) this quantity is strictly less than .95. We therefore obtain that, for σ ≤ σ∆, 1</p><formula xml:id="formula_45">|C| i∈C (κ [i] (θT )) 2 &lt; 1 |C| i∈C (κ [i] (θ0)) 2 −</formula><p>.05 with high probability. This proves the first claim. The second claim is established by an analogous argument: for fixed initialization θ0, we have</p><formula xml:id="formula_46">E tanh(θ 0 x) 2 ≤ E(θ 0 x) 2 = 4σ 2 + (θ 0 v) 2 ,</formula><p>so as above we can conclude that</p><formula xml:id="formula_47">1 |W | i∈W (κ [i] (θ0)) 2 ≤ 1 + 4σ 2 + oP (1) .</formula><p>We likewise have by another application of Lemma 9</p><formula xml:id="formula_48">1 |W | i∈W (tanh(θ T x [i] ) − ε [i] ) 2 1/2 ≥ | tanh(−θ T v) − 1| − σ(2 + 3c∆) − oP (1) ≥ 1 + tanh(.1) − σ(2 + 3c∆) − oP (1) .</formula><p>where we again have used that (ε * ) <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Memorization</head><p>To show that the labels are memorized asymptotically, it suffices to show that the classes S+ := {x [i] : ε [i] = +1} and S− := {x [i] : ε [i] = −1} are linearly separable. Indeed, it is well known that for linearly separable data, gradient descent performed on the logistic loss will yield a classifier which perfectly memorizes the labels [see, e.g. 37, Lemma 1]. It is therefore enough to establish the following theorem. Theorem 6. If p, n → ∞ and lim infp,n→∞ p/n &gt; 1 − ∆/2, then the classes S+ and S− are linearly separable with probability tending to 1.</p><p>Proof. Write X = {x <ref type="bibr" target="#b0">[1]</ref> , . . . , x [n] }. Since the samples x <ref type="bibr" target="#b0">[1]</ref> , . . . , x [n] are drawn from a distribution absolutely continuous with respect to the Lebesgue measure, they are in general position with probability 1. Let us write T+ :</p><formula xml:id="formula_49">= {x [i] : (ε * ) [i] = +1}. For each i, the example x [i] is in S+ with probability 1 − (∆/2) if i ∈ T+ or ∆/2 or if i / ∈ T+.</formula><p>We therefore have for any S ⊂ X that</p><formula xml:id="formula_50">P[S+ = S|X] = (∆/2) |T + S| (1 − (∆/2)) n−|T + S| .</formula><p>We obtain that</p><formula xml:id="formula_51">P[S+ ∈ B|X] = S∈B (∆/2) |T + S| (1 − (∆/2)) n−|T + S| = n k=0 |{S ∈ B : |T+ S| = k}| · (∆/2) k (1 − (∆/2)) n−k .<label>(15)</label></formula><p>The set {S ∈ B : |T+ S| = k} has cardinality at most n k . Moreover, n k=0 |{S ∈ B : |T+ S| = k}| = |B|. We can therefore bound the sum (15) by the following optimization problem:</p><formula xml:id="formula_52">max x 1 ,...xn n k=0 x k · (∆/2) k (1 − (∆/2)) n−k<label>(16)</label></formula><p>s.t. x k ∈ 0, n k , n k=0</p><p>x k = |B| .</p><p>Since ∆ ≤ 1, the probability (∆/2) k (1 − (∆/2)) n−k is a nonincreasing function of k. Therefore, because |B| ≤ 2 n−p−1 k=0 n−1 k ≤ 2 n−p k=0 n k , the value of (16) is less than</p><formula xml:id="formula_53">2 n−p k=0 n k (∆/2) k (1 − (∆/2)) n−k = 2 · P[Bin(n, ∆/2) ≤ (n − p)] .</formula><p>If lim sup n,p→∞ 1 − p/n &lt; ∆/2, then this probability approaches 0 by the law of large numbers. We have shown that if lim infn,p p/n &gt; 1 − ∆/2, then</p><formula xml:id="formula_54">P[S+ ∈ B|X] ≤ 2 · P[Bin(n, ∆/2) ≤ (n − p)] = o(1)</formula><p>holds X-almost surely, which proves the claim.</p><p>A.5 Additional lemmas <ref type="bibr">Lemma 7</ref>. Suppose that θ0 is initialized randomly on the sphere of radius 2.</p><p>|θ 0 v| = oP (1).</p><p>Proof. Without loss of generality, take v = e1, the first elementary basis vector. Since the coordinates of θ0 each have the same marginal distribution and θ0 2 = 2 almost surely, we must have E|θ 0 e1| 2 = 2/p. The claim follows.</p><formula xml:id="formula_55">Lemma 8. sup θ∈R d ∇LCE(θ) ≤ 1 + 2σ + oP (1) .</formula><p>Proof. Denote by α the vector with entries αi = 1</p><formula xml:id="formula_56">2 √ n [tanh(θ x [i] ) − ε [i] ]. Since | tanh(x)| ≤ 1 for all x ∈ R, we have α ≤ 1. Therefore ∇LCE(θ) = 1 √ n n i=1 x [i] αi ≤ 1 √ n X ,</formula><p>where X ∈ R p×n is a matrix whose columns are given by the vectors x <ref type="bibr">[i]</ref> . By Lemma 10, we have</p><formula xml:id="formula_57">1 √ n X = 1 n XX 1/2 ≤ 1 + 2σ + oP (1) .</formula><p>This yields the claim.</p><p>Lemma 9. Fix an initialization θ0 satisfying θ0 = 2. For any τ &gt; 0 and for I = C or I = W , we have</p><formula xml:id="formula_58">sup θ: θ−θ 0 ≤τ 1 |I| i∈I ((ε * ) [i] tanh(θ x [i] ) − tanh(θ v)) 2 1/2 ≤ σ(2 + c∆τ ) + oP (1) .</formula><p>The same claim holds with I = [n] with c∆ replaced by 2.</p><p>Proof. Let us write</p><formula xml:id="formula_59">x [i] = (ε * ) [i] (v − σz [i] ), where z [i]</formula><p>is a standard Gaussian vector. Since tanh is odd and 1-Lipschitz, we have</p><formula xml:id="formula_60">|(ε * ) [i] tanh(θ x [i] ) − tanh(θ v)| = | tanh(θ v − θ σz [i] ) − tanh(θ v)| ≤ σ|θ z [i] | .</formula><p>We therefore obtain</p><formula xml:id="formula_61">1 |I| i∈I ((ε * ) [i] tanh(θ x [i] ) − tanh(θ v)) 2 1/2 ≤ σ 1 |I| i∈I (θ z [i] ) 2 1/2 ≤ σ 1 |I| i∈I (θ 0 z [i] ) 2 1/2 + σ 1 |I| i∈I ((θ − θ0) z [i] ) 2 1/2 ≤ σ 1 |I| i∈I (θ 0 z [i] ) 2 1/2 + σ θ − θ0 1 |I| i∈I z [i] (z [i] ) .</formula><p>Taking a supremum over all θ such that θ − θ0 ≤ τ and applying Lemma 10 yields the claim.</p><p>Lemma 10. Assume p ≤ n. There exists a positive constant c∆ depending on ∆ such that for I = C or</p><formula xml:id="formula_62">I = W , 1 |I| i∈I (θ 0 z [i] ) 2 ≤ 2 + oP (1) 1 |I| i∈I z [i] (z [i] ) 1/2 ≤ c∆ + oP (1) 1 |I| i∈I x [i] (x [i] ) 1/2 ≤ 1 + σc∆ + oP (1) .</formula><p>Moreover, the same claims hold with I = [n], when c∆ can be replaced by 2.</p><p>Proof. The first claim follows immediately from the law of large numbers. For the second two claims, we first consider the case where I = [n]. Let us write Z for the matrix whose columns are given by the vectors z <ref type="bibr">[i]</ref> . Then</p><formula xml:id="formula_63">1 n i∈[n] z [i] (z [i] ) 1/2 = 1 n ZZ 1/2 = 1 √ n Z ≤ 1 + p/n + oP (1) ,</formula><p>where the last claim is a consequences of standard bounds for the spectral norm of Gaussian random matrices [see, e.g. 42]. Since p ≤ n by assumption, the claimed bound follows. When I = C or W , the same argument applies, except that we condition on the set of indices in I, which yields that, conditioned on I,</p><formula xml:id="formula_64">1 |I| i∈I z [i] (z [i] ) 1/2 ≤ 2 n/|I| + oP (1) .</formula><p>For any ∆, the random variable |I| concentrates around its expectation, which is c∆n, for some constant c∆.</p><formula xml:id="formula_65">Finally, to bound 1 |I| i∈I x [i] (x [i] ) 1/2</formula><p>, we again let X be a matrix whose columns are given by x <ref type="bibr">[i]</ref> . Then we can write X = v(ε * ) + σZ , where Z is a Gaussian matrix, as above. Therefore</p><formula xml:id="formula_66">1 n i∈[n] x [i] (x [i] ) 1/2 = 1 √ n X ≤ 1 √ n v(ε * ) + σ 1 √ n Z ≤ 1 + 2σ + oP (1) .</formula><p>The extension to I = C or W is as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clean labels</head><p>Wrong labels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Entropy</head><p>Early-learning Regularization <ref type="figure">Figure</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Early Learning and Memorization in Linear and Deep-Learning Models</head><p>In this section we provide a numerical example to illustrate the theory in Section 3, and the similarities between the behavior of linear and deep-learning models. We train the two-class softmax linear regression model described in Section 3 on data drawn from a mixture of two Gaussians in R 100 , where 40% of the labels are flipped at random. <ref type="figure" target="#fig_0">Figure A.1</ref> shows the training accuracy on the training set for examples with clean and false labels. Analogously to the deep-learning model in <ref type="figure" target="#fig_0">Figure 1</ref>, the linear model trained with cross entropy begins by learning to predict the true labels, but eventually memorizes the examples with wrong labels as predicted by our theory. The figure also shows the results of applying our proposed early-learning regularization technique with temporal ensembling. ELR prevents memorization, allowing the model to continue learning on the examples with clean labels to attain high accuracy on examples with clean and wrong labels.</p><p>As explained in Section 4.2, for both linear and deep-learning models the effect of label noise on the gradient of the cross-entropy loss for each example i is restricted to the term</p><formula xml:id="formula_67">p [i] − y [i] , where p [i]</formula><p>is the probability example assigned by the model to the example and y <ref type="bibr">[i]</ref> is the corresponding label. <ref type="figure">Figure B</ref>.1 plots this quantity for the linear model described in the previous paragraph and for the deep-learning model from <ref type="figure" target="#fig_0">Figure 1</ref>. In both cases, the label noise flips the sign of the term on the wrong labels (left column). The magnitude of this term dominates after early learning (right column), eventually producing memorization of the wrong labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Regularization Based on Kullback-Leibler Divergence</head><p>A natural alternative to our proposed regularization would be to penalize the Kullback-Leibler (KL) divergence between the the model output and the targets. This results in the following loss function in the bottom row of <ref type="figure" target="#fig_0">Figure 1</ref>, regularization based on KL divergence fails to provide robustness. When λ is small, memorization of the wrong labels leads to overfitting as in cross-entropy minimization. Increasing λ delays memorization, but does not eliminate it. Instead, the model starts overfitting the initial estimates, whether correct or incorrect, and then eventually memorizes the wrong labels (see the bottom right graph in <ref type="figure" target="#fig_0">Figure C.1)</ref>.</p><formula xml:id="formula_68">LCE(Θ) − λ n n i=1 C c=1 t [i] c log p [i] c .<label>(17)</label></formula><p>Analyzing the gradient of the cost function sheds some light on the reason for the failure of this type of regularization. The gradient with respect to the model parameters Θ equals</p><formula xml:id="formula_69">1 n n i=1 ∇N x [i] (Θ) p [i] − y [i] + λ p [i] − t [i] .<label>(18)</label></formula><p>A key difference between this gradient and the gradient of ELR is the dependence of the sign of the regularization component on the targets. In ELR, the sign of the cth entry for the ith is determined by the difference between t c * will generally tend to be smaller, because t [i] is obtained by a moving average and therefore tends to be smoother than p <ref type="bibr">[i]</ref> . Consequently, the regularization term tends to decrease p <ref type="bibr">[i]</ref> c * . This is exactly the opposite effect than desired. In contrast, ELR tends to keep p </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The Need for Early Learning Regularization</head><p>Our proposed framework consists of two components: target estimation and the early-learning regularization term. <ref type="figure" target="#fig_0">Figure D.1</ref> shows that the regularization term is critical to avoid memorization. If we just perform target estimation via temporal ensembling while training with a cross-entropy loss, eventually the targets overfit the noisy labels, resulting in decreased accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of Lemma 2</head><p>To ease notation, we ignore the i superscript, setting p := p [i] and t := t <ref type="bibr">[i]</ref> . We denote the instance-level ELR by</p><formula xml:id="formula_70">R(Θ) := log (1 − p, t ) .<label>(19)</label></formula><p>The gradient of R is</p><formula xml:id="formula_71">∇R(Θ) = 1 1 − p, t ∇ (1 − p, t ) .<label>(20)</label></formula><p>We express the probability estimate in terms of the softmax function and the deep-learning mapping Nx(Θ), p := e Nx(Θ)</p><p>C c=1 e (Nx(Θ)) c , where e Nx(Θ) denotes a vector whose entries equal the exponential of the entries of  </p><formula xml:id="formula_72">∇R(Θ) = n i=1 1 1 − p, t ∇ 1 − e Nx(Θ) , t C c=1 e (Nx(Θ)) c (21) = n i=1 −1 1 − p, t ∇ e Nx(Θ) , t · C c=1 e (Nx(Θ)) c − e Nx(Θ) , t · ∇ C c=1 e (Nx(Θ)) c C c=1 e (Nx(Θ)) c 2 (22) = n i=1 −∇Nx(Θ) 1 − p, t e Nx(Θ) t · C c=1 e (Nx(Θ)) c − e Nx(Θ) , t · e Nx(Θ) C c=1 e (Nx(Θ)) c 2 (23) = n i=1 −∇Nx(Θ) 1 − p, t</formula><p>The formula can be simplified to</p><formula xml:id="formula_74">∇R(Θ) = −∇Nx(Θ) 1 − p, t (p t − p, t · p) (25) = ∇Nx(Θ) 1 − p, t    p1 · ( p, t − t1) . . . pC · ( p, t − tC )    (26) = ∇Nx(Θ) 1 − p, t    p1 · C k=1 (t k − t1) p k . . . pC · C k=1 (t k − tC ) p k    .<label>(27)</label></formula><p>F Algorithms Algorithm 1 and Algorithm 2 provide detailed pseudocode for ELR combined with temporal ensembling (denoted simply by ELR) and ELR combined with temporal ensembling, weight averaging, two networks, and mixup data augmentation (denoted by ELR+) respectively. For CIFAR-10 and CIFAR-100, we use the sigmoid shaped function e −5(1−i/40000) 2 (i is current training step, following <ref type="bibr" target="#b39">[40]</ref>) to ramp-up the weight averaging momentum γ to the value we set as a hyper-parameter. For the other datasets, we fixed γ. For CIFAR-100, we also use previously mentioned sigmoid shaped function to ramp up the coefficient λ to the value we set as a hyper-parameter. Moreover, each entry of the labels y will also be updated by the targets t using yctc C c=1 yctc in CIFAR-100.  where α is a fixed hyperparameter used to choose the symmetric beta distribution from which we sample the ratio of the convex combination between data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Description of the Computational Experiments</head><p>Source code for the experiments is available at https://github.com/shengliu66/ELR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Dataset Information</head><p>In our experiments we apply ELR and ELR+ to perform image classification on four benchmark datasets: CIFAR-10, CIFAR-100, Clothing-1M, and a subset of WebVision. Because CIFAR-10, CIFAR-100 do not have predefined validation sets, we retain 10% of the training sets to perform validation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Data preprocessing</head><p>We apply normalization and simple data augmentation techniques (random crop and horizontal flip) on the training sets of all datasets. The size of the random crop is set to be consistent with previous works <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b16">17]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Sensitivity to Hyperparameters</head><p>The main hyperparameters of ELR are the temporal ensembling parameter β and regularization coefficient λ. As shown in the left image of <ref type="figure">Figure H</ref>.1, performance is robust to the value of β, although it is worth noting that this is only as long as the momentum of the moving average is large. The performance degrades to 38% when the model outputs are used to estimate the target without averaging (i.e. β = 0). The regularization parameter λ needs to be large enough to neutralize the gradients of the falsely labeled examples but also cannot be too large, to avoid neglecting the cross entropy term in the loss. As shown in the center image of <ref type="figure">Figure H</ref>.1, the sensitivity to λ is also quite mild. Finally, the right image of <ref type="figure" target="#fig_0">Figure H.1</ref> shows results for ELR combined with mixup data augmentation for different values of the mixup parameter α. Performance is again quite robust, unless the parameter becomes very large, resulting in a peaked distribution that produces too much mixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Training Time Analysis</head><p>In <ref type="table" target="#tab_18">Table I</ref>.1 we compare the training times of ELR and ELR+ with two state-of-the-art methods, using a single Nvidia v100 GPU. ELR+ is twice as slow as ELR. DivideMix takes more than 2 times longer than ELR+ to train. Co-teaching+ is about twice as slow as ELR+.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>c = 1 .</head><label>1</label><figDesc>As a result, performing stochastic gradient descent eventually results in memorization, as in the linear model (seeFigures 1 and A.1). Crucially, the influence of the label noise on the gradient of the cross-entropy loss is restricted to the term p [i] − y [i] (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 p− 1 k=0 n − 1 k 1 k</head><label>2111</label><figDesc>By a theorem of Schläfli [see 8, Theorem 1], there exist C(n, p) := different subsets S ⊆ X that are linearly separable from their complements. In particular, there are at most 2 n − C(n, p) = 2 partitions of X which are not separable. Write B for the bad set of non-separable subsets S ⊆ X. Conditional on X, the probability that the classes S+ and S− are not separable is just P[S+ ∈ B|X].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>A. 1 :</head><label>1</label><figDesc>Results of training a two-class softmax regression model with a traditional cross entropy loss (top row) and the proposed method (bottom row) to perform classification on 50 simulated data drawn from a mixture of two Gaussians in R 100 with σ = 0.1, where 40% of the labels are flipped at random. The plots show the fraction of examples with clean labels predicted correctly (green) and incorrectly (red) for examples with clean labels (left column) and wrong labels (right column). Analogously to the deep-learning model in Figure 1, the linear model trained with cross entropy begins by learning to predict the true labels, but eventually memorizes the examples with wrong labels. Early-learning regularization prevents memorization, allowing the model to continue learning on the examples with clean labels to attain high accuracy on examples with clean and wrong labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure C. 1</head><label>1</label><figDesc>shows the result of applying this regularization to CIFAR-10 dataset with 40% symmetric noise for different values of the regularization parameter λ, using targets computed via temporal ensembling. In contrast to ELR, which succeeds in avoiding memorization while allowing the model to learn effectively as demonstratedLinear model Neural Network Figure B.1: The effect of label noise on the gradient of the cross-entropy loss for each example i is restricted to the term p [i] − y [i] , where p [i] is the probability example assigned by the model to the example and y [i] is the corresponding label (see Section 4.2). The plots show this term (left column) and its magnitude (right column) for the same linear model as in Figure A.1 (top row) and the same ResNet-34 on CIFAR-10 as inFigure 1(bottom row) with 40% symmetric noise. On the left, we plot the entry of p [i] − y[i]  corresponding to the true class, denoted by c * , for training examples with clean (blue) and wrong (red) labels. On the right, we plot the absolute value of the entry. During early learning, the clean labels dominate, but afterwards their effects decrease and the noisy labels start to be dominant, eventually leading to memorization of the wrong labels. In all plots the curves represent the mean value, and the shaded regions are within one standard deviation of the mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>c</head><label></label><figDesc>* large, as explained in Section 4.2, which allows the model to continue learning on the clean examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure C. 1 :</head><label>1</label><figDesc>Results of training a ResNet-34 neural network with a traditional cross entropy loss regularized by KL divergence using different coefficients λ (showed in different rows) to perform classification on the CIFAR-10 dataset where 40% of the labels are flipped at random. The left column shows the fraction of examples with clean labels that are predicted correctly (green) and incorrectly (blue). The right column shows the fraction of examples with wrong labels that are predicted correctly (green), memorized (the prediction equals the wrong label, shown in red), and incorrectly predicted as neither the true nor the labeled class (blue). When λ = 1, it is analogous to the model trained without any regularization (top row inFigure 1), while when λ increases, the fraction of correctly predicted examples decreases, indicating worse performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure D. 1 :</head><label>1</label><figDesc>Validation accuracy achieved by targets estimated via temporal ensembling using the cross entropy loss and our proposed cost function. The model is a ResNet-34 trained on CIFAR-10 with 40% symmetric noise. The temporal ensembling momentum β is set to 0.7. Without the regularization term, the targets eventually overfit the noisy labels.Nx(Θ). Plugging this into Eq. (20) yields</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>e</head><label></label><figDesc>Nx</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure H. 1 :</head><label>1</label><figDesc>Test accuracy on CIFAR-10 with symmetric noise level 60%. The mean accuracy over four runs is reported, along with bars representing one standard deviation from the mean. In each experiment, the rest of hyperparameters are fixed to the values reported in Section G.4. 1 (chosen from {0.1, 2, 5} via grid search on the validation set) and the value of the weight averaging parameter γ is set to 0.997 (which is the default value in the public code of Ref.<ref type="bibr" target="#b39">[40]</ref>) except Clothing1M, which is set to 0.9999.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>± 0.12 81.88 ± 0.29 74.14 ± 0.56 53.82 ± 1.04 90.69 ± 0.17 88.59 ± 0.34 86.14 ± 0.40 80.11 ± 1.44 Bootstrap [33] 86.23 ± 0.23 82.23 ± 0.37 75.12 ± 0.56 54.12 ± 1.32 90.32 ± 0.21 88.26 ± 0.24 86.57 ± 0.35 81.21 ± 1.47 Forward [31] 87.99 ± 0.36 83.25 ± 0.38 74.96 ± 0.65 54.64 ± 0.44 90.52 ± 0.26 89.09 ± 0.47 86.79 ± 0.36 83.55 ± 0.58 GSE [56] 89.83 ± 0.20 87.13 ± 0.22 82.54 ± 0.23 64.07 ± 1.38 90.91 ± 0.22 89.33 ± 0.17 85.45 ± 0.74 76.74 ± 0.61 SL [45] 89.83 ± 0.32 87.13 ± 0.26 82.81 ± 0.61 68.12 ± 0.81 91.72 ± 0.31 90.44 ± 0.27 88.48 ± 0.46 82.51 ± 0.45 ELR 91.16 ± 0.08 89.15 ± 0.17 86.12 ± 0.49 73.86 ± 0.61 93.27 ± 0.11 93.52 ± 0.23 91.89 ± 0.22 90.12 ± 0.47 ELR 92.12 ± 0.35 91.43 ± 0.21 88.87 ± 0.24 80.69 ± 0.57 94.57 ± 0.23 93.28 ± 0.19 92.70 ± 0.41 90.35 ± 0.38 ± 0.65 37.41 ± 0.94 18.10 ± 0.82 66.54 ± 0.42 59.20 ± 0.18 51.40 ± 0.16 42.74 ± 0.61 Bootstrap [33] 58.27 ± 0.21 47.66 ± 0.55 34.68 ± 1.1 21.64 ± 0.97 67.27 ± 0.78 62.14 ± 0.32 52.87 ± 0.19 45.12 ± 0.57 Forward [31] 39.19 ± 2.61 31.05 ± 1.44 19.12 ± 1.95 8.99 ± 0.58 45.96 ± 1.21 42.46 ± 2.16 38.13 ± 2.97 34.44 ± 1.93 GSE [56] 66.81 ± 0.42 61.77 ± 0.24 53.16 ± 0.78 29.16 ± 0.74 68.36 ± 0.42 66.59 ± 0.22 61.45 ± 0.26 47.22 ± 1.15 SL [45] 70.38 ± 0.13 62.27 ± 0.22 54.82 ± 0.57 25.91 ± 0.44 73.12 ± 0.22 72.56 ± 0.22 72.12 ± 0.24 69.32 ± 0.87 ELR 74.21 ± 0.22 68.28 ± 0.31 59.28 ± 0.67 29.78 ± 0.56 74.20 ± 0.31 74.03 ± 0.31 73.71 ± 0.22 73.26 ± 0.64 ELR 74.68 ± 0.31 68.43 ± 0.42 60.05 ± 0.78 30.27 ± 0.86 74.52 ± 0.32 74.20 ± 0.25 74.02 ± 0.33 73.73 ± 0.34</figDesc><table><row><cell>Datasets (Architecture)</cell><cell>Methods</cell><cell>20%</cell><cell>Symmetric label noise 40% 60%</cell><cell>80%</cell><cell>10%</cell><cell>Asymmetric label noise 20% 30%</cell><cell>40%</cell></row><row><cell cols="3">CIFAR10 (ResNet34) 86.98 CIFAR100 Cross entropy Cross entropy 58.72 ± 0.26</cell><cell>48.20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(ResNet34)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table G.1 in the supplementary material reports additional details about the datasets, and our training, validation and test splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Cross entropy Co-teaching+<ref type="bibr" target="#b51">[52]</ref> Mixup<ref type="bibr" target="#b54">[55]</ref> PENCIL<ref type="bibr" target="#b50">[51]</ref> MD-DYR-SH<ref type="bibr" target="#b1">[2]</ref> DivideMix<ref type="bibr" target="#b21">[22]</ref> ELR+ ELR+ *</figDesc><table><row><cell>CIFAR-10</cell><cell>Sym. label noise</cell><cell>20% 50% 80% 90%</cell><cell>86.8 79.4 62.9 42.7</cell><cell>89.5 85.7 67.4 47.9</cell><cell>95.6 87.1 71.6 52.2</cell><cell>92.4 89.1 77.5 58.9</cell><cell>94.0 92.0 86.8 69.1</cell><cell>96.1 94.6 93.2 76.0</cell><cell>94.6 95.8 93.8 94.8 91.1 93.3 75.2 78.7</cell></row><row><cell></cell><cell cols="2">Asym. 40%</cell><cell>83.2</cell><cell>-</cell><cell>-</cell><cell>88.5</cell><cell>87.4</cell><cell>93.4</cell><cell>92.7 93.0</cell></row><row><cell>CIFAR-100</cell><cell>Sym. label noise</cell><cell>20% 50% 80% 90%</cell><cell>62.0 46.7 19.9 10.1</cell><cell>65.6 51.8 27.9 13.7</cell><cell>67.8 57.3 30.8 14.6</cell><cell>69.4 57.5 31.1 15.3</cell><cell>73.9 66.1 48.2 24.3</cell><cell>77.3 74.6 60.2 31.5</cell><cell>77.5 77.6 72.4 73.6 58.2 60.8 30.8 33.4</cell></row><row><cell></cell><cell cols="2">Asym. 40%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.1</cell><cell>76.5 77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison with state-of-the-art methods in test accuracy (%) on Clothing1M. All methods use a ResNet-50 architecture pretrained on ImageNet. Results of other methods are taken from the original papers (except for GCE, which is taken from [45]).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 compares</head><label>3</label><figDesc></figDesc><table><row><cell>22]</cell><cell>ELR</cell><cell>ELR+</cell></row></table><note>ELR and ELR+ to state-of-the-art methods on the Clothing1M dataset. ELR+ achieves state-of-the-art performance, slightly superior to DivideMix.D2L [27] MentorNet [17] Co-teaching [14] Iterative-CV [44] DivideMix [</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art methods trained on the mini WebVision dataset. Results of other methods are taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>Network mixup 93.04 ± 0.12 91.05 ± 0.13 87.23 ± 0.30 81.43 ± 0.52 92.09 ± 0.08 90.83 ± 0.07 76.50 ± 0.65 72.54 ± 0.35 2 Networks mixup 93.68 ± 0.51 93.51 ± 0.47 88.62 ± 0.26 84.75 ± 0.26 92.95 ± 0.05 91.86 ± 0.14 80.13 ± 0.51 73.49 ± 0.47</figDesc><table><row><cell>40%</cell><cell>80%</cell></row><row><cell>Weight Averaging</cell><cell>Weight Averaging</cell></row><row><cell>1</cell><cell></cell></row></table><note>compares ELR and ELR+ to state-of-the-art methods trained on the mini WebVision dataset and evaluated on both the WebVision and ImageNet ILSVRC12 validation sets. ELR+ achieves state-of-the-art performance, slightly superior to DivideMix, on WebVision. ELR also performs strongly, despite its simplicity. On ILSVRC12 DivideMix produces superior results (particularly in terms of top1 accuracy).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>accuracy of θt on mislabeled examples. We now show that the classifier's accuracy on the mislabeled examples improves over the first T rounds. In fact, we show that with high probability,Â(θ0) ≈ 1/2 whereaŝ A(θT ) ≈ 1.Theorem 4. There exists a σ∆ such that if σ ≤ σ∆, then Let us write x [i] = (ε * ) [i] (v − σz [i] ), where z [i] is a standard Gaussian vector. If we fix θ0, then sign(θ 0 x [i] ) = (ε * ) [i] if and only if σθ 0 z [i] &lt; θ 0 v. In particular this yields</figDesc><table><row><cell>Proof.</cell><cell></cell></row><row><cell>A(θ0) ≤ .5001</cell><cell>(12)</cell></row><row><cell>andÂ</cell><cell></cell></row><row><cell>(θT ) &gt; .9999</cell><cell>(13)</cell></row><row><cell>with high probability.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>x [i] ) − ε [i] associated with the correctly labeled examples decrease, while the coefficients on mislabeled examples increase. For simplicity, we write κ [i] := tanh(θ x [i] ) − ε [i] . Proposition 5. There exists a constant σ∆ such that, for any σ ≤ σ∆, with high probability,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>and the rest of the entries of t[i]  (seeLemma 2). In contrast, for KL divergence it depends on the difference between t This results in overfitting the target probabilities. To illustrate this, recall that for examples with clean labels, the cross-entropy term p [i] − y[i]  tends to vanish after the early-learning stage because p [i] is very close to y[i]  , allowing examples with wrong labels to dominate the gradient. Let c * denote the true class. When p</figDesc><table><row><cell>[i]</cell><cell></cell></row><row><cell>[i] c and p</cell><cell>[i]</cell></row><row><cell cols="2">[i] c  *  (correctly) approaches one, t [i]</cell></row></table><note>cc .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table G</head><label>G</label><figDesc>.1: Description of the datasets used in our computational experiments, including the training, validation and test splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table G.1 provides a detailed description of each dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>: 32 for the CIFAR datasets, 224 × 224 for Clothing1M (after resizing to 256 × 256), and 227 × 227 for WebVision.Table G.2: Training hyperparameters for ELR+ on CIFAR-10, Clothing-1M and Webvision.</figDesc><table><row><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell><cell>Clothing-1M</cell><cell></cell><cell>Webvision</cell></row><row><cell>batch size</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell></cell><cell>32</cell></row><row><cell>architecture</cell><cell>PreActResNet-18</cell><cell>PreActResNet-18</cell><cell cols="2">ResNet-50 (pretrained)</cell><cell>InceptionResNetV2</cell></row><row><cell>training epochs</cell><cell>200</cell><cell>250</cell><cell>15</cell><cell></cell><cell>100</cell></row><row><cell>learning rate (lr)</cell><cell>0.02</cell><cell>0.02</cell><cell>0.002</cell><cell></cell><cell>0.02</cell></row><row><cell>lr scheduler</cell><cell cols="2">divide 10 at 150th epoch divide 10 at 200th epoch</cell><cell cols="2">divide 10 at 7th epoch</cell><cell>divide 10 at 50th epoch</cell></row><row><cell>weight decay</cell><cell>5e-4</cell><cell>5e-4</cell><cell>1e-3</cell><cell></cell><cell>5e-4</cell></row><row><cell cols="2">Temporal ensembling momentum β</cell><cell cols="2">Regularization coefficient λ</cell><cell cols="2">mixup parameter α</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table I .</head><label>I</label><figDesc>1: Comparison of total training time in hours on CIFAR-10 with 40% symmetric label noise.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The term target is inspired by semi-supervised learning where target probabilities are used to learn on unlabeled examples<ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSF NRT-HDR Award 1922658. SL was partially supported by NSF grant DMS 2009752. CFG was partially supported by NSF Award HDR-1940097. JNW gratefully acknowledges the support of the Institute for Advanced Study, where a portion of this research was conducted.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 1: Pseudocode for ELR with temporal ensembling.</p><p>Require: {x <ref type="bibr">[i]</ref> , y [i] }, 1 ≤ i ≤ n = training data (with noisy labels) Require: β = temporal ensembling momentum, 0 ≤ β &lt; 1 Require: λ = regularization parameter Require: N x (Θ) = neural network with trainable parameters Θ t ← 0 <ref type="bibr">[n×C]</ref> initialize ensemble predictions for t in <ref type="bibr">[1, num_epochs]</ref> </p><p>proposed regularization component update Θ using stochastic gradient descent update network parameters end for end for return Θ Algorithm 2: Pseudocode for ELR+.</p><p>initialize averaged predictions Θ 1 ,Θ 2 ← 0, 0 initialize averaged weights (untrainable) for t in [1, num_epochs] do for k in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> do for each network for each minibatch B dõ B ← mixup(B, α) mixup augmentation on the mini-batch</p><p>proposed regularization component update Θ k using SGD update network parameters end for end for end for return Θ 1 , Θ 2</p><p>To apply mixup data augmentation, when processing the ith example in a mini-batch ( </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-frequency covariance estimates with noisy and asynchronous financial data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Aït-Sahalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Xiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">492</biblScope>
			<biblScope unit="page" from="1504" to="1517" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Noise-tolerant learning, the parity problem, and the statistical query model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="506" to="519" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haw-Shiuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1002" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electronic Computers</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="334" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Who said what: Modeling individual labelers improves classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Melody Y Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor Wai-Hung</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probability in Banach Spaces: isoperimetry and processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11680</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the design of convolutional neural networks for automatic detection of Alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chhavi</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Learning for Health NeurIPS Workshop</title>
		<meeting>the Machine Learning for Health NeurIPS Workshop</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-12-13" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="184" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Sudanthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning without concentration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="25" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Class noise and supervised learning in medical domains: The effect of feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tsymbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seppo</forename><surname>Puuronen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th IEEE symposium on computer-based medical systems (CBMS&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="708" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SELFIE: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08059</idno>
		<title level="m">Prestopping: How does early stopping help generalization against label noise? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Shpigel Nacson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from noisy labels by regularized estimation of annotator confusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11244" to="11253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6575" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Introduction to the non-asymptotic analysis of random matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compressed sensing</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="210" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Imae for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Iterative learning with open-set noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8688" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6835" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">LDMI: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In noisy labels</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning from multiple annotators with varying expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="327" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd annual meeting of the association for computational linguistics</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor Wai-Hung</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Training Procedure Below we describe the training procedure for ELR (i.e. the proposed approach with temporal ensembling) for the different datasets. The information for ELR+ is shown in Table G.4. In ELR+ we ensemble the outputs of two networks during inference, as is customary for methods that train two networks simultaneously</title>
		<imprint/>
	</monogr>
	<note>22, 14</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">001, and a batch size of 128. The network is trained for 120 epochs for CIFAR-10 and 150 epochs for CIFAR-100. We set the initial learning rate as 0.02, and reduce it by a factor of 100 after 40 and 80 epochs for CIFAR-10 and after 80 and 120 epochs for CIFAR-100. We also experiment with cosine annealing learning rate [26] where the maximum number of epoch for each period is set to 10, the maximum and minimum learning rate is set to 0</title>
		<idno>CIFAR-10/CIFAR-100: We use a ResNet-34 [15] and train it using SGD with a momentum of 0.9</idno>
		<imprint/>
	</monogr>
	<note>02 and 0.001 respectively, total epoch is set to 150</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The model is trained with batch size 64 and initial learning rate 0.001, which is reduced by 1/100 after 5 epochs (10 epochs in total). The optimization is done using SGD with a momentum 0.9, and weight decay 0.001. For each epoch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clothing-1m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">We use a ResNet-50 pretrained on ImageNet same as Refs</title>
		<imprint/>
	</monogr>
	<note>45, 47. we sample 2000 mini-batches from the training data ensuring that the classes of the noisy labels are balanced</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">All other optimization details are the same as for CIFAR-10</title>
		<imprint/>
	</monogr>
	<note>we use an InceptionResNetV2 as the backbone architecture. except for the weight decay (0.0005) and the batch size (32</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">4 Hyperparameters selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">10} using the validation set. The selected values are β = 0.7 and λ = 3 for symmetric noise, β = 0.9 and λ = 1 for assymetric noise on CIFAR-10, and β = 0.9 and λ = 7 CIFAR-100. For Clothing1M and WebVision we use the same values as for CIFAR-10. As shown in Section H, the performance of the proposed method seems to be robust to changes in the hyperparameters. For ELR+</title>
	</analytic>
	<monogr>
		<title level="m">We perform hyperparameter tuning on the CIFAR datasets via grid search: the temporal ensembling parameter β is chosen from {0.5, 0.7, 0.9, 0.99} and the regularization coefficient λ is chosen from {1</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>The mixup α is set to</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
