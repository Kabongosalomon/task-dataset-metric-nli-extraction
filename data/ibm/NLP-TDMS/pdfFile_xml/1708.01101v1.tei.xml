<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Feature Pyramids for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<email>wyang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wlouyang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Feature Pyramids for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Articulated human pose estimation is a fundamental yet challenging task in computer vision. The difficulty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens. Although pyramid methods are widely used to handle scale changes at inference time, learning feature pyramids in deep convolutional neural networks (DCNNs) is still not well explored. In this work, we design a Pyramid Residual Module (PRMs) to enhance the invariance in scales of DCNNs. Given input features, the PRMs learn convolutional filters on various scales of input features, which are obtained with different subsampling ratios in a multibranch network. Moreover, we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks, which achieve superior performance than plain networks in many tasks recently. Therefore, we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures. We investigate our method on two standard benchmarks for human pose estimation. Our approach obtains state-of-the-art results on both benchmarks. Code is available at https://github.com/bearpaw/PyraNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Localizing body parts for human body is a fundamental yet challenging task in computer vision, and it serves as an important basis for high-level vision tasks, e.g., activity recognition <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b53">54]</ref>, clothing parsing <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36]</ref>, human re-identification <ref type="bibr" target="#b64">[65]</ref>, and human-computer interaction. Achieving accurate localization, however, is difficult due to the highly articulated human body limbs, occlusion, change of viewpoint, and foreshortening.</p><p>Significant progress on human pose estimation has been achieved by deep convolutional neural networks (DC-NNs) <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b38">39]</ref>. In these methods, the DCNNs learn body part detectors from images warped to the similar scale based on human body size. At inference <ref type="bibr">(a)</ref> (b) (c) <ref type="figure">Figure 1</ref>. Our predictions on the LSP dataset <ref type="bibr" target="#b30">[31]</ref>. When images are warped to approximately the same scale, scales of different body parts may still be inconsistent due to camera view change and foreshortening. In (a), the scale of hand and head are larger than that of foot. In (b), the scale of foot is larger than that of head.</p><p>time, testing images should also be warped to the same scale as that for training images. Although the right scale of the full human body is provided, scales for body parts may still be inconsistent due to inter-personal body shape variations and foreshortening caused by viewpoint change and body articulation. It results in difficulty for body part detectors to localize body parts. For example, severe foreshortening is present in <ref type="figure">Figure 1</ref>. When the images are warped to the same size according to human body scale, the hand in <ref type="figure">Figure 1</ref> (a) has a larger scale than that in <ref type="figure">Figure 1 (b)</ref>. Therefore, the hand detector that can detect the hand in <ref type="figure">Figure 1</ref> (a) might not be able to detect the hand in <ref type="figure">Figure 1 (b)</ref> reliably. In DCNNs, this problem from scale change happens not only for high-level semantics in deeper layers, but also exists for low-level features in shallower layers.</p><p>To enhance the robustness of DCNNs against scale variations of visual patterns, we design a Pyramid Residual Module to explicitly learn convolutional filters for building feature pyramids. Given input features, the Pyramid Residual Module obtains features of different scales via subsampling with different ratios. Then convolution is used to learn filters for features in different scales. The filtered features are upsampled to the same resolution and are summed together for the following processing. This Pyramid Residual Module can be used as building blocks in DCNNs for learning feature pyramids at different levels of the network.</p><p>There is a trend of designing networks with branches, e.g., Inception models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b45">46]</ref> and ResNets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> for classification, ASPP-nets <ref type="bibr" target="#b8">[9]</ref> for semantic segmentation, convolutional pose machines <ref type="bibr" target="#b54">[55]</ref> and stacked hourglass networks <ref type="bibr" target="#b38">[39]</ref> for human pose estimation, in which the input of a layer is from multiple other layers or the output of a layer is used by many other layers. Our pyramid residual module also has branches. We observe that the existing weight initialization scheme, e.g., MSR <ref type="bibr" target="#b23">[24]</ref> and Xavier <ref type="bibr" target="#b20">[21]</ref> methods, are not proper for layers with branches. Therefore, we extend the current weight initialization scheme and provide theoretic derivation to show that the initialization of network parameters should take the number of branches into consideration. We also show another issue in the residual unit <ref type="bibr" target="#b25">[26]</ref>, where the variance of output of the residual unit accumulates as the depth increases. The problem is caused by the identity mapping.</p><p>Since Hourglass network, also called conv-deconv structure, is an effective structure for pose estimation <ref type="bibr" target="#b38">[39]</ref>, object detection <ref type="bibr" target="#b33">[34]</ref>, and pixel level tasks <ref type="bibr" target="#b9">[10]</ref>, we use it as the basic structure in experiments. We observe a problem of using residual unit for Hourglass: when outputs of two residual units are summed up, the output variance is approximately doubled, which causes difficulty in optimization. We propose a simple but efficient way with negligible additional parameters to solve this problem.</p><p>The main contributions are three folds:</p><p>• We propose a Pyramid Residual Module, which enhances the invariance in scales of deep models by learning feature pyramids in DCNNs with only a small increase of complexity.</p><p>• We identify the problem for initializing DCNNs including layers with multiple input or output branches. A weight initialization scheme is then provided, which can be used for many network structures including inception models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b45">46]</ref> and ResNets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>• We observe that the problem of activation variance accumulation introduced by identity mapping may be harmful in some scenarios, e.g., adding outputs of multiple residual units implemented by identity mapping <ref type="bibr" target="#b25">[26]</ref> together in the Hourglass structure. A simple yet effective solution is introduced for solving this issue.</p><p>We evaluate the proposed method on two popular human pose estimation benchmarks, and report state-of-the-art results. We also demonstrate the generalization ability of our approach on standard image classification task. Ablation study demonstrates the effectiveness of the pyramid residual module, the new initialization scheme, and the approach in handling drastic activation variance increase caused by adding residual units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human pose estimation. Graph structures, e.g., Pictorial structures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b60">61]</ref> and loopy structures <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b17">18]</ref>, have been broadly used to model the spatial relationships among body parts. All these methods were built on handcrafted features such as HOG feature <ref type="bibr" target="#b14">[15]</ref>, and their performances relied heavily on image pyramid. Recently, deep models have achieved state-of-the-art results in human pose estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40]</ref>. Among them, DeepPose <ref type="bibr" target="#b52">[53]</ref> is one of the first attempts on using DCNNs for human pose estimation. It regressed the coordinates of body parts directly, which suffered from the problem that image-to-locations is a difficult mapping to learn. Therefore, later methods modeled part locations as Gaussian peaks in score maps, and predicted the score maps with fully convolutional networks. In order to achieve higher accuracy, multi-scale testing on image pyramids was often utilized, which produced a multi-scale feature representation. Our method is a complementary to image pyramids. On the other hand, to learn a model with strong scale invariance, a multi-branch network trained on three scales of image pyramid was proposed in <ref type="bibr" target="#b50">[51]</ref>. However, when image pyramids are used for training, computation and memory linearly increases with the number of scales. In comparison, our pyramid residual module provides an efficient way of learning multi-scale features, with relatively small cost in computation and memory. DCNNs combining multiple layers.</p><p>In contrast to traditional plain networks (e.g., AlexNet <ref type="bibr" target="#b32">[33]</ref> and VGGnets <ref type="bibr" target="#b44">[45]</ref>), multi-branch networks exhibit better performance on various vision tasks. In classification, the inception models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b45">46]</ref> are one of the most successful multi-branch networks. The input of each module is first mapped to low dimension by 1 × 1 convolutions, then transformed by a set of filters with different sizes to capture various context information and combined by concatenation. ResNet <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> can be regarded as a two-branch networks with one identity mapping branch. ResNeXt <ref type="bibr" target="#b55">[56]</ref> is an extension of ResNet, in which all branches share the same topology. The implicitly learned transforms are aggregated by summation. In our work, we use multi-branch network to explore another possibility: to learn multi-scale features.</p><p>Recent methods in pose estimation, object detection and segmentation used features from multiple layers for making predictions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9]</ref>. Our approach is complementary to these works. For example, we adopt Hourglass as our basic structure, and replace its original residual units, which learn features from a single scale, with the proposed Pyramid Residual Module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight initialization.</head><p>Good initialization is essential for training deep models. Hinton and Salakhutdinov <ref type="bibr" target="#b26">[27]</ref> adopted the layer-by-layer pretraining strategy to train a deep autoencoder. Krizhevsky et al. <ref type="bibr" target="#b32">[33]</ref> initialized the weight of each layer by drawing samples from a Gaussian distribution with zero mean and 0.01 standard deviation. However, it has difficulty in training very deep networks due to the instability of gradients <ref type="bibr" target="#b44">[45]</ref>. Xavier initialization <ref type="bibr" target="#b20">[21]</ref> has provided a theoretically sound estimation of the variance of weight. It assumes that the weights are initialized close to zero, hence the nonlinear activations like Sigmoid and Tanh can be regarded as linear functions. This assumption does not hold for rectifier <ref type="bibr" target="#b37">[38]</ref> activations. Thus He et al. <ref type="bibr" target="#b23">[24]</ref> proposed an initialization scheme for rectifier networks based on <ref type="bibr" target="#b20">[21]</ref>. All the above initialization methods, however, are derived for plain networks with only one branch. We identify the problem of the initialization methods when applied for multi-branch networks. An initialization scheme for networks with multiple branches is provided to handle this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Framework</head><p>An overview of the proposed framework is illustrated in <ref type="figure">Figure.</ref> 2. We adopt the highly modularized stacked Hourglass Network <ref type="bibr" target="#b38">[39]</ref> as the basic network structure to investigate feature pyramid learning for human pose estimation . The building block of our network is the proposed Pyramid Residual Module (PRM). We first briefly review the structure of hourglass network. Then a detailed discussion of our pyramid residual module is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting Stacked Hourglass Network</head><p>Hourglass network aims at capturing information at every scale in feed-forward fashion. It first performs bottom-up processing by subsampling the feature maps, and conducts top-down processing by upsampling the feature maps with the comination of higher resolution features from bottom layers, as demonstrated in <ref type="figure">Figure.</ref> 2(b). This bottomup, top-down processing is repeated for several times to build a "stacked hourglass" network, with intermediate supervision at the end of each stack.</p><p>In <ref type="bibr" target="#b38">[39]</ref>, residual unit <ref type="bibr" target="#b25">[26]</ref> is used as the building block of the hourglass network. However, it can only capture visual patterns or semantics at one scale. In this work, we use the proposed pyramid residual module as the building block for capturing multi-scale visual patterns or semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pyramid Residual Modules (PRMs)</head><p>The objective is to learn feature pyramids across different levels of DCNNs. It allows the network to capture feature pyramids from primitive visual patterns to high-level semantics. Motivated by recent progress on residual learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, we propose a novel Pyramid Residual Module (PRM), which is able to learn multi-scale feature pyramids.</p><p>The PRM explicitly learns filters for input features with different resolutions. Let x (l) and W (l) be the input and the filter of the l-th layer, respectively. The PRM can be formulated as,</p><formula xml:id="formula_0">x (l+1) = x (l) + P(x (l) ; W (l) ),<label>(1)</label></formula><p>where P(x (l) ; W (l) ) is feature pyramids decomposed as:</p><formula xml:id="formula_1">P(x (l) ; W (l) ) = g C c=1 fc(x (l) ; w (l) fc ); w (l) g + f0(x (l) ; w (l) f 0 ).</formula><p>(</p><p>The C in (2) denotes the number of pyramid levels, f c (·) is the transformation for the c-th pyramid level, and</p><formula xml:id="formula_3">W (l) = {w (l) fc , w (l) g } C c=0</formula><p>is the set of parameters. Outputs of transformations f c (·) are summed up together, and further convolved by filters g(·). An illustration of the pyramid residual module is illustrated in <ref type="figure">Figure.</ref> 3. To reduce the computational and space complexity, each f c (·) is designed as a bottleneck structure. For example, in <ref type="figure">Figure.</ref> 3, the feature dimension is reduced by a 1 × 1 convolution, then new features are computed on a set of subsampled input features by 3 × 3 convolutions. Finally, all the new features are upsampled to the same dimension and are summed together. Generation of input feature pyramids. Max-pooling or average-pooling are widely used in DCNNs to reduce the resolution of feature maps, and to encode the translation invariance. But pooling reduces the resolution too fast and coarse by a factor of an integer of at least two, which is unable to generate pyramids gently. In order to obtain input feature maps of different resolutions, we adopt the fractional max-pooling <ref type="bibr" target="#b21">[22]</ref> to approximate the smoothing and subsampling process used in generating traditional image pyramids. The subsampling ratio of the cth level pyramid is computed as:</p><formula xml:id="formula_4">Addi�on dstride dstride 1 BN-ReLU-3x3Conv BN-ReLU- 3x3 Dilated Conv Downsampling Upsampling Ra�o 1 Ra�o Ra�o 1 Ra�o BN-ReLU-1x1Conv (a) PRM-A PRM-B = Addi�on PRM-C = Concatena�on (c) PRM-D ( ) ( +1) 0 1 (b) g n i p p a M y t i t n e d I</formula><formula xml:id="formula_5">s c = 2 −M c C , c = 0, · · · , C, M ≥ 1,<label>(3)</label></formula><p>where s c ∈ [2 −M , 1] denotes the relative resolution compared with the input features. For example, when c = 0, the output has the same resolution as its input. When M = 1, c = C, the map has half resolution of its input.</p><p>In experiments, we set M = 1 and C = 4, with which the lowest scale in pyramid is half the resolution of its input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussions</head><p>PRM for general CNNs. Our PRM is a general module and can be used as the basic building block for various CNN architectures, e.g., stacked hourglass networks <ref type="bibr" target="#b38">[39]</ref> for pose estimation, and Wide Residual Nets <ref type="bibr" target="#b63">[64]</ref> and ResNeXt <ref type="bibr" target="#b55">[56]</ref> for image classification, as demonstrated in experiments.</p><p>Variants in pyramid structure. Besides using fractional max-pooling, convolution and upsampling to learn feature pyramids, as illustrated in <ref type="figure">Figure.</ref> 3(a-b), one can also use dilated convolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b62">63]</ref> to compute pyramids, as shown in <ref type="figure">Figure.</ref> 3(c)(PRM-D). The summation of features in pyramid can also replaced by concatenation, as shown in <ref type="figure">Figure.</ref>  Weight sharing. To generate the feature pyramids, traditional methods usually apply a same handcrafted filter, e.g., HOG, on different levels of image pyramids <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. This process corresponds to sharing the weights W (l) fc across dif-ferent levels of pyramid f c (·), which is able to greatly reduce the number of parameters. Complexity. The residual unit used in <ref type="bibr" target="#b38">[39]</ref> has 256-d input and output, which are reduced to 128-d within the residual unit. We adopt this structure for the branch with original scale (i.e., f 0 in Eq. <ref type="formula" target="#formula_2">(2)</ref>). Since features with smaller resolution contain relatively fewer information, we use fewer feature channels for branches with smaller scales. For example, given a PRM with five branches and 28 feature channels for branches with smaller scale (i.e., f 1 to f 4 in Eq.(2)), the increased complexity is about only 10% compared with residual unit in terms of both parameters and GFLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Inference</head><p>We use score maps to represent the body joint locations. Denote the ground-truth locations by z = {z k } K k=1 , where z k = (x k , y k ) denotes the location of the kth body joint in the image. Then the ground-truth score map S k is generated from a Gaussian with mean z k and variance Σ as follows,</p><formula xml:id="formula_6">S k (p) ∼ N (z k , Σ),<label>(4)</label></formula><p>where p ∈ R 2 denotes the location, and Σ is empirically set as an identity matrix I. Each stack of hourglass network predicts K score maps, i.e.Ŝ = {Ŝ k } K k=1 , for K body joints. A loss is attached at the end of each stack defined by the squared error</p><formula xml:id="formula_7">L = 1 2 N n=1 K k=1 S k −Ŝ k 2 ,<label>(5)</label></formula><p>where N is the number of samples. During inference, we obtain the predicted body joint locationsẑ k from the predicted score maps generated from the last stack of hourglass by taking the locations with the maximum score as follows:</p><formula xml:id="formula_8">z k = arg max pŜ k (p), k = 1, · · · , K.<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Initialization Multi-Branch Networks</head><p>Initialization is essential to train very deep networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24]</ref>, especially for tasks of dense prediction, where Batch Normalization <ref type="bibr" target="#b29">[30]</ref> is less effective because of the small minibatch due to the large memory consumption of fully convolutional networks. Existing weight initialization methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> are designed upon the assumption of a plain networks without branches. The proposed PRM has multiple branches, and does not meet the assumption. Recent developed architectures with multiple branches, e.g., Inception models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b45">46]</ref> and ResNets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, are not plain network either. Hence we discuss how to derive a proper initialization for networks adding multiple branches. Our derivation mainly follows <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>. Forward propagation. Generally, multi-branch networks can be characterized by the number of input and output branches. <ref type="figure" target="#fig_4">Figure. 4 (a)</ref> shows an example where the lth layer has C i }. Take fullyconnected layer for example, a response is computed as:</p><formula xml:id="formula_9">y (l) = W (l) C (l) i c=1 x (l) c + b (l) ,<label>(7)</label></formula><p>x</p><formula xml:id="formula_10">(l+1) = f y (l) ,<label>(8)</label></formula><p>where f (·) is the non-linear activation function. As in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>, we assume that W (l) and x (l) are both independent and identically distributed (i.i.d.), and they are independent of each other. Therefore, we respectively denote y (l) , x (l) and w (l) as the element in y (l) , x (l) and W (l) . Then we have,</p><formula xml:id="formula_11">Var y (l) = C (l) i n (l) i Var w (l) x (l) ,<label>(9)</label></formula><p>where n (l) i is the number of elements in x (l) c for c = 1, . . . , C (l) i . Suppose w (l) has zero mean. The variance for the product of independent variables above is as follows: where α depends on the activation function f in <ref type="bibr" target="#b7">(8)</ref>. α = 0.5 for ReLU and α = 1 for Tanh and Sigmoid. In order to make the variances of the output y (l) approximately the same for different layers l, the following condition should be satisfied:</p><formula xml:id="formula_12">Var y (l) = C (l) i n (l) i Var w (l) E x (l) 2 = αC (l) i n (l) i Var w (l) Var y (l−1) , Conv / FC 1 ( ) 2 ( ) ( ) ( ) Conv / FC 1 ( ) 2 ( ) ( ) ( ) (a) (b)</formula><formula xml:id="formula_13">αC (l) i n (l) i Var w (l) = 1.<label>(10)</label></formula><p>Hence in initialization, a proper variance for W (l) should be 1/(αC <ref type="bibr">(l)</ref> and ∂L ∂y (l) by ∆x (l) and ∆y (l) respectively. During backward propagation, the gradient is computed by chain rule,</p><formula xml:id="formula_14">(l) i n (l) i ). Backward propagation. Denote ∂L ∂x</formula><formula xml:id="formula_15">∆x (l) = C (l) o c=1 W (l)T ∆y (l) ,<label>(11)</label></formula><formula xml:id="formula_16">∆y (l) = f (y (l) )∆x (l+1) .<label>(12)</label></formula><p>Suppose w (l) and ∆y (l) are i.i.d. and independent of each other, then ∆x (l) has zero mean when w (l) is initialized with zero mean and symmetric with small magnitude. Let n (l) o denote the number of output neurons. Then we have,</p><formula xml:id="formula_17">Var ∆x (l) = C (l) o n (l) o Var[w (l) ] Var[∆y (l) ].<label>(13)</label></formula><p>Denote E(f (y (l) )) = α. α = 0.5 for ReLU and α = 1 for Tanh and Sigmoid. We further assume that f (y (l) ) and ∆x (l) are independent of each other, then from Eq. <ref type="formula" target="#formula_0">(12)</ref> </p><formula xml:id="formula_18">i = C (l) o n (l)</formula><p>o . As in <ref type="bibr" target="#b20">[21]</ref>, a compromise between the forward and backward constraints is to have,</p><formula xml:id="formula_19">Var[w (l) ] = 1 α 2 (C (l) i n (l) i + C (l) o n (l) o )</formula><p>, ∀l. Special case. For plain networks with one input and one output branch, we have C <ref type="bibr" target="#b14">(15)</ref>. In this case, the result in <ref type="bibr" target="#b14">(15)</ref> degenerates to the conclusions obtained for Tanh and Sigmoid in <ref type="bibr" target="#b20">[21]</ref> and the conclusion in <ref type="bibr" target="#b23">[24]</ref> for ReLU. General case. In general, a network with branches would have C (l) i = 1 or C (l) o = 1 for some ls. Therefore, the number of input branches and output branches should be taken into consideration when initializing parameters. Specifically, if several multi-branch layers are stacked together without other operations (e.g., batch normalization,convolution, ReLU, etc.), the output variance would be increased approximately l C (l) i times by using Xavier <ref type="bibr" target="#b20">[21]</ref> or MSR <ref type="bibr" target="#b23">[24]</ref> initialization.</p><formula xml:id="formula_21">(l) i = C (l) o = 1 in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Output Variance Accumulation</head><p>Residual learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> allows us to train extremely deep neural networks due to identity mappings. But it is also the source of its drawbacks: identity mapping keeps increasing the variances of responses when the network goes deeper, which increases the difficulty of optimization.</p><p>The response of the residual unit is computed as follows:</p><formula xml:id="formula_22">x (l+1) = x (l) + F x (l) ; W (l) ,<label>(16)</label></formula><p>where F denotes the residual function, e.g., a bottleneck structure with three convolutions (1 × 1 → 3 × 3 → 1 × 1). Assume x (l) and F x (l) ; W (l) are uncorrelated, then the variance of the response of residual unit is as</p><formula xml:id="formula_23">Var x (l+1) = Var x (l) + Var F x (l+1) ; W (l) &gt; Var x (l) ,<label>(17)</label></formula><p>where Var F x (l+1) ; W (l) is positive. In <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, the identity mapping will be replaced by convolution layer when the resolution of feature maps is reduced, or when the dimension of feature channels are increased. This allows the networks to reset the variance of response to a small value, and avoid responses with very large variance, as shown in <ref type="figure">Figure.</ref> 5. The effect of increasing variance becomes more obvious in hourglass-like structures, where the responses of two residual units are summed together, as illustrated in <ref type="figure" target="#fig_6">Figure. 6(a)</ref>. Assume branches are uncorrelated, then the variance will be increased as:</p><formula xml:id="formula_24">Var x (l+1) = 2 i=1 Var x (l) i + Var Fi x (l) i ; W (l) i &gt; 2 i=1 Var x (l) i .<label>(18)</label></formula><p>Hence the output variance is almost doubled. When the network goes deeper, the variance will increase drastically.</p><p>In this paper, we use a 1 × 1 convolution preceding with batch normalization and ReLU to replace the identity mapping when the output of two residual units are summed up, as illustrated in <ref type="figure" target="#fig_6">Figure. 6(b)</ref>. This simple replacement stops the variance explosion, as demonstrated in <ref type="figure" target="#fig_6">Figure. 6(c)</ref>. In experiments, we find that breaking the variance explosion also provide a better performance (Section 5.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on Human Pose Estimation</head><p>We conduct experiments on two widely used human pose estimation benchmarks. (i) The MPII human pose dataset <ref type="bibr" target="#b1">[2]</ref>, which covers a wide range of human activities with 25k images containing over 40k people. (ii) The Leeds Sports Poses (LSP) <ref type="bibr" target="#b30">[31]</ref> and its extended training dataset, which contains 12k images with challenging poses in sports. <ref type="table">Table 1</ref>. Comparisons of PCKh@0.5 score on the MPII test set. Ours-A is trained using the training set used in <ref type="bibr" target="#b50">[51]</ref>. Ours-B is trained with the same settings but using all the MPII training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Head Sho. Elb. Wri. Hip Knee Ank. Mean Pishchulin et al. <ref type="bibr" target="#b40">[41]</ref> 74. <ref type="bibr" target="#b2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Implementation Details</head><p>Our implementation follows <ref type="bibr" target="#b38">[39]</ref>. The input image is 256 × 256 cropped from a resized image according to the annotated body position and scale. For the LSP test set, we simply use the image center as the body position, and estimate the body scale by the image size. Training data are augmented by scaling, rotation, flipping, and adding color noise. All the models are trained using Torch <ref type="bibr" target="#b13">[14]</ref>. We use RMSProp <ref type="bibr" target="#b49">[50]</ref> to optimize the network on 4 Titan X GPUs with a mini-batch size of 16 (4 per GPU) for 200 epochs. The learning rate is initialized as 7 × 10 −4 and is dropped by 10 at the 150th and the 170th epoch. Testing is conducted on six-scale image pyramids with flipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Experimental Results</head><p>Evaluation measure. Following previous work, we use the Percentage Correct Keypoints (PCK) measure <ref type="bibr" target="#b61">[62]</ref> on the LSP dataset, and use the modified PCK measure that uses the matching threshold as 50% of the head segment length (PCKh) <ref type="bibr" target="#b1">[2]</ref> on the MPII dataset. MPII Human Pose. We report the performance on MPII dataset in <ref type="table">Table 1</ref>. Ours-A is trained using the training and validation set used in <ref type="bibr" target="#b50">[51]</ref>. Ours-B is trained with the same settings but using all the MPII training set. Our approach achieves 92.0% PCKh score at threshold of 0.5, which is the new state-of-the-art result. Specifically, our method achieves 1.6% and 2.4% improvements on wrist and ankle, which are considered as the most challenging parts to be detected. Qualitative results are demonstrated in <ref type="figure">Figure. 7</ref>. Complexity. Our model increases the number of parameters by 13.5% from 23.7M to 26.9M given an eight-stack hourglass network. Our model needs 45.9 GFLOPs for a 256×256 RGB image, which is a 11.4% increase compared to hourglass network (41.2 GFLOPs). As reported in <ref type="bibr" target="#b38">[39]</ref>, deeper hourglass with more stacks hardly improves result. LSP dataset. <ref type="table">Table 2</ref> presents the PCK scores at the threshold of 0.2. We follow previous methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b28">29]</ref> to train our model by adding MPII training set to the LSP and its extended training set. Our method improves the previous best result with a large margin by 3.2%. For difficult body parts, e.g., wrist and ankle, we have 3.7% and 5.0% improvements, respectively. Our method gains a lot due to the high occurrence of foreshortening and extreme poses presented in this dataset, as demonstrated in <ref type="figure">Figure.</ref> 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Ablation Study</head><p>We conduct ablation study on the MPII validation set used in <ref type="bibr" target="#b50">[51]</ref> with a 2-stack hourglass network as the basic model. Architectures of PRM. We first evaluate different designs of PRM, as discussed in Section 3.2, with the same number of branches, and the same feature channels for each branch (e.g., 5 branches with 28 feature channels for each pyramidal branch). We use PRM-A to PRM-D, which corresponds to <ref type="figure">Figure.</ref> 3, to denote the different architectures. Specifically, PRM-A produces separate input feature maps for different levels of pyramids, while PRM-B uses shared feature maps for all levels of pyramids. PRM-C uses concatenation instead of addition to combine features generated from pyramid, which is similar to inception models. PRM-D uses dilated convolutions, which are also used in ASPP-net <ref type="bibr" target="#b8">[9]</ref>, instead of pooling to build the pyramid. The validation ac- curacy is reported in <ref type="figure">Figure. 8(a)</ref>. All the PRMs have better accuracy compared with the baseline model. We observe that the difference in accuracy between PRM-A to PRM-D is subtle, while the parameters of PRM-A/C are higher than PRM-B/B*/D ( <ref type="figure">Figure. 8(b)</ref>), and the computational complexity (GFLOPs) of PRM-A/C/D are higher than PRM-B/B*. Therefore, we use PRM-B* in the rest of the experiments. Noted that increasing the number of channels to make the baseline model has the similar model size as ours (Wide BS) would slightly improve the performance. But it is still worse than ours.</p><p>Scales of pyramids. To evaluate the trade-off between the scales of pyramids C, we vary the scales from 3 to 5, and fix the model size by tuning the feature channels in each scale. We observe that increasing scales generally improves the performance, as shown in <ref type="figure" target="#fig_8">Figure. 9(a-b)</ref>.</p><p>Weight initialization. We compare the performance of our initialization scheme with Xavier <ref type="bibr" target="#b20">[21]</ref> and MSR <ref type="bibr" target="#b23">[24]</ref> methods. The training and validation curves of accuracy vs. epoch are reported in <ref type="figure" target="#fig_8">Figure 9</ref>(c-d). It can be seen that the proposed initialization scheme achieves better performance than both methods.</p><p>Controlling variance explosion. Controlling variance explosion, as discussed in Section 4.2, obtains higher validation score (88.0) compared with the baseline model (87.6). With our pyramid residual module, the performance could be further improved to 88.5 PCKh score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on CIFAR-10 Image Classification</head><p>The CIFAR-10 dataset <ref type="bibr" target="#b31">[32]</ref> consists of 50k training images and 10k test images with size 32 × 32 drawn from 10 classes. We follow previous works for data preparation and augmentation. We incorporate the proposed pyramid branches into two state-of-the-art network architectures, i.e., Wide residual networks <ref type="bibr" target="#b63">[64]</ref> and ResNeXt <ref type="bibr" target="#b55">[56]</ref>. We add four pyramid branches with scales ranging from 0.5 to 1 into the building block of both Wide ResNet and ResNeXt. For Wide ResNet, the total width of all pyramid branches is equal to the width of the output of each residual module. For ResNeXt, we simply use the same width as its original branches for our pyramid branches. <ref type="table" target="#tab_2">Table 3</ref> shows the top-1 test error, model sizes and GFLOPs. Our method with similar or less model size (Ours-28-9 vs. WRN-28-10 and Ours-29, 8 × 64d vs. ResNeXt-29, 16 × 64d) achieve better results. A larger model with our pyramid module (Ours-29, 16 × 64d ) achieves 3.30% test error, which is the state-of-the-art result on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has proposed a Pyramid Residual Module to enhance the invariance in scales of the DCNNs. We also provide a derivation of the initialization scheme for multibranch networks, and demonstrate its theoretical soundness and efficiency through experimental analysis. Additionally, a simple yet effective method to prevent the variances of response from explosion when adding outputs of multiple identity mappings has been proposed. Our PRMs and the initialization scheme for multi-branch networks are general, and would help other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our framework. (a) demonstrates the network architecture, which has n stacks of hourglass network. Details of each stack of hourglass is illustrated in (b). Score maps of body joint locations are produced at the end of each hourglass, and a squared-error loss is also attached in each stack of hourglass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Structures of PRMs. Dashed links indicate identity mapping. (a) PRM-A produces separate input feature maps for different levels of pyramids, while (b) PRM-B uses shared input for all levels of pyramids. PRM-C use concatenation instead of addition to combine features generated from pyramids, which is similar to inception models. (c) PRM-D use dilated convolutions, which are also used in ASPP-net<ref type="bibr" target="#b8">[9]</ref>, instead of pooling to build the pyramid. The dashed trapezoids mean that the subsampling and upsampling are skipped.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3(b)(PRM-C). We discuss the performance of these variants in experiments, and show that the design in Figure. 3(b)(PRM-B) has comparable performance with others, while maintains relatively fewer parameters and smaller computational complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>i</head><label></label><figDesc>input branches and one output branch. Figure. 4 (b) shows an example where the lth layer has one input branch and C (l) o output branches. During forward propagation, C (l) i affects the variance for the output of the lth layer while C (l) o does not. At the lth layer, assume there are C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Examples of multi-branch networks when (a) the inputs might be an addition of multiple branches, or (b) the output might be forwarded to multiple branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Response variances accumulate in ResNets. This accumulation can be reset (blue bar) when the identity mappings are replaced by convolution or batch normalization (i.e., when the feature channels of feature resolutions changes between input and output features).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Top: (a) Addition of outputs of two identity mappings. (b) One identity mapping is replaced by a BN-ReLU-Conv block. Bottom: Statistics of response variances of the original hourglass network (yellow bar) and our structure (b) (red bar).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Results on the MPII (top) and the LSP dataset (bottom). Statistics of (a) accuracy, (b) number of parameters, and (c) computational complexity in terms of GFLOPs on different designs of PRMs inFigure.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Training and validation curves of PCKh scores vs. epoch on the MPII validation set. (a-b) Investigate the number of scales in the pyramid. BL stands for baseline model (two-stack hourglass network), S2 to S8 indicate PRM-B* with four scales to eight scales. (c-d) Comparison of our initialization scheme with Xavier method<ref type="bibr" target="#b20">[21]</ref> and MSR method<ref type="bibr" target="#b23">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, we have E ∆y (l) = α E ∆x (l+1) . Then we can derive that Var[∆y (l) ] = E[(∆y (l) ) 2 ] = α Var[x (l+1) ]. Therefore, from Eq.(13) we have,</figDesc><table><row><cell cols="4">Var ∆x (l) = αC (l) o n (l) o Var[w (l) ] Var[∆x (l+1) ]..</cell><cell>(14)</cell></row><row><cell cols="4">To ensure Var[∆x (l) ] = Var[∆x (l+1) ], we must have</cell></row><row><cell cols="2">Var[w (l) ] = 1/(αC o n (l)</cell><cell cols="2">(l) o ).</cell></row><row><cell>In many cases, C</cell><cell cols="2">(l) i n</cell><cell>(l)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>49.0 40.8 34.1 36.5 34.4 35.2 44.1 Tompson et al. [52] 95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6 Carreira et al.Table 2. Comparisons of PCK@0.2 score on the LSP dataset.</figDesc><table><row><cell>[8]</cell><cell>95.7 91.7 81.7 72.4 82.8 73.2 66.4 81.3</cell></row><row><cell>Tompson et al. [51]</cell><cell>96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0</cell></row><row><cell>Hu&amp;Ramanan [28]</cell><cell>95.0 91.6 83.0 76.6 81.9 74.5 69.5 82.4</cell></row><row><cell>Pishchulin et al. [42]</cell><cell>94.1 90.2 83.4 77.3 82.6 75.7 68.6 82.4</cell></row><row><cell>Lifshitz et al. [35]</cell><cell>97.8 93.3 85.7 80.4 85.3 76.6 70.2 85.0</cell></row><row><cell>Gkioxary et al. [20]</cell><cell>96.2 93.1 86.7 82.1 85.2 81.4 74.1 86.1</cell></row><row><cell>Rafi et al. [43]</cell><cell>97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3</cell></row><row><cell>Insafutdinov et al. [29]</cell><cell>96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5</cell></row><row><cell>Wei et al. [55]</cell><cell>97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5</cell></row><row><cell cols="2">Bulat&amp;Tzimiropoulos [5] 97.9 95.1 89.9 85.3 89.4 85.7 81.7 89.7</cell></row><row><cell>Newell et al. [39]</cell><cell>98.2 96.3 91.2 87.1 90.1 87.4 83.6 90.9</cell></row><row><cell>Ours-A</cell><cell>98.4 96.5 91.9 88.2 91.1 88.6 85.3 91.8</cell></row><row><cell>Ours-B</cell><cell>98.5 96.7 92.5 88.7 91.1 88.6 86.0 92.0</cell></row><row><cell>Method</cell><cell>Head Sho. Elb. Wri. Hip Knee Ank. Mean</cell></row><row><cell cols="2">Belagiannis&amp;Zisserman [3]95.2 89.0 81.5 77.0 83.7 87.0 82.8 85.2</cell></row><row><cell>Lifshitz et al. [35]</cell><cell>96.8 89.0 82.7 79.1 90.9 86.0 82.5 86.7</cell></row><row><cell>Pishchulin et al. [42]</cell><cell>97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1</cell></row><row><cell>Insafutdinov et al. [29]</cell><cell>97.4 92.7 87.5 84.4 91.5 89.9 87.2 90.1</cell></row><row><cell>Wei et al. [55]</cell><cell>97.8 92.5 87.0 83.9 91.5 90.8 89.9 90.5</cell></row><row><cell cols="2">Bulat&amp;Tzimiropoulos [5] 97.2 92.1 88.1 85.2 92.2 91.4 88.7 90.7</cell></row><row><cell>Ours</cell><cell>98.3 94.5 92.2 88.9 94.4 95.0 93.7 93.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Top-1 test error (%), model size (million) and GFLOPs on CIFAR-10. WRN-28-10 denote the Wide ResNet with depth 29 and widen factor 10. ResNeXt-29, m × nd denote ResNeXt with depth 29, cardinality m and base width n.</figDesc><table><row><cell>method</cell><cell cols="3">#params GFLOPs top-1</cell></row><row><cell>WRN-28-10 [64]</cell><cell>36.5</cell><cell>10.5</cell><cell>4.17</cell></row><row><cell>Ours-28-9</cell><cell>36.4</cell><cell>9.5</cell><cell>3.82</cell></row><row><cell>Ours-28-10</cell><cell>42.3</cell><cell>11.3</cell><cell>3.67</cell></row><row><cell>ResNeXt-29, 8 × 64d [56]</cell><cell>34.4</cell><cell>48.8</cell><cell>3.65</cell></row><row><cell>ResNeXt-29, 16 × 64d [56]</cell><cell>68.2</cell><cell>184.5</cell><cell>3.58</cell></row><row><cell>Ours-29, 8 × 64d</cell><cell>45.6</cell><cell>50.5</cell><cell>3.39</cell></row><row><cell>Ours-29, 16 × 64d</cell><cell>79.3</cell><cell>186.1</cell><cell>3.30</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid methods in image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ogden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RCA engineer</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2d human pose estimation in tv shows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marín-Jiménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical and Geometrical Approaches to Visual Motion Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="128" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Fractional max-pooling. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Zoom out-and-in network with recursive training for object proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05711</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matching-cnn meets knn: Quasiparametric human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recovering human body configurations using pairwise constraints between parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast globally optimal 2d human detection with loopy graph models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Clothing co-parsing by joint image segmentation and labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
