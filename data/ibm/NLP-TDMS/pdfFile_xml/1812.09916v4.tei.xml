<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMPROVING MMD-GAN TRAINING WITH REPULSIVE LOSS FUNCTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saman</forename><surname>Halgamuge</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">RMIT University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMPROVING MMD-GAN TRAINING WITH REPULSIVE LOSS FUNCTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization. 1</p><p>linear 4 × 4, stride 2 deconv, 256, BN, ReLU 4 × 4, stride 2 deconv, 128, BN, ReLU 4 × 4, stride 2 deconv, 64, BN, ReLU 3 × 3, stride 1 conv, 3, Tanh (b) Discriminator RGB image x ∈ [−1, 1] H×W ×3 3 × 3, stride 1 conv, 64, LReLU 4 × 4, stride 2 conv, 128, LReLU 3 × 3, stride 1 conv, 128, LReLU 4 × 4, stride 2 conv, 256, LReLU 3 × 3, stride 1 conv, 256, LReLU 4 × 4, stride 2 conv, 512, LReLU 3 × 3, stride 1 conv, 512, LReLU h × w × 512 → s, dense, linear</p><p>Table S2: DCGAN models for image generation on CelebA and LSUN-bedroom datasets. For non-saturating loss and hinge loss, s = 1; for MMD-rand, MMD-rbf, MMD-rq, s = 16. (a) Generator z ∈ R 128 ∼ N (0, I) 128 → 4 × 4 × 1024, dense, linear 4 × 4, stride 2 deconv, 512, BN, ReLU 4 × 4, stride 2 deconv, 256, BN, ReLU 4 × 4, stride 2 deconv, 128, BN, ReLU 4 × 4, stride 2 deconv, 64, BN, ReLU 3 × 3, stride 1 conv, 3, Tanh (b) Discriminator RGB image x ∈ [−1, 1] 64×64×3 3 × 3, stride 1 conv, 64, LReLU 4 × 4, stride 2 conv, 128, LReLU 3 × 3, stride 1 conv, 128, LReLU 4 × 4, stride 2 conv, 256, LReLU 3 × 3, stride 1 conv, 256, LReLU 4 × 4, stride 2 conv, 512, LReLU 3 × 3, stride 1 conv, 512, LReLU 4 × 4, stride 2 conv, 1024, LReLU 3 × 3, stride 1 conv, 1024, LReLU 4 × 4 × 512 → s, dense, linear</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative adversarial nets (GANs) <ref type="bibr" target="#b7">(Goodfellow et al. (2014)</ref>) are a branch of generative models that learns to mimic the real data generating process. GANs have been intensively studied in recent years, with a variety of successful applications <ref type="bibr" target="#b16">(Karras et al. (2018)</ref>; <ref type="bibr" target="#b21">Li et al. (2017b)</ref>; <ref type="bibr" target="#b19">Lai et al. (2017)</ref>; <ref type="bibr">Zhu et al. (2017)</ref>; <ref type="bibr" target="#b13">Ho &amp; Ermon (2016)</ref>). The idea of GANs is to jointly train a generator network that attempts to produce artificial samples, and a discriminator network or critic that distinguishes the generated samples from the real ones. Compared to maximum likelihood based methods, GANs tend to produce samples with sharper and more vivid details but require more efforts to train.</p><p>Recent studies on improving GAN training have mainly focused on designing loss functions, network architectures and training procedures. The loss function, or simply loss, defines quantitatively the difference of discriminator outputs between real and generated samples. The gradients of loss functions are used to train the generator and discriminator. This study focuses on a loss function called maximum mean discrepancy (MMD), which is well known as the distance metric between two probability distributions and widely applied in kernel two-sample test <ref type="bibr" target="#b8">(Gretton et al. (2012)</ref>). Theoretically, MMD reaches its global minimum zero if and only if the two distributions are equal. Thus, MMD has been applied to compare the generated samples to real ones directly <ref type="bibr" target="#b22">(Li et al. (2015)</ref>; <ref type="bibr" target="#b5">Dziugaite et al. (2015)</ref>) and extended as the loss function to the GAN framework recently <ref type="bibr">(Unterthiner et al. (2018)</ref>; <ref type="bibr" target="#b20">Li et al. (2017a)</ref>; ).</p><p>In this paper, we interpret the optimization of MMD loss by the discriminator as a combination of attraction and repulsion processes, similar to that of linear discriminant analysis. We argue that the existing MMD loss may discourage the learning of fine details in data, as the discriminator attempts to minimize the within-group variance of its outputs for the real data. To address this issue, we propose a repulsive loss for the discriminator that explicitly explores the differences among real data. The proposed loss achieved significant improvements over the MMD loss on image generation Published as a conference paper at ICLR 2019 tasks of four benchmark datasets, without incurring any additional computational cost. Furthermore, a bounded Gaussian kernel is proposed to stabilize the training of discriminator. As such, using a single kernel in MMD-GAN is sufficient, in contrast to a linear combination of kernels used in <ref type="bibr" target="#b20">Li et al. (2017a)</ref> and . By using a single kernel, the computational cost of the MMD loss can potentially be reduced in a variety of applications.</p><p>The paper is organized as follows. Section 2 reviews the GANs trained using the MMD loss <ref type="bibr">(MMD-GAN)</ref>. We propose the repulsive loss for discriminator in Section 3, introduce two practical techniques to stabilize the training process in Section 4, and present the results of extensive experiments in Section 5. In the last section, we discuss the connections between our model and existing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MMD-GAN</head><p>In this section, we introduce the GAN model and MMD loss. Consider a random variable X ∈ X with an empirical data distribution P X to be learned. A typical GAN model consists of two neural networks: a generator G and a discriminator D. The generator G maps a latent code z with a fixed distribution P Z (e.g., Gaussian) to the data space X : y = G(z) ∈ X , where y represents the generated samples with distribution P G . The discriminator D evaluates the scores D(a) ∈ R d of a real or generated sample a. This study focuses on image generation tasks using convolutional neural networks (CNN) for both G and D.</p><p>Several loss functions have been proposed to quantify the difference of the scores between real and generated samples: {D(x)} and {D(y)}, including the minimax loss and non-saturating loss <ref type="bibr" target="#b7">(Goodfellow et al. (2014)</ref>), hinge loss <ref type="bibr" target="#b35">(Tran et al. (2017)</ref>), Wasserstein loss ; <ref type="bibr" target="#b10">Gulrajani et al. (2017)</ref>) and maximum mean discrepancy (MMD) <ref type="bibr" target="#b20">(Li et al. (2017a)</ref>; ) (see Appendix B.1 for more details). Among them, MMD uses kernel embedding φ(a) = k(·, a) associated with a characteristic kernel k such that φ is infinite-dimensional and φ(a), φ(b) H = k(a, b). The squared MMD distance between two distributions P and Q is</p><formula xml:id="formula_0">M 2 k (P, Q) = µ P − µ Q 2 H = E a,a ∼P [k(a, a )] + E b,b ∼Q [k(b, b )] − 2E a∼P,b∼Q [k(a, b)]</formula><p>(1) The kernel k(a, b) measures the similarity between two samples a and b. <ref type="bibr" target="#b8">Gretton et al. (2012)</ref> proved that, using a characteristic kernel k, M 2 k (P, Q) ≥ 0 with equality applies if and only if P = Q.</p><p>In MMD-GAN, the discriminator D can be interpreted as forming a new kernel with k: k•D(a, b) = k(D(a), D(b)) = k D (a, b). If D is injective, k • D is characteristic and M 2 k•D (P X , P G ) reaches its minimum if and only if P X = P G <ref type="bibr" target="#b20">(Li et al. (2017a)</ref>). Thus, the objective functions for G and D could be <ref type="bibr" target="#b20">(Li et al. (2017a)</ref>; </p><formula xml:id="formula_1">): min G L mmd G = M 2 k•D (P X , P G ) = E P G [k D (y, y )] − 2E P X ,P G [k D (x, y)] + E P X [k D (x, x )] (2) min D L att D = −M 2 k•D (P X , P G ) = 2E P X ,P G [k D (x, y)] − E P X [k D (x, x )] − E P G [k D (y, y )]</formula><p>(3) MMD-GAN has been shown to be more effective than the model that directly uses MMD as the loss function for the generator G <ref type="bibr" target="#b20">(Li et al. (2017a)</ref>). <ref type="bibr" target="#b23">Liu et al. (2017)</ref> showed that MMD and Wasserstein metric are weaker objective functions for GAN than the JensenShannon (JS) divergence (related to minimax loss) and total variation (TV) distance (related to hinge loss). The reason is that convergence of P G to P X in JS-divergence and TV distance also implies convergence in MMD and Wasserstein metric. Weak metrics are desirable as they provide more information on adjusting the model to fit the data distribution <ref type="bibr" target="#b23">(Liu et al. (2017)</ref>). <ref type="bibr" target="#b27">Nagarajan &amp; Kolter (2017)</ref> proved that the GAN trained using the minimax loss and gradient updates on model parameters is locally exponentially stable near equilibrium, while the GAN using Wasserstein loss is not. In Appendix A, we demonstrate that the MMD-GAN trained by gradient descent is locally exponentially stable near equilibrium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REPULSIVE LOSS FUNCTION</head><p>In this section, we interpret the training of MMD-GAN (using L att D and L mmd G ) as a combination of attraction and repulsion processes, and propose a novel repulsive loss function for the discriminator by rearranging the components in L att D . First, consider a linear discriminant analysis (LDA) model as the discriminator. The task is to find a projection w to maximize the between-group variance w T µ x − w T µ y and minimize the withingroup variance w T (Σ x + Σ y )w, where µ and Σ are group mean and covariance.</p><p>In MMD-GAN, the neural-network discriminator works in a similar way as LDA. By minimizing L att D , the discriminator D tackles two tasks: 1) D reduces E P X ,P G [k D (x, y)], i.e., causes the two groups {D(x)} and {D(y)} to repel each other (see <ref type="figure" target="#fig_0">Fig. 1a</ref> orange arrows), or maximize betweengroup variance; and 2) D increases E P X [k D (x, x )] and E P G [k(y, y )], i.e. contracts {D(x)} and {D(y)} within each group (see <ref type="figure" target="#fig_0">Fig. 1a</ref> blue arrows), or minimize the within-group variance. We refer to loss functions that contract real data scores as attractive losses.</p><p>We argue that the attractive loss L att D (Eq. 3) has two issues that may slow down the GAN training:</p><p>1. The discriminator D may focus more on the similarities among real samples (in order to contract {D(x)}) than the fine details that separate them. Initially, G produces low-quality samples and it may be adequate for D to learn the common features of {x} in order to distinguish between {x} and {y}. Only when {D(y)} is sufficiently close to {D(x)} will D learn the fine details of {x} to be able to separate {D(x)} from {D(y)}. Consequently, D may leave out some fine details in real samples, thus G has no access to them during training. <ref type="figure" target="#fig_0">Fig. 1a</ref>, the gradients on D(y) from the attraction (blue arrows) and repulsion (orange arrows) terms in L att D (and thus L mmd G ) may have opposite directions during training. Their summation may be small in magnitude even when D(y) is far away from D(x), which may cause G to stagnate locally. Therefore, we propose a repulsive loss for D to encourage repulsion of the real data scores {D(x)}:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">As shown in</head><formula xml:id="formula_2">L rep D = E P X [k D (x, x )] − E P G [k D (y, y )]<label>(4)</label></formula><p>The generator G uses the same MMD loss L mmd G as before (see Eq. 2). Thus, the adversary lies in the fact that D contracts {D(y)} via maximizing E P G [k D (y, y )] (see <ref type="figure" target="#fig_0">Fig. 1b</ref>) while G expands {D(y)} (see <ref type="figure" target="#fig_0">Fig. 1c</ref>). Additionally, D also learns to separate the real data by minimizing E P X [k D (x, x )], which actively explores the fine details in real samples and may result in more meaningful gradients for G. Note that in Eq. 4, D does not explicitly push the average score of {D(y)} away from that of {D(x)} because it may have no effect on the pair-wise sample distances. But G aims to match the average scores of both groups. Thus, we believe, compared to the model using L mmd G and L att D , our model of L mmd G and L rep D is less likely to yield opposite gradients when {D(y)} and {D(x)} are distinct (see <ref type="figure" target="#fig_0">Fig. 1c</ref>). In Appendix A, we demonstrate that GAN trained using gradient descent and the repulsive MMD loss (L rep D , L mmd G ) is locally exponentially stable near equilibrium.</p><p>At last, we identify a general form of loss function for the discriminator D: where λ is a hyper-parameter 2 . When λ &lt; 0, the discriminator loss L D,λ is attractive, with λ = −1 corresponding to the original MMD loss L att D in Eq. 3; when λ &gt; 0, L D,λ is repulsive and λ = 1 corresponds to L rep D in Eq. 4. It is interesting that when λ &gt; 1, the discriminator explicitly contracts {D(x)} and {D(y)} via maximizing E P X ,P G [k D (x, y)], which may work as a penalty that prevents the pairwise distances of {D(x)} from increasing too fast. Note that L D,λ has the same computational cost as L att D (Eq. 3) as we only rearranged the terms in L att D .</p><formula xml:id="formula_3">L D,λ = λE P X [k D (x, x )] − (λ − 1)E P X ,P G [k D (x, y)] − E P G [k D (y, y )]<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REGULARIZATION ON MMD AND DISCRIMINATOR</head><p>In this section, we propose two approaches to stabilize the training of MMD-GAN: 1) a bounded kernel to avoid the saturation issue caused by an over-confident discriminator; and 2) a generalized power iteration method to estimate the spectral norm of a convolutional kernel, which was used in spectral normalization on the discriminator in all experiments in this study unless specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KERNEL IN MMD</head><p>For MMD-GAN, the following two kernels have been used:</p><p>• Gaussian radial basis function (RBF), or Gaussian kernel <ref type="bibr" target="#b20">(Li et al. (2017a)</ref></p><formula xml:id="formula_4">), k rbf σ (a, b) = exp(− 1 2σ 2 a − b 2 )</formula><p>where σ &gt; 0 is the kernel scale or bandwidth.</p><p>• Rational quadratic kernel </p><formula xml:id="formula_5">), k rq α (a, b) = (1 + 1 2α a − b 2 ) −α ,</formula><p>where the kernel scale α &gt; 0 corresponds to a mixture of Gaussian kernels with a Gamma(α, 1) prior on the inverse kernel scales σ −1 .</p><p>It is interesting that both studies used a linear combination of kernels with five different kernel scales, i.e., <ref type="figure" target="#fig_1">Fig. 2a</ref> and 2c for illustration). We suspect the reason is that a single kernel k(a, b) is saturated when the distance a − b is either too large or too small compared to the kernel scale (see <ref type="figure" target="#fig_1">Fig. 2b</ref> and 2d), which may cause diminishing gradients during training. Both <ref type="bibr" target="#b20">Li et al. (2017a)</ref> and  applied penalties on the discriminator parameters but not to the MMD loss itself. Thus the saturation issue may still exist. Using a linear combination of kernels with different kernel scales may alleviate this issue but not eradicate it.</p><formula xml:id="formula_6">k rbf = 5 i=1 k rbf σi and k rq = 5 i=1 k rq αi , where σ i ∈ {1, 2, 4, 8, 16}, α i ∈ {0.2, 0.5, 1, 2, 5} (see</formula><p>Inspired by the hinge loss (see Appendix B.1), we propose a bounded RBF (RBF-B) kernel for the discriminator. The idea is to prevent D from pushing {D(x)} too far away from {D(y)} and causing saturation. For L att D in Eq. 3, the RBF-B kernel is:</p><formula xml:id="formula_7">k rbf-b σ (a, b) = exp(− 1 2σ 2 max( a − b 2 , b l )) if a, b ∈ {D(x)} or a, b ∈ {D(y)} exp(− 1 2σ 2 min( a − b 2 , b u )) if a ∈ {D(x)} and b ∈ {D(y)}<label>(6)</label></formula><p>For L rep D in Eq. 4, the RBF-B kernel is:</p><formula xml:id="formula_8">k rbf-b σ (a, b) = exp(− 1 2σ 2 max( a − b 2 , b l )) if a, b ∈ {D(y)} exp(− 1 2σ 2 min( a − b 2 , b u )) if a, b ∈ {D(x)} (7)</formula><p>where b l and b u are the lower and upper bounds. As such, a single kernel is sufficient and we set σ = 1, b l = 0.25 and b u = 4 in all experiments for simplicity and leave their tuning for future work. It should be noted that, like the case of hinge loss, the RBF-B kernel is used only for the discriminator to prevent it from being over-confident. The generator is always trained using the original RBF kernel, thus we retain the interpretation of MMD loss L mmd G as a metric.</p><p>RBF-B kernel is among many methods to address the saturation issue and stabilize MMD-GAN training. We found random sampling kernel scale, instance noise <ref type="bibr" target="#b33">(Sønderby et al. (2017)</ref>) and label smoothing <ref type="bibr" target="#b34">(Szegedy et al. (2016)</ref>; <ref type="bibr" target="#b31">Salimans et al. (2016)</ref>) may also improve the model performance and stability. However, the computational cost of RBF-B kernel is relatively low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SPECTRAL NORMALIZATION IN DISCRIMINATOR</head><p>Without any Lipschitz constraints, the discriminator D may simply increase the magnitude of its outputs to minimize the discriminator loss, causing unstable training 3 . Spectral normalization divides the weight matrix of each layer by its spectral norm, which imposes an upper bound on the magnitudes of outputs and gradients at each layer of D ). However, to estimate the spectral norm of a convolution kernel,  reshaped the kernel into a matrix. We propose a generalized power iteration method to directly estimate the spectral norm of a convolution kernel (see Appendix C for details) and applied spectral normalization to the discriminator in all experiments. In Appendix D.1, we explore using gradient penalty to impose the Lipschitz constraint <ref type="bibr" target="#b10">(Gulrajani et al. (2017)</ref>; ; ) for the proposed repulsive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we empirically evaluate the proposed 1) repulsive loss L rep D (Eq. 4) on unsupervised training of GAN for image generation tasks; and 2) RBF-B kernel to stabilize MMD-GAN training. The generalized power iteration method is evaluated in Appendix C.3. To show the efficacy of L rep D , we compared the loss functions (L rep D , L mmd G ) using Gaussian kernel (MMD-rep) with (L att D , L mmd G ) using Gaussian kernel (MMD-rbf) <ref type="bibr" target="#b20">(Li et al. (2017a)</ref>) and rational quadratic kernel (MMD-rq) ), as well as non-saturating loss <ref type="bibr" target="#b7">(Goodfellow et al. (2014)</ref>) and hinge loss <ref type="bibr" target="#b35">(Tran et al. (2017)</ref>). To show the efficacy of RBF-B kernel, we applied it to both L att D and L rep D , resulting in two methods MMD-rbf-b and MMD-rep-b. The Wasserstein loss was excluded for comparison because it cannot be directly used with spectral normalization ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENT SETUP</head><p>Dataset: The loss functions were evaluated on four datasets: 1) CIFAR-10 (50K images, 32 × 32 pixels) <ref type="bibr">(Krizhevsky &amp; Hinton</ref>  <ref type="formula">(2009)</ref>); 2) STL-10 (100K images, 48 × 48 pixels) <ref type="bibr" target="#b3">(Coates et al. (2011))</ref>; 3) CelebA (about 203K images, 64 × 64 pixels) <ref type="bibr" target="#b24">(Liu et al. (2015)</ref>); and 4) LSUN bedrooms (around 3 million images, 64×64 pixels) (Yu et al. <ref type="formula" target="#formula_3">(2015)</ref>). The images were scaled to range [−1, 1] to avoid numeric issues.</p><p>Network architecture: The DCGAN ) architecture was used with hyperparameters from  (see Appendix B.2 for details). In all experiments, batch normalization (BN) <ref type="bibr" target="#b15">(Ioffe &amp; Szegedy (2015)</ref>) was used in the generator, and spectral normalization with the generalized power iteration (see Appendix C) in the discriminator. For MMD related losses, the dimension of discriminator output layer was set to 16; for non-saturating loss and hinge loss, it was 1. In Appendix D.2, we investigate the impact of discriminator output dimension on the performance of repulsive loss.  ) and thus omitted. 3 On LSUN-bedroom, MMD-rbf and MMD-rq did not achieve reasonable results and thus are omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters:</head><p>We used Adam optimizer (Kingma &amp; Ba <ref type="formula" target="#formula_3">(2015)</ref>) with momentum parameters β 1 = 0.5, β 2 = 0.999; two-timescale update rule (TTUR) <ref type="bibr" target="#b12">(Heusel et al. (2017)</ref>) with two learning rates (ρ D , ρ G ) chosen from {1e-4, 2e-4, 5e-4, 1e-3} (16 combinations in total); and batch size 64. Fine-tuning on learning rates may improve the model performance, but constant learning rates were used for simplicity. All models were trained for 100K iterations on CIFAR-10, STL-10, CelebA and LSUN bedroom datasets, with n dis = 1, i.e., one discriminator update per generator update 4 . For MMD-rbf, the kernel scales σ i ∈ {1, √ 2, 2, 2 √ 2, 4} were used due to a better performance than the original values used in <ref type="bibr" target="#b20">Li et al. (2017a)</ref>. For MMD-rq, α i ∈ {0.2, 0.5, 1, 2, 5}. For MMD-rbf-b, MMD-rep, MMD-rep-b, a single Gaussian kernel with σ = 1 was used.</p><p>Evaluation metrics: Inception score (IS) <ref type="bibr" target="#b31">(Salimans et al. (2016)</ref>), Fréchet Inception distance (FID) <ref type="bibr" target="#b12">(Heusel et al. (2017)</ref>) and multi-scale structural similarity (MS-SSIM) <ref type="bibr">(Wang et al. (2003)</ref>) were used for quantitative evaluation. Both IS and FID are calculated using a pre-trained Inception model <ref type="bibr" target="#b34">(Szegedy et al. (2016)</ref>). Higher IS and lower FID scores indicate better image quality. MS-SSIM calculates the pair-wise image similarity and is used to detect mode collapses among images of the same class <ref type="bibr" target="#b29">(Odena et al. (2017)</ref>). Lower MS-SSIM values indicate perceptually more diverse images. For each model, 50K randomly generated samples and 50K real samples were used to calculate IS, FID and MS-SSIM. <ref type="table" target="#tab_0">Table 1</ref> shows the Inception score, FID and MS-SSIM of applying different loss functions on the benchmark datasets with the optimal learning rate combinations tested experimentally. Note that the same training setup (i.e., DCGAN + BN + SN + TTUR) was applied for each loss function. We observed that: 1) MMD-rep and MMD-rep-b performed significantly better than MMD-rbf and MMD-rbf-b respectively, showing the proposed repulsive loss L rep D (Eq. 4) greatly improved over the attractive loss L att D (Eq. 3); 2) Using a single kernel, MMD-rbf-b performed better than MMD-rbf and MMD-rq which used a linear combination of five kernels, indicating that the kernel saturation may be an issue that slows down MMD-GAN training; 3) MMD-rep-b performed comparable or better than MMD-rep on benchmark datasets where we found the RBF-B kernel managed to stabilize MMD-GAN training using repulsive loss. 4) MMD-rep and MMD-rep-b performed significantly better than the non-saturating and hinge losses, showing the efficacy of the proposed repulsive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">QUANTITATIVE ANALYSIS</head><p>Additionally, we trained MMD-GAN using the general loss L D,λ (Eq. 5) for discriminator and L mmd G (Eq. 2) for generator on the CIFAR-10 dataset. <ref type="figure">Fig. 3</ref> shows the influence of λ on the performance </p><formula xml:id="formula_9">λ = −1 λ = −0.5 λ = 0 λ = 0.5 λ = 1 λ = 2</formula><formula xml:id="formula_10">(ρ D , ρ G ), in the order of (1e-4, 1e-4), (1e-4, 2e-4),...,(1e-3, 1e-3).</formula><p>The discriminator was trained using L D,λ (Eq. 5) with λ ∈ {-1, -0.5, 0, 0.5, 1, 2}, and generator using L mmd G (Eq. 2). We use the FID&gt; 30 to indicate that the model diverged or produced poor results.</p><p>of MMD-GAN with RBF and RBF-B kernel 5 . Note that when λ = −1, the models are essentially MMD-rbf (with a single Gaussian kernel) and MMD-rbf-b when RBF and RBF-B kernel are used respectively. We observed that: 1) the model performed well using repulsive loss (i.e., λ ≥ 0), with λ = 0.5, 1 slightly better than λ = −0.5, 0, 2; 2) the MMD-rbf model can be significantly improved by simply increasing λ from −1 to −0.5, which reduces the attraction of discriminator on real sample scores; 3) larger λ may lead to more diverged models, possibly because the discriminator focuses more on expanding the real sample scores over adversarial learning; note when λ 1, the model would simply learn to expand all real sample scores and pull the generated sample scores to real samples', which is a divergent process; 4) the RBF-B kernel managed to stabilize MMD-rep for most diverged cases but may occasionally cause the FID score to rise up.</p><p>The proposed methods were further evaluated in Appendix A, C and D. In Appendix A.2, we used a simulation study to show the local stability of MMD-rep trained by gradient descent, while its global stability is not guaranteed as bad initialization may lead to trivial solutions. The problem may be alleviated by adjusting the learning rate for generator. In Appendix C.3, we showed the proposed generalized power iteration (Section 4.2) imposes a stronger Lipschitz constraint than the method in , and benefited MMD-GAN training using the repulsive loss. Moreover, the RBF-B kernel managed to stabilize the MMD-GAN training for various configurations of the spectral normalization method. In Appendix D.1, we showed the gradient penalty can also be used with the repulsive loss. In Appendix D.2, we showed that it was better to use more than one neuron at the discriminator output layer for the repulsive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">QUALITATIVE ANALYSIS</head><p>The discriminator outputs may be interpreted as a learned representation of the input samples. <ref type="figure">Fig. 4</ref> visualizes the discriminator outputs learned by the MMD-rbf and proposed MMD-rep methods on CIFAR-10 dataset using t-SNE (van der Maaten <ref type="formula" target="#formula_2">(2014)</ref>). MMD-rbf ignored the class structure in data (see <ref type="figure">Fig. 4a</ref>) while MMD-rep learned to concentrate the data from the same class and separate different classes to some extent <ref type="figure">(Fig. 4b)</ref>. This is because the discriminator D has to actively learn the data structure in order to expands the real sample scores {D(x)}. Thus, we speculate that techniques reinforcing the learning of cluster structures in data may further improve the training of MMD-GAN.</p><p>In addition, the performance gain of proposed repulsive loss (Eq. 4) over the attractive loss (Eq. 3) comes at no additional computational cost. In fact, by using a single kernel rather than a linear combination of kernels, MMD-rep and MMD-rep-b are simpler than MMD-rbf and MMD-rq. Besides, given a typically small batch size and a small number of discriminator output neurons (64 and 16 in our experiments), the cost of MMD over the non-saturating and hinge loss is marginal compared to the convolution operations.</p><p>In Appendix D.3, we provide some random samples generated by the methods in our study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>This study extends the previous work on MMD-GAN <ref type="bibr" target="#b20">(Li et al. (2017a)</ref>) with two contributions. First, we interpreted the optimization of MMD loss as a combination of attraction and repulsion processes, and proposed a repulsive loss for the discriminator that actively learns the difference among real data. Second, we proposed a bounded Gaussian RBF (RBF-B) kernel to address the saturation issue. Empirically, we observed that the repulsive loss may result in unstable training, due to factors including initialization (Appendix A.2), learning rate ( <ref type="figure">Fig. 3b)</ref> and Lipschitz constraints on the discriminator (Appendix C.3). The RBF-B kernel managed to stabilize the MMD-GAN training in many cases. Tuning the hyper-parameters in RBF-B kernel or using other regularization methods may further improve our results.</p><p>The theoretical advantages of MMD-GAN require the discriminator to be injective. The proposed repulsive loss (Eq. 4) attempts to realize this by explicitly maximizing the pair-wise distances among the real samples. <ref type="bibr" target="#b20">Li et al. (2017a)</ref> achieved the injection property by using the discriminator as the encoder and an auxiliary network as the decoder to reconstruct the real and generated samples, which is more computationally extensive than our proposed approach. On the other hand, ;  imposed a Lipschitz constraint on the discriminator in MMD-GAN via gradient penalty, which may not necessarily promote an injective discriminator.</p><p>The idea of repulsion on real sample scores is in line with existing studies. It has been widely accepted that the quality of generated samples can be significantly improved by integrating labels <ref type="bibr" target="#b29">(Odena et al. (2017)</ref>; ; Zhou et al. <ref type="formula">(2018)</ref>) or even pseudo-labels generated by k-means method <ref type="bibr" target="#b9">(Grinblat et al. (2017)</ref>) in the training of discriminator. The reason may be that the labels help concentrate the data from the same class and separate those from different classes. Using a pre-trained classifier may also help produce vivid image samples ) as the learned representations of the real samples in the hidden layers of the classifier tend to be well separated/organized and may produce more meaningful gradients to the generator.</p><p>At last, we note that the proposed repulsive loss is orthogonal to the GAN studies on designing network structures and training procedures, and thus may be combined with a variety of novel techniques. For example, the ResNet architecture <ref type="bibr" target="#b11">(He et al. (2016)</ref>) has been reported to outperform the plain DCGAN used in our experiments on image generation tasks ; <ref type="bibr" target="#b10">Gulrajani et al. (2017)</ref>) and self-attention module may further improve the results (Zhang et al. <ref type="formula">(2018)</ref>). On the other hand, <ref type="bibr" target="#b16">Karras et al. (2018)</ref> proposed to progressively grows the size of both discriminator and generator and achieved the state-of-the-art performance on unsupervised training of GANs on the CIFAR-10 dataset. Future work may explore these directions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A STABILITY ANALYSIS OF MMD-GAN</head><p>This section demonstrates that, under mild assumptions, MMD-GAN trained by gradient descent is locally exponentially stable at equilibrium. It is organized as follows. The main assumption and proposition are presented in Section A.1, followed by simulation study in Section A.2 and proof in Section A.3. We discuss the indications of assumptions on the discriminator of GAN in Section A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MAIN PROPOSITION</head><p>We consider GAN trained using the MMD loss L mmd G for generator G and either the attractive loss L att D or repulsive loss L rep D for discriminator D, listed below:</p><formula xml:id="formula_11">L mmd G = M 2 k•D (P X , P G ) = E P X [k D (x, x )] − 2E P X ,P G [k D (x, y)] + E P G [k D (y, y )]<label>(S1a)</label></formula><formula xml:id="formula_12">L att D = −L mmd G (S1b) L rep D = E P X [k D (x, x )] − E P G [k D (y, y )]<label>(S1c)</label></formula><p>where k D (a, b) = k(D(a), D(b)). Let S(P ) be the support of distribution P ; let θ G ∈ Θ G , θ D ∈ Θ D be the parameters of the generator G and discriminator D respectively. To prove that GANs trained using the minimax loss and gradient updates is locally stable at the equilibrium point (θ * D , θ * G ), Nagarajan &amp; Kolter (2017) made the following assumption: Assumption 1 (Nagarajan &amp; Kolter <ref type="formula">(2017)</ref>). P θ * G = P X and ∀x ∈ S(P X ), D θ * D (x) = 0.</p><p>For loss functions like minimax and Wasserstein, D θ D (x) may be interpreted as how plausible a sample is real. Thus at equilibrium, it may be reasonable to assume all real and generated samples are equally plausible. However, D θ * D (x) = 0 also indicates that D θ * D may have no discrimination power (see Appendix A.4 for discussion). For MMD loss in Eq. S1, D θ D (x)| x∼P may be interpreted as a learned representation of the distribution P . As long as two distributions P and Q match, M 2 k•D θ D (P, Q) = 0. On the other hand, D θ D (x) = 0 is a minima solution for D but D is trained to find local maxima. Thus in contrast to Assumption 1, we assume Assumption 2. For GANs using MMD loss in Eq. S1, and random initialization on parameters, at equilibrium, D θ * D (x) is injective on S(P X ) S(P θ * G ).</p><p>Assumption 2 indicates that D θ * D (x) is not constant almost everywhere. We use a simulation study in Section A.2 to show that D θ * D (x) = 0 does not hold in general for MMD loss. Based on Assumption 2, we propose the following proposition and prove it in Appendix A.3: Proposition 1. If there exists θ * G ∈ Θ G such that P θ * G = P X , then GANs with MMD loss in Eq. S1 has equilibria (θ * G , θ D ) for any θ D ∈ Θ D . Moreover, the model trained using gradient descent methods is locally exponentially stable at (θ * G , θ D ) for any θ D ∈ Θ D .</p><p>There may exist non-realizable cases where the mapping between P Z and P X cannot be represented by any generator G θ G with θ G ∈ Θ G . In Section A.2, we use a simulation study to show that both the attractive MMD loss L att D (Eq. S1b) and the proposed repulsive loss L rep D (Eq. S1c) may be locally stable and leave the proof for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SIMULATION STUDY</head><p>In this section, we reused the example from <ref type="bibr" target="#b27">Nagarajan &amp; Kolter (2017)</ref> to show that GAN trained using the MMD loss in Eq. S1 is locally stable. Consider a two-parameter MMD-GAN with uniform latent distribution P Z over [−1, 1], generator G(z) = w 1 z, discriminator D(x) = w 2 x 2 , and Gaussian kernel k rbf 0.5 . The MMD-rbf model (L mmd G and L att D from Eq. S1b) and the MMD-rep model (L mmd G and L rep D from Eq. S1c) were tested. Each model was applied to two cases:</p><p>(a) the data distribution P X is the same as P Z , i.e., uniform over [−1, 1], thus P X is realizable; <ref type="figure" target="#fig_0">Figure S1</ref>: Streamline plots of MMD-GAN using the MMD-rbf and the MMD-rep model on distributions: P Z = U(−1, 1), P X = U(−1, 1) or P X = N (0, 1). In (a) and (b), the equilibria satisfying P G = P X lie on the line w 1 = 1. In (c), the equilibrium lies around point (1.55, 0.74); in (d), it is around (1.55, 0.32).</p><formula xml:id="formula_13">(a) MMD-rbf, PX = U(−1, 1) (b) MMD-rep, PX = U(−1, 1) (c) MMD-rbf, PX = N (0, 1) (d) MMD-rep, PX = N (0, 1)</formula><p>(b) P X is standard Gaussian, thus non-realizable for any w 1 ∈ R. <ref type="figure" target="#fig_0">Fig. S1</ref> shows that MMD-GAN are locally stable in both cases and D θ * D (x) = 0 does not hold in general for MMD loss. However, MMD-rep may not be globally stable for the tested cases: initialization of (w 1 , w 2 ) in some regions may lead to the trivial solution w 2 = 0 (see <ref type="figure" target="#fig_0">Fig. S1b</ref> and S1d). We note that by decreasing the learning rate for G, the area of such regions decreased. At last, it is interesting to note that both MMD-rbf and MMD-rep had the same nontrivial solution w 1 ≈ 1.55 for generator in the non-realizable cases (see <ref type="figure" target="#fig_0">Fig. S1c</ref> and S1d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 PROOF OF PROPOSITION 1</head><p>This section divides the proof for Proposition 1 into two parts. First, we show that GAN with the MMD loss in Eq. S1 has equilibria for any parameter configuration of discriminator D; second, we prove the model is locally exponentially stable. For convenience, we consider the general form of discriminator loss in Eq. 5:</p><formula xml:id="formula_14">L D,λ = λE P X [k D (x, x )] − (λ − 1)E P X ,P G [k D (x, y)] − E P G [k D (y, y )]<label>(S2)</label></formula><p>which has L att D and L rep D as the special cases when λ equals −1 and 1 respectively. Consider real data X r ∼ P X , latent variable Z ∼ P Z and generated variable Y g = G θ G (Z). Let x r , z, y g be their samples. Denote ∇ a b = ∂a ∂b ;</p><p>.</p><formula xml:id="formula_15">θ D = −∇ L D θ D , . θ G = −∇ L G θ G ; d g = D(G(z)), d r = D(x r )</formula><p>where L D and L G are the losses for D and G respectively. Assume an isotropic stationary kernel k(a, b) = k I ( a − b ) (Genton <ref type="formula">(2002)</ref>) is used in MMD. We first show:</p><p>Proposition 1 (Part 1). If there exists θ * G ∈ Θ G such that P θ * G = P X , the GAN with the MMD loss in Eq. S1a and Eq. S2 has equilibria (θ * G , θ D ) for any θ D ∈ Θ D .</p><formula xml:id="formula_16">Proof. Denote e i,j = a i − b j and ∇ k ei,j = ∂k(a i ,b j ) ∂e</formula><p>where k is the kernel of MMD. The gradients of MMD loss are</p><formula xml:id="formula_17">. θ D = (λ − 1)E P X ,P θ G [∇ k er,g ∇ er,g θ D ] − λE P X [∇ k er1,r2 ∇ er1,r2 θ D ] + E P θ G [∇ k eg1,g2 ∇ eg1,g2 θ D ] (S3a) . θ G = 2E P θ G ,P X [∇ k eg,r ∇ dg xg ∇ xg θ G ] − E P θ G [∇ k eg1,g2 (∇ dg1 xg1 ∇ xg1 θ G − ∇ dg2 xg2 ∇ xg2 θ G )]<label>(S3b)</label></formula><p>Note that, given i.i.d. drawn samples X = {x i } n i=1 ∼ P X and Y = {y i } n i=1 ∼ P G , an unbiased estimator of the squared MMD is <ref type="bibr" target="#b8">(Gretton et al. (2012)</ref>)</p><formula xml:id="formula_18">M 2 k (P X , P G ) = 1 n(n − 1) n i =j k(x i , x j )+ 1 n(n − 1) n i =j k(y i , y j )− 2 n(n − 1) n i =j k(x i , y j ) (S4)</formula><p>At equilibrium, consider a sequence of N samples</p><formula xml:id="formula_19">d ri = d gi = d i with N → ∞, we have . θ D ∝ (λ − 1) i =j ∇ k ei,j ∇ ei,j θ D − λ i =j ∇ k ei,j ∇ ei,j θ D + i =j ∇ k ei,j ∇ ei,j θ D = 0 . θ * G ∝ − i =j ∇ k ei,j (∇ di xi ∇ xi θ G − ∇ dj xj ∇ xj θ G ) + 2 i =j ∇ k ei,j ∇ di xi ∇ xi θ G = i =j ∇ k ei,j (∇ di xi ∇ xi θ G + ∇ dj xj ∇ xj θ G ) = 0</formula><p>where for . θ * G we have used the fact that for each term in the summation, there exists an term with i, j reversed and ∇ k ei,j = −∇ k ej,i thus the summation is zero. Since we have not assumed the status of θ D , . θ D = 0 for any θ D ∈ Θ D .</p><p>We proceed to prove the model stability. First, following Theorem 5 in <ref type="bibr" target="#b8">Gretton et al. (2012)</ref> and Theorem 4 in <ref type="bibr" target="#b20">Li et al. (2017a)</ref>, it is straightforward to see:</p><formula xml:id="formula_20">Lemma A.1. Under Assumption 2, M 2 k•D θ D</formula><p>(P X , P θ G ) ≥ 0 with the equality if and only if P X = P θ G .</p><p>Lemma A.1 and Proposition 1 (Part 1) state that at equilibrium P θ * G = P X , every discriminator D θ D and kernel k will give M 2 k•D θ D (P θ * G , P X ) = 0, thus no discriminator can distinguish the two distributions. On the other hand, we cite Theorem A.4 from Nagarajan &amp; Kolter <ref type="formula">(2017)</ref>: <ref type="formula">(2017)</ref>). Consider a non-linear system of parameters (θ, γ): θ = h 1 (θ, γ),γ = h 2 (θ, γ) with an equilibrium point at (0, 0). Let there exist such that ∀γ ∈ B (0), (0, γ) is an equilibrium. If J = ∂h1(θ,γ) ∂θ (0,0) is a Hurwitz matrix, the non-linear system is exponentially stable.</p><formula xml:id="formula_21">Lemma A.2 (Nagarajan &amp; Kolter</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we can prove:</head><p>Proposition 1 (Part 2). At equilibrium P θ * G = P X , the GAN trained using MMD loss and gradient descent methods is locally exponentially stable at (θ * G , θ D ) for any θ D ∈ Θ D .</p><p>Proof. Inspired by Nagarajan &amp; Kolter (2017), we first derive the Jacobian of the system</p><formula xml:id="formula_22">J = J DD J DG J GD J GG ∂ . θ T D /∂θ D ∂ . θ T D /∂θ G ∂ . θ T G /∂θ D ∂ . θ T G /∂θ G Denote ∆ a b = ∂ 2 a</formula><p>∂b 2 and ∆ a bc = ∂ 2 a ∂b∂c . Based on Eq. S3, we have</p><formula xml:id="formula_23">J DD =(λ − 1)E P X ,P θ G [(∆ er,g θ D ) T ⊗ (∇ k er,g ) T + (∇ er,g θ D ) T ∆ k er,g ∇ er,g θ D ] (S5a) − λE P X [(∆ er1,r2 θ D ) T ⊗ (∇ k er1,r2 ) T + (∇ er1,r2 θ D ) T ∆ k er1,r2 ∇ Proof. Without loss of generality, we consider D(x) = W 2 h(x) + b 2 and h(x) = f (W 1 x + b 1 ), where W 1 ∈ R d h ×d , W 2 , b 1 , b 2</formula><p>are model weights and biases, f is an activation function satisfying Assumption 3. For x ∈ S, since D(x) = c, we have h(x) ∈ null(W 2 ). Furthermore:</p><p>(a) If rank(W 1 ) &lt; d, for any δx ∈ null(W 1 ), h(x + δx) ∈ null(W 2 ).</p><p>(b) If rank(W 1 ) = d = d h , the problem h(x + δx) = k · h(x) has unique solution for any k ∈ R as long as k · h(x) is within the output range of f .</p><p>(c) If rank(W 1 ) = d &lt; d h , let U and V be two basis matrices of R d h such that W 1 x = U x T 0 T T and any vector in null(W 2 ) can be represented as V z T 0 T T , wherê x ∈ R d h ×d , z ∈ R d h ×n and n is the nullity of W 2 . Let the projected support beŜ. Thus, ∀x ∈Ŝ, there exists z such that f (U</p><formula xml:id="formula_24">x T 0 T T + b 1 ) = V z T z T c T with z c = 0.</formula><p>Consider the Jacobian:</p><formula xml:id="formula_25">J = ∂ z T z T c T ∂ x T 0 T T = V −1 ∇ΣU (S6) where ∇Σ = diag( d f d ai ) and a = [a i ] d h i=1</formula><p>is the input to activation, or pre-activations. SinceŜ is continuous and compact, it has infinite number of boundary points {x b } for d &gt; 1. Consider one boundary pointx b and its normal line δx b . Let &gt; 0 be a small scalar such thatx</p><formula xml:id="formula_26">b − δx b ∈ S andx b + δx b ∈Ŝ.</formula><p>• For linear activation, ∇Σ = I and J is constant. Then z c remains 0 forx b + δx b , i.e., there exists z such that h(x + δx) ∈ null(W 2 ).</p><p>• For nonlinear activations, assume f has N discontinuities.</p><p>Since U x T 0 T T + b 1 = c has unique solution for any vector c, the boundary points {x b } cannot yield pre-activations {a b } that all lie on the discontinuities in any of the d h directions. Though we might need to sample d N +1</p><p>h points in the worst case to find an exception, there are infinite number of exceptions. Letx b be a sample where {a b } does not lie on the discontinuities in any direction. Because f is continuous, z c remains 0 forx b + δx b , i.e., there exists z such that h(x + δx) ∈ null(W 2 ).</p><p>In conclusion, we can always find δx such that x + δx / ∈ S and D(x + δx) = c.</p><p>Proposition 2 indicates that if D θ * D (x) = 0 for x ∈ S(P X ) S(P θ * G ), D θ * D cannot discriminate against fake samples with distortions to the original data. In contrast, Assumption 2 and Lemma A.1 guarantee that, at equilibrium, the discriminator trained using MMD loss function is effective against such fake samples given a large number of i.i.d. test samples <ref type="bibr" target="#b8">(Gretton et al. (2012)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SUPPLEMENTARY METHODOLOGY B.1 REPRESENTATIVE LOSS FUNCTIONS IN LITERATURE</head><p>Several loss functions have been proposed to quantify the difference between real and generated sample scores, including: (assume linear activation is used at the last layer of D)</p><p>• The Minimax loss <ref type="bibr" target="#b7">(Goodfellow et al. (2014)</ref> <ref type="figure">G(z)</ref>))] and L G = −L D , which can be derived from the JensenShannon (JS) divergence between P X and the model distribution P G .</p><formula xml:id="formula_27">): L D = E P X [Softplus(−D(x))] + E P Z [Softplus(D(</formula><p>• The non-saturating loss <ref type="bibr" target="#b7">(Goodfellow et al. (2014)</ref>), which is a variant of the minimax loss with the same L D and L G = E P Z [Softplus(−D(G(z)))].</p><p>• The Hinge loss <ref type="bibr" target="#b35">(Tran et al. (2017)</ref>):</p><formula xml:id="formula_28">L D = E P X [ReLU(1 − D(x))] + E P Z [ReLU(1 + D(G(z)))], L G = E P Z [−D(G(z))]</formula><p>, which is notably known for usage in support vector machines and is related to the total variation (TV) distance <ref type="bibr" target="#b28">(Nguyen et al. (2009)</ref>).</p><p>• The Wasserstein loss ; <ref type="bibr" target="#b10">Gulrajani et al. (2017)</ref>), which is derived from the Wasserstein distance between P X and P G :</p><formula xml:id="formula_29">L G = − E P Z [D(G(z))], L D = E P Z [D(G(z))] − E P X [D(x)]</formula><p>, where D is subject to some Lipschitz constraint.</p><p>• The maximum mean discrepancy (MMD) <ref type="bibr" target="#b20">(Li et al. (2017a)</ref>; ), as described in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 NETWORK ARCHITECTURE</head><p>For unsupervised image generation tasks on CIFAR-10 and STL-10 datasets, the DCGAN architecture from  was used. For CelebA and LSUN bedroom datasets, we added more layers to the generator and discriminator accordingly. See <ref type="table" target="#tab_0">Table S1</ref> and S2 for details. (a) Generator</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C POWER ITERATION FOR CONVOLUTION OPERATION</head><p>This section introduces the power iteration for convolution operation (PICO) method to estimate the spectral norm of a convolution kernel, and compare PICO with the power iteration for matrix (PIM) method used in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 METHOD FORMATION</head><p>For a weight matrix W , the spectral norm is defined as σ(W ) = max v 2 ≤1 W v 2 . The PIM is used to estimate σ(W ) ), which iterates between two steps:</p><formula xml:id="formula_30">1. Update u = W v/ W v 2 ; 2. Update v = W T u/ W T u 2 .</formula><p>The convolutional kernel W c is a tensor of shape h × w × c in × c out with h, w the receptive field size and c in , c out the number of input/output channels. To estimate σ(W c ),  reshaped it into a matrix W rs of shape (hwc in ) × c out and estimated σ(W rs ).</p><p>We propose a simple method to calculate W c directly based on the fact that convolution operation is linear. For any linear map T : R m → R n , there exists matrix W L ∈ R n×m such that y = T (x) can be represented as y = W L x. Thus, we may simply substitute W L = ∂y ∂x in the PIM method to estimate the spectral norm of any linear operation. In the case of convolution operation * , there exist <ref type="formula" target="#formula_7">(2016)</ref>). Thus, similar to PIM, PICO iterates between the following two steps:</p><formula xml:id="formula_31">doubly block circulant matrix W dbc such that u = W c * v = W dbc v. Consider v = W T dbc u = [ ∂u ∂v ] T u which is essentially the transpose convolution of W c on u (Dumoulin &amp; Visin</formula><formula xml:id="formula_32">1. Update u = W c * v/ W c * v 2 ; 2. Do transpose convolution of W c on u to getv; update v =v/ v 2 .</formula><p>Similar approaches have been proposed in <ref type="bibr" target="#b36">Tsuzuku et al. (2018)</ref> and <ref type="bibr">Virmaux &amp; Scaman (2018)</ref> from different angles, which we were not aware during this study. In addition, <ref type="bibr" target="#b32">Sedghi et al. (2019)</ref> proposes to compute the exact singular values of convolution kernels using FFT and SVD. In spectral normalization, only the first singular value is concerned, making the power iteration methods PIM and PICO more efficient than FFT and thus preferred in our study. However, we believe the exact method FFT+SVD <ref type="bibr" target="#b32">(Sedghi et al. (2019)</ref>) may eventually inspire more rigorous regularization methods for GAN.</p><p>The proposed PICO method estimates the real spectral norm of a convolution kernel at each layer, thus enforces an upper bound on the Lipschitz constant of the discriminator D. Denote the upper bound as LIP PICO . In this study, Leaky ReLU (LReLU) was used at each layer of D, thus LIP PICO ≈ 1 (Virmaux &amp; Scaman <ref type="formula">(2018)</ref>). In practice, however, PICO would often cause the norm of the signal passing through D to decrease to zero, because at each layer,</p><p>• the signal hardly coincides with the first singular-vector of the convolution kernel; and</p><p>• the activation function LReLU often reduces the norm of the signal.</p><p>Consequently, the discriminator outputs tend to be similar for all the inputs. To compensate the loss of norm at each layer, the signal is multiplied by a constant C after each spectral normalization. This essentially enlarges LIP PICO by C K where K is the number of layers in the DCGAN discriminator. For all experiments in Section 5, we fixed C = 1 0.55 ≈ 1.82 as all loss functions performed relatively well empirically. In Appendix Section C.3, we tested the effects of coefficient C K on the performance of several loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 COMPARISON TO PIM</head><p>PIM ) also enforces an upper bound LIP PIM on the Lipschitz constant of the discriminator D. Consider a convolution kernel W c with receptive field size h × w and stride s. Let σ PICO and σ PIM be the spectral norm estimated by PICO and PIM respectively. We empirically </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 EXPERIMENTS</head><p>In this section, we empirically evaluate the effects of coefficient C K on the performance of PICO and compare PICO against PIM using several loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment setup:</head><p>We used a similar setup as Section 5.1 with the following adjustments. Four loss functions were tested: hinge, MMD-rbf, MMD-rep and MMD-rep-b. Either PICO or PIM was used at each layer of the discriminator. For PICO, five coefficients C K were tested: 16, 32, 64, 128 and 256 (note this is the overall coefficient for K layers; K = 8 for CIFAR-10 and STL-10; K = 10 for CelebA and LSUN-bedroom; see Appendix B.2). FID was used to evaluate the performance of each combination of loss function and power iteration method, e.g., hinge + PICO with C K = 16.</p><p>Results: For each combination of loss function and power iteration method, the distribution of FID scores over 16 learning rate combinations is shown in <ref type="figure" target="#fig_1">Fig. S2</ref>. We separated well-performed learning rate combinations from diverged or poorly-performed ones using a threshold τ as the diverged cases often had non-meaningful FID scores. The boxplot shows the distribution of FID scores for goodperformed cases while the number of diverged or poorly-performed cases was shown above each box if it is non-zero. <ref type="figure" target="#fig_1">Fig. S2</ref> shows that: 1) When PICO was used, the hinge, MMD-rbf and MMD-rep methods were sensitive to the choices of C K while MMD-rep-b was robust. For hinge and MMD-rbf, higher C K may result in better FID scores and less diverged cases over 16 learning rate combinations. For MMD-rep, higher C K may cause more diverged cases; however, the best FID scores were often achieved with C K = 64 or 128.</p><p>2) For CIFAR-10, STL-10 and CelebA datasets, PIM performed comparable to PICO with C K = 128 or 256 on four loss functions. For LSUN bedroom dataset, it is likely that the performance of PIM corresponded to that of PICO with C K &gt; 256. This implies that PIM may result in a relatively loose Lipschitz constraint on deep convolutional networks.</p><p>3) MMD-rep-b performed generally better than hinge and MMD-rbf with tested power iteration methods and hyper-parameter configurations. Using PICO, MMD-rep also achieved generally better FID scores than hinge and MMD-rbf. This implies that, given a limited computational budget, the proposed repulsive loss may be a better choice than the hinge and MMD loss for the discriminator.   <ref type="table" target="#tab_3">Table S3</ref> shows the best FID scores obtained by PICO and PIM where C K was fixed at 128 for hinge and MMD-rbf, and 64 for MMD-rep and MMD-rep-b. For hinge and MMD-rbf, PICO performed significantly better than PIM on the LSUN-bedroom dataset and comparably on the rest datasets. For MMD-rep and MMD-rep-b, PICO achieved consistently better FID scores than PIM.</p><p>However, compared to PIM, PICO has a higher computational cost which roughly equals the additional cost incurred by increasing the batch size by two <ref type="bibr" target="#b36">(Tsuzuku et al. (2018)</ref>). This may be problematic when a small batch has to be used due to memory constraints, e.g., when handling high resolution images on a single GPU. Thus, we recommend using PICO when the computational cost is less of a concern.</p><p>D SUPPLEMENTARY EXPERIMENTS D.1 LIPSCHITZ CONSTRAINT VIA GRADIENT PENALTY Gradient penalty has been widely used to impose the Lipschitz constraint on the discriminator arguably since Wasserstein GAN <ref type="bibr" target="#b10">(Gulrajani et al. (2017)</ref>). This section explores whether the proposed repulsive loss can be applied with gradient penalty.</p><p>Several gradient penalty methods have been proposed for MMD-GAN.  penalized the gradient norm of witness function f w (z) = E P X [k D (z, x)] − E P G [k D (z, y)] w.r.t. the interpolated sample z = ux + (1 − u)y to one, where u ∼ U(0, 1) 9 . More recently,  proposed to impose the Lipschitz constraint on the mapping φ • D directly and derived the Scaled MMD (SMMD) as SM k (P, Q) = σ µ,k,λ M k (P, Q), where the scale σ µ,k,λ incorporates gradient and smooth penalties. Using the Gaussian kernel and measure µ = P X leads to the discriminator loss:</p><formula xml:id="formula_33">L SMMD D = L att D 1 + λE P X [ ∇D(x) 2 F ]<label>(S7)</label></formula><p>We apply the same formation of gradient penalty to the repulsive loss:</p><formula xml:id="formula_34">L rep-gp D = L rep D − 1 1 + λE P X [ ∇D(x) 2 F ]<label>(S8)</label></formula><p>where the numerator L rep D − 1 ≤ 0 so that the discriminator will always attempt to minimize both L rep D and the Frobenius norm of gradients ∇D(x) w.r.t. real samples. Meanwhile, the generator is trained using the MMD loss L mmd G (Eq. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment setup:</head><p>The gradient-penalized repulsive loss L rep-gp D (Eq. S8, referred to as MMD-repgp) was evaluated on the CIFAR-10 dataset. We found λ = 10 in  too restrictive 9 Empirically, we found this gradient penalty did not work with the repulsive loss. The reason may be the attractive loss L att D (Eq. 3) is symmetric in the sense that swapping P X and PG results in the same loss; while the repulsive loss is asymmetric and naturally results in varying gradient norms in data space. and used λ = 0.1 instead. Same as , the output dimension of discriminator was set to one. Since we entrusted the Lipschitz constraint to the gradient penalty, spectral normalization was not used. The rest experiment setup can be found in Section 5.1.  <ref type="figure" target="#fig_1">Figure S2</ref>: Boxplot of the FID scores for 16 learning rate combinations on four datasets: (a) CIFAR-10, (b) STL-10, (c) CelebA, (d) LSUN-bedroom, using four loss functions, Hinge, MMD-rbf, MMD-rep and MMD-rmb. Spectral normalization was applied to discriminator with two power iteration methods: PICO and PIM. For PICO, five coefficients C K were tested: 16, 32, 64, 128, and 256. A learning rate combination was considered diverged or poorly-performed if the FID score exceeded a threshold τ , which is 50, 80, 50, 90 for CIFAR-10, STL-10, CelebA and LSUN-bedroom respectively. The box quartiles were plotted based on the cases with FID &lt; τ while the number of diverged or poorly-performed cases (out of 16 learning rate combinations) was shown above each box if it is non-zero. We introduced τ because the diverged cases often had arbitrarily large and non-meaningful FID scores. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the gradient directions of each loss on the real sample scores {D(x)} ("r" nodes) and generated sample scores {D(y)} ("g" nodes). The blue arrows stand for attraction and the orange arrows for repulsion. When L mmd G is paired with L att D , the gradient directions of L mmd G on {D(y)} can be obtained by reversing the arrows in (a), thus are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Gaussian kernels {k rbf σi (a, b)} and their mean as a function of e = a − b , where σ i were used in our experiments; (b) derivatives of {k rbf σi (a, b)} in (a); (c) rational quadratic kernel {k rq αi (a, b)} and their mean, where α i ∈ {0.2, 0.5, 1, 2, 5}; (d) derivatives of {k rq αi (a, b)} in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>MMD-GAN trained using a single RBF kernel in L D,λ λ = −1 λ = −0.5 λ = MMD-GAN trained using and a single RBF-B kernel in L D,λ Figure 3: FID scores of MMD-GAN using (a) RBF kernel and (b) RBF-B kernel in L D,λ on CIFAR-10 dataset for 16 learning rate combinations. Each color bar represents the FID score using a learning rate combination</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t-SNE visualization of discriminator outputs {D(x)} learned by (a) MMD-rbf and (b) MMD-rep for 2560 real samples from the CIFAR-10 dataset, colored by their class labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>MMD-rbf-b (e) MMD-rep (f) MMD-rep-b Figure S4: Image generation using different loss functions on 64 × 64 LSUN bedroom dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Inception score (IS), Fréchet Inception distance (FID) and multi-scale structural similarity (MS-SSIM) on image generation tasks using different loss functions</figDesc><table><row><cell>Methods 1</cell><cell cols="2">CIFAR-10</cell><cell cols="2">STL-10</cell><cell cols="2">CelebA 2</cell><cell cols="2">LSUN-bedrom 2</cell></row><row><cell></cell><cell>IS</cell><cell>FID</cell><cell>IS</cell><cell>FID</cell><cell>FID</cell><cell>MS-SSIM</cell><cell>FID</cell><cell>MS-SSIM</cell></row><row><cell>Real data</cell><cell cols="4">11.31 2.09 26.37 2.10</cell><cell>1.09</cell><cell>0.2678</cell><cell>1.24</cell><cell>0.0915</cell></row><row><cell>Non-saturating</cell><cell cols="5">7.39 23.23 8.25 48.53 10.64</cell><cell>0.2895</cell><cell>23.66</cell><cell>0.1027</cell></row><row><cell>Hinge</cell><cell cols="5">7.33 23.46 8.24 49.44 8.60</cell><cell>0.2894</cell><cell>16.73</cell><cell>0.0946</cell></row><row><cell>MMD-rbf 3</cell><cell cols="5">7.05 28.38 8.13 57.52 13.03</cell><cell>0.2937</cell><cell></cell></row><row><cell>MMD-rq 3</cell><cell cols="5">7.22 27.00 8.11 54.05 12.74</cell><cell>0.2935</cell><cell></cell></row><row><cell>MMD-rbf-b</cell><cell cols="5">7.18 25.25 8.07 51.86 10.09</cell><cell>0.3090</cell><cell>32.29</cell><cell>0.1001</cell></row><row><cell>MMD-rep</cell><cell cols="5">7.99 16.65 9.36 36.67 7.20</cell><cell>0.2761</cell><cell>16.91</cell><cell>0.0901</cell></row><row><cell>MMD-rep-b</cell><cell cols="5">8.29 16.21 9.34 37.63 6.79</cell><cell>0.2659</cell><cell>12.52</cell><cell>0.0908</cell></row></table><note>1 The models here differ only by the loss functions and dimension of discriminator outputs. See Section 5.1.2 For CelebA and LSUN-bedroom, IS is not meaningful</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Thomas Unterthiner, Bernhard Nessler, Calvin Seward, Günter Klambauer, Martin Heusel, Hubert   Ramsauer, and Sepp Hochreiter. Coulomb GANs: Provably optimal Nash equilibria via potential fields. In ICLR, 2018.Laurens van der Maaten. Accelerating t-SNE using tree-based algorithms. J. Mach.Learn. Res., 15:  3221-3245, 2014.    </figDesc><table><row><cell>Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and</cell></row><row><cell>efficient estimation. In NIPS, 2018.</cell></row><row><cell>Zhou Wang, Eero. P. Simoncelli, and Alan. C. Bovik. Multiscale structural similarity for image</cell></row><row><cell>quality assessment. In Asilomar Conference on Signals, Systems &amp; Computers, volume 2, pp.</cell></row><row><cell>1398-1402, 2003.</cell></row><row><cell>Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a</cell></row><row><cell>large-scale image dataset using deep learning with humans in the loop. 2015. arXiv:1506.03365.</cell></row><row><cell>Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative</cell></row><row><cell>adversarial networks, 2018. arXiv:1805.08318.</cell></row></table><note>Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang, and Yong Yu. Activation maximization generative adversarial nets. In ICLR, 2018. Jun-Yan. Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, pp. 2242-2251, 2017.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table S1</head><label>S1</label><figDesc></figDesc><table><row><cell>: DCGAN models for image generation on CIFAR-10 (h = w = 4, H = W = 32) and</cell></row><row><cell>STL-10 (h = w = 6, H = W = 48) datasets. For non-saturating loss and hinge loss, s = 1; for</cell></row><row><cell>MMD-rand, MMD-rbf, MMD-rq, s = 16.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table S3 :</head><label>S3</label><figDesc>Fréchet Inception distance (FID) on image generation tasks using spectral normalization with two power iteration methods PICO and PIM PICO . Thus, LIP PIM is indefinite and may vary during training. In deep convolutional networks, PIM could potentially result in a very loose constraint on the Lipschitz constant of the network. In Appendix Section C.3, we experimentally compare the performance of PICO and PIM with several loss functions.</figDesc><table><row><cell>Methods</cell><cell>C K in PICO</cell><cell cols="2">CIFAR-10 PIM PICO PIM PICO PIM PICO PIM STL-10 CelebA LSUN-bedrom PICO</cell></row><row><cell>Hinge</cell><cell>128</cell><cell cols="2">23.60 22.89 47.10 47.24 10.02 9.08 27.38 17.20</cell></row><row><cell>MMD-rbf</cell><cell>128</cell><cell cols="2">26.56 26.50 53.17 54.23 13.06 12.81</cell></row><row><cell>MMD-rep</cell><cell>64</cell><cell>19.98 17.00 40.40 37.15 8.51</cell><cell>6.81 74.03 16.01</cell></row><row><cell>MMD-rep-b</cell><cell>64</cell><cell>18.24 16.65 39.78 37.31 7.09</cell><cell>6.42 20.12 11.22</cell></row><row><cell cols="4">found 8 that σ −1 PIM varies in the range [σ −1 PICO , kernel of size 3 × 3 and stride 1, σ −1 PIM may vary from σ −1 √ hw s σ −1 PICO ], depending on the kernel W c . For a typical PICO to 3σ −1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table S4</head><label>S4</label><figDesc></figDesc><table><row><cell cols="3">: Inception score (IS) and</cell></row><row><cell cols="3">Fréchet Inception distance (FID) on</cell></row><row><cell cols="3">CIFAR-10 dataset using gradient</cell></row><row><cell cols="3">penalty and different loss functions</cell></row><row><cell>Methods 1</cell><cell>IS</cell><cell>FID</cell></row><row><cell>Real data</cell><cell cols="2">11.31 2.09</cell></row><row><cell>SMMDGAN 2</cell><cell>7.0</cell><cell>31.5</cell></row><row><cell>SN-SMMDGAN 2</cell><cell>7.3</cell><cell>25.0</cell></row><row><cell>MMD-rep-gp</cell><cell cols="2">7.26 23.01</cell></row><row><cell cols="3">1 All methods used the same DCGAN ar-</cell></row><row><cell>chitecture.</cell><cell></cell><cell></cell></row><row><cell cols="3">2 Results from Arbel et al. (2018) Table 1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S5</head><label>S5</label><figDesc></figDesc><table><row><cell cols="4">: Inception score (IS), Fréchet</cell></row><row><cell cols="4">Inception distance (FID) on CIFAR-10</cell></row><row><cell cols="4">dataset using MMD-rep and different di-</cell></row><row><cell cols="3">mensions of discriminator outputs</cell><cell></cell></row><row><cell>Methods</cell><cell>C K</cell><cell>IS</cell><cell>FID</cell></row><row><cell>Real data</cell><cell></cell><cell cols="2">11.31 2.09</cell></row><row><cell>MMD-rep-1</cell><cell>64</cell><cell cols="2">7.43 22.43</cell></row><row><cell>MMD-rep-4</cell><cell>64</cell><cell cols="2">7.81 17.87</cell></row><row><cell>MMD-rep-16</cell><cell>32</cell><cell cols="2">8.20 16.99</cell></row><row><cell>MMD-rep-64</cell><cell>32</cell><cell cols="2">8.08 15.65</cell></row><row><cell>MMD-rep-256</cell><cell>32</cell><cell cols="2">7.96 16.61</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The weights for the three terms in L D,λ sum up to zero. This is to make sure the ∂L D,λ /∂θD is zero at equilibrium P X = PG, where θD is the parameters of D.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that training stability is different from the local stability considered in Appendix A. Training stability often refers to the ability of model converging to a desired state measured by some criterion. Local stability means that if a model is initialized sufficiently close to an equilibrium, it will converge to the equilibrium.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Increasing or decreasing n dis may improve the model performance in some cases, but it has significant impact on the computation cost. For simple and fair comparison, we set n dis = 1 in all cases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For λ &lt; 0, the RBF-B kernel in Eq. 6 was used in L D,λ .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">This include many commonly used activations like linear, sigmoid, tanh, ReLU and ELU. 7 For distributions with semi-infinite or infinite support, we consider the effective or truncated support S (P ) = {x ∈ X |P (x) ≥ }, where &gt; 0 is a small scalar. This is practical, e.g., univariate Gaussian has support in (−∞, +∞) yet a sample five standard deviations away from the mean is unlikely to be valid.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">This was obtained by optimizing σPICO/σPIM w.r.t. a variety of randomly initialized kernel Wc. Both gradient descent and Adam methods were tested with a small learning rate 1e −5 so that the error of spectral norm estimation may be ignored at each iteration.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Wei Wang is fully supported by the Ph.D. scholarships of The University of Melbourne. This work is partially funded by Australian Research Council grant DP150103512 and undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. The Facility was established with the assistance of LIEF Grant LE170100200.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>semidefinite. The eigenvectors of J GG corresponding to zero eigenvalues form null(J GG ). There may exist small distortion δθ G ∈ null(J GG ) such that P θ * G +δθ G = P θ * G . That is, P θ * G is locally constant along some directions in the parameter space of G. As a result, null(J GG ) ⊆ null(J DG ) because varying θ * G along these directions has no effect on D. Following Lemma C.3 of Nagarajan &amp; Kolter (2017), we consider eigenvalue decomposition</p><p>. Thus, the projections γ G = T G θ G are orthogonal to null(J GG ). Then, the Jacobian corresponding to the projected system has the form J = [0, J DG ; 0, J GG ] with block J DG = T D J DG T T G and J GG = T G J GG T T G , where J GG is negative definite. Moreover, on all directions exclude those described by J GG , the system is surrounded by a neighborhood of equilibia at least locally. According to Lemma A.2, the system is exponentially stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 DISCUSSION ON ASSUMPTION 1</head><p>This section shows that constant discriminator output</p><p>D may have no discrimination power. First, we make the following assumptions: Assumption 3. 1. D is a multilayer perceptron where each layer l can be factorized into an affine transform and an element-wise activation function f l . 2. Each activation function f l ∈ C 0 ; furthermore, f l has a finite number of discontinuities and f l ∈ C 06 . 3. Input data to D is continuous and its support S is compact in R d with non-zero measure in each dimension and d &gt; 1 7 .</p><p>Based on Assumption 3, we have the following proposition:</p><p>Proposition 2. If ∀x ∈ S, D(x) = c, where c is constant, then there always exists distortion δx such that x + δx ∈ S and D(x + δx) = c.</p><p>Results: <ref type="table">Table S4</ref> shows that the proposed repulsive loss can be used with gradient penalty to achieve reasonable results on CIFAR-10 dataset. For comparison, we cited the Inception score and FID for Scaled MMD-GAN (SMMDGAN) and Scaled MMD-GAN with spectral normalization (SN-SMMDGAN) from . Note that SMMDGAN and SN-SMMDGAN used the same DCGAN architecture as MMD-rep-gp, but were trained for 150k generator updates and 750k discriminator updates, much more than that of MMD-rep-gp (100k for both G and D). Thus, the repulsive loss significantly improved over the attractive MMD loss for discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 OUTPUT DIMENSION OF DISCRIMINATOR</head><p>In this section, we investigate the impact of the output dimension of discriminator on the performance of repulsive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment setup:</head><p>We used a similar setup as Section 5.1 with the following adjustments. The repulsive loss was tested on the CIFAR-10 dataset with a variety of discriminator output dimensions: d ∈ {1, 4, 16, 64, 256}. Spectral normalization was applied to discriminator with the proposed PICO method (see Appendix C) and the coefficients C K selected from <ref type="bibr">{16, 32, 64, 128, 256}</ref>.</p><p>Results: <ref type="table">Table S5</ref> shows that using more than one output neuron in the discriminator D significantly improved the performance of repulsive loss over the one-neuron case on CIFAR-10 dataset. The reason may be that using insufficient output neurons makes it harder for the discriminator to learn an injective and discriminative representation of the data (see <ref type="figure">Fig. 4b</ref>). However, the performance gain diminished when more neurons were used, perhaps because it becomes easier for D to surpass the generator G and trap it around saddle solutions. The computation cost also slightly increased due to more output neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 SAMPLES OF UNSUPERVISED IMAGE GENERATION</head><p>Generated samples on CelebA dataset are given in <ref type="figure">Fig. S3</ref> and LSUN bedrooms in </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On gradient regularizers for MMD GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikołaj</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikołaj</forename><surname>Bińkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A guide to convolution arithmetic for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<idno>arxiv:1603.07285</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Gintare Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classes of kernels for machine learning: A statistics perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Genton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="299" to="312" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Class-splitting generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><forename type="middle">L</forename><surname>Grinblat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><forename type="middle">C</forename><surname>Uzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">M</forename><surname>Granitto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07359</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">Stacked generative adversarial networks. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1866" to="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for optical flow with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="354" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><forename type="middle">Poczos</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2157" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approximation and convergence properties of generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5545" to="5553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient descent GAN optimization is locally stable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Zico</forename><surname>Vaishnavh Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5585" to="5595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On surrogate loss functions and f-divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="876" to="904" />
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The singular values of convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lipschitz-margin training: scalable certification of perturbation invariance for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Tsuzuku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
