<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking on Multi-Stage Networks for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
							<email>1liwenbo@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
							<email>wangzhicheng@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
							<email>yinbinyi@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Peng</surname></persName>
							<email>pengqixiang@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
							<email>duyuming@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzi</forename><surname>Xiao</surname></persName>
							<email>xiaotianzi@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
							<email>yugang@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<email>weiyichen@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking on Multi-Stage Networks for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing pose estimation approaches fall into two categories: single-stage and multi-stage methods. While multistage methods are seemingly more suited for the task, their performance in current practice is not as good as singlestage methods.</p><p>This work studies this issue. We argue that the current multi-stage methods' unsatisfactory performance comes from the insufficiency in various design choices. We propose several improvements, including the single-stage module design, cross stage feature aggregation, and coarse-tofine supervision. The resulting method establishes the new state-of-the-art on both MS COCO and MPII Human Pose dataset, justifying the effectiveness of a multi-stage architecture. The source code is publicly available for further research. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation problem has seen rapid progress in recent years using deep convolutional neural networks. Currently, the best performing methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">46]</ref> are pretty simple, typically based on a single-stage backbone network, which is transferred from image classification task. For example, the COCO keypoint challenge 2017 winner <ref type="bibr" target="#b8">[9]</ref> is based on Res-Inception <ref type="bibr" target="#b39">[40]</ref>. The recent simple baseline approach <ref type="bibr" target="#b45">[46]</ref> uses ResNet <ref type="bibr" target="#b16">[17]</ref>. As pose estimation requires a high spatial resolution, up sampling <ref type="bibr" target="#b8">[9]</ref> or deconvolution <ref type="bibr" target="#b45">[46]</ref> is appended after the backbone networks to increase the spatial resolution of deep features. * The first two authors contribute equally to this work. This work is done when Wenbo Li, Binyi Yin, Qixiang <ref type="bibr">Peng</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FLOPs (GiOPs)</head><p>Hourglass Single-Stage MSPN <ref type="figure">Figure 1</ref>. Pose estimation performance on COCO minival dataset of Hourglass <ref type="bibr" target="#b28">[29]</ref>, a single-stage model using ResNet <ref type="bibr" target="#b16">[17]</ref>, and our proposed MSPN under different model capacity (measured in FLOPs).</p><p>Another category of pose estimation methods adopts an multi-stage architecture. Each stage is a simple light-weight network and contains its own down sampling and up sampling paths. The feature (and heat) maps between the stages remain a high resolution. All the stages are usually supervised simultaneously to facilitate a coarse-to-fine, end-toend training. Representative works include convolutional pose machine <ref type="bibr" target="#b44">[45]</ref> and Hourglass network <ref type="bibr" target="#b28">[29]</ref>.</p><p>At an apparent look, the multi-stage architecture is more suited for the pose estimation task because it naturally enables high spatial resolution and is more flexible. Indeed, multi-stage methods are dominant on MPII <ref type="bibr" target="#b0">[1]</ref> dataset (mostly the variants of Hourglass <ref type="bibr" target="#b28">[29]</ref>). However, they are not as good as single-stage methods on the more challenging COCO dataset. Based on the previous works, it is unclear whether a multi-stage architecture is better or not.</p><p>This work aims to study this issue. We point out that the current unsatisfactory performance in multi-stage methods is mostly due to the insufficiency in various design choices. We show that the potential advantage of a multi-stage architecture can be better exploited with certain improvements on those design choices, and SOTA results can be achieved.</p><p>Specifically, we propose a multi-stage pose estimation network (MSPN) with three improvements. First, we notice that the single-stage module in the current multi-stage methods is not good. For example, a Hourglass <ref type="bibr" target="#b28">[29]</ref> module uses equal width channels in all blocks for both down and up sampling. Such a design is clearly inconsistent with the current good practice in network architecture design such as ResNet <ref type="bibr" target="#b16">[17]</ref>. We found that simply adopting the existing good network structure (GlobalNet of CPN <ref type="bibr" target="#b8">[9]</ref> in this work) as the single-stage module is sufficiently good. Second, due to the repeated down and up sampling steps, information is more likely to lose and optimization becomes more difficult. We propose to aggregate features across different stages to strengthen the information flow and mitigate the difficulty in training. Last, observing that the pose localization accuracy is gradually refined during multi-stage, we adopt a coarse-to-fine supervision strategy in accordance. Note that this is different from the commonly used multi-scale supervision in previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>With above improvements playing in synergy, the resulting multi-stage architecture significantly outperforms all previous works. This is exemplified in <ref type="figure">Figure 1</ref>. For the single-stage method, its performance becomes saturated while increasing the network capacity. As shown in <ref type="table">Table 1</ref>, Res-152 obtains 73.6 AP on COCO minival dataset and Res-254 has 74.0, only 0.4 improvement. For the representative multi-stage method Hourglass <ref type="bibr" target="#b28">[29]</ref>, only a small performance gain is obtained after using more than two stages. As illustrated in <ref type="table" target="#tab_2">Table 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pose estimation has undergone a long way as a primary research topic of computer vision. In the early days, handcrafted features are widely used in classical methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>. Recently, many approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref> take advantage of deep convolutional neural network (DCNN) <ref type="bibr" target="#b22">[23]</ref> to enhance the performance of pose estimation by a large step. In terms of network architecture, current human pose estimation methods could be divided as single-stage <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">46]</ref> and multi-stage <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b21">22]</ref> two categories.</p><p>Single-Stage Approach Single-stage methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">46]</ref> are based on backbone networks that are well tuned on image classification tasks, such as VGG <ref type="bibr" target="#b37">[38]</ref> or ResNet <ref type="bibr" target="#b16">[17]</ref>. Papandreou et al. <ref type="bibr" target="#b30">[31]</ref> designs a network to generate heat maps as well as their relative offsets to get the final predictions of the key points. He et al. <ref type="bibr" target="#b15">[16]</ref> proposes Mask R-CNN to first generate person box proposals and then apply single-person pose estimation. Chen et al. <ref type="bibr" target="#b8">[9]</ref> which is the winner of COCO 2017 keypoint challenge leverages a Cascade Pyramid Network (CPN) to refine the process of pose estimation. The proposed online hard key points mining (OHKM) loss is used to deal with hard key points. Xiao et al. <ref type="bibr" target="#b45">[46]</ref> provides a baseline method that is simple and effective in the pose estimation task. In spite of their good performance, these methods have encountered a common bottleneck. Simply increasing the model capacity does not give rise to much improvement in performance. This is illustrated in both <ref type="figure">Figure 1</ref> and <ref type="table">Table 1</ref>.</p><p>Multi-Stage Approach Multi-Stage methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b21">22]</ref> aim to produce increasingly refined estimation. They can be bottom-up or top-down. In contrary, singlestage methods are all top-down.</p><p>Bottom-up methods firstly predict individual joints in the image and then associate these joints into human instances. Cao et al. <ref type="bibr" target="#b4">[5]</ref> employs a VGG-19 <ref type="bibr" target="#b37">[38]</ref> network as a feature encoder, then the output features go through a multi-stage network resulting in heat maps and associations of the key points. Newell et al. <ref type="bibr" target="#b27">[28]</ref> proposes a network to simultaneously output key points and group assignments.</p><p>Top-down approaches first locate the persons using detectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24]</ref>. And a single person pose estimator is then used to predict the key points' locations. Wei et al. <ref type="bibr" target="#b44">[45]</ref> employs deep convolutional neural networks as feature encoder to estimate human pose. This work designs a sequential architecture composed of convolutional networks to implicitly model long-range dependencies between joints. Hourglass <ref type="bibr" target="#b28">[29]</ref> is proposed to apply intermediate supervision to repeated down sampling, up sampling processing for pose estimation task. <ref type="bibr" target="#b47">[48]</ref> adopts Hourglass and further design a Pyramid Residual Module (PRMs) to enhance the invariance in different scales. Many recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50]</ref> are based on Hourglass and propose various improvements. While these multi-stage methods work well on MPII <ref type="bibr" target="#b0">[1]</ref>, they are not competitive on the more challenging tasks on COCO <ref type="bibr" target="#b25">[26]</ref>. For example, the winners of COCO keypoint challenge on 2016 <ref type="bibr" target="#b30">[31]</ref>,  <ref type="figure" target="#fig_1">Figure 3</ref>) is adopted between adjacent stages (Section 3.2). A coarse-to-fine supervision strategy further improves localization accuracy (Section 3.3).</p><p>2017 <ref type="bibr" target="#b8">[9]</ref> are all single-stage based, as well as the recent simple baseline work <ref type="bibr" target="#b45">[46]</ref>. In this work, we propose several modifications on existing multi-stage architecture and show that the multi-stage architecture is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Stage Pose Network</head><p>We adopt the top-down approach in two steps. In the first step, an off-the-shelf human detector is adopted. Quantitive comparison of different detectors in the experiments shows that the quality of the detector is inconsequential, as long as it is sufficiently good.</p><p>In the second step, the proposed Multi-Stage Pose Network (MSPN) is applied to each detected human bounding box to produce the pose result. The network is exemplified in two stages as in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The Multi-Stage Pose Network proposes three improvements. First, we analyze the deficiency of the previous single-stage module and show why the state-of-the-art single-stage pose network can be readily exploited. Second, to reduce information loss, a feature aggregation strategy is proposed to propagate information from early stages to the later ones. Last, we introduce the usage of coarse-to-fine supervision. It adopts finer supervision in localization accuracy in later stages.</p><p>The following sections elaborate on each improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis of a Single-Stage Module</head><p>Most recent multi-stage methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref> are variants of Hourglass <ref type="bibr" target="#b28">[29]</ref>. In each module of Hourglass, the number of convolutional filters (or feature maps) remains constant during repeated down and up sampling steps. This equal-channel-width design results in a relatively poor performance seen from <ref type="figure">Figure 1</ref> since a lot of information will be lost after every down sampling.</p><p>By contrast, modern network architectures <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> are different. The number of feature maps is increased when there is a down sampling. Likewise, we note that there are some variants of Hourglass using different width channels. <ref type="bibr" target="#b27">[28]</ref> uses the same number of channels [256, 386, 512, 768] in both down and up sampling paths. However, this variant has only 71.7 AP with 15.4G FLOPs at 2 stages as shown in <ref type="table" target="#tab_2">Table 2</ref>. Compared with this setting, our proposed MSPN remains a small number of feature maps [256, 256, 256, 256] during the up sampling and allocates more computation complexity to the down sampling. In it, the number of feature maps is doubled after every spatial down sampling. It is reasonable since we aim to extract more representative features in the down sampling process and the lost information can hardly be recovered in the up sampling procedure. Therefore, increasing the capacity of down sampling unit is usually more effective. Finally, 2stage MSPN obtains 74.5 AP with 9.6G FLOPs. In this work, we adopt the ResNet-based GlobalNet of CPN <ref type="bibr" target="#b8">[9]</ref> as the single-stage module. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, it is a U-shape architecture in which features extracted from multiple scales are utilized for predictions. Note that the single stage module structure itself is not novel, but applying it in a multi-stage setting is new and shown effective in this work for the first time. In the Section 4.3.1, we also demonstrate that this module structure is general. The down sampling unit can effectively use other backbones as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross Stage Feature Aggregation</head><p>A multi-stage network is vulnerable by the information losing during repeated up and down sampling. To mitigate this issue, a cross stage feature aggregation strategy is used to propagate multi-scale features from early stages to the current stage in an efficient way.</p><p>As is shown in <ref type="figure" target="#fig_0">Figure 2</ref>, for each scale, two separate information flows are introduced from down sampling and up sampling units in the previous stage to the down sampling procedure of the current stage. It is noted that a 1 × 1 convolution is added on each flow as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Together with down-sampled features of current stage, three components are added to produce fused results. With this design, the current stage can take full advantage of prior information to extract more discriminative representations. In addition, the feature aggregation could be regarded as an extended residual design, which is helpful dealing for with the gradient vanishing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Coarse-to-fine Supervision</head><p>In the pose estimation task, context is crucial for locating the challenging poses since it provides information for invisible joints. Besides, we notice that small localization errors would seriously affect the performance of pose estimation. Accordingly, we design a coarse-to-fine supervision, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Specifically, the ground truth heat map for each joint is realized as a Gaussian in most previous works. In this work, we further propose to use different kernel sizes of the Gaussian in different stages. That is, an early stage uses a large kernel and a latter stage uses a small kernel. This strategy is based on the observation that the estimated heat maps from multi-stages are also in a similar coarse-to-fine manner. <ref type="figure" target="#fig_2">Figure 4</ref> shows an illustrative example. It demonstrates that the proposed supervision is able to refine localization accuracy gradually.</p><p>Besides, we are inspired that intermediate supervision could play an essential role in improving the performance of deep neural network from <ref type="bibr" target="#b40">[41]</ref>. Therefore, we introduce a multi-scale supervision to perform intermediate supervisions with four different scales in each stage, which could obtain substantial contextual information in various levels to help locate challenging poses. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, an online hard key points mining (OHKM) <ref type="bibr" target="#b8">[9]</ref> is applied to the largest scale supervision in each stage. L2 loss is used for heat maps on all the scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Protocol</head><p>MS COCO <ref type="bibr" target="#b25">[26]</ref> is adopted to evaluate the performance of our framework. It consists of three splits: train, validation and test. Similar to <ref type="bibr" target="#b8">[9]</ref>, we aggregate the data of train and validation parts together, and further divide it into trainval dataset (nearly 57K images and 150K person instances) and minival dataset (5k images). They are separately utilized for training and evaluating. OKS-based mAP (AP for short) is used as our evaluation metric <ref type="bibr" target="#b25">[26]</ref>.</p><p>MPII human Pose dataset <ref type="bibr" target="#b0">[1]</ref> provides around 25k images from a variety of real-world activities. There are over 40k person instances with annotated body joints, among which 12k instances are used for testing and others for training. PCKh@0.5 is used to evaluate the performance of single-person pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Human Detector. We adopt a state-of-the-art object detector MegDet <ref type="bibr" target="#b31">[32]</ref> to generate human proposals. The MegDet is trained with full categories of MS COCO dataset. Only human boxes out of the best 100 ones of all categories are selected as the input of single-person pose estimator. All the boxes are expanded to have a fixed aspect ratio of 4 : 3 for COCO.</p><p>Training. The network is trained on 8 Nvidia GTX 1080Ti GPUs with mini-batch size 32 per GPU. There are 90k iterations. Adam optimizer is adopted and the linear learning rate gradually decreases from 5e-4 to 0. The weight decay is 1e-5.</p><p>Each image will randomly go through a series of data augmentation operations including cropping, flipping, rotation, and scaling. As for cropping, instances with more than eight joints will be cropped to upper or lower bodies with equal possibility. The rotation range is −45 • ∼ 45 • , and scaling range is 0.7 ∼ 1.35. The image size is set 256 × 192 in Section 4.3 and 384 × 288 in Section 4.5 for MS COCO dataset, and 256 × 256 for MPII.</p><p>Testing. A post-Gaussian filter is applied to the estimated heat maps. Following the same strategy as <ref type="bibr" target="#b28">[29]</ref>, we average the predicted heat maps of original image with results of corresponding flipped image. Then, a quarter offset in the direction from the highest response to the second highest response is implemented to obtain the final locations of key points. The pose score is the multiplication of box score and average score of key points, same as in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we provide an in-depth analysis of each individual design in our framework.</p><p>In order to show the effectiveness of our method in a clear way, we also perform corresponding experiments on Hourglass <ref type="bibr" target="#b28">[29]</ref>. All results are reported on COCO minival dataset. The input image size is 256 × 192.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Multi-Stage Architecture</head><p>First, we evaluate how the capacity of backbone affects the performance of pose estimation. In terms of the singlestage network in <ref type="table">Table 1</ref>, we observe that its performance gets quickly saturated with the growth of backbone capacity. It is obvious that Res-101 outperforms Res-50 by 1.6 AP and costs a more 3.1G FLOPs, but there is only 0.5 gain from Res-101 to Res-152 at the cost of additional 3.7G FLOPs. For further exploration, we train a Res-254 network by adding more residual blocks on Res-152. Although the FLOPs of the network increases from 11.2G to 18.0G, there is an only 0.4 AP improvement. Therefore, it is not effective to adopt Res-152 or larger backbones for a single-stage network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Res Then, we demonstrate the effectiveness of multi-stage architecture based on the proposed single-stage module. From <ref type="table" target="#tab_2">Table 2</ref>, we can see that the performance of singlestage Hourglass <ref type="bibr" target="#b28">[29]</ref> is poor. Adding one more stage introduces a large AP margin. It shows that a multi-stage network is potential. However, the improvement becomes small when four or eight stages are employed. This indicates the necessity of a more effective single-stage module. Our single-stage model is discussed in Section 3.1 and the performance with 71.5 AP on minival dataset demonstrates the superiority of our single-stage module. And our 2-stage network further leads to a 3.0 improvement and obtains 74.5 AP. Introducing the third and fourth stage maintains a tremendous upward trend and eventually brings an impressive performance boost of 1.4 AP improvement. With similar FLOPs, Hourglass has 71.3 AP at 4 stages and 71.6 at 8 stages, only 0.3 point. These experiments indicate that MSPN successfully pushes the upper bound of existing single-stage and multi-stage networks. It obtains noticeable performance gain with more network capacity.</p><p>Finally, we testify that our approach is general. The down sampling unit of the single-stage module can effec-  <ref type="table">Table 3</ref>. Results of MSPN with smaller single-stage modules on COCO minival dataset. "L-XCP" and "S-XCP" respectively represent a small and a large Xception backbone.</p><p>tively adopt other backbones. To verify that, we compare the proposed multi-stage network against any single-stage one with similar FLOPs. We conduct more experiments on ResNet-18 and Xception <ref type="bibr" target="#b9">[10]</ref> architectures. Results are illustrated in <ref type="table">Table 3</ref>. It is clear that the 2-stage network based on Res-18 obtains a comparable result with Res-50 with smaller FLOPs. Moreover, we design two Xception <ref type="bibr" target="#b9">[10]</ref> backbones with different capacity, a large one (L-XCP) and a small one (S-XCP). The 4-stage S-XCP outperforms the single large model with 1.0 in AP with similar complexity. These results demonstrate the generality of our single-module backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Cross Stage Feature Aggregation</head><p>To address the issue that a deep multi-stage architecture is vulnerable by information losing during repeated up and down sampling procedures, we propose a cross stage feature aggregation strategy. It is adopted to fuse different level features in adjacent stages and ensure more discriminative representations for the current stage. <ref type="table">Table 4</ref> shows that the proposed feature aggregation strategy brings about a 0.3 gain from 74.2 to 74.5 for MSPN and a 0.5 improvement in terms of Hourglass, which demonstrates its effectiveness on dealing with aforementioned problems. At the same time, we can draw a conclusion that Hourglass tends to lose more information during forwarding propagation and our feature aggregation strategy can effectively mitigate this issue.  <ref type="table" target="#tab_2">Table 2</ref>. 'CTF' indicates the coarse-to-fine supervision. 'CSFA' means the cross stage feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Coarse-to-fine Supervision</head><p>In this part, we evaluate our coarse-to-fine supervision for both MSPN and Hourglass. The results are shown in <ref type="table">Ta-ble 4</ref>. It is clear that this strategy improves the performance of our network by a large margin from 73.3 to 74.2. First of all, this design aims to realize a coarse-to-fine detection procedure and the result demonstrates its effectiveness on further improving the accuracy of keypoint localization. In addition, it is reasonable that intermediate supervisions can take full advantage of contextual information across different scales. To demonstrate the applicability of this supervision in other multi-stage networks, we further apply this strategy to a 4-stage Hourglass that is comparable with our 2-stage MSPN in complexity, and finally obtains a 1.2 improvement in AP. In a word, the proposed coarse-to-fine supervision could largely boost the performance of pose estimation and be well adapted to other multi-stage networks. Furthermore, we conduct several experiments to verify which level of supervision will have higher efficiency in our network. As described in Section 3.2, we apply a Gaussian blur operation to each point on a heat map and a smaller kernel corresponds to a finer supervision. As shown in <ref type="table" target="#tab_5">Table 5</ref>, we could see that either setting-1 or setting-2 will degrade the performance compared with the proposed coarse-to-fine supervision (setting-3). Especially, setting-2 even leads to a worse performance than the setting-1, which indicates that an appropriate supervision could make a difference to the final result.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Influence of Human Detector</head><p>The human detector used in this work has a strong performance. It has 49.4 AP on COCO minival dataset. To evaluate its influence on the final pose estimation accuracy, we also test another detector with worse performance (the one in CPN <ref type="bibr" target="#b8">[9]</ref> with 41.1 AP) and an "oracle detector" using ground truth boxes, for controlled comparison. Pose estimation performance is reported in a much better detector, the pose estimation accuracy is only slightly improved. For example, there is only 0.5 gain from 41.1 detector to 49.4 one using 4-stage MSPN. This verifies that the good performance mostly comes from MSPN. The influence of detector is quite limited. The same conclusion is also drawn in <ref type="bibr" target="#b8">[9]</ref>. Note that all results of Hourglass, ResNet and Xception in Section 4.3 are based on the same 49.4 detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-art Methods</head><p>On COCO benchmark, as shown in <ref type="table">Table 7</ref>, our single model trained by only COCO data achieves 76.1 AP on testdev and outperforms other methods by a large margin in all metrics. Advocated by external data, MSPN leads to a 1.0 improvement resulting in 77.1 AP. And the ensemble model finally obtains 78.1. From <ref type="table">Table 8</ref>, it is clear that our approach obtains 76.4 AP on the test-challenge dataset and shows its significant superiority over other state-of-theart methods. Eventually, our method surpasses COCO 2017 Challenge winner CPN <ref type="bibr" target="#b8">[9]</ref> and Sample Baseline <ref type="bibr" target="#b45">[46]</ref>  MPII is another popular benchmark for pose estimation. We also validate the proposed MSPN on this dataset. The PCKh@0.5 result on MPII test dataset is shown in <ref type="table" target="#tab_7">Table 9</ref>. Our result is the new state-of-the-art 2 . Some results generated by our method are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. We can see that our MSPN handles crowd and occlu- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a Multi-Stage Pose Network (MSPN) to perform multi-person pose estimation. We first verify the effectiveness of the multi-stage pipeline with well-designed single-stage modules in MSPN. Additionally, a coarse-to-fine supervision and a cross stage feature aggregation strategy are proposed to further boost the performance of our framework. Extensive experiments have been conducted to demonstrate its effectiveness. For the first time, it is shown that a multi-stage architecture is competitive on the challenging COCO dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of Multi-Stage Pose Network (MSPN). It is composed of two single-stage modules. A cross stage aggregation strategy (zoomed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Cross Stage Feature Aggregation on a specific scale. Two 1 × 1 convolutional operations are applied to the features of previous stage before aggregation. SeeFigure 2for the overall network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of coarse-to-fine supervision. The first row shows ground-truth heat maps in different stages and the second row represents corresponding predictions and ground truth annotations. The orange line is the prediction result and the green line indicates ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>by 4.3 and 1.9 AP in test-challenge dataset respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of MSPN results on COCO minival dataset. sion situations as well as challenging poses effectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Yuming Du and Tianzi Xiao are interns at Megvii Research.</figDesc><table><row><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>6</cell><cell>10</cell><cell>14</cell><cell>18</cell><cell>22</cell></row></table><note>1 https://github.com/megvii-detection/MSPN</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, it has 71.3 AP at 4 stages and 71.6 at 8 stages, only 0.3 improvement. With similar FLOPs, MSPN has 74.5 AP at 2 stages and 75.9 at 4 stages, that is 1.4 point improvement. Therefore, MSPN has a clearly better accuracy-FLOPs tradeoff. New state-of-the-art performance is achieved. On COCO keypoint benchmark, the proposed single model achieves 76.1 average precision (AP) on test-dev. It significantly outperforms state-of-the-art algorithms. Finally, we obtain 78.1 AP on test-dev and 76.4 on test-challenge dataset, which is 4.3 AP improvement on test-challenge benchmark compared with MS COCO 2017 Challenge winner. Meanwhile, the proposed method obtains 92.6 PCKh@0.5 on MPII test dataset, which is also the best.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of Hourglass and MSPN with different number of stages on COCO minival dataset. " †" denotes the result of a variant Hourglass<ref type="bibr" target="#b27">[28]</ref> as illustrated in Section 3.1. MSPN adopts Res-50 in each single-stage module.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">-50 Res-101 Res-152 Res-254</cell></row><row><cell>AP</cell><cell cols="2">71.5</cell><cell cols="2">73.1</cell><cell>73.6</cell><cell>74.0</cell></row><row><cell cols="2">FLOPs(G)</cell><cell>4.4</cell><cell cols="2">7.5</cell><cell>11.2</cell><cell>18.0</cell></row><row><cell cols="7">Table 1. Results of single-stage networks with different backbones</cell></row><row><cell cols="4">on COCO minival dataset.</cell><cell></cell><cell></cell></row><row><cell>Stages</cell><cell cols="3">Hourglass FLOPs(G) AP</cell><cell>Stages</cell><cell cols="2">MSPN FLOPs(G) AP</cell></row><row><cell>1</cell><cell>3.9</cell><cell></cell><cell>65.4</cell><cell>1</cell><cell>4.4</cell><cell>71.5</cell></row><row><cell>2</cell><cell>6.2</cell><cell></cell><cell>70.9</cell><cell>2</cell><cell>9.6</cell><cell>74.5</cell></row><row><cell>4</cell><cell cols="2">10.6</cell><cell>71.3</cell><cell>3</cell><cell>14.7</cell><cell>75.2</cell></row><row><cell>8</cell><cell cols="2">19.5</cell><cell>71.6</cell><cell>4</cell><cell>19.9</cell><cell>75.9</cell></row><row><cell>2  †</cell><cell cols="2">15.4  †</cell><cell>71.7  †</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results of a 2-stage MSPN with different supervision strategies on COCO minival dataset. The kernel size controls the fineness of supervision and a smaller value indicates a finer setting.</figDesc><table><row><cell>Detector</cell><cell cols="3">CPN(41.1) Ours(49.4) GT</cell></row><row><cell>2-Stg MSPN</cell><cell>74.1</cell><cell>74.5</cell><cell>75.1</cell></row><row><cell>3-Stg MSPN</cell><cell>74.8</cell><cell>75.2</cell><cell>75.6</cell></row><row><cell>4-Stg MSPN</cell><cell>75.4</cell><cell>75.9</cell><cell>76.5</cell></row><row><cell cols="4">Table 6. Results of MSPN using three detectors on COCO minival</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .Table 7 .Table 8 .</head><label>678</label><figDesc>Clearly, with Method Backbone Input Size AP AP 50 AP 75 AP M AP L AR AR 50 AR 75 AR M AR L Comparisons of results on COCO test-dev dataset. "+" indicates using an ensemble model and "*" means using external data. Method Backbone Input Size AP AP 50 AP 75 AP M AP L AR AR 50 AR 75 AR M AR L Comparisons of results on COCO test-challenge dataset. "+" means using an ensemble model and "*" means using external data.</figDesc><table><row><cell>CMU Pose [5]</cell><cell>-</cell><cell>-</cell><cell cols="6">61.8 84.9 67.5 57.1 68.2 66.5 87.2 71.8 60.6 74.6</cell></row><row><cell>Mask R-CNN [16]</cell><cell>Res-50-FPN</cell><cell>-</cell><cell>63.1 87.3 68.7 57.8 71.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>G-RMI [31]</cell><cell>Res-152</cell><cell cols="7">353×257 64.9 85.5 71.3 62.3 70.0 69.7 88.7 75.5 64.4 77.1</cell></row><row><cell>AE [28]</cell><cell>-</cell><cell cols="7">512×512 65.5 86.8 72.3 60.6 72.6 70.2 89.5 76.0 64.6 78.1</cell></row><row><cell>CPN [9]</cell><cell cols="8">Res-Inception 384×288 72.1 91.4 80.0 68.7 77.2 78.5 95.1 85.3 74.2 84.3</cell></row><row><cell>Simple Base [46]</cell><cell>Res-152</cell><cell cols="3">384×288 73.7 91.9 81.1 70.3 80.0 79.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HRNet [39]</cell><cell cols="4">HRNet-W48 384×288 75.5 92.5 83.3 71.9 81.5 80.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (MSPN)</cell><cell>4×Res-50</cell><cell cols="7">384×288 76.1 93.4 83.8 72.3 81.5 81.6 96.3 88.1 77.5 87.1</cell></row><row><cell>CPN+ [9]</cell><cell cols="8">Res-Inception 384×288 73.0 91.7 80.9 69.5 78.1 79.0 95.1 85.9 74.8 84.6</cell></row><row><cell>Simple Base+* [46]</cell><cell>Res-152</cell><cell cols="7">384×288 76.5 92.4 84.0 73.0 82.7 81.5 95.8 88.2 77.4 87.2</cell></row><row><cell>HRNet* [39]</cell><cell cols="4">HRNet-W48 384×288 77.0 92.7 84.5 73.4 83.1 82.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (MSPN*)</cell><cell>4×Res-50</cell><cell cols="7">384×288 77.1 93.8 84.6 73.4 82.3 82.3 96.5 88.9 78.4 87.7</cell></row><row><cell>Ours (MSPN+*)</cell><cell>4×Res-50</cell><cell cols="7">384×288 78.1 94.1 85.9 74.5 83.3 83.1 96.7 89.8 79.3 88.2</cell></row><row><cell cols="2">Mask R-CNN* [16] ResX-101-FPN</cell><cell>-</cell><cell cols="6">68.9 89.2 75.2 63.7 76.8 75.4 93.2 81.2 70.2 82.6</cell></row><row><cell>G-RMI* [31]</cell><cell>Res-152</cell><cell cols="7">353×257 69.1 85.9 75.2 66.0 74.5 75.1 90.7 80.7 69.7 82.4</cell></row><row><cell>CPN+ [9]</cell><cell cols="8">Res-Inception 384×288 72.1 90.5 78.9 67.9 78.1 78.7 94.7 84.8 74.3 84.7</cell></row><row><cell>Sea Monsters+*</cell><cell>-</cell><cell>-</cell><cell cols="6">74.1 90.6 80.4 68.5 82.1 79.5 94.4 85.1 74.1 86.8</cell></row><row><cell>Simple Base+* [46]</cell><cell>Res-152</cell><cell cols="7">384×288 74.5 90.9 80.8 69.5 82.9 80.5 95.1 86.3 75.3 87.5</cell></row><row><cell>Ours (MSPN+*)</cell><cell>4×Res-50</cell><cell cols="7">384×288 76.4 92.9 82.6 71.4 83.2 82.2 96.0 87.7 77.5 88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>MethodHea Sho Elb Wri Hip Kne Ank MeanBulat et al. [4] 97.9 95.1 89.9 85.3 89.4 85.7 81.7 89.7 Newell et al. [29] 98.2 96.3 91.2 87.1 90.1 87.4 83.6 90.9 Tang et al. [44] 97.4 96.4 92.1 87.7 90.2 87.7 84.3 91.2 Ning et al. [30] 98.1 96.3 92.2 87.8 90.6 87.6 82.7 91.2 Luvizon et al. [27] 98.1 96.6 92.0 87.5 90.6 88.0 82.7 91.2 Chu et al. [12] 98.5 96.3 91.9 88.1 90.6 88.0 85.0 91.5 Chou et al. [11] 98.2 96.8 92.2 88.0 91.3 89.1 84.9 91.8 Chen et al. [8] 98.1 96.5 92.5 88.5 90.2 89.6 86.0 91.9 Yang et al. [48] 98.5 96.7 92.5 88.7 91.1 88.6 86.0 92.0 Ke et al. [22] 98.5 96.8 92.7 88.4 90.6 89.3 86.3 92.1 Tang et al. [42] 98.4 96.9 92.6 88.7 91.8 89.4 86.2 92.3 Sun et al. [39] 98.6 96.9 92.8 89.0 91.5 89.0 85.7 92.3 Zhang et al. [50] 98.6 97.0 92.8 88.8 91.7 89.8 86.6 92.5 Ours (MSPN) 98.4 97.1 93.2 89.2 92.0 90.1 85.5 92.6 Comparisons of results on MPII test dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://human-pose.mpi-inf.mpg.de/ results</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08050</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02439</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1702.07432</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Articulated pose estimation using discriminative armlet classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3342" to="3349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="728" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR), 2011 IEEE conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09894</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Detnet: A backbone network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06215</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02322</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge-guided deep fractal neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1246" to="1259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive pose priors for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="422" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3674" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09212</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="190" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quantized densely connected u-nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="339" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06208</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Human pose estimation with spatial contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
