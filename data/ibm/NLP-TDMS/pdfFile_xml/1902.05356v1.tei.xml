<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse and noisy LiDAR completion with RGB guidance and uncertainty</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><forename type="middle">Van</forename><surname>Gansbeke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brabandere</forename><surname>Luc</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse and noisy LiDAR completion with RGB guidance and uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes a new method to accurately complete sparse LiDAR maps guided by RGB images. For autonomous vehicles and robotics the use of LiDAR is indispensable in order to achieve precise depth predictions. A multitude of applications depend on the awareness of their surroundings, and use depth cues to reason and react accordingly. On the one hand, monocular depth prediction methods fail to generate absolute and precise depth maps. On the other hand, stereoscopic approaches are still significantly outperformed by LiDAR based approaches. The goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds which are mapped to a 2D plane. We propose a new framework which extracts both global and local information in order to produce proper depth maps. We argue that simple depth completion does not require a deep network. However, we additionally propose a fusion method with RGB guidance from a monocular camera in order to leverage object information and to correct mistakes in the sparse input. This improves the accuracy significantly. Moreover, confidence masks are exploited in order to take into account the uncertainty in the depth predictions from each modality. This fusion method outperforms the state-of-the-art and ranks first on the KITTI depth completion benchmark [21]. Our code with visualizations is available at https: // github. com/ wvangansbeke/ Sparse-Depth-Completion .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Depth completion is predicting dense depth maps from a sparse point cloud. In many computer vision applications, precise depth values are of crucial importance. In recent years this task has gained attention due to industrial demand. Other computer vision tasks, among which 3D object detection and tracking, 2D or 3D semantic segmentation and SLAM can exploit these accurate depth cues, leading to better accuracy in these fields. This work will focus on self-driving cars, while using sparse LiDAR and monocular RGB images. Here, it is desirable to accurately detect and differentiate objects close as well as far away. The LiDAR generates a point cloud of its surroundings, but the limited amount of scan lines results in a high sparsity. LiDARs with 64 scan lines are common and still expensive. The sparse and irregular spaced input points make this task stand out from others. Since a vast amount of applications use LiDAR with a limited amount of scan lines, the industrial relevance is indisputable, currently leading to a very active research domain. The reason why this task is challenging is threefold. Firstly, the input is randomly spaced which makes the usage of straightforward convolutions difficult. Secondly, the combination of multiple modalities is still an active area of research, since multiple combinations of sensor fusion are possible, namely early and/or late fusion. This paper will focus on the fusion between RGB info and the LiDAR points. Thirdly, the used annotations are only partially completed. The construction of the pixel-wise ground truth annotations is expensive after all. Our method needs to cope with this constraint.</p><p>The contributions of this paper are:</p><p>(1) Global and local information are combined in order to accurately complete and correct the sparse input. Monocular RGB images can be used as guidance for this depth completion task.</p><p>(2) Confidence maps are learned for both the global and the local branch in an unsupervised manner. The predicted depth maps are weighted by their respective confidence map. This late fusion approach is a fundamental part of the framework.</p><p>(3) This method ranks first on the KITTI depth completion benchmark with and without using RGB images. Furthermore, it does not require any additional data or postprocessing.</p><p>The structure of the manuscript will be as follows. Section 2 mentions similar prior works regarding depth completion and focuses on the existing challenges. This is followed by a detailed description of our method in section 3. We further evaluate this method on the popular KITTI dataset in section 4. To conclude, section 5 will wrap up our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Related works with regards to the depth completion task will be discussed. Attention will be given towards the handling of sparse data and the guidance of LiDAR with other modalities, in particular RGB images. arXiv:1902.05356v1 [cs.CV] 14 Feb 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Handling sparse data</head><p>Completing missing information while also correcting the input has a wide range of applications. Inpainting, denoising and superresolution can all be considered parts of the depth completion task, making depth completion relevant for those specific sub-tasks.</p><p>Older methods use handcrafted approaches in order to perform the local upsampling of the sparse input, by usage of complex interpolation techniques. Even more recently, J. Ku et al. <ref type="bibr" target="#b8">[9]</ref> have achieved impressive results without making use of convolutional neural networks (CNNs). They artificially make the input denser by morphological image processing techniques and predict the final depth from this intermediate state. These methods are however prone too errors in the LiDAR frame, making a CNN a more powerful tool for the depth completion task. It's important to know that the 3D LiDAR points are mapped to the 2D plane, making standard 2D convolution a viable option. Despite the more dense input, convolution operations are not designed to operate on this data, since only valid points ought to be considered by the network.</p><p>In fact, recent works have also shown that convolutional neural networks can achieve exciting results for this task. Jaritz et al. <ref type="bibr" target="#b6">[7]</ref> and F. Ma et al. <ref type="bibr" target="#b12">[13]</ref> both use a deep neural net, while encoding the sparse values with zeros. They argue that a deep network is necessary for this job. We argue that a combination of a local and global network is a more elegant and an intuitive solution, furthermore yielding better results.</p><p>Uhrig et al. <ref type="bibr" target="#b17">[18]</ref> propose sparsity invariant convolutions in order to take into account the sparse input. They perform normalized convolution operations by propagating the validity mask through each layer of their network with maxpooling. Now, the network can be invariant towards the degree of sparsity. Eldesokey et al. <ref type="bibr" target="#b0">[1]</ref> propose a similar solution to take into account the sparsity. Here, a confidence mask is propagated which requires a second convolution for every layer in order to perform the normalization and to generate a the confidence mask for the next layer. We also experiment with uncertainty, but on a higher level, in order to efficiently combine the feature maps extracted by the global and local network. HMS-Net <ref type="bibr" target="#b4">[5]</ref> goes even further by adopting a multi-scale network and proposing new operations for concatenating, bilinear up-sampling and adding sparse input maps. We find that those operations are not necessary if the sparsity is constant in every frame, since we notice no accuracy gains when including these modified operations. We therefore stick to the conventional convolutions in our method and show that our framework can handle sparse LiDAR data. Furthermore, adding a validity mask to the sparse input shows no effect on the output accuracy which is in line with the findings of Jaritz et al. <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Guided depth completion</head><p>By now, multiple methods already include RGB data in order to generate better results. How to combine different sensors is still an open research question. Recent works include fusion techniques in order to generate richer features and a better prior for the depth completion task. RGB data will be used to guide our local network. We now discuss recent guidance and fusion techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Guidance</head><p>In one line of work, Schneider et al. <ref type="bibr" target="#b16">[17]</ref> include RGB information in order to generate sharp edges for the depth predictions. They use pixel-wise semantic annotations to differentiate multiple objects and they use a geodesic distance measure to enforce sharp boundary edges. F. Ma et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> make use of a ResNet-based deep neural network which takes the 4D, RGB-D, as input. Furthermore, F. Ma et al. <ref type="bibr" target="#b13">[14]</ref> take a self-supervised approach which requires temporal data. They now make use of two streams in order to combine the LiDAR data and RGB images in the same feature space, leading to better results. Instead of completing the input immediately Zhang et al. <ref type="bibr" target="#b19">[20]</ref> predict surface normals by leveraging RGB data, leading to a better prior for depth completion. They finally combine these predictions with the sparse depth input to generate the complete depth maps. Like us, they found that completing sparse data from standalone sparse depth samples is a difficult task, proving the importance of RGB guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Fusion</head><p>The fusion of multimodal sensor data is not straightforward. For example Li et al. <ref type="bibr" target="#b10">[11]</ref> upsample low resolution depth maps guided by the RGB images and take a late fusion approach. In fact, different fusion techniques can be considered: early fusion, late fusion or multi-level fusion. Valada et al. <ref type="bibr" target="#b18">[19]</ref> adopt the latter technique by extracting and combining feature maps at different stages in the encoder from multiple input streams. In general most works, such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2]</ref>, show that late fusion can achieve better performance. We propose a combination of early and late fusion showing good results on the KITTI benchmark <ref type="bibr">[21]</ref>. In our work, early fusion takes the form of a guidance map for our local network extracted from global information. Uncertainty is adopted in the depth predictions to accomplish late fusion. Further, conventional fusion techniques such as adding, concatenating or multiplying feature maps are utilized. <ref type="figure">Figure 1</ref>. The framework consists of two parts: the global branch on top and the local branch below. The global path outputs three maps: a guidance map, global depth map and a confidence map. The local map predicts a confidence map and a local map by also taking into account the guidance map of the global network. The framework fuses global and local information based on the confidence maps in a late fusion approach. <ref type="figure" target="#fig_0">Figure 2</ref> shows that this structure can correct mistakes in the LiDAR input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our method acts on a projection of a 3D point cloud to a 2D plane. Here, the depth completion problem is approached as a regression problem. Our approach requires supervision by using the ground truth to train our CNN and encodes the missing LiDAR input values with zeros. The targets are reliably composed by using semi-global matching (SGM) and temporal information <ref type="bibr" target="#b17">[18]</ref>, but they are still semi-sparse (around 30% is filled). Using the sparse input and the semi-sparse ground truth, the convolutional framework makes use of global guidance information to correct artifacts and to upsample the input properly. This correction of artifacts is not explicitly addressed in previous works.</p><p>Hence, our method makes use of global and local information in order to complete the input. Since LiDAR is characterized by mistakes due to moving objects and the moving LiDAR itself, both parts are necessary in order get accurate predictions. The local network will interpret local information, whereas the global network extracts global information based on the LiDAR and RGB information. Fusion between the two networks results in a final depth map. We will later show that depth completion does not require a deep network. First, the two parts of the framework will be explained in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extracting local and global information</head><p>The global branch can be considered as a prior, namely to regularize the features extracted by the local path. Since there are mistakes in the LiDAR input frames, the global information helps the local network to detect these artifacts and reconstruct the sparse input more accurately. We speculate that that the global information is relevant. Firstly, the global network is able to detect (moving) objects and is able to detect structures in the frame that have likely the same depth. Secondly, we expect that a more gradual depth map will be computed in order to prevent sudden and wrong variations in the LiDAR input. This information can be determined by examining the RGB input since borders of objects can be extracted more easily due to its color information. Hence, semantically meaningful information can be extracted.</p><p>The local network examines the input LiDAR frame and performs the local up-sampling. To remedy the noisy LiDAR data, we fuse the LiDAR map together with the global guidance map. On the one hand, the reasoning behind this guidance technique is that the local network can further focus on the correct and confident LiDAR points. On the other hand, the global network can reason about objects, its edges and larger structures in the frame. Finally a residual learning approach has been used in order to keep improving the predictions, implemented by skip connections over the small local networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exploiting uncertainty</head><p>We make use of uncertainty in both the global and the local network. Both parts of the framework predict a confidence map. In this way the confidence map acts like a weight map for the final fusion between the two input types. Thus, the weighing is performed per pixel and completely learned by the network in an unsupervised manner. Using this technique, uncertainty in the different network paths is utilized to give more attention to a certain input type, based on the learned confidence weights. The network learns to prefer global information over local information in certain regions. In fact, in locations with accurate and sufficient LiDAR points, the local network will produce depth predictions with a high confidence, whereas global information will be utilized where the LiDAR data is incorrect or scarce, such as at the boundaries of objects. This fusion method is an effective way of combining multiple sensors which is supported by our results in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network</head><p>The global network is an encoder-decoder network based on ERFNet <ref type="bibr" target="#b15">[16]</ref> while the local network is a stacked hourglass network. The latter consists of two hourglass modules in order to learn a residual on the original depth predictions, inspired by ResNet <ref type="bibr" target="#b3">[4]</ref> and body pose estimation architectures <ref type="bibr" target="#b14">[15]</ref>, with merely 350k parameters in total. Each consists of six layers, has a small receptive field and downsamples only two times by using strided convolutions. No batch normalization <ref type="bibr" target="#b5">[6]</ref> is present in the first convolution layer and in the encoder of the first hourglass module, since the amount of zeros will skew the layer's parameters, especially when the input sparsity is not constant. The structure of the hourglass module can be found in table 1. An ERFNet-based global network has been chosen since it achieves a high accuracy on the Cityscapes' benchmark <ref type="bibr" target="#b20">[22]</ref> while still being real-time.</p><p>The global guidance map is fused with the sparse LiDAR frame, in order to exploit the global info. This resembles early fusion as a guidance for the local network. On the one hand, the global networks provides three output maps: a guidance map with global information, a depth map and a confidence map. On the other hand, the local network provides a depth map and a confidence map. By multiplying the confidence map with its depth map and adding the predictions from both networks, the final prediction is produced. The probability values for the confidence maps are calculated by utilization of the softmax function. This selection procedure allows the framework to choose pixels from the global depth map or the adjusted depth values from the stacked hourglass module. Thus, the final depth predictiond exploits the confidence maps X and Y which equates to expression 1. A visualization of the total framework can be found in <ref type="figure">figure 1</ref>.</p><formula xml:id="formula_0">d out (i, j) = e X(i,j) ·d global (i, j) + e Y (i,j) ·d local (i, j) e X(i,j) + e Y (i,j)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>For the experiments a Tesla V100 GPU was used and the code is implemented in Pytorch. We evaluate our framework by computing the loss on all pixels of the ground truth since not all input pixels of the Li-DAR are correct. The KITTI depth completion benchmark [21] is our main focus, since it resembles reallife situations accurately. The KITTI dataset <ref type="bibr" target="#b2">[3]</ref> provides 85898 frames for training, 1000 frames for evaluation and 1000 frames for testing. An ablation study is shown first, followed by a comparison with current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation study and analysis</head><p>In all cases we perform data augmentation by flipping the images vertically. Rotating and scaling the LiDAR input while resizing the RGB input had no effect on the final results due to the magnitude of KITTI's dataset. Furthermore, since the LiDAR frame does not provide any information at the top, we crop the inputs to a 1216x256 aspect ratio. We first train both parts of the framework individually and use a pretrained ERFNet on Cityscapes <ref type="bibr" target="#b20">[22]</ref> for our global network. Afterwards, guidance for the local network is added. Hence, the framework is trained end-to-end and forced to combine the predictions of the two networks based on their certainties with this late fusion approach. We adopt the Adam optimizer <ref type="bibr" target="#b7">[8]</ref> with learning rate of 10 −3 .</p><p>Multiple loss functions were implemented. Our proposed focal-MSE loss, inspired by <ref type="bibr" target="#b11">[12]</ref>, performed slightly better than the vanilla-MSE loss (by a few mm's) and also better than the popular BerHu loss <ref type="bibr" target="#b9">[10]</ref> for the depth prediction task. It is shown in equation 2. A focal term has been added in order to give wrongly predicted points during training a slightly higher weight in the loss expression. Furthermore, this regression loss is worth to try in other domains. The loss measures the correctness of the final depth map, the global -and local depth map as shown in equation 3. The weights w 1 , w 2 are both equal to 0.1 while w 3 is equal to 1.</p><formula xml:id="formula_1">λ(ŷ, y) = 1 n n i=1 (1+0.05·epoch·|y i −ŷ i |)·(y i −ŷ i ) 2 (2) Λ = w 1 · λ(ŷ global , y) + w 2 · λ(ŷ local , y) + w 3 · λ(ŷ out , y)<label>(3)</label></formula><p>Both the RMSE (root mean squared error) and the MAE (mean absolute error) are used to evaluate on the KITTI benchmark, but we mainly focus on the RMSE since it is the leading metric on the benchmark. The ablation study in table 2 shows that the combination of a global and a local network leads to impressive results. In fact, our late fusion method, based on uncertainty, contributes to a large accuracy gain. By exploiting the guidance map, we eventually outperform previous works. We furthermore stick to 2 hourglass modules so that the inference time does not increase unnecessarily. Adding batch normalization (BN) to all the convolutions in the local network increases the MAE slightly, due to the high degree of sparsity. We conclude that the local network alone can already achieve good results with only 350k parameters. However, in order to correct mistakes we exploit the global network by predicting uncertainty maps and a guidance map. <ref type="table" target="#tab_2">Table 3</ref> reports the results on the KITTI testset. We outperform F. Ma et al. <ref type="bibr" target="#b13">[14]</ref> (currently ranked first on the KITTI depth completion benchmark [21]) by a significant amount on all metrics, while the frame rate is 4 times higher. Furthermore, we also rank first on the benchmark when we only use LiDAR information in our framework (no RGB images are used) in table 3. From this testset data we conclude that the framework can extract semantically meaningful information in order to guide the local network. <ref type="figure" target="#fig_0">Figure 2</ref> displays an example from the validation set. Here, the confidence maps clearly show that the global network is more certain around edges and locations where the LiDAR sensor is incorrect (green box). This proves the effectiveness of this framework. <ref type="figure" target="#fig_1">Figure  3</ref> demonstrates the differences between our method and other state-of-the-art methods. It shows that we predict more accurate depth values around close as well as far away objects.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>RGB RMSE MAE t SparseConvs <ref type="bibr" target="#b17">[18]</ref> 1601 481 0.01 NConv-CNN <ref type="bibr" target="#b1">[2]</ref> 1268 360 0.01 Spade-sD <ref type="bibr" target="#b6">[7]</ref> 1035 248 0.04 Sparse-to-Dense <ref type="bibr" target="#b13">[14]</ref> 954 288 0.04 HMS-Net <ref type="bibr" target="#b4">[5]</ref> 937 258 0.02 FusionNet (Ours) 923 249 0.02 Spade-RGBsD <ref type="bibr" target="#b6">[7]</ref> 918 235 0.07 NConv-CNN-L1 <ref type="bibr" target="#b1">[2]</ref> 859 208 0.02 HMS-Net v2 <ref type="bibr" target="#b4">[5]</ref> 842 253 0.02 NConv-CNN-L2 <ref type="bibr" target="#b1">[2]</ref> 830 233 0.02 Sparse-to-Dense <ref type="bibr" target="#b13">[14]</ref> 815 250 0.08 FusionNet (Ours) 773 215 0.02</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a framework guided by RGB images in order to complete and correct sparse LiDAR frames. The core of the idea is leveraging global information by using a global network. Furthermore, we exploit confidence maps in order to combine both inputs based on the uncertainty in a late fusion approach. We successfully regress towards the semi-sparse ground truth annotations using our focal loss. This method takes 20 ms inference time, hence it meets the real-time requirements for self-driving cars with a large margin. Finally, we evaluated our method on the KITTI dataset where we rank first on the depth completion benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Example on the validation set. The green box shows that our framework successfully corrects the mistakes in the sparse LiDAR input frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visual comparison with state-of-the-art. The green box shows the area to focus on in the depth maps. Our method shows better results around objects. For example, on the right of the pillar, the other two methods produce incorrect depth values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Hourglass network.</figDesc><table><row><cell>Layer</cell><cell cols="2">Kernel Size/stride Filters</cell></row><row><cell>Conv/Relu</cell><cell>3x3/2</cell><cell>32</cell></row><row><cell>Conv/Relu</cell><cell>3x3/1</cell><cell>64</cell></row><row><cell>Conv/Relu</cell><cell>3x3/2</cell><cell>64</cell></row><row><cell>Conv/Relu</cell><cell>3x3/1</cell><cell>64</cell></row><row><cell>TransConv/BN/Relu</cell><cell>2x2/2</cell><cell>64</cell></row><row><cell>TransConv/BN/Relu</cell><cell>2x2/2</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on KITTI's selected validation set.</figDesc><table><row><cell>Configuration</cell><cell cols="2">RMSE [mm] MAE [mm]</cell></row><row><cell>Local Net (LiDAR)</cell><cell>995</cell><cell>268</cell></row><row><cell>Global Net (RGB)</cell><cell>3223</cell><cell>1473</cell></row><row><cell>Global Net (LiDAR)</cell><cell>1020</cell><cell>300</cell></row><row><cell>Global Net (RGB LiDAR)</cell><cell>881</cell><cell>235</cell></row><row><cell>Local+Global+Uncertainty</cell><cell>810</cell><cell>224</cell></row><row><cell>+Guidance skip</cell><cell>802</cell><cell>214</cell></row><row><cell>+BN</cell><cell>819</cell><cell>223</cell></row><row><cell>+Extra Hourglass</cell><cell>811</cell><cell>222</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art on the testset based on RMSE[mm], MAE[mm] and t[s].</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Propagating confidences through cnns for sparse data regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Confidence propagation through CNNs for guided sparse depth regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision meets Robotics: The KITTI Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">HMS-Net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08685</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse and dense data with CNNs: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00036</idno>
		<title level="m">defense of classical image processing: Fast depth completion on the cpu</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep joint image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">154169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Self-supervised sparseto-dense: Self-supervised Depth completion from LiDAR and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00275</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>European Conference on Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparsity invariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-Supervised Model Adaptation for Multimodal Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03833</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep depth completion of a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<ptr target="https://www.cityscapes-dataset.com" />
	</analytic>
	<monogr>
		<title level="j">Cityscapes</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
