<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Humboldt-Universität zu Berlin</orgName>
								<orgName type="institution" key="instit2">¶ University of Trento</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SAP ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Puscas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SAP ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SAP ML Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jähnichen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Humboldt-Universität zu Berlin</orgName>
								<orgName type="institution" key="instit2">¶ University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SAP ML Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Models trained in the context of continual learning (CL) should be able to learn from a stream of data over an indefinite period of time. The main challenges herein are: 1) maintaining old knowledge while simultaneously benefiting from it when learning new tasks, and 2) guaranteeing model scalability with a growing amount of data to learn from. In order to tackle these challenges, we introduce Dynamic Generative Memory (DGM) -synaptic plasticity driven framework for continual learning. DGM relies on conditional generative adversarial networks with learnable connection plasticity realized with neural masking. Specifically, we evaluate two variants of neural masking: applied to (i) layer activations and (ii) to connection weights directly. Furthermore, we propose a dynamic network expansion mechanism that ensures sufficient model capacity to accommodate for continually incoming tasks. The amount of added capacity is determined dynamically from the learned binary mask. We evaluate DGM in the continual class-incremental setup on visual classification tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Conventional Deep Neural Networks (DNN) fail to continually learn from a stream of data while maintaining knowledge. Specifically, reusing old knowledge in new contexts poses a severe challenge. Generally, there are several fundamental obstacles on the way to a continually trainable AI system: the problem of forgetting when learning from new data (catastrophic forgetting), lack of model scalability, i.e. the inability to scale up the model's size with a continuously growing amount of training data, and finally inability to transfer knowledge across tasks.</p><p>Several recent approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1]</ref> try to mitigate forgetting in ANNs while simulating synaptic plasticity directly in the task solving network. It is noteworthy that these methods topically tackle the task-incremental scenario, i.e. a separate classifier is trained to make predictions about each task. This further implies the availability of oracle knowledge of the task label at inference time. Such eval-uation is often referred to as multi-head evaluation in which the task label is associated with a dedicated output head. Alternatively, other approaches rely on single-head evaluation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2]</ref>. Here, the model is evaluated on all classes observed during the training, no matter which task they belong to. While single-head evaluation does not require oracle knowledge of the task label, it also does not reduce the output space of the model to the output space of the task. Thus single-head evaluation represents a harder, yet more realistic setup. Single-head evaluation is predominantly used in class-incremental setup, in which every newly introduced data batch contains examples of one to many new classes. As opposed to the task-incremental situation, models in class-incremental setup typically require the previously learned information to be replayed when learning new categories <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref>. The simplest way to accomplish this is by retaining and replaying real samples of previously seen categories to the task solver. However, retaining real samples has several intrinsic implications. First, it is very much against the notion of bio-inspired design, as natural brains do not feature the retrieval of information identical to originally exposed impressions <ref type="bibr" target="#b14">[15]</ref>. Second, as pointed out by <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22]</ref> storing raw samples of previous data can violate data privacy and memory restrictions of real-world applications. Such restrictions are particularly relevant for the vision domain with its continuously growing dataset sizes and rigorous privacy constraints.</p><p>In this work, we address the "strict" class-incremental setup. That is, we demand a classifier to learn from a stream of data with different classes occurring at different times with no access to previously seen data, i.e. no storing of real samples is allowed. Such a scenario is solely addressed by methods relying on generative memory -a generative network is used to memorize previously seen data distributions, samples of which can be replayed to the classifier at any time. Several strategies exist to avoid catastrophic forgetting in generative networks. The most successful approaches rely on deep generative replay (DGR) <ref type="bibr" target="#b29">[30]</ref> -repetitive retraining of the generator on a mix of synthesized samples of previous categories and real samples of new classes. In this work we propose Dynamic Generative Memory (DGM) with learnable connection plasticity represented by parameter level attention mechanism. As opposed to DGR, DGM features a single generator that is able to incrementally learn about new tasks without the need to replay previous knowledge.</p><p>Another important factor in the continual learning setting is the ability to scale, i.e. to maintain sufficient capacity to accommodate for a continuously growing amount of information. Given invariant resource constraints, it is inevitable that with a growing number of tasks to learn, the model capacity is depleted at some point in time. This issue is again exacerbated when simulating neural plasticity with parameter level hard attention masking. In order to guarantee sufficient capacity and constant expressive power of the underlying DNN, we keep the number of "free" parameters (i.e. to which the gradient updates can be freely applied) constant by expanding the network with exactly the number of parameters that were blocked for the previous task.</p><p>Our contribution is twofold: (a) we introduce Deep Generative Memory (DGM) -an adversarially trainable generative network that features neural plasticity through efficient learning of a sparse attention masks for the network weights (DGMw) or layer activations (DGMa); To the best of our knowledge we are the first to introduce weight level masks that are learned simultaneously with the base network; Furthermore, we conduct it in an adversarial context of a generative model; DGM is able to incrementally learn new information during adversarial training without the need to replay previous knowledge to its generator. (b) We propose an adaptive network expansion mechanism, facilitating resource efficient continual learning. In this context, we compare the proposed method to the state-of-the-art approaches for continual learning. Finally, we demonstrate that DGMw accommodates for higher efficiency, better parameter reusability and slower network growth than DGMa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Among the first works dealing with catastrophic forgetting in the context of lifelong learning are <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>, who tackle this problem by employing shallow neural networks, whereas our method makes use of modern deep architectures. Lately, a wealth of works dealing with catastrophic forgetting in context of DNNs have appeared in the literature, see e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b23">24]</ref>. Thus, EWC <ref type="bibr" target="#b8">[9]</ref> and RWalk <ref type="bibr" target="#b1">[2]</ref> rely on Fisher's information to identify parameters that carry most of the information about previously learned tasks, and apply structural regularization to "discourage" change of these parameters. <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b0">[1]</ref> identify important parameter segments based on the sensitivity of the loss or the learned prediction function to changes in the parameter space. Instead of relying on "soft" regularization techniques, <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b25">[26]</ref> propose to dedicate separate parameter subspaces to separate tasks. Serrà et al. <ref type="bibr" target="#b28">[29]</ref> pro-pose a hard attention to the task (HAT) mechanism. HAT finds dedicated parameter subspaces for all tasks in a single network while allowing them to mutually overlap. The optimal solution is then found in the corresponding parameter subspace of each task. All of these methods have been proposed for a "task-incremental learning" setup. In our work we specifically propose a method to overcome catastrophic forgetting within the "class-incremental" setup. Notably, a method designed for class-incremental learning can be generally applied in a task-incremental setup.</p><p>Several continuous learning approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8]</ref>, address catastrophic forgetting in the class-incremental setting, i.e. by storing raw samples of previously seen data and making use of them during the training on subsequent tasks. Thus, iCarl <ref type="bibr" target="#b21">[22]</ref> proposes to find m most representative samples of each class whose mean feature space most closely approximates the entire feature space of the class. The final classification task is done by the means of the nearest mean-of-exemplars classifier.</p><p>Recently, there has been a growing interest in employing deep generative models for memorizing previously seen data distributions instead of storing old samples. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> rely on the idea of generative replay, which requires retraining the generator at each time step on a mixture of synthesized images of previous classes and real samples from currently available data. However, apart from being inefficient in training, these approaches are severely prone to "semantic drifting". Namely, the quality of images generated during every memory replay highly depends on the images generated during previous replays, which can result in loss of quality and forgetting over time. In contrast, we propose to utilize a single generator that is able to incrementally learn new information during the normal adversarial training without the need to replay previous knowledge. This is achieved through efficiently learning a sparse mask for the learnable units of the generator network.</p><p>Similar to our method, <ref type="bibr" target="#b27">[28]</ref> proposed to avoid retraining the generator at every time-step on previous classes by applying EWC <ref type="bibr" target="#b8">[9]</ref> in the generative network. We pursue a similar goal with the key difference of utilizing a hard attention mechanism similar to the one described by <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. All three approaches make use of the techniques originally proposed in the context of binary-valued networks <ref type="bibr" target="#b2">[3]</ref>. Herein, binary weights are specifically learned from a real-valued embedding matrix that is passed through a binarization function. To this end, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> learn to mask a pre-trained network without changing the weights of the base networks, whereas <ref type="bibr" target="#b28">[29]</ref>  new task solving network in terms of a linear combination of the parameters of a fixed base network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Similarly to <ref type="bibr" target="#b32">[33]</ref>, we propose to expand the capacity of the employed base network, in our case the samples generator. The expansion is performed dynamically with an increasing amount of attained knowledge. However, <ref type="bibr" target="#b32">[33]</ref> propose to keep track of the semantic drift in every neuron, and then expand the network by duplicating neurons that are subject to sharp changes. In contrast, we compute weights importance concurrently during the course of network training by modeling the neuron behavior using learnable binary masks. As a result, our method explicitly does not require any further network retraining after expansion.</p><p>Other approaches like <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27</ref>] try to explicitly model short and long term memory with separate networks. In contrast to these methods, our approach does not explicitly keep two separate memory locations, but rather incorporates it implicitly in a single memory network. Thus, the memory transfer occurs during the binary mask learning from nonbinary (short term) to completely binary (long term) values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dynamic Generative Memory</head><p>Adopting the notation of <ref type="bibr" target="#b1">[2]</ref>,</p><formula xml:id="formula_0">let S t = {(x t i , y t i )} n t i=1</formula><p>denote a collection of data belonging to the task t ∈ T , where x t i ∈ X is the input data and y t i ∈ y t are the ground truth labels. While in the non-incremental setup the entire dataset S = ∪ |T | t=0 S t is available at once, in an incremental setup it becomes available to the model in chunks S t specifically only during the learning of task t. Thereby, S t can be composed of a collection of items from different classes, or even from a single class only. Furthermore, at the test time the output space covers all the labels observed so far featuring the single head evaluation: Y t = ∪ t j=1 y j . We consider a continual learning setup, in which a task solving model D has to learn its parameters θ D from the data S t being available at the learning time of task t. Task solver D should be able to maintain good performance on all classes Y t seen so far during the training. A conventional DNN, while being trained on S t , would adapt its parameters in a way that exhibits good performance solely on the labels of the current task y t , the previous tasks would be forgotten. To overcome this, we introduce a Generative Memory component G, who's task is to memorize previously seen data distributions. As visualized in <ref type="figure">Fig. 1</ref>, samples of the previously seen classes are synthesized by G and replayed to the task solver D at each step of continual learning to maintain good performance on the entire Y t . We train a generative adversarial network (GAN) <ref type="bibr" target="#b4">[5]</ref> and a sparse mask for the weights of its generator simultaneously. The learned masks model connection plasticity of neurons, thus avoiding overwriting of important units by restricting SGD updates to the parameter segments of G that exhibit free capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Binary Masks</head><p>We consider a generator network G θ G consisting of L layers, and a discriminator network D θ D . In our approach, D θ D serves as both: a discriminator for generated fake samples of the currently learned task (L adv. ) and as a classifier for the actual learning problem (L cls .) following the AC-GAN <ref type="bibr" target="#b18">[19]</ref> architecture. The system has to continually learn T tasks. During the SGD based training of task t, we learn a set of binary masks M t = [m t 1 , ..., m t L ] for the weights of each layer. Output of a fully connected layer l is obtained by combining the binary mask m t l with the layer weights:</p><formula xml:id="formula_1">y t l = σ act [(m t l • W l ) x], W l ∈ R n×p ,<label>(1)</label></formula><p>for σ act being some activation function. W l is the weight matrix applied between layer l and l − 1, · • · corresponds to the Hadamard product. In DGMw m t l is shaped identically to W l , whereas in case of DGMa the mask m t l is shaped as 1×p and should be expanded to the size of W l . Extension to more complex models such as e.g. CNNs is straightforward.</p><p>A single binary mask for a layer l and task t is given by:</p><formula xml:id="formula_2">m t l = σ(s · e t l ),<label>(2)</label></formula><p>where e t l is a real-valued mask embeddings matrix, s is a positive scaling parameter s ∈ R + , and σ a thresholding function σ : R → [0, 1]. Similarly to <ref type="bibr" target="#b28">[29]</ref> we use the sigmoid function as a pseudo step-function to ensure gradient flow to the embeddings e. In training of DGMw, we anneal the scaling parameter s incrementally during epoch i from 1/s i max to s i max (local annealing). s i max is similarly adjusted over the course of I epochs from 1/s max to s max (global annealing with s max being a fixed meta-parameter). The annealing the scheme is largely adopted from <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_3">s i max = 1 s max + (s max − 1 s max ) i − 1 I − 1 (3) s = 1 s i max + (s i max − 1 s i max ) b − 1 B − 1 .<label>(4)</label></formula><p>Here b ∈ {1, . . . , B} is the batch index and B the number of batches in each epoch of SGD training. DGMa only features global annealing of s, as it showed better performance. In order to prevent the overwriting of the knowledge related to previous classes in the generator network, gradients g l w.r.t. the weights of each layer l are multiplied by the reverse of the cumulated mask m ≤t l :</p><formula xml:id="formula_4">g l = [1 − m ≤t l ]g l , m ≤t l = max(m t l , m t−1 l ),<label>(5)</label></formula><p>where g l corresponds to the new gradient matrix and m ≤t l is the cumulated mask. Analogously to <ref type="bibr" target="#b28">[29]</ref>, we promote sparsity of the binary mask by adding a regularization term R t to the loss function of the AC-GAN <ref type="bibr" target="#b18">[19]</ref> generator:</p><formula xml:id="formula_5">R t (M t , M t−1 ) = L−1 l=1 N l i=1 m t l,i (1 − m &lt;t l,i ) L−1 l=1 N l i=1 1 − m &lt;t l,i ,<label>(6)</label></formula><p>where N l is the number of parameters of layer l. Here, parameters that were reserved previously are not penalized, promoting reuse of units over reserving new ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dynamic Network Expansion</head><p>As discussed by <ref type="bibr" target="#b32">[33]</ref>, significant domain shift between tasks leads to rapid network capacity exhaustion, manifesting in decreasing expressive power of the underlying network and ultimately in catastrophic forgetting. In case of DGM this effect will be caused by decreasing number of "free " parameters over the course of training due to parameter reservation. To avoid this effect, we take measures to ensure constant number of free parameters for each task.</p><p>DGMa. Consider a network layer l with an input vector of size n, an output vector of size p, and the mask m 1 l ∈ [0, 1] 1×p initialized with mask elements m 1 l of all neurons of the layer set to 0.5 (real-valued embeddings e 1 l are initialized with 0). After the initial training cycle on task 1, the number of free output neurons in layer l will decrease to p − δ 1 , where δ t is the number of neurons reserved for a generation task t, here t = 1. After the training cycle, the number of output neurons p of the layer l will be expanded by δ 1 . This guarantees that the free capacity of the layer is kept constant at p neurons for each learning cycle.</p><p>DGMw. In case of DGMw, after the initial training cycle the number of free weights will decrease to np − δ 1 , with δ 1 corresponding to the number of weights reserved for the generation task 1. The number of output neurons p is expanded by δ 1 /n. The number of free weights of the layer is kept constant, which can be verified by the following equation: (p + δ t /n)n − δ t = np. In practice we extend the number of output neurons by δ t /n . The number of free weight parameters in layer l is thus either np, if δ t /n ∈ Z, or np + p, otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training of DGM</head><p>The proposed system combines the joint learning of three tasks: a generative, a discriminative and finally, a classification task in the strictly class-incremental setup.</p><p>Using task labels as conditions, the generator network must learn from a training set X t = {X t 1 , ..., X t N } to generate images for task t. To this end, AC-GAN's conditional generator synthesizes images x t = G θ G (t, z, M t ), where θ G represents the parameters of the generator network, z denotes a random noise vector. The parameters corresponding to each task are optimized in an alternating fashion. As such, the generator optimization problem can be seen as minimizing L G = L t s − L t c + λ RU R t , with L c a cross entropy classification loss calculated on the the auxiliary output, L s a discriminative loss function used on the adversarial output layer of the network (implemented to be compliant with architectural requirements of WGAN <ref type="bibr" target="#b5">[6]</ref>) , and R t the regularizer term expanded upon in equation 6. To promote efficient parameter utilization, taking into consideration the proportion of the network already in use, the regularization weight λ RU is multiplied by the ratio α = St S f ree , where S t is the size of the network before training on task t, and S f ree is the number of free neurons. This ensures that less parameters are reused during early stages of training, and more during the later stages when the model already has gained a certain level of maturity.</p><p>The discriminator is optimized similarly through minimizing L D = L t c + L t s + λ GP L t gp , where L t gp represents a gradient penalty term implemented as in <ref type="bibr" target="#b5">[6]</ref> to ensure a more stable training process.  <ref type="table">Table 1</ref>: Comparison to the benchmark presented by <ref type="bibr" target="#b1">[2]</ref> (episodic memory with real samples) and <ref type="bibr" target="#b30">[31]</ref> (generative memory) of approaches evaluated in class-incremental setup. Joint training (JT) represents the upper bound (* aka. direct accuracy -classifier trained on real and tested on generated data <ref type="bibr" target="#b30">[31]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We perform experiments measuring the classification accuracy of our system in a strictly class-incremental setup on the following benchmark datasets: MNIST <ref type="bibr" target="#b10">[11]</ref>, SVHN <ref type="bibr" target="#b16">[17]</ref>, CIFAR-10 <ref type="bibr" target="#b9">[10]</ref>, and ImageNet-50 <ref type="bibr" target="#b24">[25]</ref>. Similarly to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2]</ref> we report an average accuracy (A t ) over the heldout test sets of classes 0...t seen so far during the training.</p><p>Datasets. The MNIST and SVHN datasets are composed of 60000 and 99289 images respectively, containing digits. The main difference is in the complexity and variance of the data used. SVHN's images are cropped photos containing house numbers and as such present varying viewpoints, illuminations, etc. CIFAR10 contains 60000 labeled images, split into 10 classes, roughly 6k images per class. Finally, we use a subset of the iILSVRC-2012 dataset containing 50 classes with 1300 images per category. All images are further resized to 32 x 32 before use.</p><p>Implementation details. We make use of the same architecture for the MNIST and SVHN experiments, a 3layer DCGAN <ref type="bibr" target="#b19">[20]</ref>, with the generator's number of parameters modified to be proportionally smaller than in <ref type="bibr" target="#b30">[31]</ref> (approx. 10% of the DCGAN's generator size used by <ref type="bibr" target="#b30">[31]</ref> for DGMw, and 44% for DGMa on MNIST and SVHN). The projection and reshape operation is performed with a convolutional layer instead of a fully connected one. For the CIFAR-10 experiments, we use the ResNet architecture proposed by <ref type="bibr" target="#b5">[6]</ref>. For the ImageNet-50 benchmark, the discriminator features a ResNet-18 architecture. All are modified to function as an AC-GAN <ref type="bibr" target="#b18">[19]</ref>.</p><p>All datasets are used to train a classification network in an incremental way. The performance of our method is evaluated quantitatively through comparison with benchmark  methods. Note that we compare our method mainly to the approaches that rely on the idea of generative memory replay, e.g. replaying generator synthesized samples of previous classes to the task solver without storing real samples of old data. For the sake of fairness, we only consider benchmarks evaluated in class-incremental single-head evaluation setup. Hereby, to best of our knowledge <ref type="bibr" target="#b30">[31]</ref> represent the state-of-the-art benchmark followed by <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b27">[28]</ref>. Next, we relax the strict incremental setup and allow partial storage of real samples of previous classes. Here we compare to the iCarl <ref type="bibr" target="#b21">[22]</ref>, which is the state-of-the-art method for continual learning with storing real samples. Results. A quantitative comparison of both variants of the proposed approach with other methods is listed in Tab. 1. We use joint training (JT) as an upper performance bound, where the task solver D is trained in a non-incremental fashion on all real samples without adversarial training being involved. The first set of methods evaluated by <ref type="bibr" target="#b1">[2]</ref> do not adhere to the   <ref type="figure">Fig. (a)</ref> illustrates the ratio of newly blocked and reused neurons over the total number of used neurons for a task t. <ref type="figure">Fig. (b)</ref> illustrates trajectories of mask value change for DGMa for a selected layer of G (bold line -layer occupation). strictly incremental setup, and thus make use of stored samples, which is often referred to as "episodic memory". The second set of methods we compare with do not store any real data samples. Our method outperforms the state of the art <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref> on the MNIST and SVHN benchmarks through the integration of the memory learning mechanism directly into the generator, and the expansion of said network as it saturates to accommodate new information. We yield an increase in performance over <ref type="bibr" target="#b30">[31]</ref>, a method that is based on a replay strategy for the generator and does not provide dynamic expansion mechanism of the memory network, leading to increased training time and sensitivity to semantic drift. As it can be observed for both, our method and <ref type="bibr" target="#b30">[31]</ref>, the accuracy reported between 5 and 10-tasks of the MNIST benchmark has changed a little, suggesting that for this dataset and evaluation methodology both approaches have largely curbed the effects of catastrophic forgetting. DGM reached a comparable performance to JT on MNIST (A 5 ) using the same architecture. This suggests that the incremental training methodology forced the network to learn a generalization ability comparable to the one it would learn given all the real data. Given the high accuracy reached on the MNIST dataset largely gives rise to questions concerning saturation, we opted to perform a further evaluation on the more visually diverse SVHN dataset. In this context, increased data diversity translates to more difficult generation and susceptibility to catastrophic forgetting. In fact, as can be seen in Tab. 1, the difference between 5-and 10-task accuracies is significantly larger in all methods than what can be observed in the MNIST experiments. DGM strongly outperforms all other methods on the SVHN benchmark. This can be attributed primarily to the efficient network expansion that allows for more redundancy in reserving representative neurons, and a less destructive joint use of neurons between tasks. Additionally, replay based methods (like <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>) can be potentially prone to generation of samples that represent class mixtures, especially for classes that semantically interfere with each other. DGM is immune to  this problematic, since no generative replay is involved in the generator's training. Thus, DGM becomes more stable in the face of catastrophic forgetting. The quality of the generated images after 10 stages of incremental training for MNIST and SVHN can be observed in <ref type="figure" target="#fig_6">Fig. 5</ref>. The generator is able to provide informative and diverse samples. Finally, in the ImageNet-50 benchmark, we incrementally add 50 classes with 10 classes per step and evaluate the classification performance of DGM using single-head evaluation. The dynamics of the top-5 classification accuracy of our system is provided in <ref type="figure" target="#fig_2">Fig. 2</ref>. Looking at the qualitative results shown in <ref type="figure" target="#fig_6">Fig. 5</ref>, it can be observed that generated samples clearly feature class discriminative features which are not forgotten after incremental training on 5 tasks of the benchmark. Nevertheless, for each newly learned task the discriminator network's classification layer is extended with 10 new outputs, making the complexity of the classification problem to grow constantly (from 10-way classification to 50-way classification). With the more complex ImageNet samples also the generation task becomes much harder than in datasets like MNIST and SVHN. These factors negatively impact the classification performance of the task solver presented in <ref type="figure" target="#fig_2">Fig. 2</ref>, where DGMw performs significantly worse than the JT upper bound. Next, we relax the strict incremental setup and allow the DGM to partially store real samples of previous classes. We compare the performance of DGM to the state-of-the-art iCarl <ref type="bibr" target="#b21">[22]</ref> 1 . Noteworthy, iCarl relies only on storing real samples of previous classes introducing a smart sample selection strategy. We define a ratio of stored real and total replayed samples r = n r /N , where N is the total number of samples replayed per class and n r is the number of randomly selected real samples stored per each previously seen class. To keep the number of replayed samples balanced with the number of real samples, N is set to be equal to the average number of samples per class in the currently observed data chunk S t . Furthermore, similarly to iCarl <ref type="bibr" target="#b21">[22]</ref> we define K to be the total number of real samples that can be stored by the algorithm at any point of time. We compare DGMw with iCarl for different values of K allowing the storage of K/|Y t | samples per class.</p><p>From Tab. 2 we observe that DGM is outperformed by iCarl when no real samples are replayed (i.e. r = 0) after 50 classes in top-1 and after 30 and 50 classes in top-5 accuracy. DGMw with r = 0 outperforms iCarl with K = 1000 in top-1 accuracy after 30 classes. Furthermore, we observe that adding real samples to the replay loop boosts DGM's classification accuracy beyond the iCarl's one. Thus, already for r = 0.1 the performance of our system can be improved significantly. We now consider DGM and iCarl with the same memory size K (we test for K = 1000 and K = 2000). Here DGM outperforms iCarl in top-1 accuracy after 30 classes, and almost reaches it in Top-5 accuracy. This is largely attributed to the advantage of DGM using generated samples additionally to the stored real once. Yet, a significant performance drop is observed after learning 5 tasks (A 50 ), where DGMw is outperformed by iCarl. This can be attributed to (a) the fact that the number of samples replayed per class decreases over time due to fixed K and increasing number of classes (e.g. for K = 2000, 66  samples are played per class after seeing 30 classes, and 40 samples after 50 classes), as well as (b) iCarl's smart samples selection strategy that favors samples that better approximate the mean of all training samples per class. Such samples selection strategy appears to works better in a situation where the number of real samples available per class decreases over time. It is noteworthy that iCarl's samples selection strategy can also be applied to DGM. Growth Pattern Analysis. One of the primary strengths of DGM is an efficient generator network expansion component, removing which would lead to the inability of the generator to accommodate for memorizing new task. Performance of DGM is directly related to how the network parameters are reserved during incremental learning,which ultimately depends on the generator's ability to generalize from previously learned tasks. <ref type="figure" target="#fig_5">Fig. 4</ref> reports network growth against the number of tasks learned. We find that learning masks directly for the layer weights (DGMw) significantly slows down the network growth. Furthermore, one can observe the high efficiency of DGM's sub-linear growth pattern as compared to the worst-case linear growth scenario. Interestingly, as shown in Tab.3, after incrementally learning 10 classes the final number of generator's base network's parameters is lower than the one of the benchmarked MeRGAN <ref type="bibr" target="#b30">[31]</ref>. More specifically, we observe the final network's size reduction of 26% on MNIST, and 23% on SVHN as compared to MeRGAN's fixed generator. In general, growth pattern of DGM depends on various factors: e.g. initialization size, similarity and order of classes etc.. A rather low saturation tendency of DGM's growth pattern observed in <ref type="figure" target="#fig_5">Fig. 4</ref> can be attributed to the fact that with growing amount of information stored in the network, selecting relevant knowledge becomes increasingly hard.</p><p>Plasticity Evolution Analysis. We analyze how learning is accomplished within a given task t, and how this further affects the wider algorithm. For a given task t, its binary mask M t is initialized with the scaling parameter s = 1. <ref type="figure" target="#fig_4">Fig. 3(b)</ref> shows the learning trajectories of the mask values over the learning time of task t. Here, at task initialization of DGMa the mask is completely non-binary (all mask values are 0.5). As training progresses, the scaling parameter s is annealed, the network is encouraged to search for the most efficient parameter constellation (epoch 2-10). But with most mask values near 0 (most of the units are not used, high efficiency is reached), the network's capacity to learn is greatly curtailed. The optimization process pushes the mask to become less sparse, the number of non-zero mask values is steadily increasing until the optimal mask constellation is found, a trend observed in the segment between the epoch 10 and 55. This behaviour can be seen as a short-term memory formation -if learning was stopped at e.g. epoch 40 only a relatively small fraction of learnable units would be masked in a binary way, the units with nonbinary mask values would be still partially overwritten during the subsequent learning resulting in forgetting. A transition from short to the long-term memory occurs largely within the epochs 45-65. Here the most representative units are selected and reserved by the network, parameters that have not made this transition are essentially left as unused for the learning task t. Finally, the optimal neuron constellation is optimized for the given task from epoch 60 onwards.</p><p>For a given task t, masked units (neurons in DGMa, network weights in DGMw) can be broadly divided into three types: (i) units that are not used at all (U) [masked with 0] , (ii) units that are newly blocked for the task (N B t ), (iii) units that have been reused from previous tasks (R t ). <ref type="figure" target="#fig_4">Figure 3</ref>(a) presents the evolution of the ratio of the (N B t ) and (R t ) types over the total number of units blocked for the task t. Of particular importance is that the ratio of reused units is increasing between tasks, while the ratio of newly blocked units is decreasing. These trends can be justified by the network learning to generalize better, leading to a more efficient capacity allocation for new tasks.</p><p>Memory Usage Analysis. We evaluate the viability of generative memory usage from the perspective of required disc space. Storing the generator for the ImageNet-50 benchmark (weights and masks) corresponds to the disc space requirement of 228M B. Thereby storing the preprocessed training samples of ImageNet-50 results in the required disc space of 315M B. In this particular case storing the generator 27.5% more memory efficient than storing the training samples. Naturally, this effect will become more pronounced for larger datasets.</p><p>As discussed in Sec. 4, DGMw features a more efficient network growth pattern as compared to DGMa. Yet, DGMw's attention masks are shaped identically to the weight matrices and thus require more memory. Tab. 4 gives an overview of the required disc space for different components of DGMa and DGMw (masks are stored in a sparse form). Less total disc space is required to store DGMw's model as compared to DGMa, which suggests that DGMw's model growth efficiency compensates for the higher memory required for storing its masks. During the training, DGMw still exhibits a larger memory consumption, as the real-valued mask embeddings for the currently learned task must be kept in memory in a non-sparse form.  <ref type="table">Table 4</ref>: Disc space required to store different components of DGMw and DGMa in Megabytes (MB) after 5 and 10 tasks. Compared models exhibit comparable performance on MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we study the continual learning problem in a single-head, strictly incremental context. We propose a Dynamic Generative Memory approach for class-incremental continual learning. Our results suggest that DGM successfully overcomes catastrophic forgetting by making use of a conditional generative adversarial model where the generator is used as a memory module endowed with neural masking. We find that neural masking works more efficient when applied directly to layers' weights instead of activations. Future work will address the limitations of the DGM including missing backward knowledge transfer and limited saturation of the network growth pattern.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>G 1 …Figure 1 :</head><label>11</label><figDesc>(HAT) features binary masklearning for the layer activations simultaneously to the training of the base network. While DGMa features HATlike layer activation masking, DGMw accomplishes binary mask learning directly on the weights of the generator. Other works propose to use non-binary filters to define a Dynamic Generative Memory: auxiliary output of D is trained on the real samples of the current task t and synthesized sample of previously seen tasks 1...t − 1. Adversarial training is accomplished with real and fake samples of the current task. Connection plasticity simulated with binary mask applied to the generators weights or activations is learned simultaneously to the adversarial training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Top-5 performance of DGMw together with upper and lower performance bounds measured for ImageNet-50 benchmark. DGM+real denotes variation with different ratios of real samples added to the replay loop (25%-75% of samples being real)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Mask learning trajectories DGMa</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Masks learning dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Network growth (numb. of neurons) in the incremental MNIST setup. Comparison of best performing DGMw and comparable DGMa (DGMw A10: 96.46%, DGMa -96.98%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Images generated by DGM for MNIST(top), SVHN(middle) after training on 10 tasks, and ImageNet(bottom) after 5 tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of DGM and iCarl for different values of r and memory size K.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of generator's base network's size (number of parameters) of DGMw and MeRGAN.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use classes of ImageNet-50 with 32 × 32 resolution with the iCarl implementation under https://github.com/srebuffi/iCaRL</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno>abs/1711.09601</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno>abs/1801.10112</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><forename type="middle">B</forename><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00363</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep generative dual memory network for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fearnet: Brain-inspired model for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10563</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>abs/1612.00796</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning without forgetting. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno>abs/1606.09282</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Piggyback: Adding multiple tasks to a single, fixed network by learning to mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06519</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adding new tasks to a single network with weight trasformations using binary masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11119</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Synapses and memory storage. Cold Spring Harbor perspectives in biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mayford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siegelbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Kandel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">5751</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of Learning and Motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10628</idno>
		<title level="m">Variational continual learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratcliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="page" from="285" to="308" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<idno>abs/1611.07725</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Incremental learning through deep adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>abs/1606.04671</idno>
	</analytic>
	<monogr>
		<title level="j">Progressive neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06370</idno>
		<title level="m">Progress &amp; compress: A scalable framework for continual learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beatson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08395</idno>
		<title level="m">Continual learning in generative adversarial nets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Overcoming catastrophic forgetting with hard attention to the task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Surís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<idno>abs/1801.01423</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Memory Replay GANs: learning to generate images from new categories without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Incremental classifier learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno>abs/1802.00853</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improved multitask learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>abs/1703.04200</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
