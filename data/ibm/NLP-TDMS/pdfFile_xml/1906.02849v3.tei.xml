<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-scale self-guided attention for medical image segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sinha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Dolz</surname></persName>
						</author>
						<title level="a" type="main">Multi-scale self-guided attention for medical image segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional neural networks</term>
					<term>Deep learning</term>
					<term>Medical image segmentation</term>
					<term>Deep attention</term>
					<term>Self-attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Even though convolutional neural networks (CNNs) are driving progress in medical image segmentation, standard models still have some drawbacks. First, the use of multi-scale approaches, i.e., encoder-decoder architectures, leads to a redundant use of information, where similar low-level features are extracted multiple times at multiple scales. Second, long-range feature dependencies are not efficiently modeled, resulting in nonoptimal discriminative feature representations associated with each semantic class. In this paper we attempt to overcome these limitations with the proposed architecture, by capturing richer contextual dependencies based on the use of guided self-attention mechanisms. This approach is able to integrate local features with their corresponding global dependencies, as well as highlight interdependent channel maps in an adaptive manner. Further, the additional loss between different modules guides the attention mechanisms to neglect irrelevant information and focus on more discriminant regions of the image by emphasizing relevant feature associations. We evaluate the proposed model in the context of semantic segmentation on three different datasets: abdominal organs, cardiovascular structures and brain tumors. A series of ablation experiments support the importance of these attention modules in the proposed architecture. In addition, compared to other state-of-the-art segmentation networks our model yields better segmentation performance, increasing the accuracy of the predictions while reducing the standard deviation. This demonstrates the efficiency of our approach to generate precise and reliable automatic segmentations of medical images. Our code is made publicly available at: https://github.com/sinAshish/Multi-Scale-Attention</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic segmentation of medical images is a crucial step in diagnosis, treatment and follow-up of many diseases. Despite the automation of this task has been widely studied in the past, manual annotations are still typically used in clinical practice, which is a time-consuming and prone to inter and intra-observer variability process. Thus, there is a high demand on accurate and reliable automatic segmentation methods that allow to improve the work flow efficiency in clinical scenarios, alleviating the workload of radiologists and other medical experts.</p><p>Recently, convolutional neural networks (CNNs) have achieved state-of-the-art performance in a breadth of visual recognition tasks, becoming very popular due to their powerful, nonlinear feature extraction capabilities. These deep models dominate the literature in medical image segmentation A. Sinha is with the Indian Institute of Technology Roorkee, India. e-mail: asinha@mt.iitr.ac.in. J. Dolz is with the√âcole de technologie Superieure, Montreal, Canada. email:jose.dolz@etsmtl.ca.</p><p>Manuscript received XXX; revised XXX. <ref type="bibr" target="#b0">[1]</ref> and have achieved outstanding performance in a broad span of applications, including brain <ref type="bibr" target="#b1">[2]</ref> or cardiac <ref type="bibr" target="#b2">[3]</ref> imaging, for example, becoming the de facto solution for these problems. In this scenario, fully convolutional neural networks <ref type="bibr" target="#b3">[4]</ref> or encoder-decoder architectures <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> are typically the standard choice. These architectures are commonly composed of a contracting path, which collapses an input image into a set of high-level features, and an expanding path, where highlevel features are used to reconstruct a pixel-wise segmentation mask at a single <ref type="bibr" target="#b3">[4]</ref> or multiple upsampling steps <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Nevertheless, despite their strong representation power, these multi-scale approaches lead to a redundant use of information flow, e.g., similar low-level features are extracted multiple times at different levels within the network. Furthermore, the discriminative power of the learned feature representations for pixel-wise recognition may be insufficient for some challenging tasks, such as medical image segmentation. Recent works to improve the discriminative ability of feature representations include the use of multi-scale context fusion <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Zhao et al. <ref type="bibr" target="#b7">[8]</ref> proposed a pyramid network that exploited global information at different scales by aggregating feature maps generated by multiple dilated convolutional blocks. Aggregation of contextual multi-scale information can also be achieved through pooling operations <ref type="bibr" target="#b10">[11]</ref>. Even though these strategies may help to capture objects at different scales, contextual dependencies for all image regions are homogeneous and non-adaptive, ignoring the difference between local representation and contextual dependencies for different categories. Further, these multi-context representations are manually designed, lacking flexibility to model the multi-context representations. This makes that longrange object relationships in the whole image cannot be fully leveraged in these approaches, which is of pivotal importance in many medical imaging segmentation problems.</p><p>Alternatively, attention mechanisms have been widely studied in deep CNNs for many computer vision tasks in order to efficiently integrate local and global features, including human pose estimation <ref type="bibr" target="#b11">[12]</ref>, emotion recognition <ref type="bibr" target="#b12">[13]</ref>, text detection <ref type="bibr" target="#b13">[14]</ref>, object detection <ref type="bibr" target="#b14">[15]</ref> and classification <ref type="bibr" target="#b15">[16]</ref>. Unlike standard multi-scale features fusion approaches, which compress an entire image into a static representation, attention allows the network to focus on the most relevant features without additional supervision, avoiding the use of multiple similar feature maps and highlighting salient features that are useful for a given task. Semantic segmentation networks have also benefited from attention modules, which has resulted in enhanced models for pixel-wise recognition tasks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. For example, Chen et al. <ref type="bibr" target="#b16">[17]</ref> proposed an attention mechanism to weight multi-scale features extracted at different scales in the context of natural scene segmentation. This method improved the segmentation performance over classical average and max-pooling techniques to merge multiscale features predictions.</p><p>Despite the growing interest on integrating attention mechanisms in image segmentation networks for natural scenes, their adoption in medical images remains scarce <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, being limited to simple attention models. Thus, in this work, we explore more complex attention mechanisms that can boost the performance of standard deep networks for the task of medical image segmentation. Specifically, we propose a multi-scale guided attention network for medical image segmentation. First, the multi-scale approach generates stacks at different resolutions containing different semantics. While lower-level stacks focus on local appearance, higher-level stacks will encode global representations. This multi-scale strategy encourages that attention maps generated at different resolutions encode different semantic information. Then, at each scale, a stack of attention modules will gradually remove noisy areas and emphasize those regions that are more relevant to the semantic descriptions of the targets. Each attention module contains two independent self-attention mechanisms, which focus on modelling position and channel feature dependencies, respectively. This duple allows to model wider and richer contextual representations and improve dependencies between channel maps, resulting in enhanced feature representations. We validate our method in three different segmentation tasks: abdominal organ, cardiovascular structures and brain tumor. Results show that the proposed architecture improves the segmentation performance by successfully modeling rich contextual dependencies over local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Medical image segmentation</head><p>Even though segmentation of medical images has been widely studied in the past <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> it is undeniable that CNNs are driving progress in this field, leading to outstanding performances in many applications. Most available medical image segmentation architectures are inspired from the well-known fully convolutional neural network (FCN) <ref type="bibr" target="#b3">[4]</ref> or UNet <ref type="bibr" target="#b4">[5]</ref>. In FCN the fully connected layers of standard classification CNNS are replaced by convolutional layers to achieve dense pixel prediction at one forward step. To recover the original resolution of the input image, the prediction is upsampled in a single step. Further, to improve the prediction capabilities, skip connections are included in the network by employing the intermediate feature maps. On the other hand, UNet contains contractive and expansive paths created using the combination of convolutional layers with pooling and upsampling layers. Skip connections are used to concatenate the features from contractive and expansive path layers. Many extensions of these networks have been proposed to solve pixel-wise segmentation problems in a wide range of applications <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep attention</head><p>Attention mechanisms aim at emphasizing important local regions captured in local features and filtering irrelevant infor-mation transferred by global features, improving the modeling of long-range dependencies. These modules have therefore become an essential part of models that need to capture global dependencies. The integration of these attention modules has been proved very successful in many vision problems, such as image captioning <ref type="bibr" target="#b39">[40]</ref>, image question-answering <ref type="bibr" target="#b40">[41]</ref> or classification <ref type="bibr" target="#b41">[42]</ref>. Self-attention <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> has recently attracted the attention of researchers, as it exhibits a good ability to model long-range dependencies while maintaining computational and statistical efficiency. In these modules, the response at each position is calculated by attending to all positions and taking their weighted average in an embedding space. For image vision problems, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> integrated selfattention to model the relation of local features with their corresponding global dependencies. For instance, the point-wise spatial attention network (PSANet) proposed in <ref type="bibr" target="#b17">[18]</ref> allows a flexible and dynamic aggregation of long-range contextual information by connecting each position in the feature map with all the others through self-adaptive attention maps.</p><p>Recent works have indicated that attention features generated in a single step may still contain noise introduced from regions that are irrelevant for a given class, leading to suboptimal results <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b45">[46]</ref>. To overcome this issue, some works have investigated the use of progressive multiple attention layers in the context of visual question answering <ref type="bibr" target="#b40">[41]</ref> or zero shot learning <ref type="bibr" target="#b45">[46]</ref>. This strategy gradually filters undesired noise and emphasizes the regions highly relevant for the class semantic representations. To the best of our knowledge, the application of stacked attention modules remains unexplored in semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Medical image segmentation with deep attention</head><p>Even though attention mechanisms are becoming popular on many vision problems, the literature on medical image segmentation with attention remains scarce, with simple attention modules <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Wang et al. <ref type="bibr" target="#b22">[23]</ref> employed attention modules at multiple resolutions to combine local deep attention features (DAF) with global context for prostate segmentation on Ultrasound images. To model longrange dependencies local and global features were combined in a simple attention module, which contains three convolutional layers followed by a softmax function to create the attention map. A similar attention module, composed of two convolutional layers followed by a softmax, was integrated in a hierarchical aggregation framework integrated in UNet for left atrial segmentation <ref type="bibr" target="#b23">[24]</ref>. More recently, additive attention gate modules were integrated in the skip connections of the decoding path of UNet with the goal of better model complimentary information from the encoder <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Target structures on medical imaging typically present intra and inter-class diversity on size, shape and texture, particularly if images are processed in 2D. Traditional CNNs for segmentation have a local receptive field, which results in the generation of local feature representations. As long-range The guided attention modules will generate attentive features at multiple scales, removing noisy areas and helping the network to emphasize the regions that are more relevant to the semantic classes. contextual information is not properly encoded, local features representations may lead to potential differences between features corresponding to the pixels with the same label <ref type="bibr" target="#b18">[19]</ref>. This may introduce intra-class inconsistency that can ultimately impact on the recognition performance <ref type="bibr" target="#b46">[47]</ref>. To tackle with this problem, we investigate attention mechanisms to build associations between features. First, global context is captured by employing a multi-scale strategy. Then, learned features at multiple scales are fed into the guided attention modules, which are composed by a stack of spatial and channel selfattention modules. While the spatial and channel self-attention modules will help to adaptively integrate local features with their global dependencies, the stack of attention modules will help to gradually filter noise out emphasizing on relevant information. The overview of the proposed framework is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-scale attention maps</head><p>Multi-scale features are known to be useful in computer vision problems even before the deep learning era <ref type="bibr" target="#b47">[48]</ref>. In the context of deep segmentation networks, the integration of multi-scale features has demonstrated astonishing performance <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Inspired by these works we make use of learned features at multiple scales, which help to encode both global and local context. Specifically we follow the multi-scale strategy recently proposed in <ref type="bibr" target="#b22">[23]</ref>. In this setting, features at multiple scales are denoted as F s , where s indicates the level in the architecture <ref type="figure" target="#fig_0">(Fig. 1</ref>). Since features come at different resolutions for each level s, they are upsampled to a common resolution by employing bilinear interpolation, leading to enlarged feature maps F s . Then, F s from all the scales are concatenated forming a tensor that is convolved to create a common multiscale feature map, F M S = conv([F 0 , F 1 , F 2 , F 3 ]). Thus, F M S encodes low-level detail information from shallow layers as well as high-level semantics learned in deeper layers. Then, this new multi-scale feature map is combined with each of the feature maps at different scales s and fed into the guided attention modules to generate the attention features A s :</p><formula xml:id="formula_0">A s = AttM od s (conv([F s , F M S ]))<label>(1)</label></formula><p>where AttM od represents each guided attention module (Section III-D). As the multi-scale feature maps F M S are combined at each individual layer, complementary low-level information and high-level semantics from F M S are encoded jointly, resulting in a more powerful representation. In the following sections we detail how the attentive features A s are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial and Channel self-attention modules</head><p>As introduced earlier, receptive fields in traditional segmentation deep models are reduced to a local vicinity. This limits the capabilities of modeling wider and richer contextual representations. On the other hand, channel maps can be considered as class-specific responses, where different semantic responses are associated with each other. Thus, another strategy to enhance the feature representation of specific semantics is to improve the dependencies between channel maps <ref type="bibr" target="#b50">[51]</ref>. To address these limitations of standard CNNs we employ the position and channel attention modules recently proposed in <ref type="bibr" target="#b18">[19]</ref>, which are depicted in <ref type="figure">Figure 2</ref>.</p><p>Position attention module (PAM): Let denote F ‚àà R C√óW √óH an input feature map to the attention module, where C, W, H represent the channel, width and height dimensions, respectively. In the upper branch F is passed through a convolutional block, resulting in a feature map F p 0 ‚àà R C √óW √óH , where C is equal to C/8 1 . Then, F p 0 is reshaped to a feature map of shape (W √ó H) √ó C . In the second branch, the input feature map F follows the same operations and then is transposed, resulting in F p 1 ‚àà R C √ó(W √óH) . Both maps are multiplied and softmax is applied on the resulted matrix to generate the spatial attention map S p ‚àà R (W √óH)√ó(W √óH) :</p><formula xml:id="formula_1">s p i,j = exp (F p 0,i ¬∑ F p 1,j ) W √óH i=1 exp (F p 0,i ¬∑ F p 1,j )<label>(2)</label></formula><p>where s p i,j evaluates the impact of the i th position on the j th position. The input F is fed into a different convolutional block in the third branch, resulting in F p 2 ‚àà R C√ó(W √óH) , which has the same shape as F . As in the other branches, F p 2 is reshaped becoming F p 2 ‚àà R C√ó(W √óH) . Then it is multiplied by a permuted version of the spatial attention map S, whose output is reshaped to a R C√ó(W √óH) . The attention feature map corresponding to the position attention module, i.e., F P AM , can be therefore formulated as follows:</p><formula xml:id="formula_2">F P AM,j = Œª p W √óH i=1 s p ij F p 2,j + F j (3)</formula><p>As in <ref type="bibr" target="#b18">[19]</ref>, the value of Œª p is initialized to 0 and it is gradually learned to give more importance to the spatial attention map. Thus, the position attention module selectively aggregates global context to the learned features, guided by the spatial attention map.</p><formula xml:id="formula_3">(C/8)xWxH (WxH)xC/8 (C/8)x(WxH) Softmax (WxH) x (WxH) Cx(WxH) CxWxH Cx(WxH) Cx(WxH) (WxH)xC Cx(WxH) Softmax CxC Cx(WxH) CxWxH CxWxH</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention features</head><p>Channel attention module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position attention module</head><p>Reshape Permute</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CxWxH</head><p>Convolution CxWxH CxWxH <ref type="figure">Fig. 2</ref>: Details of the position and channel attention modules inspired by <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel attention module (CAM):</head><p>The pipeline of the channel attention module is depicted at the bottom of <ref type="figure">Figure 2</ref>. The input F ‚àà R C√óW √óH is reshaped in the first two branches of the CAM, and permuted in the second branch, leading to F c 0 ‚àà R (W √óH)√óC and F c 1 ‚àà R C√ó(W √óH) , respectively. Then, we perform a matrix multiplication between F c 0 and F c 1 , and obtain the channel attention map S c ‚àà R C√óC as:</p><formula xml:id="formula_4">s c i,j = exp (F c 0,i ¬∑ F c 1,j ) C i=1 exp (F c 0,i ¬∑ F c 1,j )<label>(4)</label></formula><p>where the impact of the i th channel on the j th is given by s c i,j . This is then multiplied by a transposed version of the input F , i.e., F c 2 , whose result is reshaped to R C√ó(W √óH) . Then, the final channel attention map is obtained as:</p><formula xml:id="formula_5">F CAM,j = Œª c C i=1 s c ij F c 2,j + F j<label>(5)</label></formula><p>where Œª c controls the importance of the channel attention map over the input feature map F . Similarly to Œª p , Œª c is initially set to 0 and gradually learned. This formulation aggregates weighted versions of the features of all the channels into the original features, highlighting class-dependent feature maps and increasing feature discriminability between classes. At the end of both attention modules, the new generated features are fed into a convolutional layer before performing an element-wise sum operation to generate the position-channel attention features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Guiding attention</head><p>Inspired by recent work to stack attention modules in the context of image classification <ref type="bibr" target="#b45">[46]</ref>, we propose to add progressive refinement of attentive features through sequential refinement modules. The intuition is that this sequential refinement will progressively weight the importance of different local regions, while masking out irrelevant noise. Particularly, given the feature map F at the input of the guided attention module at scale s-generated by concatenating F M S and F s -, it generates attention features via a multi-step refinement <ref type="figure" target="#fig_1">(Fig.  3</ref>). In the first step, F is used by the position and channel attention modules to generate self-attention features. In parallel, we integrate an encoder-decoder network that compresses the input features F into a compacted representation in the latent space <ref type="bibr" target="#b45">[46]</ref>. The objective is that the class information can be embedded into the subsequent guided attention modules by forcing the latent representation of encoder-decoders to be close, which is formulated as:</p><formula xml:id="formula_6">L G = M ‚àí1 i E i (F i‚àí1 A ) ‚àí E i+1 (F i A ) 2 2<label>(6)</label></formula><p>where E i (¬∑) is the encoded representation of the i-th encoder-decoder network, F i A denotes the attention features generated after the i-th dual attention module and M the number of iterations. Note that F i‚àí1 A are the features at the input of the semantic guided attention module, F . Specifically, the feature maps reconstructed in the first encoder-decoder (n = 0) are combined with the self-attention features generated by the first attention module through a matrix-multiplication to generate F SA . In addition, to ensure that the reconstructed features correspond to the features at the input of the positionchannel attention modules, the output of the encoders are forced to be close to their input:</p><formula xml:id="formula_7">L Rec = M i F i ‚àíF i 2 2<label>(7)</label></formula><p>whereF i are the reconstructed feature maps, i.e., D i (E i (F )) of the i-th encoder-decoder networks.</p><p>As the guided attention module is applied at multiple scales, the combined guided loss for all the modules will be:</p><formula xml:id="formula_8">L G T otal = S s=0 L s G<label>(8)</label></formula><p>Similarly, the total reconstruction loss becomes:  where L Rec1 and L Rec2 are the reconstruction losses for the encoder-decoder architectures in the first and second block of the guided attention module.</p><formula xml:id="formula_9">L Rec T otal = S s=0 L s Rec (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Deep supervision</head><p>While the attention modules do not require auxiliary objective functions, we found that the use of extra supervision at each scale <ref type="bibr" target="#b51">[52]</ref> improved the segmentation performance of the proposed model, which is in line with similar works in the literature <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><formula xml:id="formula_10">L Seg T otal = S s=0 L s Seg F + S s=0 L s Seg A<label>(10)</label></formula><p>where the first term refers to the segmentation results at the raw features F s and the second term evaluates the segmentation result provided by the attention features. In all the cases, the multi-class cross-entropy between the network prediction and the ground truth labels is employed as segmentation loss. The final objective function to optimize becomes:</p><formula xml:id="formula_11">L T otal = Œ±L Seg T otal + Œ≤L G T otal + Œ≥L Rec T otal<label>(11)</label></formula><p>where Œ±, Œ≤ and Œ≥ control the importance of each term in the main loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setting</head><p>Datasets: We employ three public segmentation benchmarks to evaluate our method. First, the abdominal MRI dataset from the Combined Healthy Abdominal Organ Segmentation (CHAOS) Challenge <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Particularly, we focus on the segmentation of abdominal organs (spleen, liver and kidneys) on MRI (T1-DUAL in phase), which includes scans from 20 subjects for training with their corresponding ground truth annotations, and 20 for testing without annotations. Scans have a resolution of 256√ó256 pixels per slice, and between 26 and 50 slices. Since testing labels are not provided within the dataset, we employed the training dataset for our experiments, splitting it into subsets of 13, 2 and 5 subjects that were used for training, validation and testing. We repeated the process 3 times selecting different subjects and report the average results over the three folds. Then, we evaluated our approach on the task of whole-heart and great vessel segmentation from 3D Cardiovascular MRI in congenital heart disease, provided in the HVSMR 2016 Challenge <ref type="bibr" target="#b55">[56]</ref>. Particularly, the myocardium and the blood pool are targeted in this scenario. The training set consists on 10 MRI Axial scans with their corresponding manual segmentations. Image dimensions varied across subjects, with an average of 390 √ó 390 √ó 165 voxel volumes. We report results on the training data, by employing a 5-fold cross-validation strategy, where each fold contains 6 scans for training, 2 for validation and 2 for testing. To increase the variability of the data, we rotate, flipped and mirrored the images randomly, but without augmenting the dataset size. For the third task, we employed the brain segmentation dataset provided in the Medical Segmentation Decathlon Challenge 2 . Particularly, this dataset contains multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w) from the BRATS'16 and BRATS'17 Challenges <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. The focus of this dataset is on the segmentation of necrotic (TC) and active areas (ET), as well as oedema (ED) in brain gliomas. We employed 484 scans that were split into training (388 scans), validation (48 scans) and testing <ref type="bibr">(48 scans)</ref>. Similarly to previous tasks, we rotate, flipped and mirrored the images randomly, but without augmenting the dataset size.</p><p>Network architectures: The multi-scale strategy in the proposed network is based on the recently work in <ref type="bibr" target="#b22">[23]</ref>, and is considered as the lower baseline in our experiments. First, we perform an ablation study on the different proposed modules to evaluate the impact of each choice in the segmentation performance. The first two networks -i.e., Proposed (PAM) and Proposed (CAM)extend the baseline by replacing the attention module by either the spatial or the channel selfattention module <ref type="figure">(Fig. 2)</ref>, respectively. Then, both modules are combined simultaneously, leading to the Proposed (DANet) model. In the next model -i.e., Proposed (MS-Dual)the attention features generated by the dual attention module are refined in a multi-step process, where a second dual attention module is included. Last, the proposed architecture, referred to as Proposed (MS-Dual-Guided) extends the Proposed (MS-Dual) model by incorporating the semantic guidance ( <ref type="figure" target="#fig_1">Fig. 3)</ref>. We also evaluated the impact of different elements, other than the attention modules, on the proposed multi-scale architectures. First, we remove the deep supervision (first term in eq. 10) on our model. Second, instead of using an encoder-decoder structure to reconstruct the input features at each dual attention module, we remove this and replace the eq. 7 by the mean error square loss between the input and the output of each attention module. This models is referred to as w/out encoder-decoder (dist). And last, we also investigated the effect of not having an encoder-decoder, i.e., no guidance, in the refinements steps, which is referred to as w/out encoder-decoder). In addition, we evaluated the impact of having multiple refinements steps n, with n = 1, 2, 3 and 5.</p><p>Furthermore we compared the performance of the pro-posed network to other state-of-the-art architectures integrating attention: Attention UNet <ref type="bibr" target="#b24">[25]</ref>, DANet <ref type="bibr" target="#b18">[19]</ref> and Pyramidal Attention Network (PAN) <ref type="bibr" target="#b19">[20]</ref>.</p><p>Training and implementation details: We train all the networks using Adam optimizer with mini-batch of size 8, and with Œ≤ 1 and Œ≤ 2 set to 0.9 and 0.99, respectively. While most of the networks converged during the first 250 epochs, we found that PAN <ref type="bibr" target="#b19">[20]</ref> and DANet <ref type="bibr" target="#b18">[19]</ref> needed around 400 epochs to achieve the best results. The learning rate is initially set to 0.001 and multiplied by 0.5 after 50 epochs without improvement on the validation set. As a segmentation objective function, we employ the cross-entropy error at each pixel over all the categories for all the networks. Furthermore, as introduced in Section III, we use the objective function in eq. (11) in the proposed architecture, with Œ±, Œ≤ and Œ≥ set empirically to 1, 0.25 and 0.1, respectively. As input of the networks we employed 2D axial images of size 256 √ó 256. Experiments were performed in a server equipped with a Titan V. The code of our model is made publicly available at https://github.com/sinAshish/Multi-Scale-Attention.</p><p>Evaluation: Similarity between ground truth and CNN segmentations is assessed by employing several comparison metrics. First, we resort to the widely used Dice similarity coefficient (DSC) to compare volumes based on their overlap. Further, we also assess the segmentation performance based on the volume similarity (VS). Additionally, to measure the sensitivity to segmentation outline, we considered the use of the mean surface distance (MSD). The formulation of these metrics is detailed in the Supplemental materials. Since interslice distances and x-y spacing for each individual scan are not provided, we report these results on voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Ablation study on the proposed attention modules: To validate the individual contribution of different components to the segmentation performance, we perform an ablation experiment under different settings. <ref type="table" target="#tab_2">Table I</ref> reports the results of the different attention modules. Compared to the baseline, we observe that by integrating either a spatial (PAM) or an attention module (CAM) at each scale in the baseline architecture the performance improves between 2-3% in terms of overlapping and volume similarity, and between 12-18% in terms of surface distances, as average. On the other hand, having both modules in parallel -i.e., Proposed (DANet)brings slightly better results in terms of DSC, but achieves lower performance when employing the surface distance metric. However, despite the lower average performance on the MSD, the proposed DANet model still achieves better results in 3 out of 4 structures compared to the channel attention model. This trend is repeated on the DSC metric, where DANet surpasses the proposed CAM architecture in the same 3 structures: liver and both left and right kidneys. This suggests that, even though both spatial and channel attention bring an improvement on the performance, the channel attention module contributes more than the spatial attention when they are combined. If features generated by the proposed DANet model are refined in a second step -network referred to as Proposed(MS-Dual)-the average results are further improved by nearly 0.7% and 10% in volume and distance-based metrics, respectively. Last, the introduction of the semantic-guided loss -Proposed (MS-Dual-Guided)results in an additional boost on performance, yielding to the best values in the three metrics: 86.75%(DSC), 93.85%(VS) and 0.66 voxels (MSD). These results represent an improvement of 4.5%, 4% and 26% in DSC, VS and MSD, respectively, compared to the baseline in <ref type="bibr" target="#b22">[23]</ref>, showing the efficiency of the proposed attention network compared to individual attention components.   The impact of the refinement steps, as well as of the several elements on both MS-Dual and MS-Dual-Guided models is reported in <ref type="table" target="#tab_2">Table II</ref>. First, we can observe that increasing the number of refinement steps does not typically improve the performance of the methods. Indeed, best results are often obtained with only two attention guided modules. We argue that progressively refining feature maps may produce an excessive focus to the attentive regions, leading to strongly mined attentive features. This has an adverse effect, as the attentive features may concentrate in the most discriminative areas, not covering the whole extent of the object. Further, we observe that the proposed model including guided-attention outperforms all the variants, particularly in the distance-based metric. In addition, we provide a comparison in terms of complexity, whose results are depicted in <ref type="table" target="#tab_2">Table VIII, in  Supplemental Materials.</ref> Comparison to state-of-the-art: The experimental results obtained by several state-of-the-art segmentation networks are reported in <ref type="table" target="#tab_2">Table III</ref>. In the first dataset (top), compared to other networks that were proposed in the context of medical image segmentation -i.e.,UNet <ref type="bibr" target="#b4">[5]</ref>, Attention UNet <ref type="bibr" target="#b24">[25]</ref> and DAF <ref type="bibr" target="#b22">[23]</ref>-our network achieves a mean improvement of 5.6%, 4.3% and 2.0% (in terms of DSC), 4.9%, 4.2% and 2.1% (on VS) and 25%, 26% and 6% (on MSD), respectively. This difference in performance could be explained by the fact that the attention modules integrated in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b24">[25]</ref> are much simpler than those proposed in our architecture. On the other hand, attention modules on general computer vision tasks have attracted more attention, resulting in more elaborated strategies which typically achieve better segmentation results. Among these architectures, the PAN model <ref type="bibr" target="#b19">[20]</ref> with ResNet101 as backbone -the same as ours-achieved the best results for segmentation networks proposed for natural scenes. Despite these competitive results, the proposed model still outperforms the PAN architecture by 2.4%, 1.9% and 12% in DSC, VS and MSD. As PAN <ref type="bibr" target="#b19">[20]</ref> also employed a multi-scale architecture, these differences suggest that the use of dual self-attention and a guided refinement module can actually improve the performance of segmentation networks. Similarly, the proposed model outperforms other networks in the second and third datasets (middle and bottom), indicating that it can be broadly applied to segmentation of medical images in general. Individual per-class scores for both datasets are given in Tables V, VI and VII in Supplemental Material. In addition to these values, we also depict the distribution of DSC, VS and MSD values on the 15 subjects used for evaluation in CHAOS for all the models ( <ref type="figure">Fig. 7</ref> in Supplemental Material).</p><p>Qualitative evaluation: To visualize the impact of the different attention modules, <ref type="figure" target="#fig_3">Fig. 4</ref> displays the segmentation results on three CHAOS subjects. Despite the similar results reported on <ref type="table" target="#tab_2">Table III</ref> for several architectures, the qualitative results depict interesting findings. First, we can observe that UNet typically under-segments certain organs and gets confused easily. For example, in the second row it confused the small bowels with the spleen, while the spleen is not even present in that slice. Integrating attention can overcome some of these limitations and improve the segmentation performance by focusing the attention to relevant areas. This can be observed in the results obtained by the other networks, which, up to some extent, reduce the amount of false positives. Nevertheless, it produces smoother segmentations, resulting in a loss of fine grained details. An interesting result is the segmentation in the last row, where all the models except the proposed network get confused to segment the left kidney. While DANet and PAN models confuse the left kidney with the right one, DAF is not able to detect any relevant region related to the kidneys in that area. In addition, both UNet and   UNet with attention models generate segmentations of the left kidney that contain three organs, i.e., left and right kidneys and spleen, which is anatomically not plausible. Unlike all these models, the proposed architecture does not get distracted by ambiguous regions and some misclassified structures are now correctly classified. Similar results are observed on the segmentations obtained in the BRATS'18 images <ref type="figure">(Fig. 5</ref>). Particularly, we can see that the proposed network obtains finer details than the other architectures. For example, small ramifications on the oedema (in green) are better captured by the proposed model (second row). Likewise, segmentation of necrotic areas (in red) achieved by our method is closer to the ground truth, specially when the region has a complex shape (first row). These visual results indicate that our approach can successfully recover finer segmentation details, while avoiding getting distracted in ambiguous regions. The selective integration of spatial information and among channel maps followed by a guided attention module helps to capture context information. This demonstrates that the proposed multi-scale guided attention model can efficiently encode complimentary information to accurately segment medical images.</p><p>Visual inspection of feature maps: Showing the performance difference through ablation studies and quantitative evaluations alone may not be enough to fully understand the benefits and behaviour of novel models. Although the proposed modules contribute to the performance improvement, as shown in the results, it is interesting to investigate whether different modules work as expected. To this end, we analyze some attended feature maps from both the spatial and channel  <ref type="figure">Fig. 5</ref>: Results on three subjects on the BRATS Challenge dataset. In these figures, the following tumor structures are depicted: oedema (green), enhancing core (yellow) and necrotic or tumor core (red).</p><p>attention modules <ref type="figure" target="#fig_4">(Fig. 6</ref>). We find that the response of specific semantic classes is more noticeable after the second guided attention modules, i.e., PAM 2 and CAM 2 attentive features. While spatial and channel attention can highlight specific class semantics in the first step of the guided module (second and third column), some non-targeted regions are still highlighted on the semantic maps. Furthermore, highest values are also more spread over the entire image. Contrary, the proposed guided attention module generates features (fourth and fifth columns) that better focus on the specific regions of the structures of interest. Particularly, it can be observed that there exist feature maps whose highlighted areas concentrate on a single organ, avoiding ambiguous regions that might result on misclassification of some regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we introduced a novel attention architecture for the task of medical image segmentation. This model incorporates a multi-scale strategy to combine semantic information at different levels and self-attention modules to progressively aggregate relevant contextual features. Last, a guided refinement module filters noisy regions and help the network to focus on relevant class-specific regions in the image. To validate our approach we conducted experiments on three different segmentation tasks: abdominal organ, cardiovascular structures and brain tumor. We provided extensive experiments to evaluate the impact of the individual components of the proposed architecture. Besides, we compared our model to existing approaches that integrate attention, which have been recently proposed for natural scene <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and medical image <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref> segmentation. Experiment results showed that the proposed model outperformed all previous approaches both quantitative and qualitatively, which may be explained by the enhanced ability to model rich contextual dependencies over local features. This demonstrates the efficiency of our approach to provide precise and reliable automatic segmentations of medical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics: formulation</head><p>In this section, we give the formal definition of the metrics employed to evaluate the proposed architecture. a) Dice Similarity Coefficient (DSC): Given two volumes A and B, their DSC can be defined as:</p><formula xml:id="formula_12">DSC = 2 |A ‚à© B| |A| + |B|<label>(12)</label></formula><p>In this metric, values close to 1 indicate high degree of overlapping, whereas near 0 represent not overlapping at all. b) Volume Similarity (VS): Further, we also assess the segmentation performance based on the volume similarity, which is formulated as:</p><formula xml:id="formula_13">VS = 1 ‚àí abs(A ‚àí B)/(A + B)<label>(13)</label></formula><p>c) Mean Surface Distance (MSD): The MSD between contours A and B is defined as follows:</p><formula xml:id="formula_14">MSD = 1 |A| + |B| a‚ààA min b‚ààB d(a, b) + b‚ààB min a‚ààA d(b, a)<label>(14)</label></formula><formula xml:id="formula_15">MSD = 1 |A| + |B| a‚ààA d(a, b) + b‚ààB d(b, a)<label>(15)</label></formula><p>where d(a, b) is the distance between a point a on the surface A and the surface B, which is given by the minimum of the Euclidean norm: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional results</head><p>Tables IV, V and VI report the extended version of the experimental results on the ablation study and comparison to other state-of-the-art networks. In these tables, individual results on single organs are also included to provide the reader a wider view of the performance of the different methods. We can observe that the proposed architecture is consistently outperforming other models, ranking either first or second in almost all the organs for all the evaluation metrics. The only exception is the result obtained for liver segmentation in terms of volume similarity, where all the models obtain almost identical results.</p><p>In addition to the values reported on Tables IV and V in the Supplemental Material, we also depict the distribution of DSC, VS and MSD values on the 15 subjects used for evaluation for all the models <ref type="figure">(Fig. 7)</ref>. In these plots, we can first observe the impact of the different attention modules in the segmentation performance of the proposed model. As we progressively include the proposed attention modules in the baseline network, the segmentation performance improves, which is reflected in a better distribution of segmentation accuracy values with a smaller variance. This difference on results distribution is more prominent when comparing the proposed network with other state-of-the-art networks, which are represented in bluish box plots. We can also observe that this pattern is constant across organs and metrics, suggesting that the proposed attention network achieves better and more robust segmentation results than current state-of-the-art architectures.  <ref type="figure">Fig. 7</ref>: These plots depict the distributions of the different evaluation metrics for the four organs segmented. Bluish colors represent the results obtained by other state-of-the-art networks, whereas the results obtained by our proposed models are displayed in with the brownish boxplots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Convergence:</head><p>We have also compared the different architectures in terms of convergence, whose results are depicted in <ref type="figure">Fig. 8</ref>. Particularly, the mean DSC value over the four structures on one of the validation folds is shown for each of the networks. It can be observed that, even though most of the networks achieve results which may be considered 'similar' -up to some extent-the convergence behaviour is totally different. While there are three networks with similar convergence curves -i.e., UNet, DANet and DAF-, PAN needs more iterations to convergence, ultimately performing better than these networks after nearly 400 epochs. On the other hand, we found that attention UNet and the proposed network presented the fastest convergence, achieving their best results at epoch <ref type="bibr" target="#b47">48</ref>       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Overview of the proposed multi-scale guided attention network. We resort to ResNet-101 to extract dense local features. Four feature maps with different sizes -acquired from the outputs of [Res-2, Res-3, Res-4, Res-5]-are employed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>An illustration of the semantic guided attention module with 2 refinement steps. For each scale s this module provides a set of attentive features, i.e., A s .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>CHAOS</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Results on three subjects on the CHAOS Challenge dataset. The proposed multi-scale guided attention network achieves qualitatively better results than other state-of-the-art networks that also integrate attention modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization results of the channel maps. For each row, we show an input image, and the corresponding channel maps from the outputs of spatial (PAM) and channel (CAM) attention module at guided module of theFig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Dice Similarity coefficient (%) (b) Volume similarity (%) (c) Average surface distance (voxels)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table><row><cell>Model</cell><cell>DSC (%)</cell><cell>VS (%)</cell><cell>MSD (voxels)</cell></row><row><cell>1 Refinement step</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS-Dual (No guidance)</cell><cell>85.75 (¬±5.08)</cell><cell>92.72 (¬±3.65)</cell><cell>0.71 (¬±0.28)</cell></row><row><cell>MS-Dual-Guided</cell><cell>86.34 (¬±5.17)</cell><cell>93.47 (¬±3.78)</cell><cell>0.68 (¬±0.29)</cell></row><row><cell>w/out deep supervision</cell><cell>84.71 (¬±4.86)</cell><cell>91.39 (¬±3.55)</cell><cell>0.75 (¬±0.17)</cell></row><row><cell>w/out encoder-decoder (dist)</cell><cell>85.92 (¬±5.17)</cell><cell>92.94 (¬±4.04)</cell><cell>0.76 (¬±0.34)</cell></row><row><cell>2 Refinement steps</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS-Dual (No guidance)</cell><cell>86.17 (¬±5.78)</cell><cell>92.74 (¬±4.76)</cell><cell>0.67 (¬±0.30)</cell></row><row><cell>MS-Dual-Guided</cell><cell>86.75 (¬±5.05)</cell><cell>93.85 (¬±3.50)</cell><cell>0.66 (¬±0.27)</cell></row><row><cell>w/out deep supervision</cell><cell>83.51 (¬±5.52)</cell><cell>91.80 (¬±3.66)</cell><cell>0.75 (¬±0.16)</cell></row><row><cell>w/out encoder-decoder (dist)</cell><cell>86.67 (¬±4.98)</cell><cell>93.67 (¬±3.38)</cell><cell>0.77 (¬±0.31)</cell></row><row><cell>3 Refinement steps</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS-Dual (No guidance)</cell><cell>86.26 (¬±5.71)</cell><cell>93.62 (¬±4.72)</cell><cell>0.71 (¬±0.34)</cell></row><row><cell>MS-Dual-Guided</cell><cell>86.14 (¬±5.89)</cell><cell>93.50 (¬±3.98)</cell><cell>0.67 (¬±0.36)</cell></row><row><cell>w/out deep supervision</cell><cell>83.22 (¬±5.72)</cell><cell>90.95 (¬±4.31)</cell><cell>0.80 (¬±0.17)</cell></row><row><cell>w/out encoder-decoder (dist)</cell><cell>85.88 (¬±4.78)</cell><cell>93.23 (¬±3.71)</cell><cell>0.79 (¬±0.39)</cell></row><row><cell>5 Refinement steps</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS-Dual (No guidance)</cell><cell>86.33 (¬±4.98)</cell><cell>93.74 (¬±3.91)</cell><cell>0.71 (¬±0.31)</cell></row><row><cell>MS-Dual-Guided</cell><cell>86.30 (¬±5.05)</cell><cell>93.16 (¬±4.11)</cell><cell>0.68 (¬±0.22)</cell></row><row><cell>w/out deep supervision</cell><cell>83.88 (¬±5.78)</cell><cell>91.03 (¬±3.66)</cell><cell>0.87 (¬±0.34)</cell></row><row><cell>w/out encoder-decoder (dist)</cell><cell>86.16 (¬±4.23)</cell><cell>92.98 (¬±2.93)</cell><cell>0.80 (¬±0.31)</cell></row></table><note>Ablation study on different attention modules on the Chaos dataset. The values show the average result of the experiments averaged over the 3 folds. Best and second best results are represented in red and blue bold, respectively.Proposed (MS-Dual and MS-Dual-Guided)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Ablation study on different elements on the MS-Dual and MS-Dual-Guided architectures evaluated on the Chaos dataset. The values show the average result of the experiments on the 3 folds. Best results are represented in red bold, while blue is used to highlight the second best performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison to other state-of-the-art architectures on the four analyzed datasets. Best and second best results are represented in red and blue bold, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>and 73, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DSC (%)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Liver</cell><cell>Kidney R</cell><cell>Kidney L</cell><cell>Spleen</cell><cell>Mean</cell></row><row><cell>Baseline (DAF [23])</cell><cell>91.66 (¬±2.99)</cell><cell>79.28 (¬±18.68)</cell><cell>83.63 (¬±7.56)</cell><cell>75.35 (¬±20.41)</cell><cell>82.48 (¬±6.06)</cell></row><row><cell>Proposed (PAM)</cell><cell>91.89 (¬±4.29)</cell><cell>85.47 (¬±7.04)</cell><cell>86.84 (¬±6.53)</cell><cell>73.65 (¬±22.62)</cell><cell>84.46 (¬±6.68)</cell></row><row><cell>Proposed (CAM)</cell><cell>92.58 (¬±2.65)</cell><cell>84.52 (¬±9.34)</cell><cell>86.38 (¬±6.27)</cell><cell>76.84 (¬±20.56)</cell><cell>85.08 (¬±5.62)</cell></row><row><cell>Proposed (DANet)</cell><cell>92.60 (¬±3.20)</cell><cell>85.29 (¬±7.96)</cell><cell>87.74 (¬±6.37)</cell><cell>76.44 (¬±22.17)</cell><cell>85.52 (¬±5.86)</cell></row><row><cell>Proposed (MS-Dual)</cell><cell>92.62 (¬±3.08)</cell><cell>86.29 (¬±5.98)</cell><cell>88.82 (¬±4.84)</cell><cell>76.96 (¬±19.87)</cell><cell>86.17 (¬±5.78)</cell></row><row><cell>Proposed (MS-Dual-Guided)</cell><cell>92.46 (¬±2.82)</cell><cell>87.96 (¬±6.46)</cell><cell>88.01 (¬±6.16)</cell><cell>78.61 (¬±18.69)</cell><cell>86.75 (¬±5.05)</cell></row><row><cell></cell><cell></cell><cell cols="3">Volume similarity (VS) (%)</cell><cell></cell></row><row><cell></cell><cell>Liver</cell><cell>Kidney R</cell><cell>Kidney L</cell><cell>Spleen</cell><cell>Mean</cell></row><row><cell>Proposed( DAF [23])</cell><cell>96.69 (¬±3.21)</cell><cell>86.75 (¬±16.41)</cell><cell>90.29 (¬±8.39)</cell><cell>84.98 (¬±14.42)</cell><cell>89.68 (¬±4.48)</cell></row><row><cell>Proposed (PAM)</cell><cell>96.62 (¬±4.62)</cell><cell>92.83 (¬±7.43)</cell><cell>93.96 (¬±6.46)</cell><cell>83.93 (¬±20.54)</cell><cell>91.84 (¬±4.77)</cell></row><row><cell>Proposed (CAM)</cell><cell>97.25 (¬±2.95)</cell><cell>93.78 (¬±6.04)</cell><cell>93.98 (¬±5.48)</cell><cell>83.72 (¬±20.97)</cell><cell>92.18 (¬±5.07)</cell></row><row><cell>Proposed (DANet)</cell><cell>97.04 (¬±3.03)</cell><cell>94.50 (¬±5.96)</cell><cell>93.43 (¬±7.03)</cell><cell>83.30 (¬±22.53)</cell><cell>92.07 (¬±5.23)</cell></row><row><cell>Proposed (MS-Dual)</cell><cell>97.47 (¬±3.07)</cell><cell>93.30 (¬±4.11)</cell><cell>95.27 (¬±4.89)</cell><cell>84.90 (¬±16.86)</cell><cell>92.74 (¬±4.76)</cell></row><row><cell>Proposed (MS-Dual-Guided)</cell><cell>96.44 (¬±3.15)</cell><cell>96.14 (¬±3.15)</cell><cell>94.95 (¬±4.48)</cell><cell>87.87 (¬±15.23)</cell><cell>93.85 (¬±3.50)</cell></row><row><cell></cell><cell></cell><cell cols="3">Average Surface Distance (MSD) (voxels)</cell><cell></cell></row><row><cell></cell><cell>Liver</cell><cell>Kidney R</cell><cell>Kidney L</cell><cell>Spleen</cell><cell>Mean</cell></row><row><cell>Baseline( DAF [23])</cell><cell>0.64 (¬±0.29)</cell><cell>0.97 (¬±1.08)</cell><cell>0.63 (¬±0.25)</cell><cell>1.45 (¬±2.04)</cell><cell>0.92 (¬±0.33)</cell></row><row><cell>Proposed (PAM)</cell><cell>0.55 (¬±0.19)</cell><cell>0.56 (¬±0.23)</cell><cell>0.55 (¬±0.21)</cell><cell>1.54 (¬±2.40)</cell><cell>0.80 (¬±0.43)</cell></row><row><cell>Proposed (CAM)</cell><cell>0.58 (¬±0.22)</cell><cell>0.57 (¬±0.24)</cell><cell>0.52 (¬±0.20)</cell><cell>1.29 (¬±1.64)</cell><cell>0.74 (¬±0.32)</cell></row><row><cell>Proposed (DANet)</cell><cell>0.54 (¬±0.19)</cell><cell>0.56 (¬±0.19)</cell><cell>0.50 (¬±0.18)</cell><cell>1.49 (¬±2.29)</cell><cell>0.77 (¬±0.41)</cell></row><row><cell>Proposed (MS-Dual)</cell><cell>0.53 (¬±0.18)</cell><cell>0.51 (¬±0.14)</cell><cell>0.46 (¬±0.14)</cell><cell>1.19 (¬±1.42)</cell><cell>0.67 (¬±0.30)</cell></row><row><cell>Proposed (MS-Dual-Guided)</cell><cell>0.54 (¬±0.16)</cell><cell>0.48 (¬±0.18)</cell><cell>0.48 (¬±0.14)</cell><cell>1.13 (¬±1.24)</cell><cell>0.66 (¬±0.27)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study on different proposed attention modules on the Chaos dataset (multi-organ segmentation on MRI task). The values show the average result of the experiments averaged over the 3 folds. Best results are represented in red bold, while blue is used to highlight the second best performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DSC (%)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Liver</cell><cell>Kidney R</cell><cell>Kidney L</cell><cell>Spleen</cell><cell>Mean</cell></row><row><cell>UNet [5]</cell><cell>90.94 (¬±4.01)</cell><cell>79.14 (¬±15.23)</cell><cell>82.51 (¬±7.48)</cell><cell>71.95 (¬±21.61)</cell><cell>81.14 (¬±7.88)</cell></row><row><cell>DANet [19]</cell><cell>91.69 (¬±4.07)</cell><cell>83.85 (¬±9.40)</cell><cell>84.49 (¬±8.60)</cell><cell>75.54 (¬±16.08)</cell><cell>83.89 (¬±9.54)</cell></row><row><cell>PAN (ResNet34) [20]</cell><cell>91.99 (¬±2.98)</cell><cell>81.51 (¬±9.03)</cell><cell>83.62 (¬±6.21)</cell><cell>73.70 (¬±19.97)</cell><cell>82.70 (¬±6.51)</cell></row><row><cell>PAN (ResNet101)[20]</cell><cell>92.13 (¬±3.51)</cell><cell>85.02 (¬±5.16)</cell><cell>85.36 (¬±4.87)</cell><cell>74.84 (¬±21.23)</cell><cell>84.34 (¬±6.17)</cell></row><row><cell>DAF [23]</cell><cell>91.66 (¬±2.99)</cell><cell>79.28 (¬±18.68)</cell><cell>83.63 (¬±7.56)</cell><cell>75.35 (¬±20.41)</cell><cell>82.48 (¬±6.06)</cell></row><row><cell>UNet Attention [25]</cell><cell>92.02 (¬±1.93)</cell><cell>84.33 (¬±5.91)</cell><cell>85.57 (¬±4.09)</cell><cell>77.18 (¬±15.95)</cell><cell>84.77 (¬±5.27)</cell></row><row><cell>Proposed (MS-Dual-Guided)</cell><cell>92.46 (¬±2.82)</cell><cell>87.96 (¬±6.46)</cell><cell>88.01 (¬±6.16)</cell><cell>78.61 (¬±18.69)</cell><cell>86.75 (¬±5.05)</cell></row><row><cell></cell><cell></cell><cell cols="3">Volume similarity (VS) (%)</cell><cell></cell></row><row><cell></cell><cell>Liver</cell><cell>Kidney R</cell><cell>Kidney L</cell><cell>Spleen</cell><cell>Mean</cell></row><row><cell>UNet [5]</cell><cell>95.54 (¬±4.43)</cell><cell>87.68 (¬±5.77)</cell><cell>89.55 (¬±4.68)</cell><cell>83.28 (¬±14.78)</cell><cell>89.01 (¬±4.82)</cell></row><row><cell>DANet [19]</cell><cell>96.90 (¬±4.18)</cell><cell>92.88 (¬±5.12)</cell><cell>91.52 (¬±6.73)</cell><cell>84.37 (¬±16.15)</cell><cell>91.42 (¬±4.52)</cell></row><row><cell>PAN (ResNet34) [20]</cell><cell>96.56 (¬±3.55)</cell><cell>90.89 (¬±5.64)</cell><cell>91.83 (¬±7.75)</cell><cell>81.98 (¬±20.67)</cell><cell>90.32 (¬±5.27)</cell></row><row><cell>PAN (ResNet101) [20]</cell><cell>96.99 (¬±3.64)</cell><cell>93.77 (¬±4.63)</cell><cell>92.69 (¬±6.88)</cell><cell>84.24 (¬±17.37)</cell><cell>91.93 (¬±4.71)</cell></row><row><cell>DAF [23]</cell><cell>96.69 (¬±3.21)</cell><cell>86.75 (¬±16.41)</cell><cell>90.29 (¬±8.39)</cell><cell>84.98 (¬±14.42)</cell><cell>89.68 (¬±4.48)</cell></row><row><cell>UNet Attention [25]</cell><cell>96.95 (¬±1.89)</cell><cell>92.29 (¬±6.41)</cell><cell>91.79 (¬±3.53)</cell><cell>85.94 (¬±11.88)</cell><cell>91.74 (¬±3.91)</cell></row><row><cell>Proposed (MS-Dual-Guided)</cell><cell>96.44 (¬±3.15)</cell><cell>96.14 (¬±3.15)</cell><cell>94.95 (¬±4.48)</cell><cell>87.87 (¬±15.23)</cell><cell>93.85 (¬±3.50)</cell></row><row><cell></cell><cell></cell><cell cols="3">Average Surface Distance (MSD) (voxels)</cell><cell></cell></row><row><cell></cell><cell>Liver</cell><cell>Kidney R</cell><cell>Kidney L</cell><cell>Spleen</cell><cell>Mean</cell></row><row><cell>UNet [5]</cell><cell>0.59 (¬±0.18)</cell><cell>0.69 (¬±0.38)</cell><cell>0.61 (¬±0.19)</cell><cell>1.76 (¬±2.57)</cell><cell>0.91 (¬±0.49)</cell></row><row><cell>DANet [19]</cell><cell>0.61 (¬±0.27)</cell><cell>0.65 (¬±0.31)</cell><cell>0.67 (¬±0.30)</cell><cell>1.17 (¬±0.94)</cell><cell>0.78 (¬±0.23)</cell></row><row><cell>PAN (ResNet34)[20]</cell><cell>0.62 (¬±0.25)</cell><cell>0.75 (¬±0.31)</cell><cell>0.69 (¬±0.21)</cell><cell>1.37 (¬±1.43)</cell><cell>0.86 (¬±0.29)</cell></row><row><cell>PAN (ResNet101) [20]</cell><cell>0.57 (¬±0.22)</cell><cell>0.61 (¬±0.19)</cell><cell>0.64 (¬±0.15)</cell><cell>1.30 (¬±1.47)</cell><cell>0.78 (¬±0.31)</cell></row><row><cell>DAF [23]</cell><cell>0.64 (¬±0.29)</cell><cell>0.97 (¬±1.08)</cell><cell>0.63 (¬±0.25)</cell><cell>1.45 (¬±2.04)</cell><cell>0.92 (¬±0.33)</cell></row><row><cell>UNet Attention [25]</cell><cell>0.57 (¬±0.25)</cell><cell>0.61 (¬±0.23)</cell><cell>0.56 (¬±0.18)</cell><cell>1.15 (¬±1.01)</cell><cell>0.72 (¬±0.24)</cell></row><row><cell>Proposed (MS-Dual-Guided)</cell><cell>0.54 (¬±0.16)</cell><cell>0.48 (¬±0.18)</cell><cell>0.48 (¬±0.14)</cell><cell>1.13 (¬±1.24)</cell><cell>0.66 (¬±0.27)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Comparison of the proposed network to other state-of-the-art architectures on the CHAOS dataset (multi-organ segmentation on MRI task). The values show the average result of the experiments averaged over the 3 folds. Best results are represented in red bold, while blue is used to highlight the second best performance.</figDesc><table><row><cell></cell><cell cols="3">Volume similarity (VS)</cell></row><row><cell></cell><cell>Myocardium</cell><cell>Blood Pool</cell><cell>Mean</cell></row><row><cell>UNet [5]</cell><cell>91.05 (¬±9.75)</cell><cell>95.78 (¬±4.04)</cell><cell>93.41 (¬±6.44)</cell></row><row><cell>DANet [19]</cell><cell>91.80 (¬±8.95)</cell><cell>97.50 (¬±3.01)</cell><cell>94.65 (¬±4.45)</cell></row><row><cell>PAN (ResNet34) [20]</cell><cell>90.58 (¬±10.89)</cell><cell>96.93 (¬±3.66)</cell><cell>93.76 (¬±5.85)</cell></row><row><cell>PAN (ResNet101) [20]</cell><cell>91.42 (¬±7.59)</cell><cell>97.23 (¬±2.36)</cell><cell>94.33 (¬±3.69)</cell></row><row><cell>DAF [23]</cell><cell>91.73 (¬±6.30)</cell><cell>96.89 (¬±2.33)</cell><cell>94.31 (¬±3.21)</cell></row><row><cell>UNet Attention [25]</cell><cell>92.52 (¬±7.66)</cell><cell>96.69 (¬±2.20)</cell><cell>94.61 (¬±4.17)</cell></row><row><cell>Proposed</cell><cell>92.08 (¬±4.39)</cell><cell>96.82 (¬±2.76)</cell><cell>94.45 (¬±2.39)</cell></row><row><cell></cell><cell cols="3">Average Surface Distance (MSD)</cell></row><row><cell></cell><cell>Myocardium</cell><cell>Blood pool</cell><cell>Mean</cell></row><row><cell>UNet [5]</cell><cell>1.82 (¬±1.48)</cell><cell>1.55 (¬±1.08)</cell><cell>1.68 (¬±1.28)</cell></row><row><cell>DANet [19]</cell><cell>1.23 (¬±0.51)</cell><cell>1.32 (¬±0.46)</cell><cell>1.27 (¬±0.46)</cell></row><row><cell>PAN (ResNet34)[20]</cell><cell>1.97 (¬±1.84)</cell><cell>1.26 (¬±0.48)</cell><cell>1.62 (¬±1.19)</cell></row><row><cell>PAN (ResNet101) [20]</cell><cell>1.33 (¬±0.53)</cell><cell>1.15 (¬±0.30)</cell><cell>1.24 (¬±0.38)</cell></row><row><cell>DAF [23]</cell><cell>1.41 (¬±0.45)</cell><cell>1.44 (¬±0.46)</cell><cell>1.48 (¬±0.50)</cell></row><row><cell>UNet Attention [25]</cell><cell>1.24 (¬±0.42)</cell><cell>1.25 (¬±0.39)</cell><cell>1.25 (¬±0.42)</cell></row><row><cell>Proposed</cell><cell>1.15 (¬±0.33)</cell><cell>1.24 (¬±0.43)</cell><cell>1.19 (¬±0.37)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Comparison of the proposed network to other state-of-the-art architectures on the HVSMR 2016 dataset. The values show the average result of the experiments on the 5 folds.Fig. 8: Evolution of the mean validation DSC over time.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Average Surface Distance (MSD) (voxels)</cell><cell></cell></row><row><cell></cell><cell>ED</cell><cell>ET</cell><cell>TC</cell><cell>-</cell><cell>Mean</cell></row><row><cell>UNet [5]</cell><cell>0.99 (¬±0.33)</cell><cell>2.37 (¬±1.74)</cell><cell>1.56 (¬±1.34)</cell><cell>-</cell><cell>1.65 (¬±0.57)</cell></row><row><cell>DANet [19]</cell><cell>0.67 (¬±0.16)</cell><cell>1.43 (¬±0.95)</cell><cell>0.78 (¬±0.25)</cell><cell>-</cell><cell>0.95 (¬±0.33)</cell></row><row><cell>PAN (ResNet34)[20]</cell><cell>0.86 (¬±0.20)</cell><cell>2.29 (¬±1.87)</cell><cell>1.10 (¬±0.47)</cell><cell>-</cell><cell>1.42 (¬±0.52)</cell></row><row><cell>PAN (ResNet101) [20]</cell><cell>0.74 (¬±0.19)</cell><cell>1.79 (¬±1.35)</cell><cell>0.96 (¬±0.48)</cell><cell>-</cell><cell>1.17 (¬±0.47)</cell></row><row><cell>DAF [23]</cell><cell>0.76 (¬±0.17)</cell><cell>1.84 (¬±1.33)</cell><cell>1.02 (¬±0.66)</cell><cell>-</cell><cell>1.21 (¬±0.46)</cell></row><row><cell>UNet Attention [25]</cell><cell>0.69 (¬±0.18)</cell><cell>1.58 (¬±1.12)</cell><cell>0.79 (¬±0.29)</cell><cell>-</cell><cell>1.02 (¬±0.40)</cell></row><row><cell>Proposed (MS-Dual-Guided)</cell><cell>0.58 (¬±0.14)</cell><cell>1.40 (¬±1.02)</cell><cell>0.71 (¬±0.31)</cell><cell>-</cell><cell>0.90 (¬±0.36)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Comparison of the proposed network to other state-of-the-art architectures on the BRATS 2018 dataset (multiorgan segmentation on MRI task). The values show the average result of the experiments averaged over the 3 folds. Best results are represented in red bold, while blue is used to highlight the second best performance.</figDesc><table><row><cell></cell><cell cols="2">Model complexity</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2"># Params</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1 Iter</cell><cell>2 Iter</cell><cell>3 Iter</cell><cell>5 Iter</cell></row><row><cell>UNet</cell><cell>31,030,853</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PAN (ResNet34)</cell><cell>21,323,991</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PAN (ResNet101)</cell><cell>42,675,415</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UNet Attention</cell><cell>34,877,681</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DANet (ResNet101)</cell><cell>68,475,961</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Proposed(DAF)</cell><cell>43,482,179</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Proposed(PAM)</cell><cell>43,486,343</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Proposed(CAM)</cell><cell>43,485,543</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Proposed(DANet)</cell><cell>43,980,179</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MS-Dual (No guidance)</cell><cell>-</cell><cell cols="4">43,485,831 44,411,103 45,337,675 47,190,819</cell></row><row><cell>MS-Dual-Guided</cell><cell>-</cell><cell cols="4">50,531,399 58,499,679 66,470,539 82,412,259</cell></row><row><cell>MS-Dual-Guided (No Deep Sup)</cell><cell>-</cell><cell cols="4">50,530,099 58,498,379 66,467,939 82,407,059</cell></row><row><cell>MS-Dual-Guided (Dist)</cell><cell>-</cell><cell cols="4">43,485,831 44,411,103 45,337,675 47,190,819</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VIII :</head><label>VIII</label><figDesc>Model complexity, measured in number of parameters, for the evaluated models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the superscript p to indicate that the feature map belongs to the position attention module. Similarly, we will employ the superscript c for the channel attention module features.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://medicaldecathlon.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We wish to thank NVIDIA for its kind donation of the Titan V GPU used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: Is the problem solved?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An attention model for group-level emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 on International Conference on Multimodal Interaction</title>
		<meeting>the 2018 on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="611" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN with pyramid attention network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9215" to="9223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PSANet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BiSeNet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep gated attention networks for large-scale streetlevel scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="702" to="714" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep attentional features for prostate segmentation in ultrasound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention based hierarchical aggregation network for 3D left atrial segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Statistical Atlases and Computational Models of the Heart</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention gated networks: Learning to leverage salient regions in medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ASDNet: Attention based semi-supervised deep networks for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Concurrent spatial and channel squeeze &amp; excitationin fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="421" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Statistical shape models for 3D medical image segmentation: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Meinzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="543" to="563" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segmentation algorithms of subcortical brain structures on MRI for radiotherapy and radiosurgery: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Massoptier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vermandel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRBM</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Esophagus segmentation in CT via 3D fully convolutional neural network and random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fechter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6341" to="6352" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from CT volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Q learning driven CT pancreas segmentation with geometry-aware U-Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Man</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="456" to="470" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparing fully automated state-of-the-art cerebellum parcellation from magnetic resonance images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional neural network with shape prior applied to cardiac mri segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1119" to="1128" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiregion segmentation of bladder cancer structures in MRI with progressive dilated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5482" to="5493" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accurate esophageal gross tumor volume segmentation in PET/CT using two-stream chained 3D deep network fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="182" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">OBELISK-Net: Fewer layers to solve 3D multi-organ segmentation with sparse deformable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouteldja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Abdominal multi-organ segmentation with organ-attention networks and statistical fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="88" to="102" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Areas of attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1242" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>T√§ckstr√∂m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>in In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stacked semantics-guided attention model for fine-grained zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5995" to="6004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and finegrained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SCA-CNN: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exploring brushlet based 3D textures in transfer function specification for direct volume rendering of abdominal organs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Selver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="174" to="187" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Segmentation of abdominal organs from MR images using multi-level hierarchical classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Selvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Faculty of Engineering and Architecture of Gazi University</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="533" to="546" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Segmentation of abdominal organs from CT using a multi-level, hierarchical neural network strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Selver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer methods and programs in biomedicine</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="830" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Interactive whole-heart segmentation in congenital heart disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Pace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Moghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">170117</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<idno>¬±2.87) - 92.66 (¬±6.86) Proposed (MS-Dual-Guided) 98.54 (¬±1.76) 82.91 (¬±20.17) 97.78 (¬±2.56) - 93.08 (¬±7.20</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
