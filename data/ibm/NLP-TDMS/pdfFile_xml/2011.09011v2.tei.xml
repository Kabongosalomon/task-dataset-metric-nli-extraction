<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
							<email>cygong@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
							<email>vchandra@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) has shown great promise in designing state-of-the-art (SOTA) models that are both accurate and efficient. Recently, two-stage NAS, e.g. BigNAS, decouples the model training and searching process and achieves remarkable search efficiency and accuracy. Two-stage NAS requires sampling from the search space during training, which directly impacts the accuracy of the final searched models. While uniform sampling has been widely used for its simplicity, it is agnostic of the model performance Pareto front, which is the main focus in the search process, and thus, misses opportunities to further improve the model accuracy. In this work, we propose At-tentiveNAS that focuses on improving the sampling strategy to achieve better performance Pareto. We also propose algorithms to efficiently and effectively identify the networks on the Pareto during training. Without extra re-training or post-processing, we can simultaneously obtain a large number of networks across a wide range of FLOPs. Our discovered model family, AttentiveNAS models, achieves top-1 accuracy from 77.3% to 80.7% on ImageNet, and outperforms SOTA models, including BigNAS and Oncefor-All networks. We also achieve ImageNet accuracy of 80.1% with only 491 MFLOPs. Our training code and pretrained models are available at https://github. com/facebookresearch/AttentiveNAS. Recent NAS advancements decouple the parameter training and architecture optimization into two separate stages <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43]</ref>:</p><p>• The first stage optimizes the parameters of all candidate networks in the search space through weightsharing, such that all networks simultaneously reach superior performance at the end of training.</p><p>• The second stage leverages typical search algorithms, such as evolutionary algorithms, to find the best performing models under various resource constraints.</p><p>Such NAS paradigm has delivered state-of-the-art empirical results with great search efficiency <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref>. The success of the two-stage NAS heavily relies on the candidate network training in the first stage. To achieve superior performance for all candidates, candidate networks are sampled from the search space during training, followed by optimizing each sample via one-step stochastic gradient descent (SGD). The key aspect is to figure out which network to sample at each SGD step. Existing methods often use a uniform sampling strategy to sample all networks</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have achieved remarkable empirical success. However, the rapid growth of network size and computation cost imposes a great challenge to bring DNNs to edge devices <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref>. Designing networks that are both accurate and efficient becomes an important but challenging problem.</p><p>Neural architecture search (NAS) <ref type="bibr" target="#b44">[45]</ref> provides a powerful tool for automating efficient DNN design. NAS requires optimizing both model architectures and model parameters, creating a challenging nested optimization problem. Conventional NAS algorithms leverage evolutionary search <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> or reinforcement learning <ref type="bibr" target="#b33">[34]</ref>, these NAS algorithms can be prohibitively expensive as thousands of models are required to be trained in a single experiment.</p><p>with equal probabilities <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref>. Though promising results have been demonstrated, the uniform sampling strategy makes the training stage agnostic of the searching stage. More specifically, while the searching stage focuses on the set of networks on the Pareto front of accuracy and inference efficiency, the training stage is not tailored towards improving the Pareto front and regards each network candidate with equal importance. This approach misses the opportunity of further boosting the accuracy of the networks on the Pareto during the training stage.</p><p>In this work, we propose AttentiveNAS to improve the baseline uniform sampling by paying more attention to models that are more likely to produce a better Pareto front. We specifically answer the following two questions:</p><p>• Which sets of candidate networks should we sample during the training?</p><p>• How should we sample these candidate networks efficiently and effectively without introducing too much computational overhead to the training?</p><p>To answer the first question, we explore two different sampling strategies. The first strategy, denoted as BestUp, investigates a best Pareto front aware sampling strategy following the conventional Pareto-optimal NAS, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>. BestUp puts more training budgets on improving the current best Pareto front. The second strategy, denoted as WorstUp, focuses on improving candidate networks that yield the worst-case performance trade-offs. We refer to these candidate networks as the worst Pareto models. This sampling strategy is similar to hard example mining <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref> by viewing networks on the worst Pareto front as hard training examples. Pushing the limits of the worst Pareto set could help update the least optimized parameters in the weight-sharing network, allowing all the parameters to be fully trained.</p><p>The second question is also non-trivial as determining the networks on both the best and the worst Pareto front is not straightforward. We propose two approaches to leverage 1) the training loss and 2) the accuracy predicted by a pretrained predictor as the proxy for accuracy comparison. The overall contribution can be summarized as follows:</p><p>• We propose a new strategy, AttentiveNAS, to improve existing two-stage NAS with attentive sampling of networks on the best or the worst Pareto front. Different sampling strategies, including BestUp and WorstUp, are explored and compared in detail.</p><p>• We propose two approaches to guide the sampling to the best or the worst Pareto front efficiently during training.</p><p>• We achieve state-of-the-art ImageNet accuracy given the FLOPs constraints for the searched Attentive-NAS model family. For example, AttentiveNAS-A0 achieves 2.1% better accuracy compared to Mo-bileNetV3 with fewer FLOPs, while AttentiveNAS-A2 achieves 0.8% better accuracy compared to FBNetV3 with 10% fewer FLOPs. AttentiveNAS-A5 reaches 80.1% accuracy with only 491 MFLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work and Background</head><p>NAS is a powerful tool for automating efficient neural architecture design. NAS is often formulated as a constrained optimization problem:</p><formula xml:id="formula_0">min α∈A L(W * α ; D val ), s.t. W * α = arg min Wα L(W α ; D trn ), FLOPs(α) &lt; τ.<label>(1)</label></formula><p>Here W α is the DNN parameters associated with network configuration α. A specifies the search space. D trn and D val represents the training dataset and validation dataset, repetitively. L(·) is the loss function, e.g., the cross entropy loss for image classification. FLOPs(α) measures the computational cost induced by the network α, and τ is a resource threshold. In this work, we consider FLOPs as a proxy for computational cost. Other resource considerations, such as latency and energy, can also be incorporated into Eqn. (1) easily. Solving the constrained optimization problem in Eqn. (1) is notoriously challenging. Earlier NAS solutions often build on reinforcement learning <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> or evolutionary algorithms <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref>. These methods require enumerating an excessively large number of DNN architectures {α} and training their corresponding model parameters {W α } from scratch to get accurate performance estimations, and thus are extremely computationally expensive.</p><p>More recent NAS practices have made the search more efficient through weight-sharing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>. They usually train a weight-sharing network and sample the candidate sub-networks by inheriting the weights directly to provide efficient performance estimation. This helps alleviate the heavy computational burden of training all candidate networks from scratch and accelerates the NAS process significantly.</p><p>To find the small sub-networks of interest, weightsharing based NAS often solve the constrained optimization in Eqn. (1) via continuous differentiable relaxation and gradient descent <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>. However, these methods are often sensitive to the hyper-parameter choices, e.g., random seeds or data partitions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>; the performance rank correlation between different DNNs varies significantly across different trials <ref type="bibr" target="#b39">[40]</ref>, necessitating multiple rounds of trialsand-errors for good performance. Furthermore, the model </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer-1</head><p>Layer-2 <ref type="figure">Figure 2</ref>. An illustration of the architecture sampling procedure in training two-stage NAS. At each training step, a single or several sub-networks are sampled from a pre-defined search space. In our implementation, a sub-network is specified by a set of choices of input resolution, channel widths, depths, kernel sizes, and expansion ratio. For example, in this case, the configuration of the selected sub-network is highlighted with solid borderlines. Images are from ImageNet <ref type="bibr" target="#b11">[12]</ref>.</p><p>weights inherited from the weight-sharing network are often sub-optimal. Hence, it is usually required to re-train the discovered DNNs from scratch, introducing additional computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Two-stage NAS</head><p>The typical NAS goal Eqn. (1) limits the search scope to only small sub-networks, yielding a challenging optimization problem that cannot leverage the benefits of overparameterization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. In addition, NAS optimization defined in Eqn. (1) is limited to one single resource constraint. Optimizing DNNs under various resource constraints often requires multiple independent searches.</p><p>To alleviate the aforementioned drawbacks, recently, a series of NAS advances propose to breakdown the constrained optimization problem (1) into two separate stages: 1) constraint-free pre-training -jointly optimizing all possible candidate DNNs specified in the search space through weight sharing without considering any resource constraints; 2) resource-constrained search -identifying the best performed sub-networks under given resource constraints. Recent work in this direction include BigNAS <ref type="bibr" target="#b42">[43]</ref>, SPOS <ref type="bibr" target="#b14">[15]</ref>, FairNAS <ref type="bibr" target="#b7">[8]</ref>, OFA <ref type="bibr" target="#b2">[3]</ref> and HAT <ref type="bibr" target="#b36">[37]</ref>.</p><p>Constraint-free pre-training (stage 1): The goal of the constraint-free pre-training stage is to learn the parameters of the weight-sharing network. This is often framed as solving the following optimization problem:</p><formula xml:id="formula_1">min W E α∈A L(W α ; D trn ) + γR(W ),<label>(2)</label></formula><p>where W represents the shared weights in the network. W α is a sub-network of W specified by architecture α and R(W ) is the regularization term. An example of R(W ), proposed in BigNAS <ref type="bibr" target="#b42">[43]</ref>, is formulated as follows,</p><formula xml:id="formula_2">R(W ) = L(w αs ; D trn ) + L(w α l ; D trn ) + η W 2 2 ,<label>(3)</label></formula><p>where α s and α l represents the smallest and the largest candidate sub-networks in the search space A, respectively. η is the weight decay coefficient. This is also referred to as the sandwich training rule in <ref type="bibr" target="#b42">[43]</ref>.</p><p>In practice, the expectation term in Eqn. (2) is often approximated with n uniformly sampled architectures and solved by SGD ( <ref type="figure">Figure 2</ref>). Note that both smaller and larger DNNs are jointly optimized in Eqn. <ref type="bibr" target="#b1">(2)</ref>. This formulation allows to transfer knowledge from larger networks to smaller networks via weight-sharing and knowledge distillation, hence improving the overall performance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Resource-constrained searching (stage 2): After the pre-training in stage 1, all candidate DNNs are fully optimized. The next step is to search DNNs that yield the best performance and resource trade-off as follows,</p><formula xml:id="formula_3">{α * i } = arg min αi∈A L(W * αi ; D val ),<label>(4)</label></formula><formula xml:id="formula_4">s.t. FLOPs(α i ) &lt; τ i , ∀i.</formula><p>Here W * is the optimal weight-sharing parameters learned in stage 1. The overall search cost of this stage is often low, since there is no need for re-training or fine-tuning. Furthermore, Eqn. (4) naturally supports a wide range of deployment constraints without the need of further modifications, yielding a more flexible NAS framework for machine learning practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NAS via Attentive Sampling</head><p>The goal of NAS is to find the network architectures with the best accuracy under different computation constraints. Although optimizing the average loss over α ∈ A in Eqn. (2) seems to be a natural choice, it is not tailored for improving the trade-off between task performance and DNN resource usage. In practice, one often pays more interest to Pareto-optimal DNNs that form the best trade-offs as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.  Adapting the constraint-free pre-training goal in Eqn. <ref type="bibr" target="#b1">(2)</ref> for better solutions in Eqn. <ref type="formula" target="#formula_3">(4)</ref> is not yet explored for twostage NAS in the literature. Intuitively, one straightforward idea is to put more training budgets on models that are likely to form the best Pareto set, and train those models with more data and iterations. In practice, increasing the training budget has been shown to be an effective technique in improving DNN performance.</p><p>However, it may also be important to improve the worst performing models. Pushing the performance limits of the worst Pareto set ( <ref type="figure" target="#fig_2">Figure 3</ref>) may lead to a better optimized weight-sharing graph, such that all trainable components (e.g., channels) reach their maximum potential in contributing to the final performance. In addition, the rationale of improving on the worst Pareto architectures is similar to hard example mining <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>, by viewing the worst Pareto sub-networks as difficult data examples. It can lead to more informative gradients and better exploration in the architecture space, thus yielding better NAS performance.</p><p>In this work, we study a number of Pareto-aware sampling strategies for improving two-stage NAS. We give a precise definition of the best Pareto architecture set and the worst Pareto architecture set in section 3.1 and then present our main algorithm in section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sub-networks of Interest</head><p>Best Pareto architecture set: Given an optimization state W (the parameters of our weight-sharing graph), a subnetwork α is considered as a best Pareto architecture if there exists no other architecture a ∈ A that achieves better performance while consuming less or the same computational cost, i.e., ∀α ∈ A, if FLOPs(α ) ≤ FLOPs(α), then,</p><formula xml:id="formula_5">L(W α ; D val ) &gt; L(W α ; D val ).</formula><p>Worst Pareto architecture set: Similarly, we define an architecture α as a worst Pareto architecture if it is always dominated in accuracy by other architectures with the same or larger FLOPs, i.e., L(W α ; D val ) &lt; L(W α ; D val ) for any α satisfies FLOPs(α ) ≥ FLOPs(α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pareto-attentive pre-training</head><p>In Eqn. <ref type="bibr" target="#b1">(2)</ref>, all candidate networks are optimized with equal probabilities. We reformulate (2) with a Paretoattentive objective such that the optimization focus on either the best or the worst Pareto set. We first rewrite the expectation in Eqn. (2) as an expected loss over FLOPs as follows,</p><formula xml:id="formula_6">min W E π(τ ) E π(α|τ ) L(W α ; D trn ) ,<label>(5)</label></formula><p>where τ denotes the FLOPs of the candidate network. It is easy to see that Eqn. (5) reduces to Eqn. (2) by setting π(τ ) as the prior distribution of FLOPs specified by the search space A and π(α | τ ) as a uniform distribution over architectures conditioned on FLOPs τ . Here, we drop the regularization term R(W ) for simplicity. Pareto-aware sampling can be conducted by setting π(α | τ ) to be an attentive sampling distribution that always draws best or worst Pareto architectures. This optimization goal is formulated as follows,</p><formula xml:id="formula_7">min W E π(τ ) π(α|τ ) γ(α)L(W α ; D trn ) ,<label>(6)</label></formula><p>where γ(α) is defined to be 1 if and only if α is a candidate network on the best or the worst Pareto front, otherwise 0.</p><p>To solve this optimization, in practice, we can approximate the expectation over π(τ ) with n Monte Carlo samples of FLOPs {τ o }. Then, for each targeted FLOPs τ o , we can approximate the summation over</p><formula xml:id="formula_8">π(α | τ o ) with k sampled architectures {a 1 , · · · , a k } ∼ π(α | τ o ) such that FLOPs(α i ) = τ o , ∀1 ≤ i ≤ k as follows, min W 1 n n τo∼π(τ ) k αi∼π(α|τo) γ(α i )L(W αi ; D trn ) . (7)</formula><p>Let P (α) denote the performance estimation of a model α with parameters W α . If the goal is to focus on best Pareto architectures, we assign γ(</p><formula xml:id="formula_9">α i ) = I(P (α i ) &gt; P (α j ), ∀ j = i), where I(·) is an indicator function. If the goal is to focus on worst Pareto architectures, we set γ(α i ) = I(P (α i ) &lt; P (α j ), ∀ j = i).</formula><p>Algorithm 1 provides a meta-algorithm of our attentive sampling based NAS framework, dubbed as AttentiveNAS. We denote the sampling strategy of always selecting the best performing architecture to train as Bestup and the strategy of always selecting the worst performing architecture to train as WorstUp.</p><p>An ideal choice for the performance estimator P (α) is to set it as the negative validation loss, i.e., P (α) = −L(W α ; D val ). However, this is often computationally expensive since the validation set could be large. In this Algorithm Draw a min-batch of data <ref type="bibr">4:</ref> for i ← 1 : n do 5:</p><p>Sample a target FLOPs τ 0 according the FLOPs prior distribution specified by the search space A 6:</p><p>Uniformly sample k subnetworks {α 1 , · · · , α k } following the FLOPs constraint τ 0 7:</p><p>(a) if BestUp-k: select the sub-network with the best performance to train according to P 8:</p><p>(b) if WorstUp-k: select the sub-network with the worst performance to train according to P 9: end for 10:</p><p>Compute additional regularization terms and backpropagate; see Eqn. <ref type="bibr" target="#b6">(7)</ref>. 11: end while work, we experiment with a number of surrogate performance metrics that could be computed efficiently, including predicted accuracy given by pre-trained accuracy predictors or mini-batch losses. Our approximation leads to a variety of attentive architecture sampling implementations, as we discuss in the following experimental results section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we describe our implementation in detail and compare with prior art NAS baselines. Additionally, we provide comparisons of training and search time cost in Appendix E. We evaluate the inference latency and transfer learning performance of our AttentiveNAS models in Appendix F and G, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Search Space</head><p>We closely follow the prior art search space design in FBNetV3 <ref type="bibr" target="#b9">[10]</ref> with a number of simplifications. In particular, we use the same meta architecture structure in FBNetV3 but reduce the search range of channel widths, depths, expansion ratios and input resolutions. We also limit the largest possible sub-network in the search space to be less than 2, 000 MFLOPs and constrain the smallest sub-network to be larger than 200 MFLOPs. In particular, our smallest and largest model has 203 MFLOPs and 1, 939 MFLOPs, respectively. The search space is shown in Appendix D.</p><p>Note that our search space leads to better DNN solutions compared to those yield by the BigNAS <ref type="bibr" target="#b42">[43]</ref> search space. Compared with the BigNAS search space, our search space contains more deeper and narrower sub-networks, which achieves higher accuracy under similar FLOPs constraints. We provide detailed comparisons in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training and Evaluation</head><p>Sampling FLOPs-constrained architectures: One key step of AttentiveNAS is to draw architecture samples following different FLOPs constraints (see Eqn. <ref type="bibr" target="#b6">(7)</ref> or step 6 in Algorithm 1). At each sampling step, one needs to first draw a sample of target FLOPs τ 0 according to the prior distribution π(τ ); and then sample k architectures {a 1 , · · · , a k } from π(α | τ 0 ).</p><p>In practice, π(τ ) can be estimated offline easily. We first draw a large number of m sub-networks from the search space randomly (e.g. m ≥ 10 6 ). Then, the empirical approximation of π(τ ) can be estimated aŝ</p><formula xml:id="formula_10">π(τ = τ 0 ) = #(τ = τ 0 ) m ,</formula><p>where #(τ = τ 0 ) is the total number of architecture samples that yield FLOPs τ 0 . We also round the real FLOPs following a step t to discretize the whole FLOPs range. We fix t = 25 MFLOPs in our experiments.</p><p>To draw an architecture sample given a FLOPs constraint, a straightforward strategy is to leverage rejection sampling, i.e., draw samples uniformly from the entire search space and reject samples if the targeted FLOPs constraint is not satisfied. This naive sampling strategy, however, is inefficient especially when the search space is large.</p><p>To speedup the FLOPs-constrained sampling process, we propose to approximate π(α | τ ) empirically. Assume the network configuration is represented by a vector of discrete variables α = [o 1 , · · · , o d ] ∈ R d , where each element o i denotes one dimension in the search space, e.g., channel width, kernel size, expansion ratio, etc. See <ref type="table">Table 2</ref> for a detailed description of our search space. Letπ(α | τ ) denote an empirical approximation of π(α | τ ), for simplicity, we relax,π</p><formula xml:id="formula_11">(α | τ = τ 0 ) ∝ iπ (o i | τ = τ 0 ).</formula><p>Let #(o i = k, τ = τ 0 ) be the number of times that the pair (o i = k, τ = τ 0 ) appears in our architecture-FLOPs sample pool. Then, we can approximateπ(o i | τ = τ 0 ) as follows,</p><formula xml:id="formula_12">π(o i = k | τ 0 ) = #(o i = k, τ = τ 0 ) #(τ = τ 0 ) .</formula><p>Now, to sample a random architecture under a FLOPs constraint, we directly leverage rejection sampling fromπ(α | τ ), which yields much higher sampling efficiency than sampling from whole search space directly. To further reduce the training overhead, we conduct the sampling process in an asynchronous mode on CPUs, which does not slow down the training process on GPUs.</p><p>Training details: We closely follow the BigNAS <ref type="bibr" target="#b42">[43]</ref> training settings. See Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MFLOPs</head><p>Acc predicted (s0, ep30) Acc predicted (s0, ep30) Acc predicted (s0, ep30) <ref type="figure">Figure 4</ref>. Rank correlation between the predicted accuracy and the actual accuracy estimated on data. Here acc predicted is the accuracy prediction by using our accuracy predictor and acc actual denotes the real model accuracy estimated on its corresponding testing data partition by reusing the weight-sharing parameters. s0 and s1 denotes random partition with seed 0 and seed 1, respectively. ep30 and 360 denotes 30 epochs of training and 360 epochs training, respectively.</p><p>Evaluation: To ensure a fair comparison between different sampling strategies, we limit the number of architectures to be evaluated to be the same for different algorithms. We use evolutionary search on the ImageNet validation set to search promising sub-networks following [37] 1 . We fix the initial population size to be 512, and set both the mutate and cross over population size to be 128. We run evolution search for 20 iterations and the total number of architectures to be evaluated is 5248.</p><p>Note that when comparing with prior art NAS baselines, we withheld the original validation set for testing and subsampled 200K training examples for evolutionary search. See section 4.5 for more details.</p><p>Since the running statistics of batch normalization layers are not accumulated during training, we calibrate the batch normalization statistics before evaluation following <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attentive Sampling with Efficient Performance Estimation</head><p>The attentive sampling approach requires selecting the best or the worst sub-network from a set of sampled candidates. Exact performance evaluation on a validation set is computationally expensive. In this part, we introduce two efficient algorithms for sub-network performance estimation:</p><p>• Minibatch-loss as performance estimator: for each architecture, use the training loss measured on the current mini-batch of training data as the proxy performance metric;</p><p>• Accuracy predictor as performance estimator: train an accuracy predictor on a validation set; then for each architecture, use the predicted accuracy given by the accuracy predictor as its performance estimation.</p><p>The first approach is intuitive and straightforward. For the second approach, it is widely observed in the literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref> that the performance rank correlation between different sub-networks learned via weight-sharing varies significantly across different runs, resulting in extremely low Kendall's τ values. If this is still the case for the twostage NAS, a pre-trained accuracy predictor cannot generalize well across different setups. Hence, it is important to first understand the performance variation of candidate subnetworks in different training stages and settings.</p><p>Settings for training accuracy predictors: We proceed as follows: 1) we first split the original training dataset into 90% of training and 10% of testing; 2) we conduct the constraint-free pre-training on the sub-sampled training set. We limit the training to be 30 epochs, hence only introducing less than 10% of the full two-stage NAS computation time. Once the training is done, we randomly sample 1024 sub-networks and evaluate their performance on the subsampled testing data partition; 3) we split the 1024 pairs of sub-networks and their accuracies into equally sized training and evaluation subsets. We train a random forest regressor with 100 trees as the accuracy predictor and set the maximum depth to be 15 per tree.</p><p>Results on the effectiveness of accuracy predictors: For all testing sub-networks, we measure the rank correlation (Kendall's τ ) between their predicted accuracies and their actual accuracies measured on the subsampled testing dataset.</p><p>As shown in <ref type="figure">Figure 4 (a)</ref>, the Kendall's τ between the predicted accuracies and the actual accuracies is 0.89, which indicates a very high rank correlation.</p><p>Since the weight-sharing parameters are constantly updated at each training step (Eqn. <ref type="formula">(7)</ref>), would the performance rank between different sub-networks remains stable throughout the training stage? To verify, we further ex- MFLOPs (±10) <ref type="figure">Figure 6</ref>. Comparison of Pareto-set performance with the Uniform sampling baseline. tend the step 2) above for 360 epochs and measure the rank correlation between the predicted accuraries and their actual accuraries on the testing sub-networks set. <ref type="figure">Figure 4</ref> (b) shows that the accuracy predictor trained via early stopping at epoch 30 also provides a good estimation in predicting the actual accuracy measured via using the weightsharing parameters learned at epoch 360, yielding a high rank correlation of 0.87. Our results also generalize to different random data partitions. As shown in <ref type="figure">Figure 4</ref> (c), we use the accuracy predictor trained on data partition with random seed 0 to predict the architecture performance on data partition with random seed 1. The Kendall' τ is 0.88, indicating significant high rank correlation. Our findings provide abundant evidence that justifies the choice of using pre-trained accuracy predictors for sub-network performance estimation in Algorithm 1. It also shows the robustness of the weight-sharing NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">NAS with Efficient Attentive Sampling</head><p>Settings: AttentiveNAS requires specifying: 1) the attentive architecture set, either the best Pareto front (denoted as BestUp) or the worst Pareto front (denoted as WorstUp);</p><p>2) the number of candidate sub-networks (k) to be evaluated at each sampling step, see Step 6 in Algorithm 1; and 3) the performance estimator, e.g., the minibatch loss based performance estimation (denoted as loss) or the predicted accuracies based performance estimation (denoted as acc). We name our sampling strategies accordingly in the following way,</p><formula xml:id="formula_13">{BestUp / WorstUp} 1) attentive architecture set − k 2) #candidates ({loss/acc}) 3) performance estimator ,</formula><p>In general, we would like to set k to be a relative large number for better Pareto frontier approximation. For our accuracy predictor based implementation, we set k = 50 as default, yielding sample strategies BestUp-50 (acc) and WorstUp-50 (acc).</p><p>We also study an extreme case, for which we generate the potential best or worst Pareto architecture set in an offline mode. Specifically, we first sample 1 million random sub-networks and use our pretrained accuracy predictor to predict the best or the worst Pareto set in an offline mode. This is equivalent to set k as a large number. We use BestUp-1M (acc) and WorstUp-1M (acc) to denote the algorithms that only sample from the offline best or the offline worst Pareto set, respectively.</p><p>For our minibatch loss based sampling strategies BestUp-k (loss) and WorstUp-k (loss), these methods require to forward the data batch for k − 1 more times compared with the Uniform baseline (k = 1). We limit k = 3 in our experiments to reduce the training overhead.</p><p>Results: We summarize our results in <ref type="figure" target="#fig_4">Figure 5</ref> and Figure 6. In <ref type="figure" target="#fig_4">Figure 5</ref>, we group architectures according to their FLOPs and visualize five statistics for each group of sub-networks, including the minimum, the first quantile, the median, the third quantile and the maximum accuracy. In <ref type="figure">Figure 6</ref>, we report the maximum top-1 accuracy achieved by different sampling strategies on various FLOPs regimes. For visualization clarity, we plot the relative top-1 accuracy gain over the Uniform baseline. We have the following observations from the experimental results: 1) As shown in <ref type="figure" target="#fig_4">Figure 5</ref> (a) and (b), pushing up the worst performed architectures during training leads to a higher low-bound performance Pareto. The minimum and the first quartile accuracy achieved by WorstUp-50 (acc) and WorstUp-1M (acc) are significantly higher than those achieved by BestUp-50 (acc), BestUp-1M (acc)) and Uniform.</p><p>2) WorstUp-1M (acc) consistently outperforms over BestUp-1M (acc) in <ref type="figure" target="#fig_4">Figure 5</ref> (a) and (b). Our findings challenge the traditional thinking of NAS by focusing only on the best Pareto front of sub-networks, e.g., in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>3) Improving models on the worst Pareto front leads to a better performed best Pareto front. For example, as we can see from <ref type="figure" target="#fig_4">Figure 5</ref> and 6, WorstUp-50 (acc) outperforms Uniform around 0.3% of top-1 accuracy on the 200 ± 10 MFLOPs regime. WorstUp-1M (acc) also improves on the Uniform baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>As we can see from <ref type="figure">Figure 6</ref>, the best Pareto front focused sampling strategies are mostly useful at medium FLOPs regimes. BestUp-50 (acc) starts to outperform WorstUp-50 (acc) and Uniform when the model size is greater than 400 MFLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Both</head><p>WorstUp-3 (loss) and BestUp-3 (loss) improves on Uniform, further validating the advantage of our attentive sampling strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6)</head><p>As we can see from <ref type="figure">Figure 6</ref>, BestUp-3 (loss) achieves the best performance in general. Compared with BestUp-50 (acc) and BestUp-1M (acc), BestUp-3 (loss) yields better exploration of the search space; while comparing with Uniform, BestUp-3 (loss) enjoys better exploitation of the search space. Our findings suggest that a good sampling strategy needs to balance the exploration and exploitation of the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with Prior NAS Approaches</head><p>In this section, we pick our winning sampling strategy BestUp-3 (loss) (denoted as AttentiveNAS in Table 1), and compare it with prior art NAS baselines on ImageNet, including FBNetV2 <ref type="bibr" target="#b35">[36]</ref>, FBNetV3 <ref type="bibr" target="#b9">[10]</ref>, Mo-bileNetV2 <ref type="bibr" target="#b27">[28]</ref>, MobileNetV3 <ref type="bibr" target="#b16">[17]</ref>, OFA <ref type="bibr" target="#b2">[3]</ref>, FairNAS <ref type="bibr" target="#b7">[8]</ref>, Proxyless <ref type="bibr" target="#b3">[4]</ref>, MnasNet <ref type="bibr" target="#b33">[34]</ref>, NASNet <ref type="bibr" target="#b45">[46]</ref>, Efficient-Net <ref type="bibr" target="#b34">[35]</ref> and BigNAS <ref type="bibr" target="#b42">[43]</ref>.</p><p>For fair comparison, we withhold the original ImageNet validation set for testing and randomly sample 200k Ima-geNet training examples as the validation set for searching. Since all models are likely to overfit at the end of training, we use the weight-sharing parameter graph learned at epoch 30 for performance estimation and then evaluate the discovered best Pareto set of architectures on the unseen original ImageNet validation set. We follow the evolutionary search protocols described in Section 4. <ref type="bibr" target="#b1">2</ref> We summarize our results in both <ref type="table" target="#tab_2">Table 1</ref> and <ref type="figure" target="#fig_0">Figure 1</ref>. AttentiveNAS significantly outperforms all baselines, establishing new SOTA accuracy vs. FLOPs trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Method MFLOPs Top-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>200-300 (M)</head><p>AttentiveNAS-A0 203 77.3 MobileNetV2 0.75× <ref type="bibr" target="#b27">[28]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a variety of attentive sampling strategies for training two-stage NAS. We show that our attentive sampling can improve the accuracy significantly compared to the uniform sampling by taking the performance Pareto into account. Our method outperforms priorart NAS approaches on the ImageNet dataset, establishing new SOTA accuracy under various of FLOPs constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training settings</head><p>We use the sandwich sampling rule and always train the smallest and biggest sub-networks in the search space as regularization (see Eqn. <ref type="formula" target="#formula_2">(3)</ref>). We set n = 2 in Eqn. <ref type="bibr" target="#b6">(7)</ref>. This way, at each iteration, a total of 4 sub-networks are evaluated. We use in-place knowledge distillation, i.e., all smaller sub-networks are supervised by the largest sub-network. To handle different input resolutions, we always fetch training patches of a fixed size (e.g., 224x224 on ImageNet) and then rescale them to our target resolution with bicubic interpolation.</p><p>We use SGD with a cosine learning rate decay. All the training runs are conducted with 64 GPUs and the mini-batch size is 32 per GPU. The base learning rate is set as 0.1 and is linearly scaled up for every 256 training samples. We use AutoAugment <ref type="bibr" target="#b8">[9]</ref> for data augmentation and set label smoothing coefficient to 0.1. Unless specified, we train the models for 360 epochs. We use momentum of 0.9, weight decay of 10 −5 , dropout of 0.2 after the global average pooling layer, and stochastic layer dropout of 0.2. We don't use synchronized batch-normalization. Following <ref type="bibr" target="#b42">[43]</ref>, we only enable weight decay and dropout for training the largest DNN model. All other smaller sub-networks are trained without regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robustness of two-stage NAS</head><p>We also study the robustness and stability of stage 1 constraint-free NAS pre-training w.r.t. different data partitions, initializations and training epochs.</p><p>We follow the experimental setting in settings 4.3. Specifically, 1) we randomly partitioned the original ImageNet training set into 90% for training and 10% for testing. We then train on the subsampled training set. 2) After training, we randomly sample 1024 sub-networks and evaluate their performance on their corresponding testing data partition.</p><p>In <ref type="figure">Figure 7</ref>, we show that our two-stage NAS training is quite robust, achieving reproducible results across a variety of training settings. Specifically, in <ref type="figure">Figure 7</ref> (a), we terminate early at epoch 30, the Kendall's tau value is 0.94 between two different runs. We further train for 360 epochs, in <ref type="figure">Figure 7</ref> (b), we observe a high rank correlation of 0.96 between different trials. Furthermore, in <ref type="figure">Figure 7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sampling efficiency</head><p>Our attentive sampling requires to sample architectures under different FLOPs constraints. Given a randomly drawn FLOPs constraint, naive uniformly sampling requires of an average of 50,878 trials to sample an architecture that satisfies the constraint due to the enormous size of the search space. In section 4.2, we construct a proposal distributionπ(α | τ ) in an offline mode to accelerate this sampling process. In <ref type="figure" target="#fig_6">Figure 8</ref>, we show the average sampling trials for sampling targeted architectures under constraints is about 12 by sampling fromπ, hence computationally extremely efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparisons of search space</head><p>Our search space is defined in <ref type="table">Table 2</ref>. Note that our search space is adapted from FBNetV3 <ref type="bibr" target="#b9">[10]</ref>. Compared to the search space used in BigNAS <ref type="bibr" target="#b42">[43]</ref>, our search space contains more deeper and narrower sub-networks. We compare the uniform sampling strategy performance on both search spaces. Specifically, we follow the evaluation flow described in section 4.2. The search space proposed in <ref type="bibr" target="#b2">[3]</ref> is not evaluated here as its training pipeline requires complicated progressive network shrinking and carefully tuned hyper-parameters for each training stage.  <ref type="table">Table 2</ref>. An illustration of our search space. MBConv refers to inverted residual block <ref type="bibr" target="#b27">[28]</ref>. MBPool denotes the efficient last stage <ref type="bibr" target="#b16">[17]</ref>. SE represents the squeeze and excite layer <ref type="bibr" target="#b18">[19]</ref>. Width represents the channel width per layer. Depth denotes the number of repeated MBConv blocks. Kernel size and expansion ratio is the filter size and expansion ratio for the depth-wise convolution layer used in each MBConv block. We use swish activation.</p><p>Top-1 accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparisons of training and search time</head><p>Overall, our method yields a computationally efficient NAS framework: 1) compared with RL-based solutions [e.g., <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>, our method builds on weight-sharing <ref type="bibr" target="#b34">[35]</ref>, alleviating the burden of training thousands of sub-networks from scratch or on proxy tasks; 2) when comparing with conventional differentiable NAS approaches, e.g., DARTS <ref type="bibr" target="#b22">[23]</ref>, ProxylessNAS <ref type="bibr" target="#b3">[4]</ref>, etc, our method simultaneously finds a set of Pareto optimal networks for various deployment scenarios, e.g., N different FLOPs requirements, with just one single training. While typical differentiable NAS solutions need to repeat the NAS procedure for each deployment consideration; 3) no fine-tuning or re-training is needed for our method. The networks can be directly sampled from the weight-sharing graph in contrast to Once-for-All (OFA) etc, which usually requires to fine-tune the sub-networks.</p><p>As different methods use different hardware for training, it makes wall-clock time comparison challenging. We report the total number of training epochs performed on ImageNet by each method in <ref type="table">Table 3</ref>. For our method, we train for 360 epochs and our training strategy requires back-propagating through 4 sub-networks at each iteration, which is roughly about 4× slower in per batch time. As we can see from <ref type="table">Table 3</ref>, our method yields the lowest training cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Total training epochs on ImageNet (N=40) MnasNet <ref type="bibr" target="#b33">[34]</ref> 40,000N = 1,600k ProxylessNAS <ref type="bibr" target="#b3">[4]</ref> 200N (weight-sharing graph training) + 300N (retraining) = 20k OFA <ref type="bibr" target="#b2">[3]</ref> 590 (weight-sharing graph training) + 75N (finetuning) = 3.59k AttentiveNAS (ours) 360 ×4 (weight-sharing graph training) = 1.44k <ref type="table">Table 3</ref>. Overview of training cost. Here N denotes the number of deployment cases. Following OFA, we consider N = 40. Similar to OFA, our method also includes an additional stage of evolutionary search (evaluation with fixed weights, no back-propagation), which amounts to less than 10% of the total training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional results on inference latency</head><p>Our attentive sampling could be naturally adapted for other metrics, e.g., latency. In this work, we closely follow the conventional NAS evaluation protocols in the literature and report the accuracy vs. FLOPs Pareto as examples to demonstrate the effectiveness of our method. In table 4, we use GPU latency as an example and provide additional latency comparisons on both 2080 Ti and V100 GPUs. Compared with EfficientNet models <ref type="bibr" target="#b34">[35]</ref>, our model yield better latency vs. ImageNet validation accuracy trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Batch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Transfer learning results</head><p>We evaluate the transfer learning performance of our AttentiveNAS-A1 and AttentiveNAS-A4 model on standard benchmarks, including Oxford Flowers <ref type="bibr" target="#b23">[24]</ref>, Stanford Cars <ref type="bibr" target="#b21">[22]</ref> and Food-101 <ref type="bibr" target="#b1">[2]</ref>.</p><p>Specifically, we closely follow the training settings and strategies in <ref type="bibr" target="#b19">[20]</ref>, where the best learning rate and the weight decay are searched on a hold-out subset (20%) of the training data. All models are fine-tuned for 150 epochs with a batch size of 64. We use SGD with momentum of 0.9, label smoothing of 0.1 and dropout of 0.5. All images are resized to the size used on ImageNet. As we can see from <ref type="table">Table 5</ref>, our models yield the best transfer learning accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MFLOPs  <ref type="table">Table 5</ref>. Results (%) on transfer learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>TopFigure 1 .</head><label>1</label><figDesc>Comparison of AttentiveNAS with prior NAS approaches<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref> on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Best</head><label></label><figDesc>Pareto architecture set Worst Pareto architecture set Task performance Model size (e.g., FLOPs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An illustration of best and worst Pareto architecture set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Kendall's τ = 0.89(b) Kendall's τ = 0.87 (c) Kendall's τ = 0.88Acc actual (s0, ep30)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Results on ImageNet of different sampling strategies. Each box plot shows the the performance summarization of sampled architecture within the specified FLOPs regime. From left to right, each horizontal bar represents the minimum accuracy, the first quartile, the sample median, the sample third quartile and the maximum accuracy, respectively.Relative acc w.r.t. Uniform</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(c), we show the performance measured at epoch 30 also correlates well with the performance measured at the end of training. The rank correlation is 0.88. Our results are in alignment with the findings in FairNAS<ref type="bibr" target="#b7">[8]</ref>.Kendall's τ = 0.94Kendall's τ = 0.96 Kendall's τ = 0.88Acc actual (s1, ep30) s0, ep30) Acc actual (s0, ep360) Acc actual (s0, ep30) (a) Rank correlation at ep30 (b) Rank correlation at ep360 (c) Rank correlation wrt training epochsFigure 7. An illustration of robustness of stage 1 training. S0 and s1 denote random data partition with seed 0 and seed 1, respectively. Ep30 and ep360 denote 30 training epochs and 360 training epochs, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>An illustration of mean of the number of trials to sample architectures under constraints along with its standard derivation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of the effectiveness of search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1</head><label></label><figDesc>AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling 1: Input: Search space A; performance estimator P 2: while not converging do</figDesc><table /><note>3:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison with prior NAS approaches on ImageNet.</figDesc><table><row><cell></cell><cell></cell><cell>208</cell><cell>69.8</cell></row><row><cell></cell><cell>MobileNetV3 1.0× [17]</cell><cell>217</cell><cell>75.2</cell></row><row><cell></cell><cell>FBNetv2 [36]</cell><cell>238</cell><cell>76.0</cell></row><row><cell></cell><cell>BigNAS [43]</cell><cell>242</cell><cell>76.5</cell></row><row><cell></cell><cell>AttentiveNAS-A1</cell><cell>279</cell><cell>78.4</cell></row><row><cell></cell><cell>MNasNet [34]</cell><cell>315</cell><cell>75.2</cell></row><row><cell></cell><cell>AttentiveNAS-A2</cell><cell>317</cell><cell>78.8</cell></row><row><cell></cell><cell>Proxyless [4]</cell><cell>320</cell><cell>74.6</cell></row><row><cell></cell><cell>FBNetv2 [36]</cell><cell>325</cell><cell>77.2</cell></row><row><cell>300-400 (M)</cell><cell cols="2">FBNetv3 [10] MobileNetV3 1.25× [17] 356 343</cell><cell>78.0 76.6</cell></row><row><cell></cell><cell>AttentiveNAS-A3</cell><cell>357</cell><cell>79.1</cell></row><row><cell></cell><cell>OFA (#75ep) [3]</cell><cell>389</cell><cell>79.1</cell></row><row><cell></cell><cell>EfficientNet-B0 [35]</cell><cell>390</cell><cell>77.1</cell></row><row><cell></cell><cell>FairNAS [8]</cell><cell>392</cell><cell>77.5</cell></row><row><cell></cell><cell>MNasNet [34]</cell><cell>403</cell><cell>76.7</cell></row><row><cell></cell><cell>BigNAS [43]</cell><cell>418</cell><cell>78.9</cell></row><row><cell></cell><cell>FBNetv2 [36]</cell><cell>422</cell><cell>78.1</cell></row><row><cell>400-500 (M)</cell><cell>AttentiveNAS-A4</cell><cell>444</cell><cell>79.8</cell></row><row><cell></cell><cell>OFA (#75ep) [3]</cell><cell>482</cell><cell>79.6</cell></row><row><cell></cell><cell>NASNet [46]</cell><cell>488</cell><cell>72.8</cell></row><row><cell></cell><cell>AttentiveNAS-A5</cell><cell>491</cell><cell>80.1</cell></row><row><cell></cell><cell>EfficientNet-B1 [35]</cell><cell>700</cell><cell>79.1</cell></row><row><cell>&gt;500 (M)</cell><cell>AttentiveNAS-A6 FBNetV3 [10]</cell><cell>709 752</cell><cell>80.7 80.4</cell></row><row><cell></cell><cell>EfficientNet-B2 [35]</cell><cell cols="2">1000 80.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Inference latency comparison.</figDesc><table><row><cell></cell><cell cols="4">-size 2080 Ti (ms) V100 (ms) Top-1</cell></row><row><cell>Efficientnet (B0)</cell><cell>128</cell><cell>21.51± 0.27</cell><cell>13.13± 0.30</cell><cell>77.3</cell></row><row><cell>AttentiveNAS-A1</cell><cell>128</cell><cell>19.13± 0.26</cell><cell>12.32± 0.26</cell><cell>78.4</cell></row><row><cell>Efficientnet (B1)</cell><cell>128</cell><cell>28.87± 0.45</cell><cell>19.71 ± 0.40</cell><cell>80.3</cell></row><row><cell>AttentiveNAS-A6</cell><cell>128</cell><cell>23.43± 0.37</cell><cell>15.99± 0.33</cell><cell>80.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https : / / github . com / mit -han -lab / hardwareaware-transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<idno>PMLR, 2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalization bounds of stochastic gradient descent for wide and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10836" to="10846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching toward pareto-optimal device-aware neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-Chieh</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Huan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chieh</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-Aided Design</title>
		<meeting>the International Conference on Computer-Aided Design</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Ting-Wu Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pareco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11752</idno>
		<title level="m">Pareto-aware channel optimization for slimmable neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02049</idno>
		<title level="m">Joint architecture-recipe search using neural acquisition function</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards efficient network design through platform-aware model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11398" to="11407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Nas-bench-102: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Maxup: A simple way to improve generalization of neural network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09024</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Chakraborty, and Erik Learned-Miller. Unsupervised hard example mining from videos for improved object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Souyoung</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="307" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Collecting a large-scale dataset of fine-grained cars. Second Workshop on Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Elizaveta Ivanova, Ilya Kalinovskiy, and Eugene Luckyanets. Hard example mining with auxiliary embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Melnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Oleinik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single-path nas: Designing hardware-efficient convnets in less than 4 hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Exploiting the potential of standard convolutional autoencoders for image restoration by evolutionary search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00370</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic class-based hard example mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7251" to="7259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12965" to="12974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hat: Hardware-aware transformers for efficient natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14187</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1379" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperança</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12522</idno>
		<title level="m">Nas evaluation is frustratingly hard</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7105" to="7114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Universally slimmable networks and improved training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1803" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11142</idno>
		<title level="m">Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search with big single-stage models</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
