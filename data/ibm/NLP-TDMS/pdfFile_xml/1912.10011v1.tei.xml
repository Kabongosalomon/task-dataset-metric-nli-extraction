<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Model for Data-to-Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Rebuffel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIP6</orgName>
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">BNP Paribas</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIP6</orgName>
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">BNP Paribas</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIP6</orgName>
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Criteo AI Lab</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Model for Data-to-Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data-to-text · Hierarchical encoding · Language Generation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transcribing structured data into natural language descriptions has emerged as a challenging task, referred to as "data-to-text". These structures generally regroup multiple elements, as well as their attributes. Most attempts rely on translation encoder-decoder methods which linearize elements into a sequence. This however loses most of the structure contained in the data. In this work, we propose to overpass this limitation with a hierarchical model that encodes the data-structure at the element-level and the structure level. Evaluations on RotoWire show the effectiveness of our model w.r.t. qualitative and quantitative metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge and/or data is often modeled in a structure, such as indexes, tables, key-value pairs, or triplets. These data, by their nature (e.g., raw data or long time-series data), are not easily usable by humans; outlining their crucial need to be synthesized. Recently, numerous works have focused on leveraging structured data in various applications, such as question answering <ref type="bibr" target="#b26">[24,</ref><ref type="bibr" target="#b36">34]</ref> or table retrieval <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">32]</ref>. One emerging research field consists in transcribing data-structures into natural language in order to ease their understandablity and their usablity. This field is referred to as "data-to-text" <ref type="bibr" target="#b8">[8]</ref> and has its place in several application domains (such as journalism <ref type="bibr" target="#b24">[22]</ref> or medical diagnosis <ref type="bibr" target="#b27">[25]</ref>) or wide-audience applications (such as financial <ref type="bibr" target="#b28">[26]</ref> and weather reports <ref type="bibr" target="#b32">[30]</ref>, or sport broadcasting <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">39]</ref>). As an example, <ref type="figure">Figure 1</ref> shows a data-structure containing statistics on NBA basketball games, paired with its corresponding journalistic description.</p><p>Designing data-to-text models gives rise to two main challenges: 1) understanding structured data and 2) generating associated descriptions. Recent datato-text models <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b41">39]</ref> mostly rely on an encoder-decoder architecture <ref type="bibr" target="#b1">[2]</ref> in which the data-structure is first encoded sequentially into a fixed-size vectorial representation by an encoder. Then, a decoder generates words conditioned on this representation. With the introduction of the attention mechanism <ref type="bibr" target="#b21">[19]</ref> on one hand, which computes a context focused on important elements from the input at each decoding step and, on the other hand, the copy mechanism <ref type="figure">Fig. 1</ref>: Example of structured data from the RotoWire dataset. Rows are entities (either a team or a player) and each cell a record, its key being the column label and its value the cell content. Factual mentions from the table are boldfaced in the description. <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b35">33]</ref> to deal with unknown or rare words, these systems produce fluent and domain comprehensive texts. For instance, Roberti et al. <ref type="bibr" target="#b33">[31]</ref> train a characterwise encoder-decoder to generate descriptions of restaurants based on their attributes, while Puduppully et al. <ref type="bibr" target="#b30">[28]</ref> design a more complex two-step decoder: they first generate a plan of elements to be mentioned, and then condition text generation on this plan. Although previous work yield overall good results, we identify two important caveats, that hinder precision (i.e. factual mentions) in the descriptions:</p><p>1. Linearization of the data-structure. In practice, most works focus on introducing innovating decoding modules, and still represent data as a unique sequence of elements to be encoded. For example, the table from <ref type="figure">Figure 1</ref> would be linearized to [(Hawks, H/V, H), ..., (Magic, H/V, V), ...], effectively leading to losing distinction between rows, and therefore entities. To the best of our knowledge, only Liu et al. <ref type="bibr" target="#b19">[17,</ref><ref type="bibr" target="#b20">18]</ref> propose encoders constrained by the structure but these approaches are designed for single-entity structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Arbitrary ordering of unordered collections in recurrent networks (RNN).</head><p>Most data-to-text systems use RNNs as encoders (such as GRUs or LSTMs), these architectures have however some limitations. Indeed, they require in practice their input to be fed sequentially. This way of encoding unordered sequences (i.e. collections of entities) implicitly assumes an arbitrary order within the collection which, as demonstrated by Vinyals et al. <ref type="bibr" target="#b39">[37]</ref>, significantly impacts the learning performance.</p><p>To address these shortcomings, we propose a new structured-data encoder assuming that structures should be hierarchically captured. Our contribution focuses on the encoding of the data-structure, thus the decoder is chosen to be a classical module as used in <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b41">39]</ref>. Our contribution is threefold:</p><p>-We model the general structure of the data using a two-level architecture, first encoding all entities on the basis of their elements, then encoding the data structure on the basis of its entities; -We introduce the Transformer encoder <ref type="bibr" target="#b38">[36]</ref> in data-to-text models to ensure robust encoding of each element/entities in comparison to all others, no matter their initial positioning; -We integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder.</p><p>We report experiments on the RotoWire benchmark <ref type="bibr" target="#b41">[39]</ref> which contains around 5K statistical tables of NBA basketball games paired with humanwritten descriptions. Our model is compared to several state-of-the-art models. Results show that the proposed architecture outperforms previous models on BLEU score and is generally better on qualitative metrics.</p><p>In the following, we first present a state-of-the art of data-to-text literature (Section 2), and then describe our proposed hierarchical data encoder (Section 3). The evaluation protocol is presented in Section 4, followed by the results (Section 5). Section 6 concludes the paper and presents perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Until recently, efforts to bring out semantics from structured-data relied heavily on expert knowledge <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">30]</ref>. For example, in order to better transcribe numerical time series of weather data to a textual forecast, Reiter et al. <ref type="bibr" target="#b32">[30]</ref> devise complex template schemes in collaboration with weather experts to build a consistent set of data-to-word rules.</p><p>Modern approaches to the wide range of tasks based on structured-data (e.g. table retrieval <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b43">41]</ref>, table classification <ref type="bibr" target="#b9">[9]</ref>, question answering <ref type="bibr" target="#b12">[12]</ref>) now propose to leverage progress in deep learning to represent these data into a semantic vector space (also called embedding space). In parallel, an emerging task, called "data-to-text", aims at describing structured data into a natural language description. This task stems from the neural machine translation (NMT) domain, and early work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b41">39]</ref> represent the data records as a single sequence of facts to be entirely translated into natural language. Wiseman et al. <ref type="bibr" target="#b41">[39]</ref> show the limits of traditional NMT systems on larger structured-data, where NMT systems fail to accurately extract salient elements.</p><p>To improve these models, a number of work <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b42">40]</ref> proposed innovating decoding modules based on planning and templates, to ensure factual and coherent mentions of records in generated descriptions. For example, Puduppully et al. <ref type="bibr" target="#b30">[28]</ref> propose a two-step decoder which first targets specific records and then use them as a plan for the actual text generation. Similarly, Li et al. <ref type="bibr" target="#b18">[16]</ref> proposed a delayed copy mechanism where their decoder also acts in two steps: 1) using a classical LSTM decoder to generate delexicalized text and 2) using a pointer network <ref type="bibr" target="#b40">[38]</ref> to replace placeholders by records from the input data.</p><p>Closer to our work, very recent work <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b31">29]</ref> have proposed to take into account the data structure. More particularly, Puduppully et al. <ref type="bibr" target="#b31">[29]</ref> follow entity-centric theories <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b22">20]</ref> and propose a model based on dynamic entity representation at decoding time. It consists in conditioning the decoder on entity representations that are updated during inference at each decoding step. On the other hand, Liu et al. <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b19">17]</ref> rather focus on introducing structure into the encoder. For instance, they propose a dual encoder <ref type="bibr" target="#b19">[17]</ref> which encodes separately the sequence of element names and the sequence of element values. These approaches are however designed for single-entity data structures and do not account for delimitation between entities.</p><p>Our contribution differs from previous work in several aspects. First, instead of flatly concatenating elements from the data-structure and encoding them as a sequence <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b41">39]</ref>, we constrain the encoding to the underlying structure of the input data, so that the delimitation between entities remains clear throughout the process. Second, unlike all works in the domain, we exploit the Transformer architecture <ref type="bibr" target="#b38">[36]</ref> and leverage its particularity to directly compare elements with each others in order to avoid arbitrary assumptions on their ordering. Finally, in contrast to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">29]</ref> that use a complex updating mechanism to obtain a dynamic representation of the input data and its entities, we argue that explicit hierarchical encoding naturally guides the decoding process via hierarchical attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Encoder Model for Data-to-Text</head><p>In this section we introduce our proposed hierarchical model taking into account the data structure. We outline that the decoding component aiming to generate descriptions is considered as a black-box module so that our contribution is focused on the encoding module. We first describe the model overview, before detailing the hierarchical encoder and the associated hierarchical attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation and General Overview</head><p>Let's consider the following notations:</p><p>• An entity e i is a set of J i unordered records {r i,1 , ..., r i,j , ..., r i,Ji }; where record r i,j is defined as a pair of key k i,j and value v i,j . We outline that J i might differ between entities.</p><p>• A data-structure s is an unordered set of I entities e i . We thus denote s := {e 1 , ..., e i , ..., e I }.</p><p>• For each data-structure, a textual description y is associated. We refer to the first t words of a description y as y 1:t . Thus, the full sequence of words can be noted as y = y 1:T .</p><p>• The dataset D is a collection of N aligned (data-structure, description) pairs (s, y). For instance, <ref type="figure">Figure 1</ref> illustrates a data-structure associated with a description. The data-structure includes a set of entities (Hawks, Magic, Al Horford, Jeff Teague, ...). The entity Jeff Teague is modeled as a set of records {(PTS, 17), (REB, 0), (AST, 7) ...} in which, e.g., the record (PTS, 17) is characterized by a key (PTS) and a value <ref type="bibr" target="#b19">(17)</ref>.</p><p>For each data-structure s in D, the objective function aims to generate a descriptionŷ as close as possible to the ground truth y. This objective function optimizes the following log-likelihood over the whole dataset D:</p><formula xml:id="formula_0">arg max θ L(θ) = arg max θ (s,y)∈D log P (ŷ = y | s; θ)<label>(1)</label></formula><p>where θ stands for the model parameters and P (ŷ = y | s; θ) the probability of the model to generate the adequate description y for table s. During inference, we generate the sequenceŷ * with the maximum a posteriori probability conditioned on table s. Using the chain rule, we get:</p><formula xml:id="formula_1">y * 1:T = arg max y 1:T T t=1 P (ŷ t |ŷ 1:t−1 ; s; θ)<label>(2)</label></formula><p>This equation is intractable in practice, we approximate a solution using beam search, as in <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b41">39]</ref>.</p><p>Our model follows the encoder-decoder architecture <ref type="bibr" target="#b1">[2]</ref>. Because our contribution focuses on the encoding process, we chose the decoding module used in <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b41">39]</ref>: a two-layers LSTM network with a copy mechanism. In order to supervise this mechanism, we assume that each record value that also appears in the target is copied from the data-structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input. We now describe the hierarchical encoder and the hierarchical attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Encoding Model</head><p>As outlined in Section 2, most previous work <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b42">40</ref>] make use of flat encoders that do not exploit the data structure. To keep the semantics of each element from the data-structure, we propose a hierarchical encoder which relies on two modules. The first one (module A in <ref type="figure" target="#fig_0">Figure 2)</ref> is called low-level encoder and encodes entities on the basis of their records; the second one (module B), called high-level encoder, encodes the data-structure on the basis of its underlying entities. In the low-level encoder, the traditional embedding layer is replaced by a record embedding layer as in <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b41">39]</ref>. We present in what follows the record embedding layer and introduce our two hierarchical modules.</p><p>Record Embedding Layer. The first layer of the network consists in learning two embedding matrices to embed the record keys and values. Keys k i,j are embedded to k i,j ∈ R d and values v i,j to v i,j ∈ R d , with d the size of the embedding. As in previous work <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b41">39]</ref>, each record embedding r i,j is computed by a linear projection on the concatenation [k i,j ; v i,j ] followed by a non linearity:</p><formula xml:id="formula_2">r i,j = ReLU(W r [k i,j ; v i,j ] + b r )<label>(3)</label></formula><p>where W r ∈ R 2d×d and b r ∈ R d are learnt parameters. The low-level encoder aims at encoding a collection of records belonging to the same entity while the high-level encoder encodes the whole set of entities. Both the low-level and high-level encoders consider their input elements as unordered. We use the Transformer architecture from <ref type="bibr" target="#b38">[36]</ref>. For each encoder, we have the following peculiarities:</p><p>the Low-level encoder encodes each entity e i on the basis of its record embeddings r i,j . Each record embedding r i,j is compared to other record embeddings to learn its final hidden representation h i,j . Furthermore, we add a special record [ENT] for each entity, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> as the last record. Since entities might have a variable number of records, this token allows to aggregate final hidden record representations {h i,j } Ji j=1 in a fixedsized representation vector h i .</p><p>the High-level encoder encodes the data-structure on the basis of its entity representation h i . Similarly to the Low-level encoder, the final hidden state e i of an entity is computed by comparing entity representation h i with each others. The data-structure representation z is computed as the mean of these entity representations, and is used for the decoder initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical attention</head><p>To fully leverage the hierarchical structure of our encoder, we propose two variants of hierarchical attention mechanism to compute the context fed to the decoder module.</p><p>• Traditional Hierarchical Attention. As in <ref type="bibr" target="#b31">[29]</ref>, we hypothesize that a dynamic context should be computed in two steps: first attending to entities, then to records corresponding to these entities. To implement this hierarchical attention, at each decoding step t, the model learns a first set of attention scores α i,t over entities e i and a second set of attention scores β i,j,t over records r i,j belonging to entity e i . The α i,t scores are normalized to form a distribution over all entities e i , and β i,j,t scores are normalized to form a distribution over records r i,j of entity e i . Each entity is then represented as a weighted sum of its record embeddings, and the entire data structure is represented as a weighted sum of the entity representations. The dynamic context is computed as:</p><formula xml:id="formula_3">c t = I i=1 (α i,t j β i,j,t r i,j ) (4) where α i,t ∝ exp(d t W α e i ) and β i,j,t ∝ exp(d t W β h i,j )<label>(5)</label></formula><p>where d t is the decoder hidden state at time step t, W α ∈ R d×d and W β ∈ R d×d are learnt parameters, i α i,t = 1, and for all i ∈ {1, ..., I} j β i,j,t = 1.</p><p>• Key-guided Hierarchical Attention. This variant follows the intuition that once an entity is chosen for mention (thanks to α i,t ), only the type of records is important to determine the content of the description. For example, when deciding to mention a player, all experts automatically report his score without consideration of its specific value. To test this intuition, we model the attention scores by computing the β i,j,t scores from equation <ref type="formula" target="#formula_3">(5)</ref> solely on the embedding of the key rather than on the full record representation h i,j :</p><formula xml:id="formula_4">β i,j,t ∝ exp(d t W a2 k i,j )<label>(6)</label></formula><p>Please note that the different embeddings and the model parameters presented in the model components are learnt using Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Rotowire dataset</head><p>To evaluate the effectiveness of our model, and demonstrate its flexibility at handling heavy data-structure made of several types of entities, we used the Ro-toWire dataset <ref type="bibr" target="#b41">[39]</ref>. It includes basketball games statistical tables paired with journalistic descriptions of the games, as can be seen in the example of <ref type="figure">Figure 1</ref>. The descriptions are professionally written and average 337 words with a vocabulary size of 11.3K. There are 39 different record keys, and the average number of records (resp. entities) in a single data-structure is 628 (resp. 28). Entities are of two types, either team or player, and player descriptions depend on their involvement in the game. We followed the data partitions introduced with the dataset and used a train/validation/test sets of respectively 3, 398/727/728 (data-structure, description) pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation metrics</head><p>We evaluate our model through two types of metrics. The BLEU score <ref type="bibr" target="#b25">[23]</ref> aims at measuring to what extent the generated descriptions are literally closed to the ground truth. The second category designed by <ref type="bibr" target="#b41">[39]</ref> is more qualitative.</p><p>BLEU Score. The BLEU score <ref type="bibr" target="#b25">[23]</ref> is commonly used as an evaluation metric in text generation tasks. It estimates the correspondence between a machine output and that of a human by computing the number of co-occurrences for ngrams (n ∈ 1, 2, 3, 4) between the generated candidate and the ground truth. We use the implementation code released by <ref type="bibr" target="#b29">[27]</ref>.</p><p>Information extraction-oriented metrics. These metrics estimate the ability of our model to integrate elements from the table in its descriptions. Particularly, they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ. To do so, we follow the protocol presented in <ref type="bibr" target="#b41">[39]</ref>. First, we apply an information extraction (IE) system trained on labeled relations from the gold descriptions of the RotoWire train dataset. Entity-value pairs are extracted from the descriptions. For example, in the sentence Isaiah Thomas led the team in scoring, totaling 23 points [...]., an IE tool will extract the pair (Isaiah Thomas, 23, PTS). Second, we compute three metrics on the extracted information:</p><p>• Relation Generation (RG) estimates how well the system is able to generate text containing factual (i.e., correct) records. We measure the precision and absolute number (denoted respectively RG-P% and RG-#) of unique relations r extracted fromŷ 1:T that also appear in s.</p><p>• Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. We measure the precision and recall (denoted respectively CS-P% and CS-R%) of unique relations r extracted fromŷ 1:T that are also extracted from y 1:T .</p><p>• Content Ordering (CO) analyzes how well the system orders the records discussed in the description. We measure the normalized Damerau-Levenshtein distance <ref type="bibr" target="#b2">[3]</ref> between the sequences of records extracted fromŷ 1:T that are also extracted from y 1:T .</p><p>CS primarily targets the "what to say" aspect of evaluation, CO targets the "how to say it" aspect, and RG targets both. Note that for CS, CO, RG-% and BLEU metrics, higher is better; which is not true for RG-#. The IE system used in the experiments is able to extract an average of 17 factual records from gold descriptions. In order to mimic a human expert, a generative system should approach this number and not overload generation with brute facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare our hierarchical model against three systems. For each of them, we report the results of the best performing models presented in each paper.</p><p>• Wiseman <ref type="bibr" target="#b41">[39]</ref> is a standard encoder-decoder system with copy mechanism.</p><p>• Li <ref type="bibr" target="#b18">[16]</ref> is a standard encoder-decoder with a delayed copy mechanism: text is first generated with placeholders, which are replaced by salient records extracted from the table by a pointer network.</p><p>• Puduppully-plan <ref type="bibr" target="#b30">[28]</ref> acts in two steps: a first standard encoder-decoder generates a plan, i.e. a list of salient records from the table; a second standard encoder-decoder generates text from this plan.</p><p>• Puduppully-updt <ref type="bibr" target="#b31">[29]</ref>. It consists in a standard encoder-decoder, with an added module aimed at updating record representations during the generation process. At each decoding step, a gated recurrent network computes which records should be updated and what should be their new representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model scenarios</head><p>We test the importance of the input structure by training different variants of the proposed architecture:</p><p>• Flat, where we feed the input sequentially to the encoder, losing all notion of hierarchy. As a consequence, the model uses standard attention. is closest to Wiseman, with the exception that we use a Transformer to encode the input sequence instead of an RNN.</p><p>• Hierarchical-kv is our full hierarchical model, with traditional hierarchical attention, i.e. where attention over records is computed on the full record encoding, as in equation <ref type="formula" target="#formula_3">(5)</ref>.</p><p>• Hierarchical-k is our full hierarchical model, with key-guided hierarchical attention, i.e. where attention over records is computed only on the record key representations, as in equation <ref type="formula" target="#formula_4">(6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation details</head><p>The decoder is the one used in <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b41">39]</ref> with the same hyper-parameters. For the encoder module, both the low-level and high-level encoders use a two-layers multi-head self-attention with two heads. To fit with the small number of record keys in our dataset <ref type="bibr" target="#b41">(39)</ref>, their embedding size is fixed to 20. The size of the record value embeddings and hidden layers of the Transformer encoders are both set to 300. We use dropout at rate 0.5. The models are trained with a batch size of 64. We follow the training procedure in <ref type="bibr" target="#b38">[36]</ref> and train the model for a fixed number of 25K updates, and average the weights of the last 5 checkpoints (at every 1K updates) to ensure more stability across runs. All models were trained with the Adam optimizer <ref type="bibr" target="#b14">[13]</ref>; the initial learning rate is 0.001, and is reduced by half every 10K steps. We used beam search with beam size of 5 during inference. All the models are implemented in OpenNMT-py <ref type="bibr" target="#b15">[14]</ref>. All code is available at https://github.com/KaijuML/data-to-text-hierarchical</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Our results on the RotoWire testset are summarized in <ref type="table">Table 1</ref>. For each proposed variant of our architecture, we report the mean score over ten runs, as well as the standard deviation in subscript. Results are compared to baselines <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b41">39]</ref> and variants of our models. We also report the result of the oracle (metrics on the gold descriptions). Please note that gold descriptions trivially obtain 100% on all metrics expect RG, as they are all based on comparison with themselves. RG scores are different, as the IE system is imperfect and fails to extract accurate entities 4% of the time. RG-# is an absolute count.</p><p>Ablation studies To evaluate the impact of our model components, we first compare scenarios Flat, Hierarchical-k, and Hierarchical-kv. As shown in <ref type="table">Table 1</ref>, we can see the lower results obtained by the Flat scenario compared to the other scenarios (e.g. BLEU 16.7 vs. 17.5 for resp. Flat and Hierarchical-k ), suggesting the effectiveness of encoding the data-structure using a hierarchy. This is expected, as losing explicit delimitation between entities makes it harder a) for the encoder to encode semantics of the objects contained in the table and b) for the attention mechanism to extract salient entities/records. Second, the comparison between scenario Hierarchical-kv and Hierarchical-k shows that omitting entirely the influence of the record values in the attention mechanism is more effective: this last variant performs slightly better in all metrics excepted CS-R%, reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in Section 3.3: once an entity is selected, facts about this entity are relevant based on their key, not value which might add noise. To illustrate this intuition, we depict in <ref type="figure" target="#fig_1">Figure 3</ref> attention scores (recall α i,t and β i,j,t from equations <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula" target="#formula_4">(6)</ref>) for both variants Hierarchical-kv and Hierarchical-k. We particularly focus on the timestamp where the models should mention the number of points scored during the first quarter of the game. Scores of Hierarchical-k are sharp, with all of the weight on the correct record (PTS QTR1, 26) whereas scores of Hierarchical-kv are more distributed over all PTS QTR records, ultimately failing to retrieve the correct one.</p><p>Comparison w.r.t. baselines. From a general point of view, we can see from <ref type="table">Table 1</ref> that our scenarios obtain significantly higher results in terms of BLEU The Atlanta Hawks ( 46 -12 ) defeated the Orlando Magic ( 19 -41 ) 95 -88 on Monday at Philips Arena in Atlanta. The Hawks got out to a quick start in this one, out -scoring the Magic 28 -16 in the first quarter alone. Along with the quick start, the Hawks were able to hold off the Magic late in the fourth quarter, out -scoring the Magic 19 -21. The Hawks were led by Nikola Vucevic, who went 10 -for -16 from the field and 0 -for -0 from the three-point line to score a team -high of 21 points, while also adding 15 rebounds in 37 minutes. It was his second double -double in a row, a stretch where he's averaging 22 points and 17 rebounds. Notching a doubledouble of his own, Al Horford recorded 17 points ( 7 -9 FG , 0 -0 3Pt , 3 -4 FT ), 13 rebounds and four steals. He's now averaging 15 points and 6 rebounds on the year. Paul Millsap had a strong showing , posting 20 points ( 8 -17 FG , 4 -7 3Pt , 0 -2 FT ), four rebounds and three blocked shots. He's been a pleasant surprise for the Magic in the second half, as he's averaged 14 points and 5 rebounds over his last three games. DeMarre Carroll was the other starter in double figures, finishing with 15 points ( 6 -12 FG , 3 -6 3Pt ), eight rebounds and three steals. He's had a nice stretch of three games , averaging 24 points, 3 rebounds and 2 assists over that span. Tobias Harris was the only other Magic player to reach double figures, scoring 15 points ( 5 -9 FG , 2 -4 3Pt , 3 -4 FT ). The Magic 's next game will be at home against the Miami Heat on Wednesday, while the Magic will travel to Charlotte to play the Hornets on Wednesday. over all models; our best model Hierarchical-k reaching 17.5 vs. 16.5 against the best baseline. This means that our models learns to generate fluent sequences of words, close to the gold descriptions, adequately picking up on domain lingo. Qualitative metrics are either better or on par with baselines. We show in <ref type="figure" target="#fig_2">Figure  4</ref> a text generated by our best model, which can be directly compared to the gold description in <ref type="figure">Figure 1</ref>. Generation is fluent and contains domain-specific expressions. As reflected in <ref type="table">Table 1</ref>, the number of correct mentions (in green) outweights the number of incorrect mentions (in red). Please note that, as in previous work <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b41">39]</ref>, generated texts still contain a number of incorrect facts, as well hallucinations (in blue): sentences that have no basis in the input data (e.g. "[...] he's now averaging 22 points [...]."). While not the direct focus of our work, this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts.</p><p>Specifically, regarding all baselines, we can outline the following statements.</p><p>• Our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture Wiseman, reinforcing the crucial role of structure in data semantics and saliency. The analysis of RG metrics shows that Wiseman seems to be the more naturalistic in terms of number of factual mentions (RG#) since it is the closest scenario to the gold value (16.83 vs. 17.31 for resp. Wiseman and Hierarchical-k ). However, Wiseman achieves only 75.62% of precision, effectively mentioning on average a total of 22.25 records (wrong or accurate), where our model Hierarchical-k scores a precision of 89.46%, leading to 23.66 total mentions, just slightly above Wiseman.</p><p>• The comparison between the Flat scenario and Wiseman is particularly interesting. Indeed, these two models share the same intuition to flatten the data-structure. The only difference stands on the encoder mechanism: bi-LSTM vs. Transformer, for Wiseman and Flat respectively. Results shows that our Flat scenario obtains a significant higher BLEU score (16.7 vs. 14.5) and generates fluent descriptions with accurate mentions (RG-P%) that are also included in the gold descriptions (CS-R%). This suggests that introducing the Transformer architecture is promising way to implicitly account for data structure.</p><p>• Our hierarchical models outperform the two-step decoders of Li and Puduppully-plan on both BLEU and all qualitative metrics, showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder (i.e., planning or templating). While our models sensibly outperform in precision at factual mentions, the baseline Puduppully-plan reaches 34.28 mentions on average, showing that incorporating modules dedicated to entity extraction leads to over-focusing on entities; contrasting with our models that learn to generate more balanced descriptions.</p><p>• The comparison with Puduppully-updt shows that dynamically updating the encoding across the generation process can lead to better Content Ordering (CO) and RG-P%. However, this does not help with Content Selection (CS) since our best model Hierarchical-k obtains slightly better scores. Indeed, Puduppullyupdt updates representations after each mention allowing to keep track of the mention history. This guides the ordering of mentions (CO metric), each step limiting more the number of candidate mentions (increasing RG-P%). In contrast, our model encodes saliency among records/entities more effectively (CS metric). We note that while our model encodes the data-structure once and for all, Puduppully-updt recomputes, via the updates, the encoding at each step and therefore significantly increases computation complexity. Combined with their RG-# score of 30.11, we argue that our model is simpler, and obtains fluent description with accurate mentions in a more human-like fashion.</p><p>We would also like to draw attention to the number of parameters used by those architectures. We note that our scenarios relies on a lower number of parameters (14 millions) compared to all baselines (ranging from 23 to 45 millions). This outlines the effectiveness in the design of our model relying on a structure encoding, in contrast to other approach that try to learn the structure of data/descriptions from a linearized encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>In this work we have proposed a hierarchical encoder for structured data, which 1) leverages the structure to form efficient representation of its input; 2) has strong synergy with the hierarchical attention of its associated decoder. This results in an effective and more light-weight model. Experimental evaluation on the RotoWire benchmark shows that our model outperforms competitive baselines in terms of BLEU score and is generally better on qualitative metrics. This way of representing structured databases may lead to automatic inference and enrichment, e.g., by comparing entities. This direction could be driven by very recent operation-guided networks <ref type="bibr" target="#b37">[35,</ref><ref type="bibr" target="#b23">21]</ref>. In addition, we note that our approach can still lead to erroneous facts or even hallucinations. An interesting perspective might be to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We would like to thank the H2020 project AI4EU (825619) which partially supports Laure Soulier and Patrick Gallinari.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Diagram of the proposed hierarchical encoder. Once the records are embedded, the low-level encoder works on each entity independently (A); then the high-level encoder encodes the collection of entities (B). In circles, we represent the hierarchical attention scores: the α scores at the entity level and the β scores at the record level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Right: Comparison of a generated sentence from Hierarchical-k and Hierarchicalkv. Left: Attention scores over entities (top) and over records inside the selected entity (bottom) for both variants, during the decoding of respectively 26 or 31 (circled in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Text generated by our best model. Entites are boldfaced, factual mentions are in green, erroneous mentions in red and hallucinations are in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.2 76.62 1 18.54 .6 31.67 .7 42.9 1 36.42 .4 14.64 .3 14M Hierarchical-kv 17 .3 89.04 1 21.46 .9 38.57 1.2 51.50 .9 44.19 .7 18.70 .7 14M Hierarchical-k 17.5 .3 89.46 1.4 21.17 1.4 39.47 1.4 51.64 1 44.7 .6 18.90 .7 14M</figDesc><table><row><cell></cell><cell>BLEU</cell><cell>RG</cell><cell></cell><cell>CS</cell><cell></cell><cell>CO</cell><cell>Nb</cell></row><row><cell></cell><cell>P%</cell><cell>#</cell><cell>P%</cell><cell>R%</cell><cell>F1</cell><cell>Params</cell></row><row><cell cols="3">Gold descriptions 100 96.11 17.31</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Wiseman</cell><cell cols="6">14.5 75.62 16.83 32.80 39.93 36.2 15.62 45M</cell></row><row><cell>Li</cell><cell cols="2">16.19 84.86 19.31</cell><cell cols="4">30.81 38.79 34.34 16.34</cell><cell>-</cell></row><row><cell>Pudupully-plan</cell><cell cols="2">16.5 87.47 34.28</cell><cell cols="2">34.18 51.22</cell><cell>41</cell><cell>18.58 35M</cell></row><row><cell cols="3">Puduppully-updt 16.2 92.69 30.11</cell><cell cols="4">38.64 48.51 43.01 20.17 23M</cell></row><row><cell cols="7">Flat 16.7 Table 1: Evaluation on the RotoWire testset using relation generation (RG) count (#)</cell></row><row><cell cols="7">and precision (P%), content selection (CS) precision (P%) and recall (R%), content</cell></row><row><cell cols="5">ordering (CO), and BLEU. -: number of parameters unavailable.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>This variant</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A surprisingly effective out-of-the-box char2char model on the E2E NLG challenge dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dymetman</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W17-5519/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="158" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cite arxiv:1409.0473Comment: Accepted at ICLR 2015 as oral presentation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An improved error model for noisy channel spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075218.1075255</idno>
		<ptr target="https://doi.org/10.3115/1075218.1075255" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
	<note>ACL &apos;00, Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to sportscast: A test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1390156.1390173</idno>
		<ptr target="http://doi.acm.org/10.1145/1390156.1390173" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
	<note>ICML &apos;08</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1204</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1204" />
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2250" to="2260" />
		</imprint>
	</monogr>
	<note>Neural text generation in stories using entity representations as context</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable column concept determination for web tables using large knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.14778/2536258.2536271</idno>
		<ptr target="http://dl.acm.org/citation.cfm?doid=2536258.2536271" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1606" to="1617" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR&apos;19</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR&apos;19<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1029" to="1032" />
		</imprint>
	</monogr>
	<note>Table2vec: Neural Word and Entity Embeddings for Table Population and Retrieval</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3331184.3331333</idno>
		<ptr target="http://dl.acm.org/citation.cfm?doid=3331184.3331333" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3241691.3241693" />
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="170" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Tabvec: Table vectors for classification of web tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghasemi-Gol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Szekely</surname></persName>
		</author>
		<idno>abs/1802.06290</idno>
		<ptr target="http://arxiv.org/abs/1802.06290" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Centering: A framework for modelling the coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weinstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-01" />
			<publisher>CIS</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Reports</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1014</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural multi-step reasoning for question answering on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grnarova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval -40th</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<idno type="DOI">10.1007/978-3-319-76941-7_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-76941-752" />
		<title level="m">Proceedings</title>
		<meeting>null<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-03-26" />
			<biblScope unit="page" from="611" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">the 3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cite arxiv:1412.6980Comment: Published as a conference paper at</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OpenNMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<idno type="DOI">10.18653/v1/P17-4012</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-4012" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural text generation from structured data with application to the biography domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1128</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1128" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Point precisely: Towards ensuring the precision of data in generated texts using delayed copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="1044" to="1055" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical encoder with auxiliary supervision for neural table-to-text generation: Learning better representation for tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016786</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.33016786" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6786" to="6793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Table-to-text Generation by Structure-aware Seq2seq Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1166</idno>
		<ptr target="https://www.aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Thompson</surname></persName>
		</author>
		<title level="m">Rhetorical Structure Theory: Toward a functional theory of text organization. Text -Interdisciplinary Journal for the Study of Discourse</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Operation-guided neural networks for high fidelity data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D18-1422/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11-04" />
			<biblScope unit="page" from="3879" to="3889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Oremus</surname></persName>
		</author>
		<ptr target="https://slate.com/technology/2014/03/quakebot-los-angeles-times-robot-journalist-writes-article-on-la-earthquake.html" />
		<title level="m">The First News Report on the L.A. Earthquake Was Written by a Robot</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>ACL &apos;02</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1142</idno>
		<ptr target="https://www.aclweb.org/anthology/P15-1142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Making Effective Use of Healthcare Data Using Data-to-Text Technology: Methodologies and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pauws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-01" />
			<biblScope unit="page" from="119" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interacting with financial data using natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Smiley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bretz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schilder</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2911451.2911457</idno>
		<ptr target="http://doi.acm.org/10.1145/2911451.2911457" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1121" to="1124" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;16</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6319</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-6319" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Data-to-text generation with entity modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1195/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-08-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2023" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Choosing words in computer-generated weather forecasts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Davy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2005.06.006</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2005.06.006" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="137" to="169" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Copy mechanism and tailored training for character-based data-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bonetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cancelliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<idno>abs/1904.11838</idno>
		<ptr target="http://arxiv.org/abs/1904.11838" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Finding related tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://i.stanford.edu/∼anishds/publications/sigmod12/modi255i-dassarma.pdf" />
		<imprint>
			<date type="published" when="2012" />
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1099" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Table Cell Search for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web -WWW &apos;16</title>
		<meeting>the 25th International Conference on World Wide Web -WWW &apos;16</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="771" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Neural arithmetic logic units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1808.00508</idno>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1808.html#abs-1808-00508" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3295222.3295349" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06391" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5866-pointer-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
		<ptr target="https://www.aclweb.org/anthology/D17-1239" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning neural templates for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1356</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1356" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="3174" to="3187" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Web Table Extraction, Retrieval and Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR&apos;19</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval -SIGIR&apos;19</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1409" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
