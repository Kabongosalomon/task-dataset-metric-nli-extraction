<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Path Aggregation Network for Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research YouTu Lab</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong ยง Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Path Aggregation Network for Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.</p><p>These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1 st place in the COCO 2017 Challenge Instance Segmentation task and the 2 nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github. com/ShuLiu1993/PANet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Instance segmentation is one of the most important and challenging tasks. It aims to predict class label and pixelwise instance mask to localize varying numbers of instances presented in images. This task widely benefits autonomous vehicles, robotics, video surveillance, to name a few.</p><p>With the help of deep convolutional neural networks, several frameworks for instance segmentation, e.g., <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b38">38]</ref>, were proposed where performance grows rapidly <ref type="bibr" target="#b12">[12]</ref>. Mask R-CNN <ref type="bibr" target="#b21">[21]</ref> is a simple and effective system for instance segmentation. Based on Fast/Faster R-CNN <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b51">51]</ref>, a fully convolutional network (FCN) is used for mask prediction, along with box regression and classification. To achieve high performance, feature pyramid network (FPN) <ref type="bibr" target="#b35">[35]</ref> is utilized to extract in-network feature hierarchy, where a top-down path with lateral connections is augmented to propagate semantically strong features.</p><p>Several newly released datasets <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b45">45</ref>] make large room for algorithm improvement. COCO <ref type="bibr" target="#b37">[37]</ref> consists of 200k images. Lots of instances with complex spatial layout are captured in each image. Differently, Cityscapes <ref type="bibr" target="#b7">[7]</ref> and MVD <ref type="bibr" target="#b45">[45]</ref> provide street scenes with a large number of traffic participants in each image. Blur, heavy occlusion and extremely small instances appear in these datasets.</p><p>There have been several principles proposed for designing networks in image classification that are also effective for object recognition. For example, shortening information path and easing information propagation by clean residual connection <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref> and dense connection <ref type="bibr" target="#b26">[26]</ref> are useful. Increasing the flexibility and diversity of information paths by creating parallel paths following the splittransform-merge strategy <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b6">6]</ref> is also beneficial.</p><p>Our Findings Our research indicates that information propagation in state-of-the-art Mask R-CNN can be further improved. Specifically, features in low levels are helpful for large instance identification. But there is a long path from low-level structure to topmost features, increasing difficulty to access accurate localization information. Further, each proposal is predicted based on feature grids pooled from one feature level, which is assigned heuristically. This process can be updated since information discarded in other levels may be helpful for final prediction. Finally, mask prediction is made on a single view, losing the chance to gather more diverse information.</p><p>Our Contributions Inspired by these principles and observations, we propose PANet, illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, for instance segmentation.</p><p>First, to shorten information path and enhance feature pyramid with accurate localization signals existing in low-levels, bottom-up path augmentation is created. In fact, features in low-layers were utilized in the systems of <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b14">14]</ref>. But propagating low-level features to enhance entire feature hierarchy for instance recognition was not explored. Second, to recover broken information path between each proposal and all feature levels, we develop adaptive feature pooling. It is a simple component to aggregate features from all feature levels for each proposal, avoiding arbitrarily assigned results. With this operation, cleaner paths are created compared with those of <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b62">62]</ref>.</p><p>Finally, to capture different views of each proposal, we augment mask prediction with tiny fully-connected (fc) layers, which possess complementary properties to FCN originally used by Mask R-CNN. By fusing predictions from these two views, information diversity increases and masks with better quality are produced.</p><p>The first two components are shared by both object detection and instance segmentation, leading to much enhanced performance of both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>With PANet, we achieve state-ofthe-art performance on several datasets. With ResNet-50 <ref type="bibr" target="#b23">[23]</ref> as the initial network, our PANet tested with a single scale already outperforms champion of COCO 2016 Challenge in both object detection <ref type="bibr" target="#b27">[27]</ref> and instance segmentation <ref type="bibr" target="#b33">[33]</ref> tasks. Note that these previous results are achieved by larger models <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b58">58]</ref> together with multi-scale and horizontal flip testing.</p><p>We achieve the 1 st place in COCO 2017 Challenge Instance Segmentation task and the 2 nd place in Object Detection task without large-batch training. We also benchmark our system on Cityscapes and MVD, which similarly yields top-ranking results, manifesting that our PANet is a very practical and top-performing framework. Our code and models are available at https://github.com/ ShuLiu1993/PANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Instance Segmentation There are mainly two streams of methods in instance segmentation. The most popular one is proposal-based. Methods in this stream have a strong connection to object detection. In R-CNN <ref type="bibr" target="#b17">[17]</ref>, object propos-als from <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b68">68]</ref> were fed into the network to extract features for classification. While Fast/Faster R-CNN <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b51">51]</ref> and SPPNet <ref type="bibr" target="#b22">[22]</ref> sped up the process by pooling features from global feature maps. Earlier work <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref> took mask proposals from MCG <ref type="bibr" target="#b1">[1]</ref> as input to extract features while CFM <ref type="bibr" target="#b9">[9]</ref>, MNC <ref type="bibr" target="#b10">[10]</ref> and Hayder et al. <ref type="bibr" target="#b20">[20]</ref> merged feature pooling to network for faster speed. Newer design was to generate instance masks in networks as proposal <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b8">8]</ref> or final result <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b41">41]</ref>. Mask R-CNN <ref type="bibr" target="#b21">[21]</ref> is an effective framework falling in this stream. Our work is built on Mask R-CNN and improves it from different aspects.</p><p>Methods in the other stream are mainly segmentationbased. They learned specially designed transformation <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b59">59]</ref> or instance boundaries <ref type="bibr" target="#b30">[30]</ref>. Then instance masks were decoded from predicted transformation. Instance segmentation by other pipelines also exists. DIN <ref type="bibr" target="#b2">[2]</ref> fused predictions from object detection and semantic segmentation systems. A graphical model was used in <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b65">65]</ref> to infer the order of instances. RNN was utilized in <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b50">50]</ref> to propose one instance in each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-level Features</head><p>Features from different layers were used in image recognition. SharpMask <ref type="bibr" target="#b49">[49]</ref>, Peng et al. <ref type="bibr" target="#b47">[47]</ref> and LRR <ref type="bibr" target="#b14">[14]</ref> fused feature maps for segmentation with finer details. FCN <ref type="bibr" target="#b44">[44]</ref>, U-Net <ref type="bibr" target="#b54">[54]</ref> and Noh et al. <ref type="bibr" target="#b46">[46]</ref> fused information from lower layers through skip-connections. Both TDM <ref type="bibr" target="#b56">[56]</ref> and FPN <ref type="bibr" target="#b35">[35]</ref> augmented a top-down path with lateral connections for object detection. Different from TDM, which took the fused feature map with the highest resolution to pool features, SSD <ref type="bibr" target="#b42">[42]</ref>, DSSD <ref type="bibr" target="#b13">[13]</ref>, MS-CNN <ref type="bibr" target="#b5">[5]</ref> and FPN <ref type="bibr" target="#b35">[35]</ref> assigned proposals to appropriate feature levels for inference. We take FPN as a baseline and much enhance it.</p><p>ION <ref type="bibr" target="#b4">[4]</ref>, Zagoruyko et al. <ref type="bibr" target="#b62">[62]</ref>, Hypernet <ref type="bibr" target="#b31">[31]</ref> and Hypercolumn <ref type="bibr" target="#b19">[19]</ref> concatenated feature grids from different layers for better prediction. But a sequence of operations, i.e., normalization, concatenation and dimension reduction are needed to get feasible new features. In comparison, our design is much simpler.</p><p>Fusing feature grids from different sources for each proposal was also utilized in <ref type="bibr" target="#b52">[52]</ref>. But this method extracted feature maps on input with different scales and then conducted feature fusion (with the max operation) to improve feature selection from the input image pyramid. In contrast, our method aims at utilizing information from all feature levels in the in-network feature hierarchy with single-scale input. End-to-end training is enabled.</p><p>Larger Context Region Methods of <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b62">62]</ref> pooled features for each proposal with a foveal structure to exploit context information from regions with different resolutions. Features pooled from a larger region provide surrounding context. Global pooling was used in PSPNet <ref type="bibr" target="#b67">[67]</ref> and ParseNet <ref type="bibr" target="#b43">[43]</ref> to greatly improve quality of semantic segmentation. Similar trend was observed by Peng et al. <ref type="bibr" target="#b47">[47]</ref> where global convolutionals were utilized. Our mask prediction branch also supports accessing global information. But the technique is completely different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Framework</head><p>Our framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Path augmentation and aggregation is conducted for improving performance. A bottom-up path is augmented to make low-layer information easier to propagate. We design adaptive feature pooling to allow each proposal to access information from all levels for prediction. A complementary path is added to the mask-prediction branch. This new structure leads to decent performance. Similar to FPN, the improvement is independent of the CNN structure, e.g., <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b23">23</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bottom-up Path Augmentation</head><p>Motivation The insightful point <ref type="bibr" target="#b63">[63]</ref> that neurons in high layers strongly respond to entire objects while other neurons are more likely to be activated by local texture and patterns manifests the necessity of augmenting a top-down path to propagate semantically strong features and enhance all features with reasonable classification capability in FPN.</p><p>Our framework further enhances the localization capability of the entire feature hierarchy by propagating strong responses of low-level patterns based on the fact that high response to edges or instance parts is a strong indicator to accurately localize instances. To this end, we build a path with clean lateral connections from the low level to top ones. Therefore, there is a "shortcut" (dashed green line in <ref type="figure" target="#fig_0">Figure 1</ref>), which consists of less than 10 layers, across these levels. In comparison, the CNN trunk in FPN gives a long path (dashed red line in <ref type="figure" target="#fig_0">Figure 1</ref>) passing through even 100+ layers from low layers to the topmost one.</p><p>Augmented Bottom-up Structure Our framework first accomplishes bottom-up path augmentation. We follow FPN to define that layers producing feature maps with the same spatial size are in the same network stage. Each feature level corresponds to one stage. We also take ResNet <ref type="bibr" target="#b23">[23]</ref> as the basic structure and use {P 2 , P 3 , P 4 , P 5 } to denote feature levels generated by FPN. Our augmented path starts from the lowest level P 2 and gradually approaches P 5 as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). From P 2 to P 5 , the spatial size is gradually down-sampled with factor 2. We use {N 2 , N 3 , N 4 , N 5 } to denote newly generated feature maps corresponding to {P 2 , P 3 , P 4 , P 5 }. Note that N 2 is simply P 2 , without any processing.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, each building block takes a higher resolution feature map N i and a coarser map P i+1 through lateral connection and generates the new feature map N i+1 . Each feature map N i first goes through a 3 ร 3 convolutional layer with stride 2 to reduce the spatial size. Then each element of feature map P i+1 and the down-sampled map are added through lateral connection. The fused feature map is then processed by another 3 ร 3 convolutional layer to generate N i+1 for following sub-networks. This is an iterative process and terminates after approaching P 5 . In these building blocks, we consistently use channel 256 of feature maps. All convolutional layers are followed by a ReLU <ref type="bibr" target="#b32">[32]</ref>. The feature grid for each proposal is then pooled from new feature maps, i.e., {N 2 , N 3 , N 4 , N 5 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive Feature Pooling</head><p>Motivation In FPN <ref type="bibr" target="#b35">[35]</ref>, proposals are assigned to different feature levels according to the size of proposals. It makes small proposals assigned to low feature levels and large proposals to higher ones. Albeit simple and effective, it still could generate non-optimal results. For example, two proposals with 10-pixel difference can be assigned to different levels. In fact, these two proposals are rather similar.</p><p>Further, importance of features may not be strongly correlated to the levels they belong to. High-level features are generated with large receptive fields and capture richer context information. Allowing small proposals to access these features better exploits useful context information for prediction. Similarly, low-level features are with many fine details and high localization accuracy. Making large proposals access them is obviously beneficial. With these thoughts, we propose pooling features from all levels for each pro- posal and fusing them for following prediction. We call this process adaptive feature pooling.</p><p>We now analyze the ratio of features pooled from different levels with adaptive feature pooling. We use max operation to fuse features from different levels, which lets network select element-wise useful information. We cluster proposals into four classes based on the levels they were assigned to originally in FPN. For each set of proposals, we calculate the ratio of features selected from different levels. In notation, levels 1 โ 4 represent low-to-high levels. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the blue line represents small proposals that were assigned to level 1 originally in FPN. Surprisingly, nearly 70% of features are from other higher levels. We also use the yellow line to represent large proposals that were assigned to level 4 in FPN. Again, 50%+ of the features are pooled from other lower levels. This observation clearly indicates that features in multiple levels together are helpful for accurate prediction. It is also a strong support of designing bottom-up path augmentation.</p><p>Adaptive Feature Pooling Structure Adaptive feature pooling is actually simple in implementation and is demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>(c). First, for each proposal, we map them to different feature levels, as denoted by dark grey regions in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Following Mask R-CNN <ref type="bibr" target="#b21">[21]</ref>, ROIAlign is used to pool feature grids from each level. Then a fusion operation (element-wise max or sum) is utilized to fuse feature grids from different levels.</p><p>In following sub-networks, pooled feature grids go through one parameter layer independently, which is followed by the fusion operation, to enable network to adapt features. For example, there are two fc layers in the box branch in FPN. We apply the fusion operation after the first layer. Since four consecutive convolutional layers are used in mask prediction branch in Mask R-CNN, we place fusion operation between the first and second convolutional layers. Ablation study is given in Section 4.2. The fused feature grid is used as the feature grid of each proposal for further prediction, i.e., classification, box regression and mask prediction. A detailed illustration of adaptive feature pooling on box branch is shown by <ref type="figure">Figure 6</ref> in Appendix.</p><p>Our design focuses on fusing information from innetwork feature hierarchy instead of those from different feature maps of input image pyramid <ref type="bibr" target="#b52">[52]</ref>. It is simpler compared with the process of <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b31">31]</ref>, where L-2 normalization, concatenation and dimension reduction are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fully-connected Fusion</head><p>Motivation Fully-connected layers, or MLP, were widely used in mask prediction in instance segmentation <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b34">34]</ref> and mask proposal generation <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49]</ref>. Results of <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b33">33]</ref> show that FCN is also competent in predicting pixelwise masks for instances. Recently, Mask R-CNN <ref type="bibr" target="#b21">[21]</ref> applied a tiny FCN on the pooled feature grid to predict corresponding masks avoiding competition between classes.</p><p>We note fc layers yield different properties compared with FCN where the latter gives prediction at each pixel based on a local receptive field and parameters are shared at different spatial locations. Contrarily, fc layers are location sensitive since predictions at different spatial locations are achieved by varying sets of parameters. So they have the ability to adapt to different spatial locations. Also prediction at each spatial location is made with global information of the entire proposal. It is helpful to differentiate instances <ref type="bibr" target="#b48">[48]</ref> and recognize separate parts belonging to the same object. Given properties of fc and convolutional layers different from each other, we fuse predictions from these two types of layers for better mask prediction.</p><p>Mask Prediction Structure Our component of mask prediction is light-weighted and easy to implement. The mask branch operates on pooled feature grid for each proposal. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the main path is a small FCN, which consists of 4 consecutive convolutional layers and 1 deconvolutional layer. Each convolutional layer consists of 256 3 ร 3 filters and the deconvolutional layer up-samples feature with factor 2. It predicts a binary pixel-wise mask for each class independently to decouple segmentation and classification, similar to that of Mask R-CNN. We further create a short path from layer conv3 to a fc layer. There are two 3 ร 3 convolutional layers where the second shrinks channels to half to reduce computational overhead.</p><p>A fc layer is used to predict a class-agnostic foreground/background mask. It not only is efficient, but also allows parameters in the fc layer trained with more samples, leading to better generality. The mask size we use is 28 ร 28 so that the fc layer produces a 784 ร 1 ร 1 vector. This vector is reshaped to the same spatial size as the mask predicted by FCN. To obtain the final mask prediction, mask of each class from FCN and foreground/background prediction from fc are added. Using only one fc layer, instead of multiple of them, for final prediction prevents the issue of collapsing the hidden spatial feature map into a short feature vector, which loses spatial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compare our method with state-of-the-arts on challenging COCO <ref type="bibr" target="#b37">[37]</ref>, Cityscapes <ref type="bibr" target="#b7">[7]</ref> and MVD <ref type="bibr" target="#b45">[45]</ref> datasets. Our results are top ranked in all of them. Comprehensive ablation study is conducted on the COCO dataset. We also present our results on COCO 2017 Instance Segmentation and Object Detection Challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We re-implement Mask R-CNN and FPN based on Caffe <ref type="bibr" target="#b29">[29]</ref>. All pre-trained models we use in experiments are publicly available. We adopt image centric training <ref type="bibr" target="#b16">[16]</ref>. For each image, we sample 512 region-of-interests (ROIs) with positive-to-negative ratio 1 : 3. Weight decay is 0.0001 and momentum is set to 0.9. Other hyper-parameters slightly vary according to datasets and we detail them in respective experiments. Following Mask R-CNN, proposals are from an independently trained RPN <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b51">51]</ref> for convenient ablation and fair comparison, i.e., the backbone is not shared with object detection/instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on COCO</head><p>Dataset and Metrics COCO <ref type="bibr" target="#b37">[37]</ref> dataset is among the most challenging ones for instance segmentation and object detection due to the data complexity. It consists of 115k images for training and 5k images for validation (new split of 2017). 20k images are used in test-dev and 20k images are used as test-challenge. Ground-truth labels of both testchallenge and test-dev are not publicly available. There are 80 classes with pixel-wise instance mask annotation. We train our models on train-2017 subset and report results on val-2017 subset for ablation study. We also report results on test-dev for comparison.</p><p>We follow the standard evaluation metrics, i.e., AP, AP 50 , AP 75 , AP S , AP M and AP L . The last three measure performance with respect to objects with different scales. Since our framework is general to both instance segmentation and object detection, we also train independent object detectors. We report mask AP, box ap AP bb of an independently trained object detector, and box ap AP bbM of the object detection branch trained in the multi-task fashion.</p><p>Hyper-parameters We take 16 images in one image batch for training. The shorter and longer edges of the images are 800 and 1000, if not specially noted. For instance segmentation, we train our model with learning rate 0.02 for 120k iterations and 0.002 for another 40k iterations. For object detection, we train one object detector without the mask prediction branch. Object detector is trained for 60k iterations with learning rate 0.02 and another 20k iterations with learning rate 0.002. These parameters are adopted from Mask R-CNN and FPN without any fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Segmentation Results</head><p>We report performance of our PANet on test-dev for comparison, with and without multi-scale training. As shown in <ref type="table">Table 1</ref>, our PANet with ResNet-50 trained on multi-scale images and tested on single-scale images already outperforms Mask R-CNN and Champion in 2016, where the latter used larger model ensembles and testing tricks <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b62">62]</ref>. Trained and tested with image scale 800, which is same as that of Mask R-CNN, our method outperforms the single-model state-of-the-art Mask R-CNN with nearly 3 points under the same initial models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection Results</head><p>Similar to the way adopted in Mask R-CNN, we also report bounding box results inferred from the box branch.  Component Ablation Studies First, we analyze importance of each proposed component. Besides bottom-up path augmentation, adaptive feature pooling and fully-connected fusion, we also analyze multi-scale training, multi-GPU synchronized batch normalization <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b28">28]</ref> and heavier head. For multi-scale training, we set longer edge to 1, 400 and the other to range from 400 to 1, 400. We calculate mean and variance based on all samples in one batch across all GPUs, do not fix any parameters during training, and make all new layers followed by a batch normalization layer, when using multi-GPU synchronized batch normalization. The heavier head uses 4 consecutive 3 ร 3 convolutional layers shared by box classification and box regression, instead of two fc layers. It is similar to the head used in <ref type="bibr" target="#b36">[36]</ref> but the convolutional layers for box classification and box regression branches are not shared in their case. Our ablation study from the baseline gradually to all components incorporated is conducted on val-2017 subset and is shown in <ref type="table">Table 3</ref>. ResNet-50 <ref type="bibr" target="#b23">[23]</ref> is our initial model. We report performance in terms of mask AP, box ap AP bb of an independently trained object detector and box ap AP bbM of box branch trained in the multi-task fashion.</p><p>1) Re-implemented Baseline. Our re-implemented Mask R-CNN performs comparable with the one described in original paper and our object detector performs better.</p><p>2) Multi-scale Training &amp; Multi-GPU Sync. BN. These two techniques help the network to converge better and increase the generalization ability.</p><p>3) Bottom-up Path Augmentation. With or without adap-tive feature pooling, bottom-up path augmentation consistently improves mask AP and box ap AP bb by more than 0.6 and 0.9 respectively. The improvement on instances with large scale is most significant. This verifies usefulness of information from lower feature levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Adaptive Feature</head><p>Pooling. With or without bottomup path augmentation, adaptive feature pooling consistently improves performance. The performance in all scales generally improves, in accordance with our observation that features in other layers are also useful in final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Fully-connected Fusion.</head><p>Fully-connected fusion aims at predicting masks with better quality. It yields 0.7 improvement in terms of mask AP. It is general for instances at all scales. 6) Heavier Head. Heavier head is quite effective for box ap AP bbM of bounding boxes trained with multi-task fashion. While for mask AP and independently trained object detector, the improvement is minor.</p><p>With all these components in PANet, improvement on mask AP is 4.4 over baselines. Box ap AP bb of independently trained object detector increases 4.2. They are significant. Small-and medium-size instances contribute most. Half improvement is from multi-scale training and multi-GPU sync. BN, which are effective strategies to help train better models.</p><p>Ablation Studies on Adaptive Feature Pooling We conduct ablation studies on adaptive feature pooling to find where to place the fusion operation and the most appropri-   <ref type="table">Table 5</ref>. Ablation study on fully-connected fusion on val-2017 in terms of mask AP.</p><p>ate fusion operation. We place it either between ROIAlign and fc1, represented by "fu.fc1fc2" or between fc1 and fc2, represented by "fc1fu.fc2" in <ref type="table" target="#tab_3">Table 4</ref>. Similar settings are also applied to mask prediction branch. For feature fusing, max and sum operations are tested. As shown in <ref type="table" target="#tab_3">Table 4</ref>, adaptive feature pooling is not sensitive to the fusion operation. Allowing a parameter layer to adapt feature grids from different levels, however, is of great importance. We use max as fusion operation and use it behind the first parameter layer in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies on Fully-connected Fusion</head><p>We investigate performance with different ways to instantiate the augmented fc branch. We consider two aspects, i.e., the layer to start the new branch and the way to fuse predictions from the new branch and FCN. We experiment with creating new paths from conv2, conv3 and conv4, respectively. "max", "sum" and "product" operations are used for fusion. We take our reimplemented Mask R-CNN with bottom-up path augmentation and adaptive feature pooling as the baseline. Corresponding results are shown in <ref type="table">Table 5</ref>. They clearly show that staring from conv3 and taking sum for fusion produces the best results.</p><p>COCO 2017 Challenge With PANet, we participated in the COCO 2017 Instance Segmentation and Object Detection Challenges. Our framework reaches the 1 st place in Instance Segmentation task and the 2 nd place in Object Detection task without large-batch training. As shown in <ref type="table" target="#tab_5">Tables  6 and 7</ref>, compared with last year champions, we achieve 9.1% absolute and 24% relative improvement on instance segmentation. While for object detection, 9.4% absolute and 23% relative improvement is yielded.</p><p>The top performance comes with a few more details in PANet. First, we use deformable convolutions where DCN <ref type="bibr" target="#b11">[11]</ref> is adopted. The common testing tricks <ref type="bibr">[23, 33, 10,</ref> AP AP 50 AP 75 AP S AP M AP L Champion 2015 <ref type="bibr" target="#b10">[10]</ref> 28.4 51. <ref type="bibr" target="#b6">6</ref> 28.   <ref type="figure" target="#fig_3">4d)</ref> is used as the base model to generate proposals. We train these models with different random seeds, with and without balanced sampling <ref type="bibr" target="#b55">[55]</ref> to enhance diversity between models. Detection results we submitted are acquired by tightening instance masks. We show a few visual results in <ref type="figure">Figure 5</ref> -most of our predictions are with high quality.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Cityscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Ablation Study</head><p>We compare with state-ofthe-arts on test subset in <ref type="table" target="#tab_7">Table 8</ref>. Trained on "fine-only" data, our method outperforms Mask R-CNN with "fineonly" data by 5.6 points. It is even comparable with Mask R-CNN pre-trained on COCO. By pre-training on COCO, we outperform Mask R-CNN with the same setting by 4.4 points. Visual results are shown in <ref type="figure">Figure 5</ref>.</p><p>Our ablation study to analyze the improvement on val subset is given in <ref type="table" target="#tab_8">Table 9</ref>. Based on our re-implemented baseline, we add multi-GPU synchronized batch normalization to help network converge better. It improves the accuracy by 1.5 points. With our full PANet, the performance is further boosted by 1.9 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on MVD</head><p>MVD <ref type="bibr" target="#b45">[45]</ref> is a relatively new and large-scale dataset for instance segmentation. It provides 25, 000 images on street scenes with fine instance-level annotations for 37 semantic classes. They are captured from several countries using different devices. The content and resolution vary greatly. We train our model on train subset with ResNet-50 as initial model and report performance on val and secret test subsets in terms of AP and AP 50 .</p><p>We present our results in <ref type="table" target="#tab_9">Table 10</ref>. Compared with UCenter [40] -winner on this dataset in LSUN 2017 instance segmentation challenge, our PANet with one ResNet-50 tested on single-scale images already performs comparably with the ensemble result with pre-training on COCO. With multi-scale and horizontal flip testing, which are also adopted by UCenter, our method performs even better. Qualitative results are illustrated in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented our PANet for instance segmentation. We designed several simple and yet effective components to enhance information propagation in representative pipelines. We pool features from all feature levels and shorten the distance among lower and topmost feature levels for reliable information passing. Complementary path is augmented to enrich feature for each proposal. Impressive results are produced. Our future work will be to extend our method to video and RGBD data.</p><p>B. Details on Implementing Multi-GPU Synchronized Batch Normalization.</p><p>We implement multi-GPU batch normalization on Caffe <ref type="bibr" target="#b29">[29]</ref> and OpenMPI. Given n GPUs and training samples in batch B, we first split training samples evenly into n subbatches, each is denoted as b i , assigned to one GPU. On each GPU, we calculate means ยต i based on samples in b i . AllReduce operation is then applied to gather all ยต i across all GPUs to get the mean ยต B of entire batch B. ยต B is broadcast to all GPUs. We then calculate temporary statistics on each GPU independently and apply the AllReduce operation to produce the variance ฯ 2 B of entire batch B. ฯ 2 B is also broadcast to all GPUs. As a result, each GPU has the statistics calculated on all training samples in B. We then perform normalization y m = ฮณ xmโยต B โ ฯ 2 B + + ฮฒ as in <ref type="bibr" target="#b28">[28]</ref> for each training sample. In backward operations, AllReduce operation is similarly applied to gather information from all GPUs for gradient calculation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of our framework. (a) FPN backbone. (b) Bottom-up path augmentation. (c) Adaptive feature pooling. (d) Box branch. (e) Fully-connected fusion. Note that we omit channel dimension of feature maps in (a) and (b) for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of our building block of bottom-up path augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Ratio of features pooled from different feature levels with adaptive feature pooling. Each line represents a set of proposals that should be assigned to the same feature level in FPN, i.e., proposals with similar scales. The horizontal axis denotes the source of pooled features. It shows that proposals with different sizes all exploit features from several different levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Mask prediction branch with fully-connected fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Table 3 .</head><label>3</label><figDesc>Performance in terms of mask AP, box ap AP bb of an independently trained object detector, and box ap AP bbM of the box branch trained with multi-task fashion on val-2017. Based on our re-implemented baseline (RBL), we gradually add multi-scale training (MST), multi-GPU synchronized batch normalization (MBN), bottom-up path augmentation (BPA), adaptive feature pooling (AFP), fullyconnected fusion (FF) and heavier head (HHD) for ablation studies. MRB is short for Mask R-CNN result reported in the original paper. The last line shows total improvement compared with baseline RBL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>PANet [ms-train] 36.6 / 38.2 58.0 / 60.2 39.3 / 41.4 16.3 / 19.1 38.1 / 41.1 53.1 / 52.6 ResNet-50 PANet / PANet [ms-train] 40.0 / 42.0 62.8 / 65.1 43.1 / 45.7 18.8 / 22.4 42.3 / 44.7 57.2 / 58.1 ResNeXt-101 Table 1. Comparison among PANet, winner of COCO 2016 instance segmentation challenge, and Mask R-CNN on COCO test-dev subset in terms of Mask AP, where the latter two are baselines.</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>Backbone</cell></row><row><cell>Champion 2016 [33]</cell><cell>37.6</cell><cell>59.9</cell><cell>40.4</cell><cell>17.1</cell><cell>41.0</cell><cell>56.0</cell><cell>6รResNet-101</cell></row><row><cell>Mask R-CNN [21]+FPN [35]</cell><cell>35.7</cell><cell>58.0</cell><cell>37.8</cell><cell>15.5</cell><cell>38.1</cell><cell>52.4</cell><cell>ResNet-101</cell></row><row><cell>Mask R-CNN [21]+FPN [35]</cell><cell>37.1</cell><cell>60.0</cell><cell>39.4</cell><cell>16.9</cell><cell>39.9</cell><cell>53.5</cell><cell>ResNeXt-101</cell></row><row><cell>PANet /</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>shows that our method with ResNet-50, trained and tested on single-scale images, outperforms, by a large margin, all other single-model ones even using much larger ResNeXt-101 [61] as initial model. With multi-scale training and single-scale testing, our PANet with ResNet-50 outperforms Champion in 2016, which used larger model ensemble and testing tricks. PANet [ms-train] 41.2 / 42.5 60.4 / 62.3 44.4 / 46.4 22.7 / 26.3 44.0 / 47.0 54.6 / 52.3 ResNet-50 PANet / PANet [ms-train] 45.0 / 47.4 65.0 / 67.2 48.6 / 51.8 25.4 / 30.1 48.6 / 51.7 59.1 / 60.0 ResNeXt-101 Table 2. Comparison among PANet, winner of COCO 2016 object detection challenge, RentinaNet and Mask R-CNN on COCO test-dev subset in terms of box AP, where the latter three are baselines. MRB RBL MST MBN BPA AFP FF HHD AP/AP bb /AP bbM AP 50 AP 75 AP S /AP bb S</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>AP bb</cell><cell></cell><cell cols="2">AP bb 50</cell><cell>AP bb 75</cell><cell>AP bb S</cell><cell>AP bb M</cell><cell>AP bb L</cell><cell>Backbone</cell></row><row><cell cols="3">Champion 2016 [27]</cell><cell></cell><cell>41.6</cell><cell></cell><cell></cell><cell>62.3</cell><cell>45.6</cell><cell>24.0</cell><cell>43.9</cell><cell>55.2</cell><cell>2รResNet-101 + 3รInception-ResNet-v2</cell></row><row><cell cols="3">RentinaNet [36]</cell><cell></cell><cell>39.1</cell><cell></cell><cell></cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell><cell>ResNet-101</cell></row><row><cell cols="4">Mask R-CNN [21]+FPN [35]</cell><cell>38.2</cell><cell></cell><cell></cell><cell>60.3</cell><cell>41.7</cell><cell>20.1</cell><cell>41.1</cell><cell>50.2</cell><cell>ResNet-101</cell></row><row><cell cols="4">Mask R-CNN [21]+FPN [35]</cell><cell>39.8</cell><cell></cell><cell></cell><cell>62.3</cell><cell>43.4</cell><cell>22.1</cell><cell>43.2</cell><cell>51.2</cell><cell>ResNeXt-101</cell></row><row><cell cols="12">PANet / /AP bbM S</cell><cell>AP M /AP bb M /AP bbM M</cell><cell>AP L /AP bb L /AP bbM L</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">33.6 / 33.9 / -</cell><cell>55.2 35.3</cell><cell cols="2">-/ 17.8 / -</cell><cell>-/ 37.7 / -</cell><cell>-/ 45.8 / -</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">33.4 / 35.0 / 36.4 54.3 35.5 14.1 / 18.7 / 20.0</cell><cell>35.7 / 38.9 / 39.7</cell><cell>50.8 / 47.0 / 48.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">35.3 / 35.0 / 38.2 56.7 37.9 17.6 / 20.8 / 24.3</cell><cell>38.6 / 39.9 / 42.3</cell><cell>50.6 / 44.1 / 48.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">35.7 / 37.1 / 38.9 57.3 38.0 18.6 / 24.2 / 25.3</cell><cell>39.4 / 42.5 / 43.6</cell><cell>51.7 / 47.1 / 49.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">36.4 / 38.0 / 39.9 57.8 39.2 19.3 / 23.3 / 26.2</cell><cell>39.7 / 42.9 / 44.3</cell><cell>52.6 / 49.4 / 51.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">36.3 / 37.9 / 39.6 58.0 38.9 19.0 / 25.4 / 26.4</cell><cell>40.1 / 43.1 / 44.9</cell><cell>52.4 / 48.6 / 50.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">36.9 / 39.0 / 40.6 58.5 39.7 19.6 / 25.7 / 27.0</cell><cell>40.7 / 44.2 / 45.7</cell><cell>53.2 / 49.5 / 52.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">37.6 / -/ -</cell><cell cols="3">59.1 40.6 20.3 / -/ -</cell><cell>41.3 / -/ -</cell><cell>53.8 / -/ -</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">37.8 / 39.2 / 42.1 59.4 41.0 19.2 / 25.8 / 27.0</cell><cell>41.5 / 44.3 / 47.3</cell><cell>54.3 / 50.6 / 54.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">+4.4 / +4.2 / +5.7 +5.1 +5.5 +5.1 / +7.1 / +7.0</cell><cell>+5.8 / +5.4 / +7.6</cell><cell>+3.5 / +3.6 / +5.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>SettingsAP AP 50 AP 75 AP bb AP bb</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>AP bb 75</cell></row><row><cell cols="2">baseline 35.7</cell><cell>57.3</cell><cell>38</cell><cell>37.1</cell><cell>58.9</cell><cell>40.0</cell></row><row><cell cols="2">fu.fc1fc2 35.7</cell><cell>57.2</cell><cell>38.2</cell><cell>37.3</cell><cell>59.1</cell><cell>40.1</cell></row><row><cell cols="2">fc1fu.fc2 36.3</cell><cell>58.0</cell><cell>38.9</cell><cell>37.9</cell><cell>60.0</cell><cell>40.7</cell></row><row><cell>MAX</cell><cell>36.3</cell><cell>58.0</cell><cell>38.9</cell><cell>37.9</cell><cell>60.0</cell><cell>40.7</cell></row><row><cell>SUM</cell><cell>36.2</cell><cell>58.0</cell><cell>38.8</cell><cell>38.0</cell><cell>59.8</cell><cell>40.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on adaptive feature pooling on val-2017 in terms of mask AP and box ap AP bb of the independently trained object detector.SettingsAP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell cols="2">baseline 36.9</cell><cell>58.5</cell><cell>39.7</cell><cell>19.6</cell><cell>40.7</cell><cell>53.2</cell></row><row><cell>conv2</cell><cell>37.5</cell><cell>59.3</cell><cell>40.1</cell><cell>20.7</cell><cell>41.2</cell><cell>54.1</cell></row><row><cell>conv3</cell><cell>37.6</cell><cell>59.1</cell><cell>40.6</cell><cell>20.3</cell><cell>41.3</cell><cell>53.8</cell></row><row><cell>conv4</cell><cell>37.2</cell><cell>58.9</cell><cell>40.0</cell><cell>19.0</cell><cell>41.2</cell><cell>52.8</cell></row><row><cell>PROD</cell><cell>36.9</cell><cell>58.6</cell><cell>39.7</cell><cell>20.2</cell><cell>40.8</cell><cell>52.2</cell></row><row><cell>SUM</cell><cell>37.6</cell><cell>59.1</cell><cell>40.6</cell><cell>20.3</cell><cell>41.3</cell><cell>53.8</cell></row><row><cell>MAX</cell><cell>37.1</cell><cell>58.7</cell><cell>39.9</cell><cell>19.9</cell><cell>41.1</cell><cell>52.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 6. Mask AP of COCO Instance Segmentation Challenge in different years on test-dev.</figDesc><table><row><cell></cell><cell></cell><cell>1</cell><cell>9.4</cell><cell>30.6 45.6</cell></row><row><cell cols="2">Champion 2016 [33] 37.6 59.9</cell><cell cols="2">40.4 17.1 41.0 56.0</cell></row><row><cell>Our Team 2017</cell><cell>46.7 69.5</cell><cell cols="2">51.3 26.0 49.1 64.0</cell></row><row><cell>PANet baseline</cell><cell>38.2 60.2</cell><cell cols="2">41.4 19.1 41.1 52.6</cell></row><row><cell>+DCN [11]</cell><cell>39.5 62.0</cell><cell cols="2">42.8 19.8 42.2 54.7</cell></row><row><cell>+testing tricks</cell><cell>42.0 63.5</cell><cell cols="2">46.0 21.8 44.4 58.1</cell></row><row><cell>+larger model</cell><cell>44.4 67.0</cell><cell cols="2">48.5 23.6 46.5 62.2</cell></row><row><cell>+ensemble</cell><cell>46.7 69.5</cell><cell cols="2">51.3 26.0 49.1 64.0</cell></row><row><cell></cell><cell cols="3">AP bb AP bb 50 AP bb 75 AP bb S</cell><cell>AP bb M</cell><cell>AP bb L</cell></row><row><cell cols="2">Champion 2015 [23] 37.4 59.0</cell><cell cols="2">40.2 18.3 41.7 52.9</cell></row><row><cell cols="2">Champion 2016 [27] 41.6 62.3</cell><cell cols="2">45.6 24.0 43.9 55.2</cell></row><row><cell>Our Team 2017</cell><cell>51.0 70.5</cell><cell cols="2">55.8 32.6 53.9 64.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Box AP of COCO Object Detection Challenge in different years on test-dev. 15, 39, 62], such as multi-scale testing, horizontal flip testing, mask voting and box voting, are adopted. For multiscale testing, we set the longer edge to 1, 400 and the other ranges from 600 to 1, 200 with step 200. Only 4 scales are used. Second, we use larger initial models from publicly available ones. We use 3 ResNeXt-101 (64 ร 4d) [61], 2 SE-ResNeXt-101 (32 ร 4d) [25], 1 ResNet-269 [64] and 1 SENet [25] as ensemble for bounding box and mask generation. Performance with different larger initial models are similar. One ResNeXt-101 (64 ร</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Dataset and Metrics Cityscapes<ref type="bibr" target="#b7">[7]</ref> contains street scenes captured by car-mounted cameras. There are 2, 975 training images, 500 validation images and 1, 525 testing images with fine annotations. Another 20k images are with coarse annotations, excluded for training. We report our results on val and secret test subsets. 8 semantic classes are annotated with instance masks. Each image is with size 1024 ร 2048. We evaluate results based on AP and AP 50 .Hyper-parameters We use the same set of hyperparameters as in Mask R-CNN<ref type="bibr" target="#b21">[21]</ref> for fair comparison. Specifically, we use images with shorter edge randomly sampled from {800, 1024} for training and use images with shorter edge 1024 for inference. No testing tricks or DCN is used. We train our model for 18k iterations with learning rate 0.01 and another 6k iterations with learning rate 0.001. 8 images (1 image per GPU) are in one image batch. ResNet-50 is taken as the initial model on this dataset.Figure 5. Images in each row are visual results of our model on COCO test-dev, Cityscapes test and MVD test, respectively.</figDesc><table><row><cell>Methods</cell><cell cols="5">AP [val] AP AP 50 person rider</cell><cell>car</cell><cell cols="2">truck bus train motorcycle bicycle</cell></row><row><cell>SGN [38]</cell><cell>29.2</cell><cell>25.0</cell><cell>44.9</cell><cell>21.8</cell><cell cols="3">20.1 39.4 24.8 33.2 30.8</cell><cell>17.7</cell><cell>12.4</cell></row><row><cell>Mask R-CNN [fine-only] [21]</cell><cell>31.5</cell><cell>26.2</cell><cell>49.9</cell><cell>30.5</cell><cell cols="3">23.7 46.9 22.8 32.2 18.6</cell><cell>19.1</cell><cell>16.0</cell></row><row><cell>SegNet</cell><cell>-</cell><cell>29.5</cell><cell>55.6</cell><cell>29.9</cell><cell cols="3">23.4 43.4 29.8 41.0 33.3</cell><cell>18.7</cell><cell>16.7</cell></row><row><cell>Mask R-CNN [COCO] [21]</cell><cell>36.4</cell><cell>32.0</cell><cell>58.1</cell><cell>34.8</cell><cell cols="3">27.0 49.1 30.1 40.9 30.9</cell><cell>24.1</cell><cell>18.7</cell></row><row><cell>PANet [fine-only]</cell><cell>36.5</cell><cell>31.8</cell><cell>57.1</cell><cell>36.8</cell><cell cols="3">30.4 54.8 27.0 36.3 25.5</cell><cell>22.6</cell><cell>20.8</cell></row><row><cell>PANet [COCO]</cell><cell>41.4</cell><cell>36.4</cell><cell>63.1</cell><cell>41.5</cell><cell cols="3">33.6 58.2 31.8 45.3 28.7</cell><cell>28.2</cell><cell>24.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Results on Cityscapes val subset, denoted as AP[val], and on Cityscapes test subset, denoted as AP.</figDesc><table><row><cell>Methods</cell><cell cols="2">AP AP 50</cell></row><row><cell>our re-implement</cell><cell>33.1</cell><cell>59.1</cell></row><row><cell cols="2">our re-implement + MBN 34.6</cell><cell>62.4</cell></row><row><cell>PANet</cell><cell>36.5</cell><cell>62.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Ablation study results on Cityscapes val subset. Only fine annotations are used for training. MBN is short for multi-GPU synchronized batch normalization.</figDesc><table><row><cell>Methods</cell><cell cols="4">AP [test] AP 50 [test] AP [val] AP 50 [val]</cell></row><row><cell>UCenter-Single [40]</cell><cell>-</cell><cell>-</cell><cell>22.8</cell><cell>42.5</cell></row><row><cell cols="2">UCenter-Ensemble [40] 24.8</cell><cell>44.2</cell><cell>23.7</cell><cell>43.5</cell></row><row><cell>PANet</cell><cell>-</cell><cell>-</cell><cell>23.6</cell><cell>43.3</cell></row><row><cell>PANet [test tricks]</cell><cell>26.3</cell><cell>45.8</cell><cell>24.9</cell><cell>44.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Results on MVD val subset and test subset.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Yuanhao Zhu, Congliang Xu and Qingping Fu in SenseTime for technical support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details and Strategy of Generating</head><p>Anchors on Cityscapes and MVD.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">While on MVD [45], we adopt training hyper-parameters from the winning entry [40the input image to 2400 pixels and the other ranges from 600 to 2000 pixels for multi-scale training. Scales {1600, 1800, 2000} are adopted for multi-scale testing. The RPN anchors span 7 scales</title>
		<imprint/>
	</monogr>
	<note>RPN anchors span 5 scales and 3 aspect ratios following. i.e., {8 2 , 16 2 , 32 2 , 64 2 , 128 2 , 256 2 , 512 2 }, and 5 aspect ratios, i.e., {0.2, 0.5, 1, 2, 5}. RPN is trained with the same scales as those of object detection/instance segmentation network training</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelรกez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01629</idno>
		<title level="m">Dual path networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (VOC) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">DSSD : Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Laplacian reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelรกez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>ECCV. 2014. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelรกez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boundary-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollรกr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Instancecut: From edges to instances with multicut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reversible recursive instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollรกr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollรกr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SGN: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Box aggregation for proposal decimation: Last mile of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LSUN&apos;17: insatnce segmentation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="https://research.mapillary.com/img/lsun/lsun17_scene_parsing_winners.pptx" />
	</analytic>
	<monogr>
		<title level="m">UCenter winner team</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-scale patch aggregation (MPA) for simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large kernel matters -improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollรกr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollรกr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">End-to-end instance segmentation with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Object detection networks on convolutional feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollรกr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollรกr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>ECCV. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02579</idno>
		<title level="m">Crafting GBD-Net for object detection</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollรกr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
