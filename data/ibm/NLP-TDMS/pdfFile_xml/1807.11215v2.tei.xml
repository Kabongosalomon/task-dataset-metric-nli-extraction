<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAKE: Compact and Accurate K-dimensional representation of Emotion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vielzeuf</forename><surname>Kervadec</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pateux</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lechervy</surname></persName>
						</author>
						<title level="a" type="main">CAKE: Compact and Accurate K-dimensional representation of Emotion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 * Both authors contributed equally.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerous models describing the human emotional states have been built by the psychology community. Alongside, Deep Neural Networks (DNN) are reaching excellent performances and are becoming interesting features extraction tools in many computer vision tasks. Inspired by works from the psychology community, we first study the link between the compact two-dimensional representation of the emotion known as arousalvalence, and discrete emotion classes (e.g. anger, happiness, sadness, etc.) used in the computer vision community. It enables to assess the benefits -in terms of discrete emotion inference -of adding an extra dimension to arousal-valence (usually named dominance). Building on these observations, we propose CAKE, a 3-dimensional representation of emotion learned in a multi-domain fashion, achieving accurate emotion recognition on several public datasets. Moreover, we visualize how emotions boundaries are organized inside DNN representations and show that DNNs are implicitly learning arousal-valence-like descriptions of emotions. Finally, we use the CAKE representation to compare the quality of the annotations of different public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facial expression is one of the most used human means of communication after language. Thus, the automated recognition of facial expressions -such as emotions -has a key role in affective computing, and its development could benefit human-machine interactions.</p><p>Different models are used to represent human emotion states. Ekman et al. <ref type="bibr" target="#b5">[6]</ref> propose to classify the human facial expression resulting from an emotion into six classes (resp. happiness, sadness, anger, disgust, surprise and fear) supposed to be independent across the cultures. This model has the benefit of simplicity but could be not sufficient to address the c 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.  <ref type="figure">Figure 1</ref>: Comparison of the discrete and continuous (arousal-valence) representations using AffectNet's annotations <ref type="bibr" target="#b16">[17]</ref>. Representation Size AffectNet Validation Accuracy (%) <ref type="figure">Figure 2</ref>: Influence of adding supplementary dimensions to arousal-valence when predicting emotion on AffectNet <ref type="bibr" target="#b16">[17]</ref>.</p><p>whole complexity of human affect. Moreover it suffers from serious intra-class variations as, for instance, soft smile and laughing equally belong to happiness. That is why Ekman's emotion classes are sometimes assembled into compound emotions <ref type="bibr" target="#b4">[5]</ref> (e.g. happily surprised). Others have chosen to represent emotion with an n-dimensional continuous space, as opposite to the Ekman's discrete classes. Russel has built the Circumplex Model of Affect <ref type="bibr" target="#b19">[20]</ref> in which emotion states are described by two values: arousal and valence. Arousal represents the excitation rate -the higher the arousal is, the more intense the emotion isand valence defines whether the emotion has a positive or a negative impact on the subject. Russels suggests in <ref type="bibr" target="#b19">[20]</ref> that all Ekman's emotions <ref type="bibr" target="#b5">[6]</ref> and compound emotions could be mapped in the circumplex model of affect. Furthermore, this two-dimensional approach allows a more accurate specification of the emotional state, especially by taking its intensity into account.</p><p>A third dimension has been added by Mehrabian et al. <ref type="bibr" target="#b15">[16]</ref> -the dominance -which depends on the degree of control exerted by a stimulus. Last, Ekman and Friesen <ref type="bibr" target="#b6">[7]</ref> have come up with the Facial Action Code System (FACS) using anatomically based action units. Developed for measuring facial movements, FACS is well suited for classifying facial expressions resulting from an affect.</p><p>Based on these emotion representations, several large databases of face images have been collected and annotated according to emotion. EmotioNet <ref type="bibr" target="#b7">[8]</ref> gathers faces annotated with Action Units <ref type="bibr" target="#b6">[7]</ref>; SFEW <ref type="bibr" target="#b3">[4]</ref>, FER-13 <ref type="bibr" target="#b8">[9]</ref> and RAF <ref type="bibr" target="#b14">[15]</ref> propose images in the wild annotated in basic emotions; AffecNet <ref type="bibr" target="#b16">[17]</ref> is a database annotated in both discrete emotion <ref type="bibr" target="#b5">[6]</ref> and arousal-valence <ref type="bibr" target="#b19">[20]</ref>.</p><p>The emergence of these large databases has allowed to develop automatic emotion recognition systems, such as the recent approaches based on Deep Neural Networks (DNN). Af-fectNet's authors <ref type="bibr" target="#b16">[17]</ref> use three AlexNet <ref type="bibr" target="#b12">[13]</ref> to learn respectively emotion classes, arousal and valence. In <ref type="bibr" target="#b17">[18]</ref>, the authors make use of transfer learning to counteract the smallness of the SFEW <ref type="bibr" target="#b3">[4]</ref> dataset, by pre-training their model on ImageNet <ref type="bibr" target="#b1">[2]</ref> and FER <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b0">[1]</ref> authors implement Covariance Pooling using second order statistics when training on emotion recognition (on RAF <ref type="bibr" target="#b14">[15]</ref> and SFEW <ref type="bibr" target="#b3">[4]</ref>).</p><p>Emotion labels, FACS and continuous representations have their own benefits -simplic-ity of the emotion classes, accuracy of the arousal-valence, objectivity of the FACS, etc.but also their own drawbacks -imprecision, complexity, ambiguity, etc. Therefore several authors have tried to leverage the benefits of all these representations. Khorrami et al. <ref type="bibr" target="#b10">[11]</ref> first showed that neural networks trained for expression recognition implicitly learn facial action units. Contributing to highlighting the close relation between emotion and Action Units, Pons et al. <ref type="bibr" target="#b18">[19]</ref> learned a multitask and multi-domain ResNet <ref type="bibr" target="#b9">[10]</ref> on both discrete emotion classes (SFEW <ref type="bibr" target="#b3">[4]</ref>) and Action Units (EmotioNet <ref type="bibr" target="#b7">[8]</ref>). Finally, Li et al. <ref type="bibr" target="#b14">[15]</ref> proposed a "Deep Locality-Preserving Learning" to handle the variability inside an emotion class, by making classes as compact as possible.</p><p>In this context, this paper focuses on the links between arousal-valence and discrete emotion representations for image-based emotion recognition. More specifically, the paper proposes a methodology for learning very compact embedding, with not more than 3 dimensions, performing very well on emotion classification task, making the visualization of emotions easy, and bearing similarity with the arousal-valence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Very Compact Emotion Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Some Intuitions About Emotion Representations</head><p>We first want to experimentally measure the dependence between emotion and arousalvalence as yielded in <ref type="bibr" target="#b19">[20]</ref>. We thus display each sample of the AffectNet <ref type="bibr" target="#b16">[17]</ref> validation subset in the arousal-valence space and color them according to their emotion label <ref type="figure">(Figure 1)</ref>. For instance, a face image labelled as neutral with an arousal and a valence of zero is located at the center of <ref type="figure">Figure 1</ref> and colored in blue. It clearly appears that a strong dependence exists between discrete emotion classes and arousal-valence. Obviously, it is due in part to the annotations of the AffectNet <ref type="bibr" target="#b16">[17]</ref> dataset, as the arousal-valence have been constrained to lie in a predefined confidence area based on the emotion annotation. Nevertheless, this dependence agrees with the Circumplex Model of Affect <ref type="bibr" target="#b19">[20]</ref>.</p><p>To evaluate further how arousal-valence representation is linked to emotion labels, we train a classifier made of one fully connected layer 1 (fc-layer) to infer emotion classes from arousal-valence values provided by AffectNet <ref type="bibr" target="#b16">[17]</ref> dataset. We obtain the accuracy of 83%, confirming that arousal-valence can be an excellent 2-d compact emotion representation.</p><p>This raises the question of the optimality of this 2-d representation. Would adding a third dimension to arousal-valence make the classification performance better? To address this question, we used the 512-d hidden representation of a ResNet-18 <ref type="bibr" target="#b9">[10]</ref> trained to predict discrete emotions on the AffectNet dataset <ref type="bibr" target="#b16">[17]</ref>. This representation is then projected into a more compact space using a fc-layer outputting k dimensions, which are concatenated with the arousal-valence values. On top of this representation, we add another fc-layer predicting emotion classes. The two fc-layers are finally trained using Adam optimizer <ref type="bibr" target="#b11">[12]</ref>. Adding 1 dimension to arousal-valence gives a gain of +3 points on the accuracy. It agrees with the assumption that a three-dimensional representation is more meaningful than a two-dimensional one <ref type="bibr" target="#b15">[16]</ref>. The benefit of adding more than 1 dimension is exponentially decreasing; with +512 dimensions, the gain is only of +0.6 points compared to adding 1 dimension, as shown in <ref type="figure">Figure 2</ref>.</p><p>From these observations, the use of a compact representation seems to be consistent with discrete emotion classes, as it enables an accuracy of 83% and 86% -respectively for a 2-d  <ref type="figure">Figure 3</ref>: Our approach's overview. Left: we use a ResNet-18 previously trained for discrete emotion recognition or arousal valence regression to extract 512-d hidden representations from face images. Center: using these hidden representations, CAKE or AVk representations (center) are learned to predict discrete emotions. Right: the learning process is multi-domain, predicting emotions on three different datasets with three different classifiers. Gray blocks are non-trainable weights while blue blocks are optimized weights. and a 3-d representation -and it even may allow to describe affect states with more contrast and accuracy. Even if arousal-valence is a good representation for emotion recognition, the question of its optimality has not been answered by these preliminary experiments. In other words, is it possible to learn 2-d (or 3-d) embedding better than those built on arousalvalence? We positively answer this question in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Compact and Accurate Representations of Emotions</head><p>Based on the previous observations, this section proposes a methodology for learning a compact embedding for emotion recognition from images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features extraction</head><p>The basic input of our model is an image containing one face displaying a given emotion. We first extract 512-d features specialized in emotion recognition. So as to, we detect the face, align its landmarks by applying an affine transform and crop the face region. The so-obtained face is then resized into 224 × 224 and fed to a ResNet-18 <ref type="bibr" target="#b9">[10]</ref> network <ref type="figure">(Figure 3</ref>, Features extraction). The face image is augmented (e.g. jittering, rotation), mostly to take the face detector noise into account. We also use cutout <ref type="bibr" target="#b2">[3]</ref> -consisting in randomly cutting a 45 × 45 pixels sized patch from the image -to regularize and improve the robustness of our model to facial occlusions. Our ResNet outputs 512-d features, on top of which a fc-layer can be added. At training time, we also use dropout <ref type="bibr" target="#b20">[21]</ref> regularization. The neural network can be learned from scratch on two given tasks: discrete emotion classification or arousal-valence regression.</p><p>Compact emotion encoding Compact embedding is obtained by projecting the 512-d features provided by the ResNet-18 (pretrained on discrete emotion recognition) into smaller k-dimensional spaces <ref type="figure">(Figure 3</ref>, Emotion Encoding) in which the final classification is done. The k features may be seen as a compact representation of the emotion, and the performance of the classifier can be measured for different values of k. CAKE-2, CAKE-3, etc., denote such classifiers with k = 2, k = 3, etc.</p><p>In the same fashion we can train the ResNet-18 using arousal-valence regression. In this case, the so-obtained arousal-valence regressor can be used to infer arousal-valence values from novel images and concatenate them to the k features of the embedding. Thus we reproduce here the exact experiment done in Section 2.1 in order to assess the benefit of a third (or more) dimension. The difference is that arousal-valence are not ground truth values but predicted ones. These methods are denoted as AV1, AV2, AV3, etc. for the different values of k.</p><p>Domain independent embedding As we want to ensure a generic compact enough representation, independent of the datasets, we learn the previously described model jointly on several datasets, without any further fine-tuning.</p><p>Our corpus is composed of AffectNet <ref type="bibr" target="#b16">[17]</ref>, RAF <ref type="bibr" target="#b14">[15]</ref> and SFEW <ref type="bibr" target="#b3">[4]</ref>, labelled with seven discrete emotion classes: neutral, happiness, sad, surprise, fear, disgust and anger. Our training subset is composed of those of AffectNet (283901 elts., 95.9% of total), RAF (11271 elts., 3.81% of total) and SFEW (871 elts., 0.29% of total). Our testing subset is composed of the subsets commonly used for evaluation in the literature (validation of SFEW and Af-fecNet, test of RAF).</p><p>To ease the multi-domain training, we first pre-train our features extractor model on AffectNet and freeze its weights. Then we apply the same architectures as described before, but duplicate the last fc-layer in charge of emotion classification in three dataset-specific layers <ref type="figure">(Figure 3</ref>, multi-domain learning). The whole model loss is a modified softmax cross entropy defined as follows:</p><formula xml:id="formula_0">Loss = 1 N N ∑ i=1 3 ∑ j=1 w i, j class w j dataset E(y i ,ŷ i, j )<label>(1)</label></formula><p>where j is ranging in [AffectNet, RAF, SFEW], y i is the label of i th element,ŷ i, j is the prediction of the j th classifier on the i th element, E is the softmax cross entropy loss, N is the number of elements in the batch, w i class is a weight given to the i th element of the batch depending on its emotion class and w j dataset is a weight given to the j th classifier prediction. Each sample of the multi-domain dataset is identified according to its original database, allowing to choose the correct classifier's output when computing the softmax cross entropy. where N j total is the number of elements in the j th dataset, N i, j class is the number of elements in the class of the i th element of the j th dataset and nbclass is the number of classes (7 in our case). The goal here is to fix the important class imbalance in the dataset by forcing to fit the uniform distribution, as previously done by <ref type="bibr" target="#b16">[17]</ref>.</p><p>The w dataset weight permits to take the imbalance between dataset's sizes into account.</p><formula xml:id="formula_1">w j dataset = 1 log N j total sample ∈ j th dataset 0 sample / ∈ j th dataset<label>(2)</label></formula><p>We thus define a global loss enabling to optimize the last two layers of our model (namely Emotion Encoding and Multi-domain Learning in <ref type="figure">Figure 3</ref>) on the three datasets at the same  time. The dimension k (or k + 2 in the case of the arousal-valence approach) can easily be changed and help to evaluate the interest of supplementary dimensions for emotion representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Metrics</head><p>We measure the classification performance with the accuracy and the macro F1 Score (3). Accuracy measures the number of correctly classified samples. Instead of accuracy, we prefer macro F1 score which gives the same importance to each class:</p><formula xml:id="formula_2">F 1macro = 1 N c N c ∑ i F 1i F 1i = 2 prec i · rec i prec i + rec i prec i = t p i t p i + f p i rec i = t p i t p i + f n i<label>(3)</label></formula><p>where i is the class index; prec i , rec i and F 1i are the precision, the recall and the F1-score of class i; N c is the number of classes; t p, f p and f n are the true positives, false positives and false negatives rates. All scores are averaged on 10 runs, with different initializations, and given with associated standard deviations, on our multi-domain testing subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compactness of the Representation</head><p>We first evaluate the quality of the representations in a multi-domain setting. <ref type="table" target="#tab_1">Table 1</ref> reports the F1-score of CAKE-2, AV, CAKE-3 and AV1 trained on three datasets with three different classifiers, each one being specialized on a dataset as explained in Section 2. Among the 2-d models (AV and CAKE-2), AV is better, taking benefits from the knowledge transferred from the AffectNet dataset. This is not true anymore for the 3D models, where CAKE-3 is better than AV1, probably because of its greater number of trainable parameters.</p><p>To validate the hypothesis of the important gain brought by adding a third dimension, we run the "CAKE" and "AVk" experiments with different representation sizes. To simplify Rep. Dim. RAF <ref type="bibr" target="#b14">[15]</ref> SFEW <ref type="bibr" target="#b3">[4]</ref> AffectNet <ref type="bibr" target="#b16">[17]</ref> Covariance Pooling <ref type="bibr" target="#b0">[1]</ref> 2000 79.  <ref type="table">Table 2</ref>: Accuracy of our model regarding state-of-the-art methods. The size of the representation is taken into account. Metrics are the average of per class recall for RAF and accuracy for SFEW and AffectNet.</p><p>the analysis of the results, we plot in <ref type="figure" target="#fig_3">Figure 4</ref> a multi-domain F1-score, i.e. the weighted average of the F1-scores according to the respective validation set sizes. We observe that the gain in multi-domain F1-score is exponentially decreasing for both representations -note that the representation size axis is in log scale -and thus the performance gap between a representation of size 2 and size 3 is the more important. We also observe that "CAKE" representations still seem to yield better results than "AVk" when the representation size is greater than 2. These first experiment shows that a very compact representation can yield good performances for emotion recognition. It also is in line with the "dominance" dimension hypothesis, as a third dimension brought the more significant gain in performance. After 3 dimensions, the gain is much less significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Accuracy of the Representation</head><p>To evaluate the efficiency of the CAKE-3 compact representation, we compare its accuracy with state-of-the-art approaches (Table 2) on the public datasets commonly used in the literature for evaluation (validation of SFEW and AffecNet, test of RAF). In order to get a fair comparison, we add a "Rep. Dim." column corresponding to the size of the last hidden representation -concretely, we take the penultimate fully connected output size. We report the scores under the literature's metrics, namely the mean of the per class recall for RAF <ref type="bibr" target="#b14">[15]</ref> and the accuracy for SFEW <ref type="bibr" target="#b3">[4]</ref> and AffectNet <ref type="bibr" target="#b16">[17]</ref>. To the best of the author's knowledge no other model has been evaluated before on the AffectNet's seven classes.</p><p>CAKE-3 is outperformed by Covariance Pooling <ref type="bibr" target="#b0">[1]</ref> and Deep Locality Preserving <ref type="bibr" target="#b14">[15]</ref>. Nevertheless, it is still competitive as the emotion representation is far more compact -3-d versus 2000-d -and learned in a multi-domain fashion. Moreover, we gain 1 point on RAF when we compare to models of same size (2 millions parameters), e.g. Compact Model <ref type="bibr" target="#b13">[14]</ref>. These results support the conclusion made in 3.2, as we show that a compact representation of the emotion learned by small models is competitive with larger representations. This finally underlines that facial expressions may be encoded efficiently into a 3-d vector and that using a large embedding on small datasets may lead to exploit biases of the dataset more than to learn emotion recognition.</p><p>Our experiments also allow to perform a cross-database study as done in <ref type="bibr" target="#b14">[15]</ref>. This study consists in evaluating a model trained on dataset B on a dataset A. Thereby we obtain  <ref type="table">Table 3</ref>: Cross-database evaluation on CAKE-3 model (F1-Score). <ref type="table">Table 3</ref> with the evaluation of each classifier on each dataset. Results on SFEW [4] -trained or evaluated -are constantly lower than others, with a higher standard deviation. This could be due to the insufficient number of samples in the SFEW training set or more probably to the possible ambiguity in the annotation of SFEW compared to AffectNet and RAF. Confirming this last hypothesis, the RAF classifier has the better generalization among the datasets. It is in line with the claim of Li et al. <ref type="bibr" target="#b14">[15]</ref> that RAF has a really reliable annotation with a large consensus between different annotators. Finally, it also underlines the difficulty to find a reliable evaluation of an emotion recognition system because of the important differences between datasets annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visualizing Emotion Maps</head><p>Visualizations are essential to better appreciate how DNN performs classifications, as well as to visualize emotion boundaries and their variations across datasets. Our visualization method consists in densely sampling the compact representation space -2-d or 3-d -into a mesh grid, and feeding it to a formerly trained model -AV, CAKE-2 or CAKE-3 -in order to compute a dense map of the predicted emotions. Not all the coordinates of the mesh grid belong to real emotions and some of them would never happen in real applications. The construction of the mesh grid depends on the model to be used. For the AV and the CAKE-2 models, we have simply built it using 2d vectors with all values ranging in intervals containing maximum and minimum values of the coordinates observed with real images. As the CAKE-3 model is dealing with a three-dimensional representation, it is not possible to visualize it directly on a plane figure. To overcome this issue we modify CAKE-3 into a CAKE-3-Norm representation where all the coordinates are constrained to be on the surface of the unit sphere, and visualize spherical coordinates. Even if CAKE-3-Norm shows lower performances (about 2 points less than CAKE-3), the visualization is still interesting, bringing some incentives about what has really been learned. <ref type="figure">Figure 5</ref> shows the visualization results for CAKE-3-Norm, AV and CAKE-2 representations (resp. from top to down). Each dot is located by the coordinates of its compact representation -(arousal, valence) for AV, (k 1 , k 2 ) for CAKE-2 and spherical coordinates (φ and θ ) for CAKE-3-Norm -and colored according to the classifier prediction. The per class macro F1-score is displayed inside each emotion area.</p><p>First, each compact representation -CAKE-2, CAKE-3-Norm and AV -exhibits a strong consistency across the datasets (in <ref type="figure">Figure 5</ref>, compare visualizations on the same row). Indeed, the three classifiers show a very similar organization of the emotion classes, which is demonstrating the reliability of the learned representation. Thereby, the neutral class -in blue -is always placed at the origin and tends to neighbor all other classes. It is in line with the idea of neutral as an emotion with a very low intensity. Nevertheless, we can witness small inter-dataset variations, especially on SFEW <ref type="bibr" target="#b3">[4]</ref> (in <ref type="figure">Figure 5</ref>, middle column) with disgust and fearresp. brown and purple -which are almost missing. This underlines the disparities of annotations across the datasets and confirms the need of multi-domain frameworks when wishing to achieve a more general emotion recognition model. Second, we can analyze variations between the different representations for a given dataset (in <ref type="figure">Figure 5</ref>, compare visualizations on the same column). As AV is based on arousalvalence, we observe the same emotion organization as in <ref type="figure">Figure 1</ref>. Especially, as the majority of the AffectNet's training (and validation) samples have a positive arousal, the classifier do not use the whole space (in <ref type="figure">Figure 5</ref>, second row: see green, blue and orange areas) unlike CAKE-2 and CAKE-3 which are not constrained by arousal-valence.</p><p>We can find many similarities between these three representations, but the most impressive come across when comparing CAKE-2 and AV. Despite the inequality of scaling -which causes the neutral area (blue) to be smaller in CAKE-2 -AV and CAKE-2 compact representations are very close. Indeed, the area classes are organized exactly in the same fashion. The only difference is that for AV they are disposed in a clockwise order around neutral whereas for CAKE-2 they are disposed in an anticlockwise order. This observation shows that a DNN trained on the emotion recognition classification is able to learn an arousal-valence-like representation of the emotion. It contributes -along with <ref type="bibr">Khorrami [11]</ref> who points that DNNs trained to recognize emotions are learning action units <ref type="bibr" target="#b6">[7]</ref> -to bring the dependence across the emotion representations in the forefront.     <ref type="figure">Figure 5</ref>: Visualization of CAKE-3-Norm, AV and CAKE-2. Rows indicate evaluated representation -resp. from top to down: CAKE-3-Norm, AV, CAKE-2 -and columns indicate datasets -resp. from left to right: AffectNet <ref type="bibr" target="#b16">[17]</ref>, SFEW <ref type="bibr" target="#b3">[4]</ref> and RAF <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This work proposes a comprehensive analyze on how a DNN can describe emotional states.</p><p>To this purpose, we first studied how many dimensions are sufficient to accurately represent an emotion resulting from a facial expression. We then conclude that three dimensions are a good trade-off between accuracy and compactness, agreeing with the arousal-valencedominance <ref type="bibr" target="#b19">[20]</ref>[16] psychologist model. Thereby, we came up with a DNN providing a 3-dimensional compact representation of emotion, learned in a multi-domain fashion on RAF <ref type="bibr" target="#b14">[15]</ref>, SFEW <ref type="bibr" target="#b3">[4]</ref> and AffecNet <ref type="bibr" target="#b16">[17]</ref>. We set up a comparison with the state-of-thearts and showed that our model can compete with models having much larger feature sizes. It proves that bigger representations are not necessary for emotion recognition. In addition, we implemented a visualization process enabling to qualitatively evaluate the consistency of the compact features extracted from emotion faces by our model. We thus showed that DNN trained on emotion recognition are naturally learning an arousal-valence-like <ref type="bibr" target="#b19">[20]</ref> encoding of the emotion. As a future work we plan to also apply state-of-the-art techniques -as Deep Locality Preserving Loss <ref type="bibr" target="#b14">[15]</ref> or Covariance Pooling [1] -to enhance our compact representation. In addition, nothing warranty that the learned CAKE bears the same semantic meanings as arousal-valence-dominance does: further interpreting the perceived semantic of the dimensions would therefore be an interesting piece of work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The w class weight is defined as: w i, j class =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Influence of representation size on the multi-domain F1 score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>− 10</head><label>10</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1807.11215v2 [cs.AI] 3 Aug 2018</figDesc><table><row><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Arousal</cell><cell>Arousal 0.00 Arousal</cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>− 0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">− 0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">− 0.50 − 0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">− 0.75 − 0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">− 1.00 − 1.00 − 1.00 − 1.00</cell><cell>− 0.75 − 0.75</cell><cell>− 0.50 − 0.50</cell><cell>− 0.25 − 0.25</cell><cell>0.00 Valence 0.00 Valence Valence</cell><cell>0.25 0.25</cell><cell>0.50</cell><cell>0.50</cell><cell>0.75</cell><cell>0.75</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Neut ral</cell><cell>Happy</cell><cell>Sad</cell><cell>Surprise</cell><cell>Fear</cell><cell cols="2">Disgust</cell><cell cols="2">Anger</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Neut ral</cell><cell>Happy</cell><cell>Sad</cell><cell>Surprise</cell><cell>Fear</cell><cell cols="2">Disgust</cell><cell></cell><cell cols="2">Anger</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of compact representations on AffectNet, SFEW, RAF.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(± 0.5) 27.6 (± 2.6) 53.8 (± 0.6) SFEW 35.1 (± 2.1) 34.1 (± 1.0) 47.3 (± 1.2) RAF 51.8 (± 0.4) 31.5 (± 1.7) 64.4 (± 0.6)</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell></row><row><cell>AffectNet</cell><cell>SFEW</cell><cell>RAF</cell></row><row><cell>AffectNet 58.1</cell><cell></cell><cell></cell></row><row><cell>Classifier</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">By "fully connected layer" we denote a linear layer with biases and without activation function.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Covariance pooling for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danda</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2106" to="2112" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compound facial expressions of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichuan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">201322355</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constants across cultures in the face and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace V Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Measuring facial movement. Environmental psychology and nonverbal behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace V Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="56" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprakash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">Luc</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do deep neural networks learn facial action units when doing expression recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: Amethod for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Learn. Representations</title>
		<meeting>3rd Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A compact deep learning model for robust facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Ming</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hong</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Sarkis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2584" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="292" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning for emotion recognition on small datasets using transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Wei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><forename type="middle">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on international conference on multimodal interaction</title>
		<meeting>the 2015 ACM on international conference on multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="443" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Masip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1161</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
