<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
							<email>vineetk@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Aibee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Hamid</forename><surname>Rezatofighi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting the future trajectories of multiple interacting agents in a scene has become an increasingly important problem for many different applications ranging from control of autonomous vehicles and social robots to security and surveillance. This problem is compounded by the presence of social interactions between humans and their physical interactions with the scene. While the existing literature has explored some of these cues, they mainly ignored the multimodal nature of each human's future trajectory. In this paper, we present Social-BiGAT, a graph-based generative adversarial network that generates realistic, multimodal trajectory predictions by better modelling the social interactions of pedestrians in a scene. Our method is based on a graph attention network (GAT) that learns reliable feature representations that encode the social interactions between humans in the scene, and a recurrent encoder-decoder architecture that is trained adversarially to predict, based on the features, the humans' paths. We explicitly account for the multimodal nature of the prediction problem by forming a reversible transformation between each scene and its latent noise vector, as in Bicycle-GAN. We show that our framework achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks. * indicates equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For a variety of applications, accurate pedestrian trajectory forecasting is becoming a crucial component. Autonomous vehicles such as self-driving cars, and social robotics such as delivery vehicles must be able to understand human movement to avoid collisions <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Intelligent tracking and surveillance systems used for city planning must be able to understand how crowds will interact to better manage infrastructure <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Trajectory prediction is also becoming crucial enabling downstream tasks, such as tracking and re-identification <ref type="bibr" target="#b8">[9]</ref>. However, trajectory prediction is still a challenging task because of several properties inherent to human behavior:</p><p>• Social Interactions When humans move in public, they often interact socially with other pedestrians <ref type="bibr" target="#b9">[10]</ref>. From taking actions to avoid collisions, to walking in groups, there are several ways humans interact while moving that require prediction methods to model social behavior <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. These social interactions may not be necessarily influenced by people's spatial proximity. • Scene Context Pedestrian behavior is not only dependent on the people around them, but is also highly dependent on the physical scene around them <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. This includes not just stationary obstacles that cannot be avoided, such as buildings, but also different physical cues present visually, such as sidewalks or grass which may enable or restrict human movement. • Multimodal Behavior Pedestrians may follow several plausible trajectories, as there is a rich distribution of potential human behavior <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. For example, when two pedestrians <ref type="figure">Figure 1</ref>: We show multimodal behavior for the blue pedestrian, who must make a decision about which direction they will take to avoid the red-green pedestrian group.</p><p>are walking towards each other, several modes of behavior develop, such as moving to the left or moving to the right. Within each mode, there is also a large variance, allowing pedestrians to vary features like their speed.</p><p>Prior work in trajectory forecasting have tackled several of the previously listed challenges and have informed our architectural design. Helbing et al. <ref type="bibr" target="#b18">[19]</ref> and Pellegriniet al. <ref type="bibr" target="#b19">[20]</ref> successfully demonstrated the benefit of modeling social interactions but require handcrafted rules that are less able to generalize to new scenes. Alahi et al. <ref type="bibr" target="#b9">[10]</ref> utilized recurrent architectures to consider multiple timesteps of pedestrian behavior, but do not consider the physical cues of the scene. Other prior research has also focused on understanding the physical scene. Lee et al. <ref type="bibr" target="#b14">[15]</ref> and Sadeghian et al. <ref type="bibr" target="#b15">[16]</ref> use raw scene images and soft attention on the scene to highlight important cues. Their work is limited by not considering social cues jointly with the scene.</p><p>By contrast, Gupta et al. <ref type="bibr" target="#b10">[11]</ref> and Sadeghian et al. <ref type="bibr" target="#b11">[12]</ref> utilize GANs with social mechanisms that do take into account all people in the scene. However both models fall short of learning the truly multimodal distribution of human behavior, and instead learn a single mode of behavior with high variance. Further, both models are limited by how they learn social behavior: while the former loses information by using the same social vector for all pedestrians in a scene, the latter requires a hand-defined sorting operation that may not perform optimally in all cases.</p><p>To address the limitations of these works, we propose Social-BiGAT, a GAN <ref type="bibr" target="#b21">[21]</ref> based approach to construct a generative model that can learn these essential multimodal trajectory distributions. The main contributions of this work are as follows. First, we improve the modeling of social interactions between pedestrians in a scene by introducing a flexible graph attention network <ref type="bibr" target="#b22">[22]</ref>, where all pedestrians in a scene are allowed to interact. This improves over prior works where either interactions were limited locally, or interactions were modelled using hand-defined rules. Next, we encourage generalization towards a multimodal distribution by constructing a reversible mapping between outputted trajectories and latents that represent the pedestrian behavior in a scene, as previously performed for images by Zhu et al. <ref type="bibr" target="#b23">[23]</ref>. This allows us to generate trajectories that are socially and physically acceptable, while also learning a larger multimodal trajectory distribution, despite only having access to single samples from single modes of behavior across scenes. Finally, we incorporate physical scene cues using soft attention as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> to make our model more generalizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In recent years due to the rise of popularity in development of autonomous driving systems and social robots, the problem of trajectory forecasting has received significant attention from many researchers in the community. The majority of existing works have been focused on the effects of incorporating physical features of the scene into human-space models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, as well as learning how to model social behavior between pedestrians in human-human models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">24]</ref>. Other works have approached the problem from a generative setting <ref type="bibr" target="#b10">[11]</ref> and have jointly modeled these features in one framework <ref type="bibr" target="#b11">[12]</ref>. While these works have greatly advanced the field, they have drawbacks that we address by incorporating graph attention networks <ref type="bibr" target="#b22">[22]</ref> and image translation networks <ref type="bibr" target="#b23">[23]</ref>.</p><p>Trajectory Forecasting Traditionally, pedestrian trajectory prediction has been tackled by defining handcrafted rules and energy parameters that capture human motion but fail to generalize properly <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref>. Instead of handcrafting these features, modern approaches rely on recurrent neural networks that learn these parameters directly from the data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, while incorporating some means of capturing human interaction features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>. Several of these prior methods have been limited in scope, as they often limit interactions to nearby pedestrian neighbors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref> and do not model global interactions or cannot generalize to a variable number of humans. Other approaches have explored trajectory prediction from a generative standpoint, including Lee et al. <ref type="bibr" target="#b14">[15]</ref>, Gupta et al. <ref type="bibr" target="#b10">[11]</ref>, and Sadeghian et al. <ref type="bibr" target="#b11">[12]</ref>, with their own limitations. The former only considers interactions within a limited local scope, and the latter two result in models with high variances. Specifically, although human motion is inherently multimodal, these methods are not able to expressively learn this multimodal behavior and instead learn one mode with a high variance. In our work we incorporate ideas from image to image translation to generate multimodal pedestrian trajectories. Furthermore, our model uses graph attention networks <ref type="bibr" target="#b22">[22]</ref> to more efficiently and robustly model the interactions between the agents in the scene, whereas prior research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">31]</ref> depend on hand-defined rules.</p><p>Graph Attention Networks Proposed by Velickovi et al. <ref type="bibr" target="#b22">[22]</ref>, graph attention networks (GAT) allow for the application of a self-attention based architecture over any type of structured data that can be represented as a graph. These networks build upon the prior advances of graph convolutional networks (GCN) <ref type="bibr" target="#b32">[32]</ref> by also allowing for the model to implicitly assign different importances to nodes in the graph. In our case, we can formulate pedestrian interactions as a graph, where nodes refer to human humans, and edges are these interactions; higher edge weights correspond to more important interactions. By leaving the graph fully connected, we can model both local and global interactions between humans in an efficient manner without enforcing a system like pooling <ref type="bibr" target="#b10">[11]</ref> or sorting <ref type="bibr" target="#b11">[12]</ref> that may lose important features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Translation</head><p>The field of image domain translation has gone through several seminal advancements in the past couple years. The first advancement was made with the pix2pix framework <ref type="bibr" target="#b33">[33]</ref>, which enabled translation but was limited by requiring paired training examples. Zhu et al. improved this model with CycleGAN <ref type="bibr" target="#b34">[34]</ref>, which was able to learn these domain mappings with unpaired examples from each domain through a cycle consistency loss. Newer research has focused on learning multimodality of the output: InfoGAN <ref type="bibr" target="#b35">[35]</ref> focuses on maximizing variational mutual information, while BicycleGAN <ref type="bibr" target="#b23">[23]</ref> introduces a latent noise encoder and learns a bijection between noise and output. In our model we draw upon the advancements suggested by BicycleGAN to propose a latent space encoder that allows for multimodal pedestrian trajectory generation.</p><p>3 Social-BiGAT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Formally defined, human trajectory prediction is the problem of predicting the future navigation movements of pedestrians (namely their x and y coordinates on a 2D map representation), given their prior movements and additional contextual information about the scene. We assume the route taken by each pedestrian is influenced by the location of other humans and the physical constraints on its path, as well as its own goal, which is to some extent encoded in its past course of movements. For any particular scene, the inputs to our model are twofold: 1) scene information, in the form of a top-down or side-view image of the scene, I t , and 2) the previously observed trajectory within the scene of each of the N currently visible pedestrians,</p><formula xml:id="formula_0">X i = {(x t i , y t i ) ∈ R 2 |t = 1, . . . , t obs } for ∀i ∈ {1, .</formula><p>. . , N }. Given all above inputs and the ground truth future trajectory for each pedestrian between t pred and t obs timesteps, i.e.</p><formula xml:id="formula_1">Y i = {(x t i , y t i ) ∈ R 2 |t = t obs + 1, . . . , t pred } for ∀i ∈ {1, .</formula><p>. . , N }, our goal is to learn the underlying (and potentially, multimodal) distribution which can generate feasible samples for their future trajectories, i.e.Ŷ i for ∀i ∈ {1, . . . , N }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Model</head><p>Our overall model consists of four main networks, each of which is made up of three key modules ( <ref type="figure" target="#fig_0">Figure 2</ref>). Specifically, we construct a generator, two forms of discriminators (one that operates at local pedestrian scale, and one that operates at a global scene-level scale), and a latent space encoder. Our generator is composed of a feature encoder module (Section 3.3), an attention network module (Section 3.4), and a decoder module (Section 3.5). The feature encoder module extracts encodings from raw features for use in the attention network, which in turn learns which features are most important in generation. These weighted features are then passed into the decoder module, which uses LSTMs to generate multiple timesteps of trajectories. The architecture is trained adversarially with both discriminators, as motivated by Isola et al. <ref type="bibr" target="#b33">[33]</ref> and to encourage realistic local and global trajectories, and we also train a latent scene encoder that learns to generate a mean and variance for the noise that best represents a scene jointly, as in Zhu et al. <ref type="bibr" target="#b23">[23]</ref> to encourage multimodality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Encoder</head><p>The feature encoder has two main components: a social pedestrian encoder, in order to learn representations of observed pedestrian trajectories, and a physical scene encoder, in order to learn the representation of the scene features. For the social encoder, for each pedestrian we first embed the pedestrian's relative displacements into a higher dimension using a multilayer perceptron (MLP), and then encode these pedestrian movements across timesteps into a single embedding using a LSTM, resulting in encoding V s (i) for pedestrian i. For the physical feature encoder, we simply pass the top-down image view of the scene through a convolutional neural network (CNN), resulting in V p for the scene:</p><formula xml:id="formula_2">V s (i) = LST M en (M LP emb (X i , W emb ), h en (i); W en ) (1) V p = CN N (I; W cnn )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attention Network</head><p>Much like how humans intuitively know which other pedestrians to focus on to avoid collisions, we want our model to better understand the relative weight that interactions have: we accomplish this goal by applying attention over our extracted features.</p><p>Physical Attention To apply attention over our physical features relative to a specific pedestrian, we take in V s (i), and apply soft attention, where the network is parameterized by W p and outputs context vector C p t (i):</p><formula xml:id="formula_3">C p (i) = AT T p (V p , V s (i); W p )<label>(3)</label></formula><p>Social Attention Similar to physical attention, we use as input to our social attention model the embeddings of pedestrians, V s (i). The social attention model encodes pedestrians as weighted (at-tended) sum of the neighbor pedestrians they interact with. Prior research has used either permutation invariant symmetric functions, such as max or average <ref type="bibr" target="#b10">[11]</ref>, or ordering functions such as sorting based on euclidean distance <ref type="bibr" target="#b11">[12]</ref>. In the former, the downside is that each pedestrian receives an identical joint feature representation that discards some uniqueness. While the latter technique does not suffer from this drawback, it does require setting a maximum number of pedestrians and does impose a human bias on the model that is not necessarily always true. Namely, it assumes that euclidean distance ordering is a key component of understanding social interactions.</p><p>To avoid these flaws, we utilize graph attention networks <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b36">36]</ref>. Given pedestrian i's embedding, V s (i), for all pedestrians in the scene, we apply several stacked graph attention layers. Each layer, , is applied as follows, where W gat parameterizes a shared linear transformation and a is the shared attentional mechanism:</p><formula xml:id="formula_4">e ij = a(W gat V s (i), W gat V s (j)) (4) α ij = softmax j (e ij )<label>(5)</label></formula><formula xml:id="formula_5">C s (i) = j∈N α ij W gat V s (j)<label>(6)</label></formula><p>We use the features C L s from the last GAT layer where = L as the final social features. We allow the graph of pedestrians to remain fully connected and do not apply any mask. This allows each pedestrian to interact with each other and does not impose any restriction on pedestrian orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">GAN Network</head><p>In this section we present how our feature encoder and attention network serve as core building blocks in developing the LSTM based Generative Adversarial Network (GAN). GANs typically consist of two networks that compete with each other: a generator, and a discriminator. While the generator learns to generate realistic samples from input data, the discriminator learns to discern which samples are real, and which are generated, thereby engaging in a two-player min-max game.</p><p>Generator The generator is built using a decoder LSTM. Similar to conditional GANs <ref type="bibr" target="#b37">[37]</ref>, our generator takes as input a noise vector z sampled from a multivariate normal distribution, and is conditioned on the physical scene context, C p (i), the pedestrian scene context, C s L (i), and the previous pedestrian encoding, V s (i). These are all concatenated together such that C g (i) = [V s (i), C s L (i), C p (i), z]. Generation of trajectories across multiple timesteps is then performed through a decoder LSTM, such that:</p><formula xml:id="formula_6">Y i = M LP d (LST M dec (C g (i), h dec (i); W dec ); W d )<label>(7)</label></formula><p>Discriminator The discriminator architecture mirrors that of the generator, with encoder LSTMs used to represent pedestrians, and a CNN used to represent scene features. We propose two versions of this core discriminator architecture: one at local scale, operating on pedestrians, and one at global scale, operating on an entire scene. The former performs classification directly on encodings of concatenated past and future trajectories, such that:</p><formula xml:id="formula_7">L(i) = M LP clf (LST M en (M LP emb ([X i ,Ỹ i ], W emb ), h en (i); W en ); W clf ),<label>(8)</label></formula><formula xml:id="formula_8">whereỸ i ∼ p(Y i ,Ŷ i )</formula><p>is a randomly chosen future trajectory sample from the either ground truth or predicted path.L(·) is classification score representing the sample is a ground truth (real) or predicted (fake) with the truth label L(i) = 1 and L(i) = 0, respectively.</p><p>The global discriminator performs the same classification operation, but on the global context vector for the pedestrian trajectory; namely, the concatenation of the physical scene context, C p (i), the pedestrian scene context, C s L (i), and the pedestrian encoding, V s (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Latent Encoder</head><p>In order to generate trajectories that are truly multimodal, we encourage our model to develop a bijection between the outputted trajectories and the latent space inputted to the generator. Specifically, we want to map both the latent noise to an output trajectory, as well as map that trajectory back to the <ref type="figure">Figure 3</ref>: Training process for the Social-BiGAT model. We teach the generator and discriminators using traditional adversarial learning techniques, with an additional L2 loss on generated samples to encourage consistency. We further train the latent encoder by ensuring it can recreate noise passed into the generator, and by making sure it mirrors a normal distribution.</p><p>original latent. While the former task is accomplished by a generator, we perform the latter using a latent scene encoder, as previously performed in Zhu et al. <ref type="bibr" target="#b23">[23]</ref>.</p><p>The architecture for the latent scene encoder is relatively similar to the local discriminator. First, pedestrians are encoded in the scene using a LSTM encoder. Embeddings from this LSTM are passed in two parallel MLPs that are trained to output a mean µ i and log variance σ 2 i for each pedestrian:</p><formula xml:id="formula_9">µ i = M LP µ (M LP L (V s (i), W L ), W µ ) (9) log σ 2 i = M LP σ (M LP L (V s (i), W L ), W σ )<label>(10)</label></formula><p>Means and log variances across pedestrians are max pooled together to generate a single mean and log variance representation of the latent for a given scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Losses</head><p>As illustrated in <ref type="figure">Figure 3</ref>, to train these four models we have a multistep training process, where we not only perform a transformation starting from the noise, z →Ŷ i →ẑ, but also perform a transformation starting from the trajectories, Y i → z →Ŷ i .</p><p>In the former we have two main loss terms to consider: the GAN loss (L gan 1 ) from the generator fooling the discriminator, and the discriminator correctly classifying the generator, as well as a loss term on reconstructing the noise (L z ). We calculate these as follows, where G refers to the generator, D to the discriminator and E to the latent encoder:</p><formula xml:id="formula_10">L gan 1 = E log D(X i , Y i ) + E log(1 − D(X i ,Ŷ i ))<label>(11)</label></formula><formula xml:id="formula_11">L z = ||E(Ŷ i ) − z|| 1<label>(12)</label></formula><p>In the latter, we have three additional loss terms: the GAN loss (L gan 2 ), a L2 loss on trajectories (L traj ), enforcing the generation of real samples, and a KL loss on the generated noise (L kl ) such that it resembles noise drawn from a random Gaussian:</p><formula xml:id="formula_12">L gan 2 = E log D(X i , Y i ) + E log(1 − D(X i , G(X i , E(Y i )))) (13) L traj = ||Y i − G(X i , E(Y i ))|| 2 (14) L kl = E[D kl (E(Y i )||N (0, I))]<label>(15)</label></formula><p>We ultimately combine all these loss terms using λ weights that are chosen as hyperparameters:</p><formula xml:id="formula_13">G * , D * , E * = argmin G,E argmax D [L gan 1 + λ z L z + L gan 2 + λ traj L traj + λ kl L kl ]<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform experiments on two relevant datasets: ETH <ref type="bibr" target="#b19">[20]</ref> and UCY <ref type="bibr" target="#b38">[38]</ref>. Both contain annotated trajectories of socially interacting pedestrians in real world scenes. The datasets include different types   <ref type="table">Table 2</ref>: Effect of varying K in evaluation results for generative models. We see that reducing K results in a higher average ADE/FDE across the five scenes for S-GAN-P and Sophie, due to higher distribution variances. of social interactions, ranging from group formation to collision avoidance, the type of interaction we aim to encode with our Social-BiGAT model. The datasets contain five unique scenes: Zara1, Zara2, Univ, Eth, and Hotel. We evaluate Social-BiGAT on these datasets and compare to several deterministic baselines, including a linear regressor that minimizes least square error, Linear, and a predictive model using LSTMs and social pooling, S-LSTM <ref type="bibr" target="#b9">[10]</ref>, as well as two main generative models: S-GAN-P, which applies generative modeling to social LSTMs <ref type="bibr" target="#b10">[11]</ref>, and Sophie, which applies attention networks to social GANs <ref type="bibr" target="#b11">[12]</ref>. We present evaluation results of three versions of our model: one trained without the latent scene encoder but with the graph attention network, GAT, one trained without the graph attention network but with the latent scene encoder, BiGAN, and our final model with all components included, Social-BiGAT. Models are evaluated using two main metrics: average displacement error (ADE), and final displacement error (FDE). Both are defined as the average L2 distance between the ground truth and predicted trajectories. Evaluation occurs over a timescale of 8 seconds, where the first 3.2 seconds (8 timesteps) correspond to observed data, and the last 4.2 seconds (12 timesteps) correspond to predicted future data. We evaluate using a hold-one-out cross evaluation strategy in meter space, with N -K variety loss, as previously performed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative Results</head><p>We compare our model to various baselines in <ref type="table" target="#tab_1">Table 1</ref>, reporting the average displacement error (ADE) and final displacement error (FDE) for 12 timesteps of pedestrian movement. As expected we see that both the discriminative Social LSTM baseline outperforms the simple linear model, and that the generative baselines, which are evaluated from K = 20 samples, improve upon the discriminative ones by generating a full distribution of possible human trajectories. In terms of our proposed architectures, we see that incorporating the GAT alone does indeed improve performance, as the network is able to more flexibly account for pedestrian interactions. Alternatively the BiGAN alone does not help performance. Our combined GAT and BiGAN architecture, Social-BiGAT, does however achieve the best performance of all our models, resulting in a 0.15 meter decrease in average FDE from the previous state-of-the-art model. This is due to the reduced errors for the Hotel scene compoared to other generative architectures.</p><p>While the BiGAN architecture does not help performance much when K = 20, we show in <ref type="table">Table 2</ref> that it does help improve generalization at lower settings of K. Specifically, while S-GAN-P and Sophie suffer from higher variances, causing their ADE and FDE to increase dramatically when K is lowered, Social-BiGAT's ADE and FDE increase more slowly. This suggests that the inclusion of the latent scene encoder in BiGAN and Social-BiGAT allow for the architecture to reduce the variance of the outputted trajectory distributions while also allowing for better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Results</head><p>In order to better understand the contribution of the graph attention network and bicycle training structure in improving understanding of social behavior, we visualize the generated trajectories for four scenes, comparing our proposed Social-BiGAT model to S-GAN-P and Sophie (Figure various (color-coded) pedestrians, while varying z, the noise passed into the generator. We note several modes of behavior, including avoidance versus aggressiveness (a), linearity versus curvature (b), and fast versus slow (c). 4). We draw three main conclusions from these visualizations. First, as shown in scenes 1 and 2, Social-BiGAT often has a lower variance than S-GAN-P and Sophie, suggesting that it can generate more efficiently. Second, as shown in scenes 2 and 3, the model is better able to model the interactions of people travelling in crowds or groups. Finally, as scene 4 demonstrates, the model can generate realistic trajectories for pedestrians that are attempting to avoid collisions. Each of these findings are crucial in ensuring that the model performs optimally across a wide range of social behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Space Exploration</head><p>In addition to visualizing our model's trajectories in comparison to prior generative baselines, we also visualize how trajectories change as we vary the latent z passed into the generator. As seen in <ref type="figure">Figure 5</ref>, by varying z while keeping the scene fixed, we are able to visualize several types of pedestrian movement. For instance in a), on the left of the figure, we notice the green and blue pedestrians go from a state of avoidance to aggressiveness, as they narrowly avoid a collision. In b), we note that the blue pedestrian reaches the same end destination but adjusts the linearity of their course. And finally in c), we notice that the red and green pedestrians slow down dramatically. These results help validate our hypothesis that using a Bicycle-GAN inspired architecture helps generate interpretable trajectories that more realistically follow human behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented Social-BiGAT, a novel architecture for forecasting pedestrian movements that outperforms prior state-of-the-art methods across several widely used trajectory benchmarks. Unlike prior research, our model is not only able to generate multiple trajectories for a given pedestrian, but is also able to do so for multiple humans in a multimodal fashion. Through our evaluations and visualizations we demonstrated that Social-BiGAT is able to capture the intricate social nature of pedestrian movements and that we are able to control the predictions by adjusting the latents at test time. We further introduced several important architectural improvements to the trajectory generation process: 1) we utilize a social attention graph network (GAT) to better learn pedestrian interactions through the data, and 2) we train using two discriminators that operate at local and global scale. As shown experimentally, with these design patterns our Social-BiGAT model is able to generate pedestrian trajectories that predict more realistically human motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>The research reported in this publication was supported by funding from the TRI gift, ONR (1165419-10-TDAUZ), Nvidia, and Samsung.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture for the proposal Social-BiGAT model. The model consists of a single generator, two discriminators (one at local pedestrian scale, and one at global scene scale), and a latent encoder that learns noise from scenes. The model makes use of a graph attention network (GAT) and self-attention on an image to consider the social and physical features of a scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Generated trajectories visualized for the S-GAN-P, Sophie, and Social-BiGAT models across four main scenes. Observed trajectories are shown as solid lines, ground truth future movements are shown as dashed lines, and generated samples are shown as contour maps. Different colors correspond to different pedestrians. Visualization of generated trajectories (dashed lines), given observed trajectories (solid lines) for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.94 1.09 / 2.35 0.87 / 1.62 0.70 / 1.43 0.68 / 1.29 0.72 / 1.47 0.69 / 1.29 HOTEL 0.39 / 0.72 0.79 / 1.76 0.67 / 1.37 0.76 / 1.67 0.68 / 1.40 0.54 / 1.12 0.49 / 1.01 UNIV 0.82 / 1.59 0.67 / 1.40 0.76 / 1.52 0.54 / 1.24 0.57 / 1.29 0.55 / 1.34 0.55 / 1.32 ZARA1 0.62 / 1.21 0.47 / 1.00 0.35 / 0.68 0.30 / 0.63 0.29 / 0.60 0.32 / 0.65 0.30 / 0.62 ZARA2 0.77 / 1.48 0.56 / 1.17 0.42 / 0.84 0.38 / 0.78 0.37 / 0.75 0.49 / 0.88 0.36 / 0.75</figDesc><table><row><cell></cell><cell>Discriminative</cell><cell>Generative</cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell>Dataset Lin</cell><cell>S-LSTM</cell><cell>S-GAN-P Sophie</cell><cell>GAT</cell><cell>BiGAN</cell><cell>Social-BiGAT</cell></row><row><cell cols="6">ETH 1.33 / 2AVG 0.79 / 1.59 0.72 / 1.54 0.61 / 1.21 0.54 / 1.15 0.52 / 1.07 0.52 / 1.09 0.48 / 1.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Baseline models compared to our architectures when predicting 12 future timesteps, given the previous 8. Errors reported are ADE / FDE in meters, with generative models being evaluated using K = 20 samples. BiGAT 0.476 / 0.998 0.488 /1.096 0.527 / 1.260 0.606 / 1.328 27.3% / 33.1%</figDesc><table><row><cell>Model</cell><cell>K = 20</cell><cell>K = 10</cell><cell>K = 5</cell><cell>K = 1</cell><cell>% Increase</cell></row><row><cell>S-GAN-P</cell><cell cols="5">0.558 / 1.118 0.594 / 1.214 0.650 / 1.316 0.846 / 1.758 51.6% / 57.2%</cell></row><row><cell>Sophie</cell><cell cols="5">0.526 / 1.030 0.566 / 1.122 0.604 / 1.266 0.712 / 1.456 35.3% / 41.4%</cell></row><row><cell>Social-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social scene understanding: End-to-end multi-person action localization and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Timur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3425" to="3434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Forecasting interactive dynamics of pedestrians with fictitious play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4636" to="4644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autonomous exploration, active learning and human guidance with open-source poppy humanoid robot platform and explauto library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoan</forename><surname>Mollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Oudeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirtieth Annual Conference on Neural Information Processing Systems (NIPS 2016)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An assistive household robot-doing more than just cleaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kantorovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Väre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vesa</forename><surname>Pehkonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arto</forename><surname>Laikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heikki</forename><surname>Seppälä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Assistive Technologies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="76" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of vision-based trajectory learning and analysis for surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1114" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A large-scale benchmark dataset for event recognition in surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Cuntoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Chih</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Taek</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurajit</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungtae</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3153" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video surveillance and counterterrorism: the application of suspicious activity recognition in visual surveillance systems to counterterrorism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">J</forename><surname>Regens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">N</forename><surname>Edger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligence and Counter Terrorism</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Journal of Policing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">seeing is believing&quot;: Pedestrian trajectory forecasting using visual frustum of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irtiza</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Tsesmelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><forename type="middle">Del</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1178" to="1185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10892</idno>
		<title level="m">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SoPhie: An attentive GAN for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge transfer for scene-specific motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamberto</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="697" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Desire: Distant future prediction in dynamic scenes with interacting agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdinand</forename><surname>Legros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Voisin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Vesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10061</idno>
		<title level="m">Car-net: Clairvoyant attentive recurrent network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning social etiquette: Human trajectory understanding in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Andrew</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4282</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving data association by joint modeling of pedestrian trajectories and groupings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="452" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph attention networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lió</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1710.10903</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Socially-aware large-scale crowd forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition, number EPFL-CONF-230284</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2211" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Soft+ hardwired attention: An lstm framework for human trajectory prediction and abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tharindu</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tree memory networks for modelling long-term temporal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tharindu</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mcfadyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04706</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Context-aware trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamberto</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02503</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Particle-based pedestrian path prediction using lstm-mdl models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Hug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05546</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Social ways: Learning multi-modal distributions of pedestrian trajectories with gans. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javad</forename><surname>Amirian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bernard</forename><surname>Hayet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Pettré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets. CoRR, abs/1411.1784</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
