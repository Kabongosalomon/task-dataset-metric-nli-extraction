<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Self-Learning From Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
							<email>jiangfanhan@link.</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Self-Learning From Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ConvNets achieve good results when training from clean data, but learning from noisy labels significantly degrades performances and remains challenging. Unlike previous works constrained by many conditions, making them infeasible to real noisy cases, this work presents a novel deep self-learning framework to train a robust network on the real noisy datasets without extra supervision. The proposed approach has several appealing benefits. (1) Different from most existing work, it does not rely on any assumption on the distribution of the noisy labels, making it robust to real noises. (2) It does not need extra clean supervision or accessorial network to help training. (3) A self-learning framework is proposed to train the network in an iterative end-to-end manner, which is effective and efficient. Extensive experiments in challenging benchmarks such as Cloth-ing1M and Food101-N show that our approach outperforms its counterparts in all empirical settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks (DNNs) achieve impressive results on many computer vision tasks such as image recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, semantic segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24]</ref>, object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref> and cross modality tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>. However, many of these tasks require large-scale datasets with reliable and clean annotations to train DNNs such as ImageNet <ref type="bibr" target="#b1">[2]</ref> and MS-COCO <ref type="bibr" target="#b18">[19]</ref>. But collecting large-scale datasets with precise annotations is expensive and time-consuming, preventing DNNs from being employed in real-world noisy scenarios. Moreover, most of the "ground truth annotations" are from human labelers, who also make mistakes and increase biases of the data.</p><p>An alternative solution is to collect data from the Internet by using different image-level tags as queries. These tags can be regarded as labels of the collected images. This solution is cheaper and more time-efficient than human annotations, but the collected labels may contain noises. A lot of previous work has shown that noisy labels lead to an obvious decrease in performance of DNNs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. Therefore, attentions have been concentrated on how to im-  prove the robustness of DNNs against noisy labels.</p><p>Previous approaches tried to correct the noisy labels by introducing a transition matrix <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9]</ref> into their loss functions, or by adding additional layers to estimate the noises <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>. Most of these methods followed a simple assumption to simplify the problem: There is a single transition probability between the noisy label and ground-truth label, and this probability is independent of individual samples. But in real cases, the appearance of each sample has much influence on whether it can be misclassified. Due to this assumption, although these methods worked well on hand-crafted noisy datasets such as CIFAR10 <ref type="bibr" target="#b11">[12]</ref> with manually flipped noisy labels, their performances were limited on real noisy datasets such as Clothing1M <ref type="bibr" target="#b37">[38]</ref> and Food101-N <ref type="bibr" target="#b14">[15]</ref>.</p><p>Also, noisy tolerance loss functions <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b38">39]</ref> have been developed to fight against label noises, but they had a similar assumption as the above noise correction approaches. So they were also infeasible for real-world noisy datasets. Furthermore, many approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref> solved this prob-lem by using additional supervision. For instance, some of them manually selected a part of samples and asked human labelers to clean these noisy labels. By using extra supervision, these methods could improve the robustness of deep networks against noises. The main drawback of these approaches was that they required extra clean samples, making them expensive to apply in large-scale real-world scenarios.</p><p>Among all the above work, CleanNet <ref type="bibr" target="#b14">[15]</ref> achieved the existing state-of-the-art performance on real-world dataset such as Clothing1M <ref type="bibr" target="#b37">[38]</ref>. CleanNet used "class prototype" (i.e. a representative sample) to represent each class category and decided whether the label for a sample is correct or not by comparing with the prototype. However, CleanNet also needed additional information or supervision to train.</p><p>To address the above issues, we propose a novel framework of Self-Learning with Multi-Prototypes (SMP), which aims to train a robust network on the real noisy dataset without extra supervision. By observing the characteristics of samples in the same noisy category, we conjecture that these samples have widely spread distribution. A single class prototype is hard to represent all characteristics of a category. More prototypes should be used to get a better representation of characteristics. <ref type="figure" target="#fig_1">Figure 1</ref> illustrated the case and further exploration has been conducted in the experiment. Furthermore, extra information (supervision) is not necessarily available in practice.</p><p>The proposed SMP trains in an iterative manner which contains two phases: the first phase is to train a network with the original noisy label and corrected label generated in the second phase. The second phase uses the network trained in the first stage to select several prototypes. These prototypes are used to generate the corrected label for the first stage. This framework does not rely on any assumption on the distribution of noises, which makes it feasible to real-world noises. It also does not use accessorial neural networks nor require additional supervision, providing an effective and efficient training scheme.</p><p>The contributions of this work are summarized as follows. (1) We propose an iterative learning framework SMP to relabel the noisy samples and train ConvNet on the real noisy dataset, without using extra clean supervision. Both the relabeling and training phases contain only one single ConvNet that can be shared across different stages, making SMP effective and efficient to train. (2) SMP results in interesting findings for learning from noisy data. For example, unlike previous work <ref type="bibr" target="#b14">[15]</ref>, we show that a single prototype may not be sufficient to represent a noisy class. By extracting multiple prototypes for a category, we demonstrate that more prototypes would get a better representation of a class and obtain better label-correction results. <ref type="bibr" target="#b2">(3)</ref> Extensive experiments validate the effectiveness of SMP on different real-world noisy datasets. We demonstrate new state-of-the-art performance on all these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning on noisy data. ConvNets achieved great successes when training with clean data. However, the performances of ConvNets degraded inevitably when training on the data with noisy labels <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>. The annotations provided by human labelers on websites such as Amazon Mechanical Turk <ref type="bibr" target="#b9">[10]</ref> would also introduce biases and incorrect labels. As annotating large-scale clean and unbias dataset is expensive and time-consuming, many efforts have been made to improve the robustness of ConvNets trained on noisy datasets. They can be generally summarized as three parts mentioned below.</p><p>First, the transition matrix was widely used to capture the transition probability between the noisy label and true label, i.e. the sample with a true label y has a certain probability to be mislabeled as a noisy labelỹ. Sukhbaatar et al. in <ref type="bibr" target="#b31">[32]</ref> added an extra linear layer to model the transition relationships between true and corrupted labels. Patrini et al. in <ref type="bibr" target="#b24">[25]</ref> provided a loss correction method to estimate the transition matrix by using a deep network trained on the noisy dataset. The transition matrix was estimated by using a subset of cleanly labeled data in <ref type="bibr" target="#b8">[9]</ref>. The above methods followed an assumption that the transition probability is identical between classes and is irrelevant to individual images. Therefore, these methods worked well on the noisy dataset that is created intentionally by human with label flipping such as the noisy version of CIFAR10 <ref type="bibr" target="#b11">[12]</ref>. However, when applying these approaches to real-world datasets such as Clothing1M <ref type="bibr" target="#b37">[38]</ref>, their performances were limited since the assumption above is no longer valid.</p><p>Second, another scenario was to explore the robust loss function against label noises. <ref type="bibr" target="#b3">[4]</ref> explored the tolerance of different loss functions under uniform label noises. Zhang and Sabuncu <ref type="bibr" target="#b38">[39]</ref> found that the mean absolute loss function is more robust than the cross-entropy loss function, but it has other drawbacks. Then they proposed a new loss function that benefits both of them. However, these robust loss functions had certain constraints so that they did not perform well on real-world noisy datasets.</p><p>Third, CleanNet <ref type="bibr" target="#b14">[15]</ref> designed an additional network to decide whether a label is noisy or not. The weight of each sample during network training is produced by the Clean-Net to reduce the influence of noisy labels in optimization. Ren et al. <ref type="bibr" target="#b28">[29]</ref> and Li et al. <ref type="bibr" target="#b15">[16]</ref> tried to solve noisy label training by meta-learning. Some methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> based on curriculum learning were also developed to train against label noises. CNN-CRF model was proposed by Vahdat <ref type="bibr" target="#b35">[36]</ref> to represent the relationship between noisy and clean labels. However, most of these approaches either required extra clean samples as additional information or adopted a complicated training procedure. In contrast, SMP not only corrects noisy labels without using additional clean supervision but also trains the network in an efficient endto-end manner, achieving state-of-the-art performances on both Clothing1M <ref type="bibr" target="#b37">[38]</ref> and Food101-N <ref type="bibr" target="#b14">[15]</ref> benchmarks. When equipped with a few additional information, SMP further boosts the accuracies on these datasets.</p><p>Self-learning by pseudo-labels. Pseudo-labeling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14]</ref> belongs to the self-learning scenario, and it is often used in semi-supervised learning where the dataset has a few labeled data and most of the data are unlabeled. In this case, the pseudo-labels are given to the unlabeled data by using the predictions from the model pretrained on labeled data. In contrast, when learning from noisy datasets, all data have labels, but they may be incorrect. Reed et al. <ref type="bibr" target="#b27">[28]</ref> proposed to jointly train noisy and pseudo-labels. However, the method proposed in <ref type="bibr" target="#b27">[28]</ref> over-simplifies the assumption of the noisy distribution, leading to a sub-optimal result. Joint Optimization <ref type="bibr" target="#b34">[35]</ref> completely replaced all labels by using pseudo-labels. However, <ref type="bibr" target="#b34">[35]</ref> discarded the useful information in the original noisy labels. In this work, we predict the pseudo-labels by using SMP and train deep network by using both the original labels and pseudo-labels in a selflearning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><formula xml:id="formula_0">Overview. Let D be a noisily-labeled dataset, D = {X, Y } = {(x 1 , y 1 ), ..., (x N , y N )},</formula><p>which contains N samples, and y i ∈ {1, 2, ..., K} is the noisy label corresponding to the image x i . K is the number of classes in the dataset. Since the labels are noisy, they would be incorrect, impeding model training. To this end, a neural network F(θ) with parameter θ is defined to transform the image x to the label probability distribution F(θ, x). When training on a cleanly-labeled dataset, an optimization problem is defined as</p><formula xml:id="formula_1">θ * = argmin θ L(Y, F(θ, X))<label>(1)</label></formula><p>where L represents the empirical risk. However, when Y contains noises, the solution of the above equation would be sub-optimal. When label noises are presented, all previous work that improved the model robustness can be treated as adjusting the term in Eqn. <ref type="bibr" target="#b0">(1)</ref>. In this work, we propose to attain the corrected labelŶ (X, X s ) in a self-training manner, where X s indicates a set of class prototypes to represent the distribution of classes. Our optimization objective is formulated as</p><formula xml:id="formula_2">θ * = argmin θ L(Y,Ŷ (X, X s ), F(θ, X))<label>(2)</label></formula><p>Although the corrected labelŶ (X, X s ) is more precise than the original label Y , we believe that it is still likely to misclassify the hard samples as noises. So we keep the original noisy label Y as a part of supervision in the above objective function.</p><p>The corrected labelŷ i (x i , X s ) ∈Ŷ (X, X s ) of image x i is given by a similarity metric between the image x i and the set of prototypes X s . Since the data distribution of each category is complicated, a single prototype is hard to represent the distribution of the entire class. We claim that using multi-prototypes can get a better representation of the distribution, leading to better label correction.</p><p>In the following sections, we introduce the iterative selflearning framework in details, where a deep network learns from the original noisy dataset, and then it is trained to correct the noisy labels of images. The corrected labels will supervise the training process iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Iterative Self-Learning</head><p>Pipeline. The overall framework is illustrated in <ref type="figure">Figure</ref> 2. It contains two phases, the training phase, and the label-correction phase. In the training phase, a neural network F with parameters θ is trained, taking image x as input and producing the corresponding label prediction F(θ, x). The supervision signal is composed by two branches, (1) the original noisy label y corresponding to the image x and (2) the corrected labelŷ generated by the second phase of label correction.</p><p>In the label correction phase, we extract the deep features of the images in the training set by using the network G trained in the first stage. Then we explore a selection scheme to select several class prototypes for each class. Afterward, we correct the label for each sample according to the similarity of the deep features of the prototypes. The corrected labels are then used as a part of supervision in the first training phase. The first and the second phases proceed iteratively until the training converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Phase</head><p>The pipeline of the training phase is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> (a). This phase aims to optimize the parameters θ of the deep network F. In general, the objective function is the empirical risk of cross-entropy loss, which is formulated by</p><formula xml:id="formula_3">L(F(θ, x), y) = − 1 n n i=1 log(F(θ, x i ) yi )<label>(3)</label></formula><p>where n is the mini-batch size and y i is the label corresponding to the image x i . When learning on a noisy dataset, the original label y i may be incorrect, so we introduce another corrected label as a complementary supervision. The corrected label is produced by a self-training scheme in the label correction phase. With the corrected signal, the objective loss function is</p><formula xml:id="formula_4">L total = (1 − α)L(F(θ, x), y) + αL(F(θ, x),ŷ) (4)</formula><p>where L is the cross entropy loss as shown in Eqn. duced by the second phase. The weight factor α ∈ [0, 1] controls the important weight of the two terms.</p><p>Since the proposed approach does not require extra information (typically produced by using another deep network or additional clean supervision), at the very beginning of training, we set α to 0 and train the network F by using only the original noisy label y. After a preliminary network was trained, we can step into the second phase and obtain the corrected labelŷ. At this time, α is a positive value, where the network is trained jointly by y andŷ with the objective shown in Eqn. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Label Correction Phase</head><p>In the label correction phase, we aim to obtain a corrected label for each image in the training set. These corrected labels will be used to guide the training procedure for the first phase in turn.</p><p>For label correction, the first step is to select several class prototypes for each category. Inspired by the clustering method <ref type="bibr" target="#b30">[31]</ref>, we propose the following method to pick up these prototypes. (1) We use the preliminary network trained in the first phase to extract deep features of images in the training set. In experiments, we employ the ResNet <ref type="bibr" target="#b7">[8]</ref> architecture, where the output before the fullyconnected layer is regarded as the deep features, denoted as G(x). Therefore, the relationship between F(θ, x) and G(x) is F(θ, x) = f (G(x)), where f is the operation on the fully-connected layer of ResNet. <ref type="bibr" target="#b1">(2)</ref> In order to select the class prototypes for the c-th class, we extract a set of deep features, {G(x i )} n i=1 , corresponding to a set of images {x i } n i=1 in the dataset with the same noisy label c. Then, we calculate the cosine similarity between the deep features and construct a similarity matrix S ∈ R n×n , n is the number of images with noisy label c and S ij ∈ S with</p><formula xml:id="formula_5">S ij = G(x i ) T G(x j ) ||G(x i )|| 2 ||G(x j )|| 2<label>(5)</label></formula><p>Here S ij is a measurement of the similarity between two images x i and x j . Larger S ij indicates the two images with higher similarity. Both <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b6">[7]</ref> used Euclidean distance as the similarity measurement, but we find that cosine similarity is a better choice to correct the labels. The comparisons between the Euclidean distance and the cosine similarity are provided in experiment. An issue is that the number of images n in a single category is huge e.g. n =70k for Clothing1M, making the calculation of this cosine similarity matrix S time-consuming. Furthermore, latter calculation using such a huge matrix is also expensive. So we just randomly sample m images (m &lt; n) in the same class to calculate the similarity matrix S m×m to reduce the computational cost. To select prototypes, we define a density ρ i for each image x i ,</p><formula xml:id="formula_6">ρ i = m j=1 sign(S ij − S c )<label>(6)</label></formula><p>where sign(x) is the sign function 1 . The value of S c is a constant number given by the value of an element ranked top 40% in S where the values of elements in S are ranked in an ascending order from small to large. We find that the concrete choice of S c does not have influence in the final result, because we only need the relative density of images.</p><p>Discussions. From the above definition of density ρ, the image with larger ρ has more similar images around it. These images with correct labels should be close to each other, while the images with noisy labels are usually isolated from others. The probability density with ρ for images with correct label and images with the wrong label is shown in <ref type="figure" target="#fig_4">Figure 3</ref> (a). We can find the images with correct labels are more possible to have large ρ value while those images with wrong labels appear in the region with low ρ. In other words, the images with larger density ρ have a higher probability to have the correct label in the noisy dataset and can be treated as prototypes to represent this class. If we need p prototypes for a class, we can regard the images with the top-p highest density values as the class prototypes.</p><p>Nevertheless, the above strategy to choose prototypes has a weakness, that is, if the chosen p prototypes belonging to the same class are very close to each other, the representative ability of these p prototypes is equivalent to using just a single prototype. To avoid such case, we further define a similarity measurement η i for each image x i</p><formula xml:id="formula_7">η i = max j, ρj &gt;ρi S ij , ρ i &lt; ρ max min j S ij , ρ i = ρ max<label>(7)</label></formula><p>where ρ max = max{ρ 1 , ..., ρ m }. From the definition of η, we find that for the image x i with density value equaled to ρ max (ρ i = ρ max ), its similarity measure η i is the smallest. Otherwise, for those images x i with ρ i &lt; ρ max , the similarity η i is defined as the maximum of the cosine similarity between the image i with features G(x i ) and the other image j with features G(x j ), whose density value is higher than x i (ρ j &gt; ρ i ). From the above definitions, smaller similarity value η i indicates that the features corresponding to the image i are not too close the other images with density ρ larger than it. So, the sample with high-density value ρ (probability a clean label), and low similarity value η (a clean label but moderately far away from other clean labels) can fulfill our selection criterion as the class prototypes. In experiments, we find that the samples with high density ρ ranked the top often have relatively small similarity values η.</p><p>As shown in <ref type="figure" target="#fig_4">Figure 3</ref>   other. It also proves our claim that the samples in the same class tend to gather in several clusters, so a single prototype is hard to represent an entire class and therefore more prototypes are necessary. In experiments, we select the prototypes ranking in the top with η &lt; 0.95.</p><p>After the selection of prototypes for each class, we have a prototype set {G(X 1 ), ..., G(X c ), ..., G(X K )} (represented by deep features), where X c = {x c1 , ..., x cp } is the selected images for the c-th class, p is the number of prototypes for each class, and K is the number of classes in the dataset. Given an image x, we calculate the cosine similarity between extracted features G(x) and different sets of prototypes G(X c ). The similarity score σ c for the c-th class is calculated as</p><formula xml:id="formula_8">σ c = 1 p p l=1 cos(G(x), G(x cl )), c = 1...K<label>(8)</label></formula><p>where G(x cl ) is the l-th prototype for the c-th class. Here we use the average similarity over p prototypes, instead of the maximum similarity, because we find that combination (voting) from all the prototypes might prevent misclassifying some hard samples with almost the same high similarity to different classes. Then, we obtain the corrected label y ∈ {1, . . . , K} bŷ</p><formula xml:id="formula_9">y = argmax c σ c , c = 1...K<label>(9)</label></formula><p>After getting the corrected labelŷ, we treat it as complementary supervision signal to train the neural network F in the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Iterative Self-Learning</head><p>As shown in Algorithm 1, the training phase and the label correction phase proceed iteratively. The training phase first trains an initial network by using image x with noisy label y, as no corrected labelŷ provided. Then we proceed to the label correction phase. Feature extractor in this phase shares the same network parameters as the network F in if M &lt; start epoch then <ref type="bibr">4:</ref> sample (X, Y ) from training set. <ref type="bibr" target="#b4">5</ref>:</p><formula xml:id="formula_10">θ (t+1) ← θ (t) − ξ∇L(F(θ (t) , X), Y ) 6: else 7:</formula><p>Sample {x c1 , . . . , x cm } for each class label c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Extract the feature and calculate the similarity S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Calculate the density ρ and elect the class prototypes G(X c ) for each class c. sample (X, Y,Ŷ ) from training set. <ref type="bibr" target="#b11">12</ref>:</p><formula xml:id="formula_11">θ (t+1) ← θ (t) − ξ∇((1 − α)L(F(θ (t) , X), Y ) + αL(F(θ (t) , X),Ŷ ) 13:</formula><p>end if 14: end for the training phase. We randomly sample m images from the noisy dataset for each class and extract features by F, and then the prototype selection procedure selects p prototypes for each class. Corrected labelŷ is assigned to every image x by calculating the similarity between its features G(x) and the prototypes. This corrected labelŷ is then used to train the network F in the next epoch. The above procedure proceeds iteratively until converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We employ two challenging real-world noisy datasets to evaluate our approach, Clothing1M <ref type="bibr" target="#b37">[38]</ref> and Food101-N <ref type="bibr" target="#b14">[15]</ref>. (1) Clothing1M <ref type="bibr" target="#b37">[38]</ref> contains 1 million images of clothes, which are classified into 14 categories. The labels are generated by the surrounding text of the images on the Web, so they contain many noises. The accuracy of the noisy label is 61.54%. Clothing1M is partitioned into training, validation and testing sets, containing 50k, 14k and 10k images respectively. Human annotators are asked to clean a set of 25k labels as a clean set. In our approach, they are not required to use in training. (2) Food101-N <ref type="bibr" target="#b14">[15]</ref> is a dataset to classify food. It contains 101 classes with 310k images searched from the Web. The accuracy of the noisy label is 80%. It also provides 55k verification labels (clean by humans) in the training set.</p><p>Experimental Setup. For the Clothing1M dataset, we use ResNet50 pretrained on the ImageNet. The data preprocessing procedure includes resizing the image with a short edge of 256 and randomly cropping a 224×224 patch from the resized image. We use the SGD optimizer with a momentum of 0.9. The weight decay factor is 5 × 10 −3 , and the batchsize is 128. The initial learning rate is 0.002 and decreased by 10 every 5 epochs. The total training processes contain 15 epochs. In the label correction phase, we randomly sample 1280 images for each class in the noisy training set, and 8 class prototypes are picked out for each class. For the Food-101N, the learning rate decreases by 10 every 10 epochs, and there are 30 epochs in total. The other settings are the same as that of Clothing1M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Clothing1M</head><p>We adopt the following three settings by following previous work. First, only noisy dataset is used for training without using any extra clean supervision in the training process. Second, verification labels are provided, but they are not used to train the network directly. e.g. They are used to train the accessorial network as <ref type="bibr" target="#b14">[15]</ref> or to help select prototypes in our method. Third, both noisy dataset and 50k clean labels are available for training.</p><p>We compare the results in <ref type="table">Table 1</ref>. We see that in the first case, the proposed method outperforms the others by a large margin, e.g. improving the accuracy from 69.54% to 74.45%, better than Joint Optimization <ref type="bibr" target="#b34">[35]</ref> (#3) by 2.22% and MLNT-Teacher <ref type="bibr" target="#b15">[16]</ref> (#4) by 0.98%. Our result is even better than #6 and #7 which uses extra verification labels.</p><p>For the second case, Traditional Cross-Entropy is not suitable. <ref type="bibr" target="#b24">[25]</ref> used the information to estimate the transition matrix, while CleanNet <ref type="bibr" target="#b14">[15]</ref> used the verification labels to train an additional network to predict whether the label is noisy or not. Our method uses this information to select the class prototypes. In this case, we still achieve the best result compared to all methods.</p><p>For the third case, all data (both noisy and clean) can be used for training. All the methods first train a model on the noisy dataset and then the model is finetuned using vanilla cross-entropy loss on the clean dataset. We see that our method still outperforms the others. CurriculumNet <ref type="bibr" target="#b6">[7]</ref> provides a slightly better result (81.5%) in this case. But it uses a different backbone compared with all others, so we do not consider it. Among all of these cases, our approach obtains state-of-the-art performances compared to previous   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Label Correction Accuracy. We explore the classification accuracy in the label-correction phase. <ref type="table" target="#tab_2">Table 2</ref> lists the overall accuracy in the original noisy set: the accuracy of the corrected label in the initial iterative cycle (i.e. the first time we step into the label correction phase after training the preliminary model), and accuracy of the corrected label by the final model at the end of training <ref type="bibr">(Final)</ref>. We see that the accuracy after the initial cycle already reaches 74.38%, improving the original accuracy by 12.64% (61.74% vs. 74.38%). The accuracy is further improved to 77.36% at the end of the training.</p><p>We further explore the classification accuracy for different classes as shown in <ref type="figure" target="#fig_7">Figure 4 (a)</ref>. We can find that for the most classes with the original accuracies lower than 50%, our method can improve the accuracy to higher than 60%. Even for the 5th class ("Sweater") with about 30% original accuracy, our method still improves the accuracy by 10%. Some of the noisy samples successfully corrected by our approach are shown in <ref type="figure" target="#fig_8">Figure 5</ref>.</p><p>Number of Class Prototypes p. The number of class prototypes is the key to the representation ability to a class. When p = 1, the case is similar to CleanNet <ref type="bibr" target="#b14">[15]</ref>. In our method, we use p ≥ 1. Another difference is that CleanNet attained the prototype by training an additional network. But we just need to select images as prototypes by their</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hoodie Jacket</head><p>Shirt T-Shirt Jacket Hoodie Jacket Suit</p><p>Apple pie Cup cake Edamame Dumpling Churros Ice cream Sashimi Sushi density and similarity according to the data distribution. <ref type="figure" target="#fig_7">Figure 4 (b)</ref> shows the effect of changing the number of prototypes for each class. We select five p values and evaluate the final test accuracy trained by either only using 1M noisy data or adding 25k verification information, as shown by the solid lines. To have a better observation of the influence caused by p value, we evaluate the label correction rate by the model step into the first label correction phase, which is similar to the correction accuracy discussed in the last experiment. But this time we evaluate on the testing set. This metric is easy to be evaluated, so we explore 10 p values from 1 to 10, as shown in the dotted line. When comparing these two settings, they follow the same trend.</p><p>From the result, we find that when p = 1 i.e. one prototype for each class, the accuracies are sub-optimal compared to others. When using more prototypes, the performance improves a lot, e.g. the accuracy using two prototypes outperforms using a single one by 2.04% This also proves our claim that a single prototype is not enough to represent the distribution of a class. Multiple prototypes provide more comprehensive representation to the class.</p><p>Weight factor α. Weight factor α plays an important role in the training procedure, which decides the network will concentrate on the original noisy labels Y or on the  <ref type="table">Table 3</ref>. The classification accuracy (%) on Clothing1M with different number of samples used to select prototypes for each class. Final denotes the accuracy get by the model at the end of training. Initial denotes the correct accuracy by the model just step into the first label correction phase. corrected labelsŶ . If α = 0, the network is trained by using only noisy labels without correction. Another extreme case is when α = 1, the training procedure discards the original noisy labels and only depends on the corrected labels. We study the influence of different α ranging from 0.0 to 1.0 and the test accuracy with different α is shown in <ref type="figure" target="#fig_7">Figure 4</ref> (c).</p><p>From the result, we find that training using only the noisy label Y i.e. α = 0 leads to poor performance. Although the corrected label is more precise, the model trained using only corrected labelŶ also performs sub-optimal. The model jointly trained by using the original noisy label Y and the corrected labelŶ achieves the best performance when α = 0.5. The accuracy curve also proves our claim that label correction may misrecognize some hard samples as noises. Directly replacing all noisy labels with the corrected ones would make the network focused on simple features and thus degrade the generalization ability.</p><p>Number of Samples m. To avoid massive calculation related to the similarity matrix S, we randomly select m images rather than using all of the images in the same class to compute the similarity matrix. We examine how many samples are enough to select the class prototypes to represent the class distribution well. We explore the influence of the images number m for each class.</p><p>The results are listed in <ref type="table">Table 3</ref>. Experiment setting is similar to the experiments above to study the number of class prototypes. The models are trained on the noisy dataset as well as the noisy dataset plus extra verification labels respectively. The results are the accuracy of the trained model on the test set. Besides evaluating the classification accuracy got by the final model, we also examine the correction accuracy of the model just step into the first label correction phase, which is denoted by "Initial" in the table. By analyzing the results in different cases, we see that the performance is not sensitive to the number of images m. Compared with 70k training images in Clothing1M for each class, we merely sample 2% of them and obtain the class prototypes to represent the distribution of the class well.</p><p>Prototype Selection. To explore the influence of the method used to select prototypes, we also use two other clustering methods to get the prototypes. One is the density peak by Euclidean distance <ref type="bibr" target="#b30">[31]</ref>, while the other is the Method Data Accuracy K-means++ <ref type="bibr" target="#b0">[1]</ref> 1M noisy 74.08 Density peak Euc. <ref type="bibr" target="#b30">[31]</ref> 1M noisy 74.11 Ours 1M noisy 74.45 K-means++ <ref type="bibr" target="#b0">[1]</ref> 1M noisy + 25k verify 76.22 Density peak Euc. <ref type="bibr">[</ref>  <ref type="bibr" target="#b14">[15]</ref> 83.47 3 CleanNet w sof t <ref type="bibr" target="#b14">[15]</ref> 83.95 4</p><p>Ours 85.11 widely used K-means algorithm, that is, K-means++ <ref type="bibr" target="#b0">[1]</ref>. The prototypes attained by all the methods are used to produce the corrected labels for training. Results are listed in <ref type="table" target="#tab_4">Table 4</ref>. We see that the method used to generate prototypes does not largely impact the accuracy, implying that our framework is not sensitive to the clustering method. But the selection method proposed in this work still performs better than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Food-101N</head><p>We also evaluate our method on the Food-101N <ref type="bibr" target="#b14">[15]</ref> dataset. The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. We find that our method also achieves state-of-the-art performance on Food-101N, outperforming CleanNet [15] by 1.16%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an iterative self-learning framework for learning on the real noisy dataset. We prove that a single prototype is insufficient to represent the distribution of a class and multi-prototypes are necessary. We also verify our claim that original noisy labels are helpful in the training procedure although the corrected labels are more precise. By correcting the label using several class prototypes and training the network jointly using the corrected and original noisy iteratively, this work provides an effective end-to-end training framework without using an accessorial network or adding extra supervision on a real noisy dataset. We evaluate the methods on different real noisy datasets and obtain state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>An example of solving two classes classification problem using different number of prototypes. Left: Original data distribution. Data points with the same color belong to the same class. Upper Right: The decision boundary obtained by using a single prototype for each class. Lower Right: The decision boundary obtained by two prototypes for each class. Two prototypes for each class leads to a better decision boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc><ref type="bibr" target="#b2">(3)</ref>, y is the original noisy label, andŷ is the corrected label pro-Illustration of the pipeline of iterative self-learning framework on the noisy dataset. (a) shows the training phase and (b) shows the label correction phase, where these two phases proceed iteratively. The deep network G can be shared, such that only a single model needs to be evaluated in testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b), red dots are samples with density ρ ranked in the top. Over 80% of the samples have η &gt; 0.9 and half of the samples have η &gt; 0.95. So those red dots have relative small η value and far away from each 1 We have sign(x) = 1 if x &gt; 0; sign(x) = 0 if x = 0; otherwise sign(x) = −1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>(a) The probability density with the density ρ for sample with correct label (blue line) and sample with wrong label (green line) for 1280 images sampled from the same noisy class in Cloth-ing1M dataset. (b) The distribution between similarity η and density ρ. the samples are the same as (a). Red dots are samples with top-8 highest ρ value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 Learning 1 :</head><label>11</label><figDesc>Iterative Initialize network parameter θ 2: for M = 1 : num epochs do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 :</head><label>10</label><figDesc>Get the correctedŷ for each sample x i 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>(a) The label accuracy (%) of labels in the original dataset (Original), labels corrected by the label correction phase in the first iterative cycle (Correct Initial) and labels corrected by the model at the end of training (Correct Final) for each class in Clothing1M. (b) Testing accuracy (%) with the number of prototypes p ranging from 1 to 10 for each class. The solid line denotes the accuracy got by the model at the end of training (Final). The dotted line denotes the correct accuracy by the model just step into the label correction phase for the first time (Initial). Noisy is the result of the training from noisy dataset only, noisy+verify indicates additional verification information is used. (c) Testing accuracy (%) with weight factor α ranging from 0.0 to 1.0. Noisy and noisy+verify have the same meaning as (b). Original Correct Initial Correct Final Accuracy 61.74 74.38 77.36</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Samples corrected by our method. Left: The original noisy label. Right: The right label corrected by our method. The first row from Clothing1M and the second row from Food101-N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Overall label accuracy (%) of the labels in original noisy</cell></row><row><cell>dataset (Original), accuracy of the corrected label generated by the</cell></row><row><cell>label correction phase in first iterative cycle (Correct Initial) and</cell></row><row><cell>accuracy of the corrected labels generated by the final model when</cell></row><row><cell>training ends (Correct Final).</cell></row><row><cell>methods, showing that our method is effective and suitable</cell></row><row><cell>for board situations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Final) 74.37 74.07 74.45 74.27 1M noisy (Initial) 72.04 72.03 72.09 72.05 1M noisy + 25k verify (Final) 76.43 76.49 76.44 76.55 1M noisy + 25k verify (Initial) 74.09 73.97 74.17 74.21</figDesc><table><row><cell>m</cell><cell>320</cell><cell>640 1280 2560</cell></row><row><cell>1M noisy (</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The classification accuracy (%) on Clothing1M with different cluster methods used to select the prototypes.</figDesc><table><row><cell></cell><cell cols="2">31] 1M noisy + 25k verify</cell><cell>76.05</cell></row><row><cell>Ours</cell><cell cols="2">1M noisy + 25k verify</cell><cell>76.44</cell></row><row><cell>#</cell><cell>Method</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell>Cross Entropy</cell><cell>84.51</cell></row><row><cell cols="2">2 CleanNet w hard</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The classification accuracy (%) on Food-101N compare with other methods.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported in part by SenseTime Group Limited, in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants CUHK14202217, CUHK14203118, CUHK14205615, CUHK14207814, CUHK14213616.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the eighteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deliang Fan, and Boqing Gong. A semi-supervised two-stage approach to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making risk minimization tolerant to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10477" to="10486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quality management on amazon mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panagiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD workshop on human computation</title>
		<meeting>the ACM SIGKDD workshop on human computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Show, tell and discriminate: Image captioning by self-retrieval with partially labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="338" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving referring expression grounding with cross-modal attention-guided erasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1950" to="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A study of the effect of different types of noise on the precision of supervised learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>David F Nettleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Orriols-Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fornells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="306" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Class noise and supervised learning in medical domains: The effect of feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tsymbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seppo</forename><surname>Puuronen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th IEEE Symposium on Computer-Based Medical Systems (CBMS&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="708" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<title level="m">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="issue">6191</biblScope>
			<biblScope unit="page" from="1492" to="1496" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5596" to="5605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from noisy largescale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8792" to="8802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9299" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
