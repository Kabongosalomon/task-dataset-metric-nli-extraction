<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SegFlow: Joint Learning for Video Object Segmentation and Optical Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
							<email>1chengjingchun@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SegFlow: Joint Learning for Video Object Segmentation and Optical Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes an end-to-end trainable network, SegFlow, for simultaneously predicting pixel-wise object segmentation and optical flow in videos. The proposed SegFlow has two branches where useful information of object segmentation and optical flow is propagated bidirectionally in a unified framework. The segmentation branch is based on a fully convolutional network, which has been proved effective in image segmentation task, and the optical flow branch takes advantage of the FlowNet model. The unified framework is trained iteratively offline to learn a generic notion, and fine-tuned online for specific objects. Extensive experiments on both the video object segmentation and optical flow datasets demonstrate that introducing optical flow improves the performance of segmentation and vice versa, against the state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video analysis has attracted much attention in recent years due to the numerous vision applications, such as autonomous driving <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref>, video surveillance <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref> and virtual reality <ref type="bibr" target="#b0">[1]</ref>. To understand the video contents for vision tasks, it is essential to know the object status (e.g., location and segmentation) and motion information (e.g., optical flow). In this paper, we address these problems simultaneously, i.e., video object segmentation and optical flow estimation, in which these two tasks have been known to be closely related to each other <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b34">35]</ref>. <ref type="figure">Figure 1</ref> illustrates the main idea of this paper.</p><p>For video object segmentation <ref type="bibr" target="#b24">[25]</ref>, it assumes that the object mask is known in the first frame, and the goal is to assign pixel-wise foreground/background labels through the entire video. To maintain temporally connected object segmentation, optical flow is typically used to improve the smoothness across the time <ref type="bibr" target="#b27">[28]</ref>. However, flow estimation itself is a challenging problem and is often inaccurate, and thus the provided information does not always help segmentation. For instance, when an object moves fast, the op- <ref type="figure">Figure 1</ref>. An illustration of the main idea in the proposed SegFlow model. Our model produces better segmentation results than the one without using the optical flow (Ours-flo), where the flow within the object is smooth and complete, providing a guidance to improve segmentation outputs. tical flow methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37]</ref> are not effective in capturing the movement and hence generate incomplete flow within the object (see <ref type="figure" target="#fig_1">Figure 4</ref> for an example). To overcome this problem, bringing the objectness information (i.e., segmentation) can guide the algorithm to determine where the flow should be smooth (within the object). A few algorithms have been developed to leverage both information from the objectness and motion discussed above. In <ref type="bibr" target="#b40">[41]</ref>, a method is proposed to simultaneously perform object segmentation and flow estimation, and then updates both results iteratively. However, the entire process is optimized online and is time-consuming, which limits the applicability to others tasks.</p><p>Based on the above observations, we propose a learningbased approach to jointly predict object segmentation and optical flow in videos, which allows efficient inference during testing. We design a unified, end-to-end trainable convolutional neural network (CNN), which we refer to as the SegFlow, that contains one branch for object segmentation and another one for optical flow. For each branch, we learn the feature representations for each task, where the segmentation branch focuses on the objectness and the optical flow one exploits the motion information. To bridge two branches to help each other, we propagate the learned feature representations bi-directionally. As such, these features from one branch can facilitate the other branch while obtaining useful gradient information during backpropagation.</p><p>One contribution of the proposed network is the bidirectional architecture that enables the communication between two branches, whenever the two objectives of the branches are closely related and can be jointly optimized. To train this joint network, a large dataset with both ground truths of two tasks (i.e., foreground segmentation and optical flow in this paper) is required. However, such dataset may not exist or is difficult to construct. To relax such constrains, we develop an iterative training strategy that only requires one of the ground truths at a time, so that the target function can still be optimized and converge to a solution where both tasks achieve reasonable results.</p><p>To evaluate our proposed network, we carry out extensive experiments on both the video object segmentation and optical flow datasets. We compare results on the DAVIS segmentation benchmark <ref type="bibr" target="#b28">[29]</ref> with or without providing motion information, and evaluate the optical flow performance on the Sintel <ref type="bibr" target="#b5">[6]</ref>, Flying Chairs <ref type="bibr" target="#b11">[12]</ref> and Scene Flow <ref type="bibr" target="#b25">[26]</ref> datasets. In addition, analysis on the network convergence is presented to demonstrate our training strategy. We show that the bi-directional network through feature propagation performs favorably against state-of-the-art algorithms on both video object segmentation and optical flow tasks in terms of visual quality and accuracy.</p><p>The contributions of this work are summarized below:</p><p>• We propose an end-to-end trainable framework for simultaneously predicting pixel-wise foreground object segmentation and optical flow in videos. • We demonstrate that optical flow and video object segmentation tasks are complementary, and can help each other through feature propagation in a bi-directional framework. • We develop a method to train the proposed joint model without the need of a dataset that contains both segmentation and optical flow ground truths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Video Object Segmentation. Unsupervised methods aim to segment foreground objects without any knowledge of the object (e.g., an initial object mask). Several methods have been proposed to generate object segmentation via <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42]</ref>, optical flow <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref> or superpixel <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b12">13]</ref>. To incorporate higher level information such as objectness, object proposals are used to track object segments and generate consistent regions through the video <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. However, these methods usually require heavy computational loads to generate region proposals and associate thousands of segments, making such methods only applicable to offline applications.</p><p>Semi-supervised Video Object Segmentation. Semisupervised methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b26">27]</ref> assume an object mask in the first frame is known, and track this object mask through the video. To achieve this, existing approaches focus on propagating superpixels <ref type="bibr" target="#b18">[19]</ref>, constructing graphical models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">41]</ref> or utilizing object proposals <ref type="bibr" target="#b29">[30]</ref>. Recently, CNN based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7]</ref> are developed by combining offline and online training processes on static images. Although significant performance has been achieved, the segmentation results are not guaranteed to be smooth in the temporal domain. In this paper, we use CNNs to jointly estimate optical flow and provide the learned motion representations to generate consistent segmentations across time.</p><p>Optical Flow. It is common to apply optical flow to video object segmentation to maintain motion consistency. One category of the approaches is to solve a variational energy minimization problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37]</ref> in a coarse-to-fine scheme.</p><p>To better determine the correspondences between images, matching based optimization algorithms <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b31">32]</ref> are developed, in which these methods usually require longer processing time. On the other hand, learning based methods are more efficient, which can be achieved via Gaussian mixture models <ref type="bibr" target="#b33">[34]</ref>, principle components <ref type="bibr" target="#b43">[44]</ref> or convolutional networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. Considering the efficiency and accuracy, we apply the FlowNet <ref type="bibr" target="#b11">[12]</ref> as our baseline in this work, while we propose to improve optical flow results by feeding the information from the segmentation network as guidance, which is not studied by the above approaches.</p><p>Fusion Methods. The joint problem of video segmentation and flow estimation has been studied by layered models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref>. Nevertheless, such methods rely on complicated optimization during inference, thereby limiting their applications. Recently, significant efforts have been made along the direction of video object segmentation while considering optical flow. In <ref type="bibr" target="#b20">[21]</ref>, a network that uses pre-computed optical flow as an additional input to improve segmentation results is developed. Different from this work, our model only requires images as the input, and we aim to jointly learn useful motion representations to help segmentation. Closest in scope to our work is the ObjectFlow algorithm (OFL) <ref type="bibr" target="#b40">[41]</ref> that formulates an objective function to iteratively optimize segmentation and optical flow energy functions. However, this method is optimized online and is thus computationally expensive. In addition, it requires the segmentation results before updating the estimation for optical flow. In contrast, we propose an end-to-end trainable framework for simultaneously predicting pixel-wise foreground object segmentation and optical flow. <ref type="figure">Figure 2</ref>. The proposed SegFlow architecture. Our model consists of two branches, the segmentation network based on a fullyconvolutional ResNet-101 and the flow branch using the FlowNetS <ref type="bibr" target="#b11">[12]</ref> structure. In order to construct communications between two branches, we design an architecture that bridges two networks during the up-sampling stage. Specifically, feature maps are propagated bi-directionally through concatenations at different scales with proper operations (i.e., up-sampling or down-sampling) to match the size of different features. Then an iterative training scheme is adopted to jointly optimize the loss functions for both segmentation and optical flow tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SegFlow</head><p>Our goal is to segment objects in videos, as well as estimate the optical flow between frames. Towards this end, we construct a unified model with two branches, a segmentation branch based on fully-convolutional network, and an optical flow branch based on the FlowNetS <ref type="bibr" target="#b11">[12]</ref>.</p><p>Due to the lack of datasets with both segmentation and optical flow annotations, we initialize the weights of two branches from legacy models trained on different datasets, and optimize the SegFlow on segmentation and optical flow datasets via iterative offline training and online finetuning. In the following, we first introduce the baseline model of each the segmentation and optical flow branch, and explain how we construct the joint model using the proposed bidirectional architecture. The overall architecture of our proposed joint model is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Segmentation Branch</head><p>Inspired by the effectiveness of fully-convolutional networks in image segmentation <ref type="bibr" target="#b23">[24]</ref> and the deep structure in image classification <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>, we construct our segmentation branch based on the ResNet-101 architecture <ref type="bibr" target="#b17">[18]</ref>, but modified for binary (foreground and background) segmentation predictions as follows: 1) the fully-connected layer for classification is removed, and 2) features of convolution modules in different levels are fused together for obtaining more details during up-sampling.</p><p>The ResNet-101 has five convolution modules, and each consists of several convolutional layers, Relu, skip links and pooling operations after the module. Specifically, we draw feature maps from the 3-th to 5-th convolution modules after pooling operations, where score maps are with sizes of 1/8, 1/16, 1/32 of the input image size, respectively. Then these score maps are up-sampled and summed together for predicting the final output (upper branch in <ref type="figure">Figure 2</ref>).</p><p>A pixel-wise cross-entropy loss with the softmax function E is used during optimization. To overcome imbalanced pixel numbers between foreground and background regions, we use the weighted version as adopted in <ref type="bibr" target="#b44">[45]</ref>, and the loss function is defined as:</p><formula xml:id="formula_0">L s (X t ) = −(1 − w) i,j∈f g log E(y ij = 1; θ) −w i,j∈bg log E(y ij = 0; θ),<label>(1)</label></formula><p>where i, j denotes the pixel location of foreground f g and background bg, y ij denotes the binary prediction of each pixel of the input image X at frame t, and w is computed as the foreground-background pixel-number ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optical Flow Branch</head><p>Considering the efficiency and accuracy, we choose the FlowNetS <ref type="bibr" target="#b11">[12]</ref> as our baseline for flow estimation. The optical flow branch uses an encoder-decoder architecture with additional skip links for feature fusions (feature concatenations between the encoder and decoder). In addition, a down-scaling operation is used at each step of the encoder, where each step of the decoder up-samples back the output (see the lower branch in <ref type="figure">Figure 2</ref>). Based on such structure, we find that it shares similar properties with the segmentation branch and their feature representations are in similar scales, which enables plausible connections to the segmentation model, and vice versa, where we will introduce in the next section.</p><p>To optimize the network, the optical flow branch uses an endpoint error (EPE) loss as adopted in <ref type="bibr" target="#b11">[12]</ref>, which is defined as the following:</p><formula xml:id="formula_1">L f (X t , X t+1 ) = i,j ((u ij − δ uij ) 2 + (v ij − δ vij ) 2 ), (2)</formula><p>where u ij , v ij denotes the motion at pixel (i, j) of input images from X t to X t+1 , and δ uij and δ vij are network predictions. We use the images at frame t and t + 1 as the computed optical flow should align with the segmentation output (e.g., object boundaries) at frame t, so that their information can be combined later naturally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bi-directional Model</head><p>In order to make communications between two branches as mentioned above, we propose a unified structure, SegFlow, to jointly predict segmentation and optical flow outputs. Therefore, the new optimization goal becomes to solve the following loss function that combines <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula">(2)</ref>: L(X) = L s (X) + λL f (X). As shown in <ref type="figure">Figure 2</ref>, our architecture propagates feature maps between two branches bi-directionally at different scales for the final prediction. For instance, features from each convolution module in the segmentation branch are first up-scaled (to match the size of optical flow features), and then concatenated to the optical flow branch. Similar operations are adopted when propagating features from segmentation to flow. Note that, a convolutional layer is also utilized (with the channel number equal to the output channel number) after fused features for network predictions, further regularizing the information from both the segmentation and optical flow branches.</p><p>Different from directly using final outputs to help both tasks <ref type="bibr" target="#b40">[41]</ref>, we here utilize information in the feature space. One reason is that our network is able to learn useful feature representations (e.g., objectness and motion) at different scales. In addition, with the increased model capacity but without adding too much burden for training the network, the joint model learns better representations than the single branch. For instance, the single flow network does not have the ability to learn representations similar to the segmentation branch, while our model provides the chance for two tasks sharing their representations. Note that, our bi-directional model is not limited to the current architecture or tasks, while it should be a generalized framework that can be applied to co-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network Implementation and Training</head><p>In this section, we present more details regarding how we train the proposed network. To successfully train the joint model, a large-scale dataset with both the segmentation and optical flow ground truths is required. However, it is not feasible to construct such a dataset. Instead, we develop a training procedure that only needs one of the ground truths at a time by iteratively updating both branches and gradually optimizing the target function. In addition, a data augmentation strategy is described for both tasks to enhance the diversity of data distribution and match the need of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Optimization</head><p>First, we learn a generic model by iteratively updating both branches, where the goal of the segmentation network at this stage is to segment moving objects. To focus on a certain object (using the mask in the first frame), we then finetune the model for the segmentation branch on each sequence of the DAVIS dataset for online processing.</p><p>Iterative Offline Training. To start training the joint model, we initialize two branches using the weights from ResNet-101 <ref type="bibr" target="#b17">[18]</ref> and FlowNetS <ref type="bibr" target="#b11">[12]</ref>, respectively. When optimizing the segmentation branch, we freeze the weights of the optical flow branch, and train the network on the DAVIS training set. We use SGD optimizer with batch size 1 for training, starting from learning rate 1e-8 and decreasing it by half for every 10000 iterations.</p><p>For training the optical flow branch, similarly we fix the segmentation branch and only update the weights in the flow network using the target optical flow dataset (described in Section 5.1). To balance the weights between two different losses, we use a smaller learning rate 1e-9 for the EPE loss in (2), addressing the λ in the combined loss in Section 3.3. Note that, to decide when to switch the training process to another branch, we randomly split a validation set and stop training the current branch when the error on the validation set reaches a convergence. In addition, this validation set is also used to select the best model with respect to the iteration number <ref type="bibr" target="#b11">[12]</ref>.</p><p>For this iterative learning process, each time the network focuses on one task in a branch, while obtaining useful representations from another branch through feature propagation. Then after switching to train another branch, better Given an input image, we show the flow estimation from the initial model, FlowNetS <ref type="bibr" target="#b11">[12]</ref>, and our results during optimizing the SegFlow in the first and the second round. The results are gradually improved during optimization.</p><p>features learned from the previous stage are used in the branch currently optimized. We show one example of how the network gradually move toward a convergence by iteratively training both branches in <ref type="figure" target="#fig_0">Figure 3</ref> (with three rounds).</p><p>In addition, <ref type="figure" target="#fig_1">Figure 4</ref> shows visual improvements during iteratively updating the flow estimation.</p><p>Online Training for Segmentation. The model trained offline is able to separate moving object from the video. To adapt the model on a specific object for online processing, we finetune the segmentation network using the object mask in the first frame on each individual sequence. Here, we call the process online in the semi-supervised setting, as the model is needed to update with the guidance of mask in the first frame before testing on the sequence.</p><p>Each mask is then augmented to multiple training samples for both branches to increase the data diversity (described in Section 4.2). After data augmentation, we use the same training strategy introduced in the offline stage with a fixed learning rate of 1e-10. At this stage, we note that the flow branch still provides motion representations to segmentation, but does not update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Augmentation</head><p>Segmentation We use the pre-defined training set of the DAVIS benchmark <ref type="bibr" target="#b28">[29]</ref> to train the segmentation branch. Since this training set is relatively small, we adopt affine transformations (i.e., shifting, rotation, flip) to generate one thousands samples for each frame. Since the flow branch requires two adjacent frames as the input, each affine transformation is carried out through the entire sequence to maintain the inter-frame (temporal) consistency during training (see <ref type="figure">Figure 5</ref> for an example).</p><p>Optical Flow. The flow data during offline training step is generated as the approach described for segmentation. However, when training the online model using the first <ref type="figure">Figure 5</ref>. Examples for data augmentation. The first row shows the data augmentation for segmentation with the same transform through the video for maintaining the temporal consistency. The second row presents one example of the augmented flow, where the transform is applied on the object mask to simulate the slight movement in the "next frame" (highlighted within the red rectangle), where the optical flow shows the corresponding transform. frame of a test set video, we have no access to its next frame. To solve this problem, we present an optical flow data augmentation strategy. First, we augment the first frame with the transformed method used in segmentation. Then, based on each image and its object mask, we simulate an object movement by slightly deforming the foreground object region to generate a synthesized "next frame". Since we only focus on the specific object at this online stage, the missing area caused by the object movement can be treated as occlusions and is left as empty (black) area. We find this synthesized strategy is effective for training without harming the network property (see <ref type="figure">Figure 5</ref> for an example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>We present the main experimental results with comparisons to the state-of-the-art video object segmentation and optical flow methods. More results and videos can be found in the supplementary material. The code and model are available at https://github.com/ JingchunCheng/SegFlow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset and Evaluation Metrics</head><p>The DAVIS benchmark <ref type="bibr" target="#b28">[29]</ref> is a recently-released highquality video object segmentation dataset that consists of 50 sequences and 3455 annotated frames of real-world moving objects. The videos in DAVIS are also categorized according to various attributes, such as background clutter (BC), deformation (DEF), motion blur (MB), fast motion (FM), low resolution (LR), occlusion (OCC), out-of-view (OV), scale-variation (SV), appearance change (AC), edge ambiguity (EA), camera shake (CS), heterogeneous objects (HO), interesting objects (IO), dynamic background (DB), shape complexity (SC), as shown in <ref type="figure">Figure 6</ref>. We use the pre-defined training set to optimize our framework and its validation set to evaluate the segmentation quality.</p><p>For optical flow, we first use the MPI Sintel dataset <ref type="bibr" target="#b5">[6]</ref> that contains 1041 pairs of images in synthesized scenes, with a Clean version containing images without motion blur and atmospheric effects, and a F inal version of images with complicated environment variables. Second, we use the KITTI dataset <ref type="bibr" target="#b15">[16]</ref>, which has 389 pairs of flow images for real-world driving scenes. Finally, we use the Scene Flow dataset <ref type="bibr" target="#b25">[26]</ref>, which is a large-scale synthesized dataset recently established for flow estimation. Considering the realism, we use two subsets, Monkaa and Driving, where Monkaa has a collection of 24 video sequences with more than 34000 annotations for optical flow, and Driving has 8 videos with around 17000 annotations. Similar to Sintel, Driving and Monkaa both provide two versions: Clean with clear images and F inal with more realistic ones.</p><p>Since the exact training and test sets are not specified in the Scene Flow dataset, we split our own sets for comparisons (training and validation sets do not intersect). For Monkaa, we use three videos (eating × 2, f lower storm × 2, lonetree × 2) as the validation set, and use the rest of 21 sequences for training. For Driving, 7 videos are selected for training, and use the one with the attribute of 15mm f ocallength, scene f orwards and f ast for testing. Note that, every video in both Monkaa and Driving has two views of lef t and right, which results in 63400 training and 5720 validation pairs on Monkaa, and 32744 training and 2392 validation pairs on Driving.</p><p>To evaluate the segmentation quality, we use three measures (evaluation code from DAVIS website <ref type="bibr" target="#b28">[29]</ref>): region similarity J, contour accuracy F and temporal stability T . For optical flow, we compute the average endpoint error from every pixel for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study on Segmentation</head><p>To analyze the necessity and importance of each step in the proposed framework, we carry out extensive ablation studies on DAVIS, and summarize the results in <ref type="table">Table 1</ref>. We validate our method by comparing the proposed SegFlow to the ones without online training (-ol), iterative training (-it), offline training (-of) and flow branch (-flo). The detailed settings are as follows: -ol: only uses the offline training without the supervised information in the first frame, which is categorized as unsupervised video object segmentation.</p><p>-it: only trains the model once for each of the segmentation and optical flow branches.</p><p>-of: trains the model directly on the testing video with the object mask in the first frame and its augmentations.</p><p>-flo: only uses the segmentation branch without the feature propagation from the flow network. <ref type="table">Table 1</ref> shows that the offline training plays an important role in generating better results, improving the Jmean by 21%. It demonstrates that the network needs a generic <ref type="table">Table 1</ref>. Ablation study on the DAVIS validation set. We show comparisons of the proposed SegFlow model with different components removed, i.e., online-training (ol), offline-training (of), iterative learning (it), flow data augmentation (fda), optical flow branch (flo) and segmentation data augmentation (sda).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ours model to discover moving objects before online finetuning.</p><p>The combined online and iterative strategy also improve the overall Jmean by 7.9%. Compared to the model without using the flow branch, our joint model not only improves the Jmean but also produces smooth results temporally, resulting in a significant improvement in T mean by 5.6%. We evaluate the effectiveness of our data augmentation steps in <ref type="table">Table 1</ref>. Without the data augmentation for segmentation (-sda) and augmented flow data (-fda), the performance both degrades in terms of Jmean. In addition, the T mean is worse without augmenting flow data (-fda), which shows the importance of the synthesized data described in Section 4.2. <ref type="table">Table 2</ref> shows segmentation results on the DAVIS validation set. We improve the performance by considering the prediction of the image and its flipping one, and averaging both outputs to obtain the final result, where we refer to as Ours 2 . Without adding much computational cost, we further boost the Jmean with 1.3%. We compare the proposed SegFlow model with state-of-the-art approaches, including unsupervised algorithms (FST <ref type="bibr" target="#b27">[28]</ref>, CVOS <ref type="bibr" target="#b38">[39]</ref>, KEY <ref type="bibr" target="#b21">[22]</ref>, NLC <ref type="bibr" target="#b10">[11]</ref>), and semi-supervised methods (OVOS <ref type="bibr" target="#b6">[7]</ref>, MSK <ref type="bibr" target="#b20">[21]</ref>, OFL <ref type="bibr" target="#b40">[41]</ref>, BVS <ref type="bibr" target="#b24">[25]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Segmentation Results</head><p>Among unsupervised algorithms, our SegFlow model with or without the flow branch both performs favorably against other methods with a significant improvement (more than 10% in Jmean). For semi-supervised methods, our model performs competitively against OSVOS <ref type="bibr" target="#b6">[7]</ref> and MSK <ref type="bibr" target="#b20">[21]</ref>, where their methods require additional inputs (i.e., superpixels in OSVOS and optical flow in MSK 1 <ref type="table">Table 2</ref>. Overall results on the DAVIS validation set with the comparisons to unsupervised and semi-supervised methods. with CRF refinement) to achieve higher performance, while our method only needs images as inputs. Furthermore, we show consistent improvements over the model without the flow branch, especially in the temporal accuracy (T mean), which demonstrates that feature representations learned from the flow network help the segmentation. <ref type="figure">Figure 6</ref> shows the attributes-based performance (Jmean) for different methods. Our unsupervised method (offline training) performs well on all the attributes except for Dynamic Background (DB). One possible reason is that motion representations generated from the flow branch may not be accurate due to the complexity in the background. <ref type="figure">Figure 7</ref> presents some example results for segmentation. With the flow branch jointly trained with segmentation, the model is able to recover the missing area of the object that is clearly a complete region from the flow estimation. A full comparison per sequence and more results are provided in the supplementary material. <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> show the average endpoint error of the proposed SegFlow model and the comparisons to other state-of-the-art methods, including our baseline model (FlowNetS) used in the flow branch. To validate the effectiveness of our joint training scheme, we use the pre-trained FlowNetS on the Flying Chair dataset <ref type="bibr" target="#b11">[12]</ref> as the baseline, and finetune on the target dataset using the FlowNetS and our SegFlow model for comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-Supervised</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Optical Flow Results</head><p>We note that the data layer used in <ref type="bibr" target="#b11">[12]</ref> is specifically designed for FlowNetS, and thus we cannot directly apply it to our model. Hence we report performance using var- <ref type="table">Table 3</ref>. Average endpoint errors for optical flow. FlowNetS+ft denotes the results presented in <ref type="bibr" target="#b11">[12]</ref>. FlowNetS+ft * denotes FlowNetS trained with the same data as SegFlow+ft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sintel ious training data, where FlowNetS+ft denotes the results reported in <ref type="bibr" target="#b11">[12]</ref> and FlowNetS+ft * denotes the model finetuned with the same training data as used in SegFlow. As a result, we show that our SegFlow model consistently improves endpoint errors against the results of FlowNetS+ft * , which validates the benefit of incorporating the information from the segmentation branch. On KITTI, SegFlow without any data augmentation even outperforms FlowNetS+ft that uses extensive data augmentation. However, we observe that our model slightly overfits to the data on Sintel, due to the need of data augmentation on a much smaller dataset than the others.</p><p>In <ref type="table">Table 4</ref>, we also compare the results with Scene-FlowNet <ref type="bibr" target="#b25">[26]</ref> on the training and validation sets of Monkaa and Driving, and show that our method performs favorably against it. <ref type="figure">Figure 8</ref> shows some visual comparisons of optical flow. Intuitively, the segmentation provides the information to guide the flow network to estimate the output that aligns with the segmentation output (e.g., the flow within the segmentation is smooth and complete). More results and analysis are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Runtime Analysis</head><p>For the model trained offline, the proposed SegFlow predicts two outputs (segmentation and optical flow) simulta- <ref type="figure">Figure 7</ref>. Qualitative results on the DAVIS validation set with comparisons to unsupervised and semi-supervised algorithms. <ref type="figure">Figure 8</ref>. For each input image, we show the optical flow results of the baseline FlowNetS <ref type="bibr" target="#b11">[12]</ref>, fine-tuned FlowNetS and our SegFlow model on the Scene Flow dataset. Our method produces outputs with lower endpoint error, especially with the visual improvement within the object, in which the flow is smoother than the other methods due to the guidance from the segmentation network. <ref type="table">Table 4</ref>. Average endpoint errors on the Scene Flow dataset. The evaluations for train and val on the Monkaa and Driving datasets use both f orward and backward samples, while evaluations on train+val use f orward ones with the comparison as reported in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Monkaa neously in 0.3 seconds per frame on a Titan X GPU with 12 GB memory. When taking the online training step into account, our system runs at 7.9 seconds per frame averaged over the DAVIS validation set. Compared to other methods such as OFL (30 seconds per frame for optical flow generation and 90 seconds per frame for optimization), MSK (12 seconds per frame) and OSVOS (more than 10 seconds per frame at its best performance), our method is faster and can output an additional result of optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>This paper proposes an end-to-end trainable network SegFlow for joint optimization of video object segmentation and optical flow estimation. We demonstrate that with this joint structure, both segmentation and optical flow can be improved via bi-directional feature propagations. To train the joint model, we relax the constraint of a large dataset that requires both foreground segmentation and optical flow ground truths by developing an iterative training strategy. We validate the effectiveness of our joint training scheme through extensive ablation studies and show that our method performs favorably on both the video object segmentation and optical flow tasks. The proposed model can be easily adapted to other architectures and can be used for joint training other co-related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>During offline training, (a) shows the training accuracy for object segmentation, while (b) presents the loss for optical flow, with respect to the number of training iterations (both results are obtained on a training subset). After three rounds, convergences can be observed for both segmentation and optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Iteratively improving optical flow results on DAVIS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>↑ 0.748 0.674 0.538 0.669 0.739 0.724 0.654 0.606 J Recall ↑ 0.900 0.814 0.575 0.803 0.891 0.882 0.787 0.677 J Decay ↓ 0.137 0.062 0.227 0.005 0.124 0.119 0.021 0.006 F Mean ↑ 0.745 0.667 0.515 0.658 0.741 0.735 0.640 0.604 F Recall ↑ 0.853 0.771 0.540 0.765 0.839 0.841 0.750 0.717 F Decay ↓ 0.136 0.051 0.251 0.043 0.122 0.132 0.017 0.001 T Mean ↓ 0.194 0.276 0.302 0.279 0.225 0.250 0.354 0.335</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Unsupervised Measure Ours 2 Figure 6 .</head><label>26</label><figDesc>Ours Ours-flo OSVOS MSK OFL BVS Ours-ol Ours-flo-ol OSVOS FST CVOS KEY NLC J Mean ↑ 0.761 0.748 0.724 0.798 0.797 0.680 0.600 0Attribute based evaluation on the DAVIS validation set using Jmean compared with unsupervised methods.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">With image only as the input, the Jmean of MSK<ref type="bibr" target="#b20">[21]</ref> on the DAVIS validation set is 69.8, which is much lower than ours as 74.8.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is supported in part by the NSF CAREER Grant #1149783 and NSF IIS Grant #1152576.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jump: virtual reality video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kontkanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2016" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast edge-preserving patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="500" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topology-constrained layered tracking with latent flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting and tracking moving objects for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video segmentation with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi feature path modeling for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segmenting salient objects from images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vision-based offline-online perception paradigm for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakhtiary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning the local statistics of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="137" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A fully-connected layered model of foreground and background flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust and efficient foreground analysis for real-time video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient sparse-to-dense optical flow estimation using a learned basis and layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Streaming hierarchical video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
