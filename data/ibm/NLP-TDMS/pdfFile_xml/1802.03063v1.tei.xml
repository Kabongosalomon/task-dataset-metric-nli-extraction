<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 LEARNING LATENT REPRESENTATIONS IN NEURAL NETWORKS FOR CLUSTERING THROUGH PSEUDO SUPERVISION AND GRAPH-BASED ACTIVITY REGULARIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozsel</forename><surname>Kilinc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering Department</orgName>
								<orgName type="institution">University of South Florida Tampa</orgName>
								<address>
									<postCode>33620</postCode>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismai</forename><surname>Uysal</surname></persName>
							<email>iuysal@usf.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering Department University of South Florida Tampa</orgName>
								<address>
									<postCode>33620</postCode>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 LEARNING LATENT REPRESENTATIONS IN NEURAL NETWORKS FOR CLUSTERING THROUGH PSEUDO SUPERVISION AND GRAPH-BASED ACTIVITY REGULARIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.</p><p>Published as a conference paper at ICLR 2018 auxiliary target distribution derived from the soft cluster assignments. Similarly, Joint Unsupervised Learning (JULE) <ref type="bibr" target="#b30">(Yang et al., 2016)</ref> combines agglomerative clustering with convolutional neural networks (CNN) and formulates them as a recurrent process. Although JULE proposes an end-to-end learning framework, it suffers scalability issues due to its agglomerative clustering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Clustering, the unsupervised process of grouping similar examples together, is one of the most fundamental challenges in machine learning research and has been studied extensively in different aspects such as feature selection, distance functions, grouping methods, etc. <ref type="bibr">(Aggarwal &amp; Reddy, 2014)</ref>. k-means <ref type="bibr" target="#b18">(MacQueen et al., 1967)</ref> and Gaussian Mixture Models (GMM) <ref type="bibr" target="#b2">(Bishop, 2007)</ref> are two well-known conventional clustering algorithms that are applicable to a wide range of problems. Traditionally, these methods are applied to low-level features such as raw data or gradient-orientation histograms (HOG) for images. Therefore, their distance metrics are limited to local relations in the data space and inadequate to represent hidden dependencies in latent spaces. On the other hand, spectral clustering <ref type="bibr" target="#b27">(von Luxburg, 2007)</ref> is another conventional approach producing more flexible distance metrics than k-means and GMM. However, these types of solutions are not scalable to large datasets as they need to compute the full graph Laplacian matrix.</p><p>In recent years, researchers have focused on the unsupervised learning of high-level features on which to apply clustering and shown that learning good representations is important for the accuracy and robustness of the clustering task. Deep Embedding Clustering (DEC) <ref type="bibr" target="#b28">(Xie et al., 2016)</ref> was proposed to simultaneously learn feature representations and cluster assignments using deep neural networks <ref type="bibr">(DNN)</ref>. In this approach, first DNN parameters are initialized with a layer-wise trained deep autoencoder <ref type="bibr" target="#b26">(Vincent et al., 2010)</ref> and then the initialized DNN is used to obtain the latent representation on which to perform k-means clustering for the initialization of cluster centers. This complicated initialization is followed by a challenging optimization process that minimizes the Kullback-Leibler (KL) divergence between the centroid-based probability distribution and the Novel deep generative models that can be trained via direct backpropagation have recently been proposed avoiding the difficulties in preexisting generative models such as Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN) and Deep Boltzmann Machines (DBM) that are trained by MCMC-based algorithms <ref type="bibr" target="#b8">(Hinton et al., 2006;</ref><ref type="bibr" target="#b23">Salakhutdinov &amp; Hinton, 2009</ref>). Among two canonical examples of these models, Variational Autoencoders (VAE) <ref type="bibr" target="#b14">(Kingma &amp; Welling, 2013;</ref> integrate stochastic latent variables into the conventional autoencoder architecture while Generative Adversarial Networks (GAN) <ref type="bibr" target="#b7">(Goodfellow et al., 2014)</ref> propose an adversarial training procedure implementing a min-max adversarial game between two neural networks: the discriminator and the generator. Following these advances, researchers have started to study new hybrid models with the goal of performing unsupervised clustering through deep generative models. For example, Variational Deep Embedding (VaDE) <ref type="bibr" target="#b10">(Jiang et al., 2017)</ref> proposed a clustering framework combining VAE and GMM together. Also, Gaussian Mixture Variational Autoencoder (GMVAE) <ref type="bibr" target="#b5">(Dilokthanakul et al., 2016)</ref> built upon the semi-supervised model by  to perform unsupervised clustering within the VAE framework with a Gaussian mixture as a prior distribution. GAN-based methods include: Categorical Generative Adversarial Networks (CatGAN) <ref type="bibr" target="#b24">(Springenberg, 2015)</ref>, an approach incorporating neural network classifiers with an adversarial generative model, and Adversarial Autoencoder (AAE) <ref type="bibr" target="#b19">(Makhzani et al., 2015)</ref>, a probabilistic autoencoder variant integrating traditional reconstruction error with adversarial training criterion of GANs. Besides, <ref type="bibr" target="#b21">Premachandran &amp; Yuille (2016)</ref> proposes to fuse the disentangled features learned by Information Maximizing Generative Adversarial Networks (InfoGAN), an extension to GANs that uses mutual information to induce representation, with k-means clustering.</p><p>In this paper, we propose a novel unsupervised clustering approach building upon the previous study on learning of latent annotations in a particular semi-supervised setting where a coarse level of supervision is available for all observations, i.e. parent-class labels, but the model has to learn a fine level of latent annotations, i.e. sub-classes, under each one of these parents. For clarification, assume that we are given a dataset of hand-written digits such as <ref type="bibr">MNIST (LeCun et al., 1998)</ref> where the overall task is the complete categorization of each digit, but the only available supervision is whether a digit is smaller or greater than 5. To study this particular semi-supervised setting on neural networks, <ref type="bibr" target="#b11">Kilinc &amp; Uysal (2017a)</ref> proposed a novel output layer modification, Auto-clustering Output Layer (ACOL). ACOL allows simultaneous supervised classification (per provided parentclasses) and unsupervised clustering (within each parent) where clustering is performed through Graph-based Activity Regularization (GAR) technique recently proposed in <ref type="bibr" target="#b12">Kilinc &amp; Uysal (2017b)</ref>. More specifically, as ACOL duplicates the softmax nodes at the output layer for each class, GAR allows for competitive learning between these duplicates on a traditional error-correction learning framework.</p><p>To learn latent annotations in a fully unsupervised setup, we substitute the real, yet unavailable, parentclass information with a pseudo one. More specifically, we choose a domain specific transformation to be applied to the observations in a dataset to generate examples for a pseudo parent-class. The transformed dataset constitutes the examples of that pseudo parent-class and every new transformation generates a new one. Regarding the MNIST example for this fully unsupervised setting, now we simply augment the dataset by applying a transformation to examples, e.g. rotating by 90 o , and label transformed examples as rotated and non-transformed examples as original. This new augmented dataset is provided to the network as a two-class classification problem with pseudo classes labeled as original and rotated as visualized in <ref type="figure" target="#fig_4">Figure 1</ref>. While being trained over this pseudo supervision, through ACOL and GAR, the neural network learns the latent representation distinguishing the real digit identities in an unsupervised manner.</p><p>The idea of employing an auxiliary task to learn a good data representation has been previously studied for different domains <ref type="bibr" target="#b4">(Collobert et al., 2011;</ref><ref type="bibr" target="#b1">Ahmed et al., 2008)</ref>. Most recent study, Exemplar CNN <ref type="bibr" target="#b6">(Dosovitskiy et al., 2016)</ref>, proposed to use a regularizer enforcing the feature representation to be approximately invariant to the transformations while training the network to discriminate between a set of pseudo parent-classes ("surrogate classes" with their definition). This approach requires thousands of transformations to obtain a good representation and also it cannot exploit more than 300 examples per "surrogate class" severely limiting its scalability. Furthermore, some elementary  <ref type="figure" target="#fig_4">Figure 1</ref>: Assume that we are given a dataset of hand-written digits such as MNIST where the overall task is the complete categorization of each digit. Then, we simply augment the dataset by applying a transformation to examples, e.g. rotating by 90 o , and label each of them either as original or as rotated. This new augmented dataset is provided to the network as a two-class classification problem. While being trained over this pseudo supervision, through ACOL and GAR, the neural network also learns the latent representation distinguishing the real digit identities in unsupervised an manner. transformations, such as rotation, have only a minor impact on the performance. In comparison, in our approach, only 8 pseudo parent-classes generated by rotation-based transformations provide a rich latent representation to obtain state-of-the-art unsupervised clustering performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>2.1 AUTO-CLUSTERING OUTPUT LAYER Unlike traditional output layer structure, the Auto-clustering Output Layer (ACOL) <ref type="bibr" target="#b11">(Kilinc &amp; Uysal, 2017a</ref>) defines more than one softmax node (k s duplicates) per parent-class. Outputs of k s duplicated softmax nodes that belong to the same parent are then combined in a subsequent pooling layer for the final prediction. Training is performed in the configuration shown in <ref type="figure" target="#fig_1">Figure 2</ref> where n p is the number of parent-classes. This might look like a classifier with redundant softmax nodes. However, duplicated softmax nodes of each parent are specialized using GAR throughout the training in a way that each one of n = n p k s softmax nodes represent an individual sub-class of a parent, i.e. annotation.</p><p>In order to mathematically describe this modification, let us consider a neural network with L − 1 hidden layers where l denotes the individual index for each layer such that l ∈ {0, ..., L}. Let Y (l) denote the output of the nodes at layer l. Y (0) = X is the input and f (X) = f (L) (X) = Y (L) = Y is the output of the entire network. W (l) and b (l) are the weights and biases of layer l, respectively. Then, the feedforward operation of the neural networks can be written as</p><formula xml:id="formula_0">f (l) X = Y (l) = h (l) Y (l−1) W (l) + b (l)<label>(1)</label></formula><p>where h (l) (.) is the activation function applied at layer l.</p><p>For ACOL networks, h (L−1) (.) and h (L) (.) respectively correspond to softmax and linear activation functions. Also, W (L) := [I np . . . I np ] T and b (L) := 0 where I denotes the identity matrix as ACOL simply defines constant weights between the augmented softmax layer and the pooling layer to sum up the output probabilities of the softmax nodes belonging to the same parent. Let Z denote the activities at the input of augmented softmax layer such that  corresponding to an m × n matrix where m is the number of examples and n is the total number of all softmax nodes at the augmented softmax layer such that n = n p k s , where n p is the number of parent-classes and k s is the clustering coefficient of ACOL. Then, the output of the ACOL applied network can be written in terms of Z as</p><formula xml:id="formula_1">Z := Y (L−2) W (L−1) + b (L−1)<label>(2)</label></formula><formula xml:id="formula_2">Y = softmax Z W (L)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRAPH-BASED ACTIVITY REGULARIZATION</head><p>Kilinc &amp; Uysal (2017a) adopted the Graph-based Activity Regularization (GAR) technique <ref type="bibr" target="#b12">(Kilinc &amp; Uysal, 2017b)</ref> as the unsupervised regularization term to create competition between the duplicated softmax nodes of the augmented softmax layer which ultimately results in specialized but equallyactive softmax nodes each representing a latent annotation within a parent.</p><p>The GAR technique applies the regularization over the positive part of the activities at the input of softmax nodes such that g X = B := max 0, Z</p><p>and defines two terms to turn n × n symmetric matrix N , which is defined as N := B T B, into the identity matrix. While the affinity term penalizes the non-zero off-diagonal entries of N , balance attempts to equalize diagonal entries. Let v be a 1 × n vector representing the diagonal entries of N such that v := [N 11 . . . N nn ] and V be defined as n × n symmetric matrix such that V := v T v. Then, the affinity and balance terms can be written as</p><formula xml:id="formula_4">Affinity = α B := n i =j N ij (n − 1) n i=j N ij (5) Balance = β B := n i =j V ij (n − 1) n i=j V ij<label>(6)</label></formula><p>which modifies the overall objective function of the training proposed in <ref type="bibr" target="#b11">Kilinc &amp; Uysal (2017a)</ref> as</p><formula xml:id="formula_5">L f X , t + U g X = L Y , t + c α α B + c β 1 − β B + c F ||B|| 2 F<label>(7)</label></formula><p>where L(.) is the supervised log loss function, t = [t 1 . . . t m ] T is the vector of provided parent-class labels such that t i ∈ {1, ..., n p } (recall that, in the semi-supervised setting considered in <ref type="bibr" target="#b11">Kilinc &amp; Uysal (2017a)</ref>, there is a real partial supervision available for all examples, e.g. a digit is smaller or greater than 5), U(.) is the unsupervised regularization term consisting of affinity, balance and ||B|| F (the Frobenius norm for B) that is employed to limit the denominators of both affinity and balance terms not to diminish their effects and c α , c β , c F are the weighting coefficients.</p><p>GAR has been originally proposed for the classical type of semi-supervised setting where the number of labeled observations is much smaller than the number of unlabeled observations, but all existing classes are equally represented by the available labels even at limited numbers. <ref type="bibr" target="#b12">Kilinc &amp; Uysal (2017b)</ref> have shown that defining the objective of the regularization over the matrix N yields a scalable and efficient graph-based solution and that the entire operation corresponds to propagating the available labels across the graph G M whose edges are specified by the m × m symmetric matrix M := BB T that infers the adjacency of the examples based on the predictions of the neural network. More specifically, it has been shown that as the matrix N turns into the identity matrix, G M becomes a disconnected graph including n disjoint subgraphs each of which is m /n-regular. This indicates that the strong adjacencies in the matrix M get stronger, weak ones diminish and each label is propagated to m /n examples through the strong adjacencies.</p><p>On the other hand, in the particular semi-supervised setting considered by <ref type="bibr" target="#b11">Kilinc &amp; Uysal (2017a)</ref> (i.e. a coarse level of labeling is available for all observations but the model still needs to learn a fine level of latent annotation for each one of them), when applied to an ACOL network, GAR provides that the latent information introduced by the coarse supervision is propagated from the graph G Y (whose edges are specified by m × m symmetric matrix Y Y T ) to its spanning subgraph G M to reveal deeper latent annotations. In other words, although these two graphs are made up of the same vertices (m examples) while propagating the latent information that is captured through supervised adjacency introduced by G Y across G M , GAR terms eliminate some of the edges of G Y from G M in a way that G M ultimately becomes a disconnected graph of n disjoint subgraphs each of which now corresponds to a latent annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OBJECTIVE FUNCTION</head><p>The unsupervised clustering approach proposed in this paper adopts the same framework introduced in <ref type="bibr" target="#b11">Kilinc &amp; Uysal (2017a)</ref>. Since the real parent-class labels (a digit is smaller or greater than 5) are unavailable in a fully unsupervised setting, we randomly assign pseudo parent-class labels each of which is associated with a domain specific transformation used to generate the examples of that pseudo parent-class.</p><p>In this setting, n p now corresponds to the number of pseudo parent-classes andt = [t 1 . . .t m ] T is a vector of randomly assigned pseudo parent-class labels which are uniformly distributed across n p pseudo parent-classes such thatt i ∈ {1, ..., n p }. Also, there exists a set of transformations S T = {T 1 , ..., T np } where transformation T j is used to generate the examples of the j th pseudo parentclass such thatx i = T j (x i ). S T also includes non-transformation T 1 providingx i = T 1 (x i ) = x i to ensure that the original observations are introduced to the network during training.t is associated with a vector of transformations T = [T 1 . . . T m ] T such that T i = Tt i . Let be an element-wise operation defined between the vector of transformations T and the original input</p><formula xml:id="formula_6">X = [x 1 . . . x m ] T such that X =    x 1 x 2 . . . x m     = T X =     T 1 T 2 . . . T m         x 1 x 2 . . . x m     =     T 1 (x 1 ) T 2 (x 2 ) . . . T m (x m )     =     Tt 1 (x 1 ) Tt 2 (x 2 ) . . . Tt m (x m )    <label>(8)</label></formula><p>whereX corresponds to the modified input per randomly assigned pseudo labelst. The output of the entire network and the positive part of the augmented softmax layer activities respectively become Y = f (X) and B = g(X). Then, the objective function defined in <ref type="formula" target="#formula_5">(7)</ref> can simply be adopted by substituting the real, yet unavailable, observation-label pair (X, t) with a pseudo one (X,t) such that</p><formula xml:id="formula_7">L f X ),t + U g X = L Y ,t + c α α B + c β 1 − β B + c F ||B|| 2 F<label>(9)</label></formula><p>3.2 MODIFIED Affinity AND Balance TERMS Recall that an n × n symmetric matrix N = B T B specifies the edges of the graph between the softmax duplicates and that GAR terms have been proposed to regularize the matrix N in a way that it turns into the identity matrix. While the objective of affinity, i.e. penalizing the non-zero off-diagonal entries of N , corresponds to assigning an example to only one softmax node with the probability of 1, the objective of balance, i.e. equalizing diagonal entries of N , corresponds to preventing collapsing onto a subspace of dimension less than n.</p><p>Among the off-diagonal entries of N determining the affinity cost, for each one of n softmax nodes, there exist k s − 1 entries describing its relation with the other duplicates of the same parent-class (let us define them as intra-parent entries) and (n p − 1)k s entries describing its relation with the softmax nodes belonging to other parent-classes (let us define them as inter-parent entries). While inter-parent entries are explicitly affected by the pseudo classification objective as well as the regularization, intra-parent entries do not experience the classification directly. Therefore, the affinity cost due to inter-parent entries is minimized at a different rate than the affinity cost due to intra-parent entries.</p><p>On the other hand, as it is calculated over the diagonal entries of N , the balance cost does not either experience the pseudo classification objective explicitly. As a result, due to the direct impact of the pseudo classification objective which is observed only on the affinity cost, the weighting between the regularization terms actively alters during the training and needs to be re-tuned through the hyperparameters c α and c β . This effect can be observed more clearly as n p , the number of parent-classes, increases.</p><p>To ensure a more robust regularization we introduce a modification for the affinity and balance terms:</p><p>We discard all inter-parent entries of N and represent the remaining ones as a three dimensional tensorÑ . Thus,Ñ is a k s × k s × n p tensor such thatÑ :,:,k specifies the relations between k s softmax duplicates of the k th parent-class where k ∈ {1, ..., n p }. Also,Ṽ is another k s × k s × n p tensor defined asṼ</p><formula xml:id="formula_8">:,:,k = [Ñ 1,1,k . . .Ñ ks,ks,k ] T [Ñ 1,1,k . . .Ñ ks,ks,k ]<label>(10)</label></formula><p>Then, the modified affinity and balance terms can be respectively written as </p><p>and simply correspond to calculating the original terms given in (5), (6) on each 2-D k s × k s × 1 slice ofÑ andṼ tensors and then averaging the results for n p of them.</p><p>Replacing these modified terms in (9), the overall modified objective function becomes</p><formula xml:id="formula_10">L f X ),t + U g X = L Y ,t + c αα B + c β 1 −β B + c F ||B|| 2 F<label>(13)</label></formula><p>3.3 TRAINING AND CLUSTER ASSIGNMENTS Network parameters are trained by implementing the stochastic optimization method Adam (Kingma &amp; Ba, 2014) based on the objective given in (13). After training, k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer such that One might suggest performing k-means clustering on the representation observed in the augmented softmax layer (Z or softmax(Z)) rather than F . Properties and respective clustering performances of these representation spaces are empirically demonstrated in the following sections.</p><formula xml:id="formula_11">F = Y (L−2) = f (L−2) (X)<label>(14)</label></formula><p>Algorithm 1 below describes the entire training and cluster assignment procedure. </p><formula xml:id="formula_12">Ý i,ti + cαα B i + c β 1 −β B i + cF ||Bi|| 2 F until stopping criteria is met F ←− f (L−2) (X) // Obtain latent space representation F for the original examples y ←− kmeans(F , k)</formula><p>// Assign clusters by performing k-means on F return :Cluster assignments y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETUP AND DATASETS</head><p>The models have been implemented in Python using Keras <ref type="bibr" target="#b3">(Chollet, 2015)</ref> and Theano <ref type="bibr" target="#b25">(Theano Development Team, 2016)</ref>. Open source code is available at http://github.com/ozcell/LALNets that can be used to reproduce the experimental results obtained on three benchmark image datasets, <ref type="bibr">MNIST (LeCun et al., 1998)</ref>, SVHN <ref type="bibr" target="#b20">(Netzer et al., 2011)</ref> and USPS. Specifications of these datasets are presented in <ref type="table" target="#tab_1">Table 1</ref>. All experiments have been performed on a 6-layer convolutional neural network (CNN) model whose specifications are given in <ref type="table" target="#tab_3">Table 2</ref> where coefficients of GAR terms have been chosen as k s = 20, c α = 0.1, c β = 1, c F = 0.000001. During training, pseudo supervised objective is introduced as an 8 pseudo parent-class classification problem, i.e. n p = 8, through the following rotation-based transformations: </p><formula xml:id="formula_13">T i =                          i</formula><p>For all experiments, we used a batch size of 400 and each experiment has been repeated 10 times.</p><p>To ensure that the representation obtained through the proposed approach is well-generalized for never-seen-before data, we train the neural network parameters using only the training set examples of each dataset and obtain the clustering performances using k-means with k = 10 on the latent space representation F of the untransformed test set examples (through T 1 ).   <ref type="formula" target="#formula_0">2016)</ref>, we evaluate the test performances using unsupervised clustering accuracy given as</p><formula xml:id="formula_15">ACC = max f∈F m i=1 1{t * i = f(y i )} m<label>(16)</label></formula><p>where t * i is the ground-truth label, y i is the assigned cluster, and F is the set of all possible one-to-one mappings between assignments and labels. Both metrics range between [0, 1] where a larger value indicates more precise clustering results. Color codes denote the ground-truths for the digits. From epoch 1 to epoch 400 of the unsupervised (but pseudo supervised) training, clusters become well-separated and simultaneously the clustering accuracy increases. As clearly observed from this figure, using the pseudo supervision, the neural network also reveals some hidden patterns useful to distinguish the real digit identities and ultimately learns to categorize each one of them. It is also worth noting that a high level of clustering accuracy is achieved relatively quickly (after only 50 epochs) as seen both in the t-SNE and test accuracy plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Before training</head><p>Epoch <ref type="formula" target="#formula_0">1</ref>     <ref type="bibr" target="#b10">(Jiang et al., 2017)</ref> and GMVAE <ref type="bibr" target="#b5">(Dilokthanakul et al., 2016)</ref> employ variational autoencoders while <ref type="bibr">CatGAN (Springenberg, 2015)</ref>, AAE <ref type="bibr" target="#b19">(Makhzani et al., 2015)</ref> and IMSAT <ref type="bibr" target="#b9">(Hu et al., 2017)</ref> adopt adversarial training. DEC <ref type="bibr" target="#b28">(Xie et al., 2016)</ref> simultaneously learns feature representations and cluster assignments using DNNs. On the other hand, JULE <ref type="bibr" target="#b30">(Yang et al., 2016)</ref> combines agglomerative clustering with CNNs. Also, the performances of two conventional approaches, applying k-means on raw data space and applying k-means on the autoencoder representation, are provided to show a baseline for unsupervised clustering performances. Our approach statistically significantly outperforms all the contemporary methods that reported unsupervised clustering performance on MNIST except IMSAT <ref type="bibr" target="#b9">(Hu et al., 2017)</ref> displaying very competitive performance with our approach, i.e. 98.32%(±0.08) vs. 98.40%(±0.40). However, results obtained on the SVHN dataset, i.e. 76.80%(±1.30) vs. 57.30%(±3.90), show that our approach statistically significantly outperforms IMSAT on this realistic dataset and defines the current state-of-the-art for unsupervised clustering on SVHN. Besides, the USPS dataset provides another basis of comparison between our approach and JULE.  <ref type="bibr" target="#b19">(Makhzani et al., 2015)</ref> 16 90.45%(±2.05) --AAE <ref type="bibr" target="#b19">(Makhzani et al., 2015)</ref> 30 95.90%(±1.13) --IMSAT <ref type="bibr" target="#b9">(Hu et al., 2017)</ref> 10 98.40%(±0.40) -57.30%(±3.90) k-means <ref type="bibr" target="#b28">(Xie et al., 2016)</ref> 10 53.49% --AE+k-means <ref type="bibr" target="#b28">(Xie et al., 2016)</ref> 10 <ref type="formula" target="#formula_0">81</ref>  <ref type="bibr" target="#b30">(Yang et al., 2016)</ref>, we reported unsupervised clustering performance over the full dataset for a fair comparison. † † Excerpted from <ref type="bibr" target="#b9">(Hu et al., 2017).</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">REPRESENTATION PROPERTIES</head><p>Recall that, for the 6-layer CNN model employed in the experiments, F = Y (L−2) corresponds to the output of the fully-connected layer of 2048 ReLU nodes, Z = F W L−1 + b L−1 is the input of the augmented softmax layer of 160 nodes, i.e. n = n p k s , where 8 pseudo parent-classes are represented by 20 softmax duplicates each. <ref type="figure" target="#fig_8">Figure 4</ref> provides the average value for each dimension of F , Z and softmax(Z) observed with respect to untransformed test set examples and the norm of the associated weights. Note that the representation on F is not distributed to the entire space but the weights associated to these unused dimensions do not decay. On the other hand, due to the pseudo supervision task, the output of the augmented softmax layer i.e. softmax(Z), becomes a one-hot encoded representation of which 140 dimensions, i.e. (n p − 1)k s , are inactive for the untransformed examples; however, the representation at its input is distributed to all dimensions. <ref type="figure" target="#fig_8">Figure 4</ref> also summarizes how the dimension size of F , i.e. the number of ReLU nodes in the fully-connected layer, affects the clustering performance. Decreasing the number of dimensions of F up to a point, i.e. ≈ 1024, does not significantly affect the clustering accuracy. However, further decrease beyond this point dramatically reduces the performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GRAPH INTERPRETATION OF THE LATENT INFORMATION PROPAGATION THROUGH GAR</head><p>Recall that GAR terms have been originally proposed to propagate the available labels towards the unlabeled examples in a semi-supervised setting and <ref type="bibr" target="#b11">Kilinc &amp; Uysal (2017a)</ref> have shown that these terms can also be adopted to propagate the hidden information that is introduced by a coarse level of supervision and which is useful to discover a deeper level of latent annotations. In the fully unsupervised setting considered in this paper, as no real supervision is available, hidden information useful to discover unknown clusters is now captured through the help of domain specific transformations and propagated by GAR terms as well. <ref type="figure">Figure 6</ref> visualizes the realization of this propagation using the real predictions obtained on MNIST. Colored circles denote the ground-truths for the vertices, i.e. examples, and gray lines denote the edges, i.e. non-zero weighted connections between the examples representing their similarity. Note that, for vertices in graph G Y , there are two different colors indicating true pseudo parent-class labels assigned per the applied transformation (for simplicity, out of 8, only the examples of the first two pseudo parent-classes are used for this illustration), albeit ten different colors indicating the real digit identity for vertices in graph G M . Recall that edges of these two graphs, E Y and E, are respectively inferred by matrices Y Y T and BB T where B = max(0, Z) and that G M is the spanning subgraph of G Y . That is, G M = (M, E) shares the same vertices M with graph G Y = (M, E Y ), which is constructed per the pseudo supervision; however, E is a subset of E Y as some of the edges in graph G Y , such as those between the examples of digit 0 and 1, are eliminated in graph G M due to GAR regularization terms. As training continues, pseudo supervision eliminates the edges between the examples of different pseudo parent-classes and turns graph G Y into a disconnected graph of n p = 8 disjoint subgraphs (only two of them are illustrated). Simultaneously, GAR terms eliminate the edges between the examples of the same parent-class in graph G M to discover previously unknown clusters. Ultimately, G M becomes disconnected graphs of δ disjoint subgraphs where n p ≤ δ ≤ n p k s and each disjoint subgraph corresponds to a cluster.</p><p>G Y G M <ref type="figure">Figure 6</ref>: Visualizations of the graph G Y and its spanning subgraph G M for randomly chosen 500 test examples from MNIST (this selection is performed only for the simplicity of the visualization). Colored circles denote the ground-truths for the vertices, i.e. examples, and gray lines denote the edges, i.e. non-zero weighted connections between the examples representing their similarity. Note that, for vertices in graph G Y , there are two different colors indicating true pseudo parent-class labels assigned according to the applied transformation (for simplicity, out of 8, only the examples of first two pseudo parent-classes are used for this illustration), albeit ten different colors indicating the real digit identity for vertices in graph G M . As training continues, pseudo supervision eliminates the edges between the examples of different pseudo parent-classes and turns graph G Y into a disconnected graph of n p = 8 disjoint subgraphs (only two of them are illustrated). Simultaneously, GAR terms eliminate the edges between the examples of the same parent-class in graph G M to discover previously unknown clusters. Ultimately, G M becomes disconnected graphs of δ disjoint subgraphs where n p ≤ δ ≤ n p k s and each disjoint subgraph corresponds to a cluster. This figure is best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">THE IMPACT OF THE NUMBER OF CLUSTERS k</head><p>For the quantitative clustering results, we set the number of clusters for the k-means to the number of classes assuming a prior knowledge, i.e. k = 10. To demonstrate the representation power of the proposed approach as an unsupervised clustering model, on MNIST, we deliberately choose different k values for the k-means clustering applied on the representation space F . For two different k settings i.e. 7 and 20, <ref type="figure" target="#fig_10">Figure 7</ref> illustrates a few examples of each cluster. One can see that when k is smaller than the actual number of classes, digits with similar appearances are grouped together, such as digits 4 and 9, 5 and 8, 0 and 6. When k is set to a bigger value than the number of classes, some digits are divided into subclasses based on visually identifiable image properties such as digit tilt, roundness, etc. Note the differences between upright and oblique digit 1 as shown in clusters 2 and 20, between two styles of digit 6 as shown in clusters 18 and 19, and between two styles of digit 2 as shown in clusters 7 and 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k = 7</head><p>Cluster 1 Cluster 2</p><p>Cluster 3</p><p>Cluster 4</p><p>Cluster 5</p><p>Cluster <ref type="formula" target="#formula_4">6</ref> Cluster 7 k = 20</p><p>Cluster 1 Cluster 2</p><p>Cluster <ref type="formula" target="#formula_2">3</ref> Cluster 4</p><p>Cluster 5</p><p>Cluster 6</p><p>Cluster 7</p><p>Cluster 8</p><p>Cluster 9</p><p>Cluster 10</p><p>Cluster 11</p><p>Cluster <ref type="formula" target="#formula_0">12</ref> Cluster 13</p><p>Cluster 14</p><p>Cluster 15</p><p>Cluster 16</p><p>Cluster 17</p><p>Cluster <ref type="formula" target="#formula_0">18</ref> Cluster <ref type="formula" target="#formula_0">19</ref> Cluster 20 When k is smaller than the actual number of classes, digits with similar appearances are grouped together, such as digits 4 and 9, 5 and 8, 0 and 6. When k is set to a bigger value than the number of classes, some digits are divided into subclasses based on visually identifiable image properties such as digit tilt, roundness, etc. Note the differences between upright and oblique digit 1 as shown in clusters 2 and 20, between two styles of digit 6 as shown in clusters 18 and 19, and between two styles of digit 2 as shown in clusters 7 and 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">THE IMPACT OF TRANSFORMATIONS</head><p>As the revealed unknown clusters are directly related with the captured latent information through pseudo parent-classes, choosing the right set of transformations for the clustering task of concern is crucial for the performance. <ref type="figure">Figure 8</ref> presents t-SNE visualizations of the representation spaces observed when different sets of transformations are adopted.</p><p>The first row of <ref type="figure">Figure 8</ref> illustrates the clustering results when one of four different transformation types, i.e. scaling, shearing, translation and random permutation of the pixels, is applied variably to generate 8 pseudo parent-classes. One can observe some level of grouping with scaling and shearing-based transformations; however, the clusters defined by these groupings do not represent real digit identities (as shown by the colored dots) and may indicate other features of images. On the other hand, translating the images or randomly permuting the pixel positions do not provide any useful knowledge to discover any well-defined clustering.</p><p>The second row of <ref type="figure">Figure 8</ref> presents the results obtained when rotation-based transformations listed in (15) are adopted. One can easily observe that only two or four pseudo parent-classes generated using rotation-based transformations are sufficient to obtain decent clustering representing the real digit identities. Considering that, for MNIST, the clustering accuracy obtained using all 8 transformations in <ref type="formula" target="#formula_0">(15)</ref> is 98.32%(±0.08), we have achieved 97.80%(±0.18) accuracy using S T = {T 1 , T 2 , T 3 , T 4 }, 72.52%(±6.20) accuracy using S T = {T 1 , T 2 } and 96.84%(±0.29) accuracy using S T = {T 1 , T 3 }.</p><p>Recalling that T 2 and T 3 respectively correspond to rotating the images by 90 o and 180 o , one can say that comparing the untransformed images with their 180 o rotated versions is more effective in terms of capturing the latent information that is useful to distinguish the real digit identities. In fact, T 3 alone is sufficient to achieve state-of-the-art clustering accuracy on MNIST. Adding more rotation-based transformations to S T further improves the clustering performance. To summarize, the type of the transformation generating the pseudo parent-classes is more important than their number and different transformations can reveal different clustering patterns. Therefore, finding the right transformation type for the clustering task of concern is crucial for the proposed approach in this paper and it remains an important research question how to identify the kind of transformation most optimized for the clustering task at hand.</p><p>Scaling n p = 8 Shearing n p = 8 Translation n p = 8 Random permutation n p = 8</p><p>Rotation n p = 2: S T = T 1 , T 2 Rotation n p = 2: S T = T 1 , T 3 Rotation n p = 4: S T = T 1 , , T 4 Rotation n p = 8: S T = T 1 , , T 8 <ref type="figure">Figure 8</ref>: t-SNE visualizations of the representation spaces observed when different sets of transformations are adopted. The first row illustrates the clustering results when one of four different transformation types, i.e. scaling, shearing, translation and random permutation of the pixels, is applied variably to generate 8 pseudo parent-classes. The second row presents the results obtained when rotation-based transformations listed in (15) are adopted. To summarize, the type of the transformation generating the pseudo parent-classes is more important than their number and different transformations can reveal different clustering patterns. Therefore, finding the right transformation type for the clustering task of concern is crucial for the proposed approach in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduced a novel unsupervised clustering approach building upon the previous study on an output layer modification, ACOL, which is proposed to learn latent annotations on neural networks when a partial supervision is provided. To discover unknown clusters in a fully unsupervised setup, we substitute the real, yet unavailable, partial supervision with a pseudo one. More specifically, we randomly assign pseudo parent-class labels each of which is associated with a different domain specific transformation. Each observation is modified by applying the transformation corresponding to the assigned pseudo label. Generated observation-label pairs are used to train an ACOL network that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised regularization based on GAR terms, each softmax duplicate under a parent-class is specialized as the latent information captured by the help of domain specific transformations is propagated throughout the training. Ultimately we obtain a k-means friendly latent representation. Furthermore, we demonstrate that the neural network can learn by comparing differently transformed examples and translate that knowledge to reveal unknown clusters. The proposed approach was validated on three image benchmark datasets, MNIST, SVHN and USPS, through t-SNE visualizations and unsupervised clustering accuracy exceeds those reported by well-accepted approaches in the literature. Future work will extend this approach to other domains such as sequential data. We will also explore how to optimize domain specific transformations based on known or otherwise identifiable characteristics of the dataset being considered for clustering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Neural network structure with the ACOL. Each softmax node corresponds to an individual sub-class of a parent, i.e. annotation. During feedforward operation of the network, pooling layer calculates final parent-class predictions through sub-class probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Recalling that the original examples are already introduced to the network as the examples of first pseudo parent-class through transformation T 1 , we obtain the latent space representation only for the original examples to perform k-means clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Model training and cluster assignments Input :X = [x1 . . . xm] T , np, a set of transformations ST = {T1, ..., Tn p }, batch size b, weighing coefficients cα, c β , cF , the number of clusters k repeat t ←− random(np) // Randomly assign labels across np classes T ←− [Tt 1 , ..., Tt m ] // Obtain the vector of transformations corresponding tot X ←− T X // Obtain the modified input(X1,t1), ..., (Xm /b ,tm /b ) ←− (X,t) // Shuffle and create batch pairs for i ← 1 to m /b do Take i th pair (Xi,ti) Forward propagate forÝ i = f (Xi) andBi = g(Xi) Take a gradient step for L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>al. (2017) and Yang et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3</head><label>3</label><figDesc>presents the t-SNE (Maaten &amp; Hinton, 2008) visualizations of the latent space F throughout the training for 2000 untransformed test examples from MNIST. Each group corresponds to a cluster (i.e. a digit) under the first pseudo parent-class (i.e. the class of untransformed examples including all ten digits).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>t-SNE visualization of the latent space F throughout the training for 2000 untransformed test examples from MNIST. Color codes denote the ground-truths for the digits. Note the separation of clusters from epoch 1 to epoch 400 of the unsupervised (but pseudo supervised) training. For reference, clustering accuracy for the entire test set is also provided. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>The average value for each dimension of F , Z and softmax(Z) observed with respect to untransformed test set examples and the norm of the associated weights. Note that the representation on F is not distributed to the entire space but the weights associated to these unused dimensions do not decay. On the other hand, due to the pseudo supervision task, the output of the augmented softmax layer i.e. softmax(Z), becomes a one-hot encoded representation of which 140 dimensions are inactive for the untransformed examples; however, the representation at its input is distributed to all dimensions. The last plot shows how the dimension size of F affects the clustering performance. This figure is best viewed in color.For comparison,Figure 5presents t-SNE visualizations of these latent representations observed with respect to 2000 untransformed test examples from MNIST. One can clearly see that clusters are not well-separated on one-hot encoded softmax(Z); however, separations of the clusters are quite similar and clear on the representation spaces F and Z. Hence, one can also obtain similar clustering accuracy, i.e. = 98.16% ± (0.14), by applying k-means on the representation space Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of t-SNE visualizations of the latent spaces F , Z and softmax(Z) for 2000 test examples from MNIST. Color codes denote the ground-truths for the digits and each label represents the major digit of a cluster. Clusters are not well-separated on one-hot encoded softmax(Z); however, separations of the clusters are quite similar and clear on the representation spaces F and Z. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of a few examples of each cluster for two different k settings i.e. 7 and 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Datasets used in the experiments.</figDesc><table><row><cell></cell><cell>Data type</cell><cell>Number of examples</cell><cell>Dimension</cell><cell>Number of classes</cell></row><row><cell cols="5">MNIST Image: Hand-written digits Train: 60000, Test: 10000 1 × 28 × 28 10</cell></row><row><cell>USPS</cell><cell cols="2">Image: Hand-written digits Train: 7291, Test: 2007</cell><cell cols="2">1 × 16 × 16 10</cell></row><row><cell>SVHN</cell><cell>Image: Street-view digits</cell><cell cols="3">Train: 73257, Test: 26032 3 × 32 × 32 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Specifications of the CNN model used in the experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>summarizes quantitative unsupervised clustering performances observed on three datasets in terms of unsupervised clustering accuracy (ACC). Results of a broad range of recent existing solu-tions are also presented for comparison. These solutions are grouped according to their approaches to unsupervised clustering. Following the very recent developments in deep generative models, VaDE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Quantitative unsupervised clustering performance (ACC) on MNIST, USPS and SVHN datasets. Results of a broad range of recent existing solutions are also presented for comparison. The last row demonstrates the benchmark scores of the proposed framework in this article.</figDesc><table><row><cell></cell><cell>k</cell><cell>MNIST-test</cell><cell>USPS-full  †</cell><cell>SVHN-test</cell></row><row><cell>VaDE (Jiang et al., 2017)</cell><cell cols="2">10 94.06%</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">GMVAE (Dilokthanakul et al., 2016) 10 82.31%(±3.75)</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">GMVAE (Dilokthanakul et al., 2016) 16 87.82%(±5.33)</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">GMVAE (Dilokthanakul et al., 2016) 30 92.77%(±1.60)</cell><cell>-</cell><cell>-</cell></row><row><cell>CatGAN (Springenberg, 2015)</cell><cell cols="2">20 90.30%</cell><cell>-</cell><cell>-</cell></row><row><cell>AAE</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>978-1-46-655821-2</idno>
		<ptr target="http://www.crcpress.com/product/isbn/9781466558212" />
		<title level="m">Data Clustering: Algorithms and Applications</title>
		<editor>Charu C. Aggarwal and Chandan K. Reddy</editor>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88690-7_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-88690-7_6" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2008, 10th European Conference on Computer Vision</title>
		<meeting><address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="69" to="82" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning, 5th Edition. Information science and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<ptr target="http://www.worldcat.org/oclc/71008143" />
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keras</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2078186" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep unsupervised clustering with gaussian mixture variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">A M</forename><surname>Mediano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Salimbeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shanahan</surname></persName>
		</author>
		<idno>abs/1611.02648</idno>
		<ptr target="http://arxiv.org/abs/1611.02648" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2496141</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2015.2496141" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/hu17b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/273</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/273" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-clustering output layer: Automatic learning of latent annotations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozsel</forename><surname>Kilinc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Uysal</surname></persName>
		</author>
		<idno>abs/1702.08648</idno>
		<ptr target="http://arxiv.org/abs/1702" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">GAR: an efficient and scalable graph-based activity regularization for semi-supervised learning. CoRR, abs/1705.07219</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozsel</forename><surname>Kilinc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Uysal</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1705.07219" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6114" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>abs/1511.05644</idno>
		<ptr target="http://arxiv.org/abs/1511.05644" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised learning using generative adversarial training and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittal</forename><surname>Premachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v32/rezende14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<idno>abs/1511.06390</idno>
		<ptr target="http://arxiv.org/abs/1511" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?id=1953039" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename><surname>Ulrike Von</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-007-9033-z</idno>
		<ptr target="https://doi.org/10.1007/s11222-007-9033-z" />
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v48/xieb16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/860435.860485</idno>
		<ptr target="http://doi.acm.org/10.1145/860435" />
	</analytic>
	<monogr>
		<title level="m">SIGIR 2003: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-01" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.556</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.556" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
