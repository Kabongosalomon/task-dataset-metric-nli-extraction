<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Sampling Towards Fast Graph Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<email>tong.zhang@anu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Sampling Towards Fast Graph Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCNs) have become a crucial tool on learning representations of graph vertices. The main challenge of adapting GCNs on largescale graphs is the scalability issue that it incurs heavy cost both in computation and memory due to the uncontrollable neighborhood expansion across layers. In this paper, we accelerate the training of GCNs through developing an adaptive layer-wise sampling method. By constructing the network layer by layer in a top-down passway, we sample the lower layer conditioned on the top one, where the sampled neighborhoods are shared by different parent nodes and the over expansion is avoided owing to the fixed-size sampling. More importantly, the proposed sampler is adaptive and applicable for explicit variance reduction, which in turn enhances the training of our method. Furthermore, we propose a novel and economical approach to promote the message passing over distant nodes by applying skip connections. Intensive experiments on several benchmarks verify the effectiveness of our method regarding the classification accuracy while enjoying faster convergence speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Learning, especially Convolutional Neural Networks (CNNs), has revolutionized various machine learning tasks with grid-like input data, such as image classification <ref type="bibr" target="#b0">[1]</ref> and machine translation <ref type="bibr" target="#b1">[2]</ref>. By making use of local connection and weight sharing, CNNs are able to pursue translational invariance of the data. In many other contexts, however, the input data are lying on irregular or non-euclidean domains, such as graphs which encode the pairwise relationships. This includes examples of social networks <ref type="bibr" target="#b2">[3]</ref>, protein interfaces <ref type="bibr" target="#b3">[4]</ref>, and 3D meshes <ref type="bibr" target="#b4">[5]</ref>. How to define convolutional operations on graphs is still an ongoing research topic.</p><p>There have been several attempts in the literature to develop neural networks to handle arbitrarily structured graphs. Whereas learning the graph embedding is already an important topic <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, this paper mainly focus on learning the representations for graph vertices by aggregating their features/attributes. The closest work to this vein is the Graph Convolution Network (GCN) <ref type="bibr" target="#b8">[9]</ref> that applies connections between vertices as convolution filters to perform neighborhood aggregation. As demonstrated in <ref type="bibr" target="#b8">[9]</ref>, GCNs have achieved the state-of-the-art performance on node classification.</p><p>An obvious challenge for applying current graph networks is the scalability. Calculating convolutions requires the recursive expansion of neighborhoods across layers, which however is computationally prohibitive and demands hefty memory footprints. Even for a single node, it will quickly cover a large portion of the graph due to the neighborhood expansion layer by layer if particularly the graph is dense or powerlaw. Conventional mini-batch training is unable to speed up the convolution computations, since every batch will involve a large amount of vertices, even the batch size is small.  <ref type="figure">Figure 1</ref>: Network construction by different methods: (a) the node-wise sampling approach; (b) the layer-wise sampling method; (c) the model considering the skip-connection. To illustrate the effectiveness of the layer-wise sampling, we assume that the nodes denoted by the red circle in (a) and (b) have at least two parents in the upper layer. In the node-wise sampling, the neighborhoods of each parent are not seen by other parents, hence the connections between the neighborhoods and other parents are unused. In contrast, for the layer-wise strategy, all neighborhoods are shared by nodes in the parent layer, thus all between-layer connections are utilized.</p><p>To avoid the over-expansion issue, we accelerate the training of GCNs by controlling the size of the sampled neighborhoods in each layer (see <ref type="figure">Figure 5</ref>). Our method is to build up the network layer by layer in a top-down way, where the nodes in the lower layer 1 are sampled conditionally based on the upper layer's. Such layer-wise sampling is efficient in two technical aspects. First, we can reuse the information of the sampled neighborhoods since the nodes in the lower layer are visible and shared by their different parents in the upper layer. Second, it is easy to fix the size of each layer to avoid over-expansion of the neighborhoods, as the nodes of the lower layer are sampled as a whole.</p><p>The core of our method is to define an appropriate sampler for the layer-wise sampling. A common objective to design the sampler is to minimize the resulting variance. Unfortunately, the optimal sampler to minimize the variance is uncomputable due to the inconsistency between the top-down sampling and the bottom-up propagation in our network (see § 4.2 for details). To tackle this issue, we approximate the optimal sampler by replacing the uncomputable part with a self-dependent function, and then adding the variance to the loss function. As a result, the variance is explicitly reduced by training the network parameters and the sampler.</p><p>Moreover, we explore how to enable efficient message passing across distant nodes. Current methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> resort to random walks to generate neighborhoods of various steps, and then take integration of the multi-hop neighborhoods. Instead, this paper proposes a novel mechanism by further adding a skip connection between the (l+1)-th and (l−1)-th layers. This short-cut connection reuses the nodes in the (l − 1)-th layer as the 2-hop neighborhoods of the (l + 1)-th layer, thus it naturally maintains the second-order proximity without incurring extra computations.</p><p>To sum up, we make the following contributions in this paper: I.We develop a novel layer-wise sampling method to speed up the GCN model, where the between-layer information is shared and the size of the sampling nodes is controllable. II. The sampler for the layer-wise sampling is adaptive and determined by explicit variance reduction in the training phase. III. We propose a simple yet efficient approach to preserve the second-order proximity by formulating a skip connection across two layers. We evaluate the performance of our method on four popular benchmarks for node classification, including Cora, Citeseer, Pubmed <ref type="bibr" target="#b10">[11]</ref> and Reddit <ref type="bibr" target="#b2">[3]</ref>. Intensive experiments verify the effectiveness of our method regarding the classification accuracy and convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>While graph structures are central tools for various learning tasks (e.g. semi-supervised learning in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref>), how to design efficient graph convolution networks has become a popular research topic. Graph convolutional approaches are often categorized into spectral and non-spectral classes <ref type="bibr" target="#b12">[13]</ref>. The spectral approach first proposed by <ref type="bibr" target="#b13">[14]</ref> defines the convolution operation in Fourier domain. Later, <ref type="bibr" target="#b14">[15]</ref> enables localized filtering by applying efficient spectral filters, and <ref type="bibr" target="#b15">[16]</ref> employs Chebyshev expansion of the graph Laplacian to avoid the eigendecomposition. Recently, GCN is proposed in <ref type="bibr" target="#b8">[9]</ref> to simplify previous methods with first-order expansion and re-parameterization trick. Nonspectral approaches define convolution on graph by using the spatial connections directly. For instance, <ref type="bibr" target="#b16">[17]</ref> learns a weight matrix for each node degree, the work by <ref type="bibr" target="#b17">[18]</ref> defines multiple-hop neighborhoods by using the powers series of a transition matrix, and other authors <ref type="bibr" target="#b18">[19]</ref> extracted normalized neighborhoods that contain a fixed number of nodes.</p><p>A recent line of research is to generalize convolutions by making use of the patch operation <ref type="bibr" target="#b19">[20]</ref> and self-attention <ref type="bibr" target="#b12">[13]</ref>. As opposed to GCNs, these methods implicitly assign different importance weights to nodes of a same neighborhood, thus enabling a leap in model capacity. Particularly, Monti et al. <ref type="bibr" target="#b19">[20]</ref> presents mixture model CNNs to build CNN architectures on graphs using the patch operation, while the graph attention networks <ref type="bibr" target="#b12">[13]</ref> compute the hidden representations of each node on graph by attending over its neighbors following a self-attention strategy.</p><p>More recently, two kinds of sampling-based methods including GraphSAGE <ref type="bibr" target="#b2">[3]</ref> and FastGCN <ref type="bibr" target="#b20">[21]</ref> were developed for fast representation learning on graphs. To be specific, GraphSAGE computes node representations by sampling neighborhoods of each node and then performing a specific aggregator for information fusion. The FastGCN model interprets graph convolutions as integral transforms of embedding functions and samples the nodes in each layer independently. While our method is closely related to these methods, we develop a different sampling strategy in this paper. Compared to GraphSAGE that is node-wise, our method is based on layer-wise sampling as all neighborhoods are sampled as altogether, and thus can allow neighborhood sharing as illustrated in <ref type="figure">Figure 5</ref>. In contrast to FastGCN that constructs each layer independently, our model is capable of capturing the between-layer connections as the lower layer is sampled conditionally on the top one. We detail the comparisons in § 6. Another related work is the control-variate-based method by <ref type="bibr" target="#b21">[22]</ref>. However, the sampling process of this method is node-wise, and the historical activations of nodes are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Notations and Preliminaries</head><p>Notations. This paper mainly focuses on undirected graphs. Let G = (V, E) denote the undirected graph with nodes v i ∈ V, edges (v i , v j ) ∈ E, and N defines the number of the nodes. The adjacency matrix A ∈ R N ×N represents the weight associated to edge (v i , v j ) by each element A ij . We also have a feature matrix X ∈ R N ×D with x i denoting the D-dimensional feature for node v i .</p><p>GCN. The GCN model developed by Kipf and Welling <ref type="bibr" target="#b8">[9]</ref> is one of the most successful convolutional networks for graph representation learning. If we define h (l) (v i ) as the hidden feature of the l-th layer for node v i , the feed forward propagation becomes</p><formula xml:id="formula_0">h (l+1) (v i ) = σ N j=1â (v i , u j )h (l) (u j )W (l) , i = 1, · · · , N,<label>(1)</label></formula><p>whereÂ = (â(v i , u j )) ∈ R N ×N is the re-normalization of the adjacency matrix; σ(·) is a nonlinear function;</p><formula xml:id="formula_1">W (l) ∈ R D (l) ×D (l−1)</formula><p>is the filter matrix in the l-th layer; and we denote the nodes in the l-th layer as u j to distinguish them from those in the (l + 1)-th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive Sampling</head><p>Eq. (1) indicates that, GCNs require the full expansion of neighborhoods for the feed forward computation of each node. This makes it computationally intensive and memory-consuming for learning on large-scale graphs containing more than hundreds of thousands of nodes. To circumvent this issue, this paper speeds up the feed forward propagation by adaptive sampling. The proposed sampler is adaptable and applicable for variance reduction.</p><p>We first re-formulate the GCN update to the expectation form and introduce the node-wise sampling accordingly. Then, we generalize the node-wise sampling to a more efficient framework that is termed as the layer-wise sampling. To minimize the resulting variance, we further propose to learn the layer-wise sampler by performing variance reduction explicitly. Lastly, we introduce the concept of skip-connection, and apply it to enable the second-order proximity for the feed-forward propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">From Node-Wise Sampling to Layer-Wise Sampling</head><p>Node-Wise Sampling. We first observe that Eq (1) can be rewritten to the expectation form, namely,</p><formula xml:id="formula_2">h (l+1) (v i ) = σ W (l) (N (v i )E p(uj |vi) [h (l) (u j )]),<label>(2)</label></formula><p>where we have included the weight matrix W (l) into the function σ(·) for concision;</p><formula xml:id="formula_3">p(u j |v i ) = a(v i , u j )/N (v i ) defines the probability of sampling u j given v i , with N (v i ) = N j=1â (v i , u j ).</formula><p>A natural idea to speed up Eq. (2) is to approximate the expectation by Monte-Carlo sampling. To be specific, we estimate the expectation</p><formula xml:id="formula_4">µ p (v i ) = E p(uj |vi) [h (l) (u j )] withμ p (v i ) given bŷ µ p (v i ) = 1 n n j=1 h (l) (û j ),û j ∼ p(u j |v i ).<label>(3)</label></formula><p>By setting n N , the Monte-Carlo estimation can reduce the complexity of (1) from O(|E|D (l) D (l−1) ) (|E| denotes the number of edges) to O(n 2 D (l) D (l−1) ) if the numbers of the sampling points for the (l + 1)-th and l-th layers are both n.</p><p>By applying Eq. (3) in a multi-layer network, we construct the network structure in a top-down manner: sampling the neighbours of each node in the current layer recursively (see <ref type="figure">Figure 5</ref> (a)). However, such node-wise sampling is still computationally expensive for deep networks, because the number of the nodes to be sampled grows exponentially with the number of layers. Taking a network with depth d for example, the number of sampling nodes in the input layer will increase to O(n d ), leading to significant computational burden for large d 2 .</p><p>Layer-Wise Sampling. We equivalently transform Eq. (2) to the following form by applying importance sampling, i.e.,</p><formula xml:id="formula_5">h (l+1) (v i ) = σ W (l) (N (v i )E q(uj |v1,··· ,vn) [ p(u j |v i ) q(u j |v 1 , · · · , v n ) h (l) (u j )]),<label>(4)</label></formula><p>where q(u j |v 1 , · · · , v n ) is defined as the probability of sampling u j given all the nodes of the current layer (i.e., v 1 , · · · , v n ). Similarly, we can speed up Eq. (4) by approximating the expectation with the Monte-Carlo mean, namely, computing</p><formula xml:id="formula_6">h (l+1) (v i ) = σ W (l) (N (v i )μ q (v i )) witĥ µ q (v i ) = 1 n n j=1 p(û j |v i ) q(û j |v 1 , · · · , v n ) h (l) (û j ),û j ∼ q(û j |v 1 , · · · , v n ).<label>(5)</label></formula><p>We term the sampling in Eq. (5) as the layer-wise sampling strategy. As opposed to the node-wise method in Eq. <ref type="formula" target="#formula_4">(3)</ref> where the nodes {û j } n j=1 are generated for each parent v i independently, the sampling in Eq. (5) is required to be performed only once. Besides, in the node-wise sampling, the neighborhoods of each node are not visible to other parents; while for the layer-wise sampling all sampling nodes {û j } n j=1 are shared by all nodes of the current layer. This sharing property is able to enhance the message passing at utmost. More importantly, the size of each layer is fixed to n, and the total number of sampling nodes only grows linearly with the network depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Explicit Variance Reduction</head><p>The remaining question for the layer-wise sampling is how to define the exact form of the sampler q(u j |v 1 , · · · , v n ). Indeed, a good estimator should reduce the variance caused by the sampling process, since high variance probably impedes efficient training. For simplicity, we concisely denote the distribution q(u j |v 1 , · · · , v n ) as q(u j ) below.</p><p>According to the derivations of importance sampling in <ref type="bibr" target="#b22">[23]</ref>, we immediately conclude that Proposition 1. The variance of the estimatorμ q (v i ) in Eq. (5) is given by</p><formula xml:id="formula_7">Var q (μ q (v i )) = 1 n E q(uj ) [ (p(u j |v i )|h (l) (u j )| − µ q (v i )q(u j )) 2 q 2 (u j ) ].<label>(6)</label></formula><p>The optimal sampler to minimize the variance Var q(uj ) (μ q (v i )) in Eq. (6) is given by</p><formula xml:id="formula_8">q * (u j ) = p(u j |v i )|h (l) (u j )| N j=1 p(u j |v i )|h (l) (u j )| .<label>(7)</label></formula><p>Unfortunately, it is infeasible to compute the optimal sampler in our case. By its definition, the sampler q * (u j ) is computed based on the hidden feature h (l) (u j ) that is aggregated by its neighborhoods in previous layers. However, under our top-down sampling framework, the neural units of lower layers are unknown unless the network is completely constructed by the sampling.</p><p>To alleviate this chicken-and-egg dilemma, we learn a self-dependent function of each node to determine its importance for the sampling. Let g(x(u j )) be the self-dependent function computed based on the node feature x(u j ). Replacing the hidden function in Eq. <ref type="formula" target="#formula_8">(7)</ref> with g(x(u j )) arrives at</p><formula xml:id="formula_9">q * (u j ) = p(u j |v i )|g(x(u j ))| N j=1 p(u j |v i )|g(x(u j ))| ,<label>(8)</label></formula><p>The sampler by Eq. <ref type="formula" target="#formula_9">(8)</ref> is node-wise and varies for different v i . To make it applicable for the layer-wise sampling, we summarize the computations over all nodes {v i } n i=1 , thus we attain</p><formula xml:id="formula_10">q * (u j ) = n i=1 p(u j |v i )|g(x(u j ))| N j=1 n i=1 p(u j |v i )|g(x(v j ))| .<label>(9)</label></formula><p>In this paper, we define g(x(u j )) as a linear function i.e. g(x(u j )) = W g x(u j ) parameterized by the matrix W g ∈ R 1×D . Computing the sampler in Eq. <ref type="formula" target="#formula_10">(9)</ref> is efficient, since computing p(u j |v i ) (i.e. the adjacent value) and the self-dependent function g(x(u j )) is fast.</p><p>Note that applying the sampler given by Eq. (9) not necessarily results in a minimal variance. To fulfill variance reduction, we add the variance to the loss function and explicitly minimize the variance by model training. Suppose we have a mini-batch of data pairs</p><formula xml:id="formula_11">{(v i , y i )} n i=1 ,</formula><p>where v i is the target nodes and y i is the corresponded ground-true label. By the layer-wise sampling (Eq. <ref type="formula" target="#formula_10">(9)</ref>), the nodes of previous layer are sampled given {v i } n i=1 , and this process is recursively called layer by layer until we reaching the input domain. Then we perform a bottom-up propagation to compute the hidden features and obtain the estimated activation for node v i , i.e.μ q (v i ). Certain nonlinear and soft-max functions are further added onμ q (v i ) to produce the predictionȳ(μ q (v i ))). By taking the classification loss and variance (Eq. <ref type="formula" target="#formula_7">(6)</ref>) into account, we formulate a hybrid loss as</p><formula xml:id="formula_12">L = 1 n n i=1 L c (y i ,ȳ(μ q (v i ))) + λVar q (μ q (v i ))),<label>(10)</label></formula><p>where L c is the classification loss (e.g., the crossing entropy); λ is the trade-off parameter and fixed as 0.5 in our experiments. Note that the activations for other hidden layers are also stochastic, and the resulting variances should be reduced. In Eq. (10) we only penalize the variance of the top layer for efficient computation and find it sufficient to deliver promising performance in our experiments.</p><p>To minimize the hybrid loss in Eq. <ref type="formula" target="#formula_0">(10)</ref>, it requires to perform gradient calculations. For the network parameters, e.g. W (l) in Eq. <ref type="formula" target="#formula_2">(2)</ref>, the gradient calculation is straightforward and can be easily derived by the automatically-differential platform, e.g., TensorFlow <ref type="bibr" target="#b23">[24]</ref>. For the parameters of the sampler, e.g. W g in Eq. (9), calculating the gradient is nontrivial as the sampling process (Eq. <ref type="formula" target="#formula_6">(5)</ref>) is nondifferential. Fortunately, we prove that the gradient of the classification loss with respect to the sampler is zero. We also derive the gradient of the variance term regarding the sampler, and detail the gradient calculation in the supplementary material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Preserving Second-Order Proximities by Skip Connections</head><p>The GCN update in Eq. (1) only aggregates messages passed from 1-hop neighborhoods. To allow the network to better utilize information across distant nodes, we can sample the multi-hop neighborhoods for the GCN update in a similar way as the random walk <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. However, the random walk requires extra sampling to obtain distant nodes which is computationally expensive for dense graphs. In this paper, we propose to propagate the information over distant nodes via skip connections.</p><p>The key idea of the skip connection is to reuse the nodes of the (l − 1)-th layer to preserve the second-order proximity (see the definition in <ref type="bibr" target="#b6">[7]</ref>). For the (l + 1)-th layer, the nodes of the (l − 1)-th layer are actually the 2-hop neighborhoods. If we further add a skip connection from the (l − 1)-th to the (l + 1)-th layer, as illustrated in <ref type="figure">Figure 5</ref> (c), the aggregation will involve both the 1-hop and 2-hop neighborhoods. The calculations along the skip connection are formulated as</p><formula xml:id="formula_13">h (l+1) skip (v i ) = n j=1â skip (v i , s j )h (l−1) (s j )W (l−1) skip , i = 1, · · · , n,<label>(11)</label></formula><p>where s = {s j } n j=1 denote the nodes in the (l − 1)-th layer. Due to the 2-hop distance between v i and s j , the weightâ skip (v i , s j ) is supposed to be the element ofÂ 2 . Here, to avoid the full computation ofÂ 2 , we estimate the weight with the sampled nodes of the l-th layer, i.e.,</p><formula xml:id="formula_14">a skip (v i , s j ) ≈ n k=1â (v i , u k )â(u k , s j ).<label>(12)</label></formula><p>Instead of learning a free W (l−1) skip in Eq. (11), we decompose it to be W</p><formula xml:id="formula_15">(l−1) skip = W (l−1) W (l) ,<label>(13)</label></formula><p>where W (l) and W (l−1) are the filters of the l-th and (l − 1)-th layers in original network, respectively. The output of skip-connection will be added to the GCN layer (Eq.(1)) before nonlinearity.</p><p>By the skip connection, the second-order proximity is maintained without extra 2-hop sampling. Besides, the skip connection allows the information to pass between two distant layers thus enabling more efficient back-propagation and model training.</p><p>While the designs are similar, our motivation of applying the skip connection is different to the residual function in ResNets <ref type="bibr" target="#b0">[1]</ref>. The purpose of employing the skip connection in <ref type="bibr" target="#b0">[1]</ref> is to gain accuracy by increasing the network depth. Here, we apply it to preserve the second-order proximity.</p><p>In contrast to the identity mappings used in ResNets, the calculation along the skip-connection in our model should be derived specifically (see Eq. <ref type="formula" target="#formula_0">(12)</ref> and Eq. <ref type="formula" target="#formula_0">(13)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions and Extensions</head><p>Relation to other sampling methods. We contrast our approach with GraphSAGE <ref type="bibr" target="#b2">[3]</ref> and Fast-GCN <ref type="bibr" target="#b20">[21]</ref> regarding the following aspects:</p><p>1. The proposed layer-wise sampling method is novel. GraphSAGE randomly samples a fixed-size neighborhoods of each node, while FastGCN constructs each layer independently according to an identical distribution. As for our layer-wise approach, the nodes in lower layers are sampled conditioned on the upper ones, which is capable of capturing the between-layer correlations.</p><p>2. Our framework is general. Both GraphSAGE and FastGCN can be categorized as the specific variants of our framework. Specifically, the GraphSAGE model is regarded as a node-wise sampler in Eq (3) if p(u j |v i ) is defined as the uniform distribution; FastGCN can be considered as a special layer-wise method by applying the sampler q(u j ) that is independent to the nodes {v i } n i=1 in Eq. (5). 3. Our sampler is parameterized and trainable for explicit variance reduction. The sampler of GraphSAGE or FastGCN involves no parameter and is not adaptive for minimizing variance. In contrast, our sampler modifies the optimal importance sampling distribution with a self-dependent function. The resulting variance is explicitly reduced by fine-tuning the network and sampler.</p><p>Taking the attention into account. The GAT model <ref type="bibr" target="#b12">[13]</ref> applies the idea of self-attention to graph representation learning. Concisely, it replaces the re-normalization of the adjacency matrix in Eq. (1) with specific attention values, i.e., h (l+1)</p><formula xml:id="formula_16">(v i ) = σ( N j=1 a((h (l) (v i ), (h (l) (u j ))h (l) (v j )W (l) ), where a(h (l) (v i ), h (l) (u j )</formula><p>) measures the attention value between the hidden features v i and u j , which is derived as a(h (l) (v i ), h (l) (u j )) = SoftMax(LeakyReLU(W 1 h (l) (v i ), W 2 h (l) (u j ))) by using the LeakyReLU nonlinearity and SoftMax normalization with parameters W 1 and W 2 .</p><p>It is impracticable to apply the GAT-like attention mechanism directly in our framework, as the probability p(u j |v i ) in Eq. (9) will become related to the attention value a(h (l) (v i ), h (l) (u j )) that is determined by the hidden features of the l-th layer. As discussed in § 4.2, computing the hidden features of lower layers is impossible unless the network is already built after sampling. To solve this issue, we develop a novel attention mechanism by applying the self-dependent function similar to Eq. (9). The attention is computed as</p><formula xml:id="formula_17">a(x(v i ), x(u j )) = 1 n ReLu(W 1 g(x(v i )) + W 2 g(x(u j ))),<label>(14)</label></formula><p>where W 1 and W 2 are the learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We evaluate the performance of our methods on the following benchmarks: (1) categorizing academic papers in the citation network datasets-Cora, Citeseer and Pubmed <ref type="bibr" target="#b10">[11]</ref>; <ref type="formula" target="#formula_2">(2)</ref>   community different posts belong to in Reddit <ref type="bibr" target="#b2">[3]</ref>. These graphs are varying in sizes from small to large. Particularly, the number of nodes in Cora and Citeseer are of scale O(10 3 ), while Pubmed and Reddit contain more than 10 4 and 10 5 vertices, respectively. Following the supervised learning scenario in FastGCN <ref type="bibr" target="#b20">[21]</ref>, we use all labels of the training examples for training. More details of the benchmark datasets and more experimental evaluations are presented in the supplementary material.</p><p>Our sampling framework is inductive in the sense that it clearly separates out test data from training. In contrast to the transductive learning where all vertices should be provided, our approach aggregates the information from each node's neighborhoods to learn structural properties that can be generalized to unseen nodes. For testing, the embedding of a new node may be either computed by using the full GCN architecture or approximated through sampling as is done in model training. Here we use the full architecture as it is more straightforward and easier to implement. For all datasets, we employ the network with two hidden layers as usual. The hidden dimensions for the citation network datasets (i.e., Cora, Citeseer and Pubmed) are set to be 16. For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by <ref type="bibr" target="#b2">[3]</ref>. The numbers of the sampling nodes for all layers excluding the top one are set to 128 for Cora and Citeseer, 256 for Pubmed and 512 for Reddit. The sizes of the top layer (i.e. the stochastic mini-batch size) are chosen to be 256 for all datasets. We train all models using early stopping with a window size of 30, as suggested by <ref type="bibr" target="#b8">[9]</ref>. Further details on the network architectures and training settings are contained in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Alation Studies on the Adaptive Sampling</head><p>Baselines. The codes of GraphSAGE <ref type="bibr" target="#b2">[3]</ref> and FastGCNN <ref type="bibr" target="#b20">[21]</ref> provided by the authors are implemented inconsistently; here we re-implement them based on our framework to make the comparisons more fair <ref type="bibr" target="#b2">3</ref> . In details, we implement the GraphSAGE method by applying the node-wise strategy with a uniform sampler in Eq. <ref type="formula" target="#formula_4">(3)</ref>, where the number of the sampling neighborhoods for each node are set to 5. For FastGCN, we adopt the Independent-Identical-Distribution (IID) sampler proposed by <ref type="bibr" target="#b20">[21]</ref> in Eq. <ref type="formula" target="#formula_6">(5)</ref>, where the number of the sampling nodes for each layer is the same as our method. For consistence, the re-implementations of GraphSAGE and FastGCN are named as Node-Wise and IID in our experiments. We also implement the Full GCN architecture as a strong baseline. All compared methods shared the same network structure and training settings for fair comparison. We have also conducted the attention mechanism introduced in § 6 for all methods.</p><p>Comparisons with other sampling methods. The random seeds are fixed and no early stopping is used for the experiments here. <ref type="figure">Figure 5</ref> reports the converging behaviors of all compared methods during training on Cora, Citeseer and Reddit 4 . It demonstrates that our method, denoted as Adapt, converges faster than other sampling counterparts on all three datasets. Interestingly, our method even outperforms the Full model on Cora and Reddit. Similar to our method, the IID sampling is also layer-wise, but it constructs each layer independently. Thanks to the conditional sampling, our method achieves more stable convergent curve than the IID method as <ref type="figure">Figure 5</ref> shown. It turns out that considering the between-layer information helps in stability and accuracy.</p><p>Moreover, we draw the training time in <ref type="figure" target="#fig_1">Figure 3 (a)</ref>. Clearly, all sampling methods run faster than the Full model. Compared to the Node-Wise method, our approach exhibits a higher training speed due to the more compact architecture. To show this, suppose the number of nodes in the top layer is n, then <ref type="table">Table 1</ref>: Accuracy Comparisons with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cora</head><p>Citeseer Pubmed Reddit KLED <ref type="bibr" target="#b24">[25]</ref> 0.8229 -0.8228 -2-hop DCNN <ref type="bibr" target="#b17">[18]</ref> 0.8677 -0.8976 -FastGCN <ref type="bibr" target="#b20">[21]</ref> 0.8500 0.7760 0.8800 0.9370 GraphSAGE <ref type="bibr" target="#b2">[3]</ref> 0.8220 0.7140 0.8710 0.9432 Full 0.8664 ± 0.0011 0.7934 ± 0.0026 0.9022 ± 0.0008 0.9568 ± 0.0069 IID 0.8506 ± 0.0048 0.7387 ± 0.0078 0.8200 ± 0.0114 0.8611 ± 0.0437 Node-Wise 0.8202 ± 0.0133 0.7734 ± 0.0081 0.9002 ± 0.0017 0.9449 ± 0.0026 Adapt (no vr) 0.8588 ± 0.0062 0.7942 ± 0.0022 0.9060 ± 0.0024 0.9501 ± 0.0047 Adapt 0.8744 ± 0.0034 0.7966 ± 0.0018 0.9060 ± 0.0016 0.9627 ± 0.0032 for the Node-Wise method the input, hidden and top layers are of sizes 25n, 5n and n, respectively, while the numbers of the nodes in all layers are n for our model. Even with less sampling nodes, our model still surpasses the Node-Wise method by the results in <ref type="figure">Figure 5</ref>.</p><p>How important is the variance reduction? To justify the importance of the variance reduction, we implement a variant of our model by setting the trade-off parameter as λ = 0 in Eq. <ref type="bibr" target="#b9">(10)</ref>. By this, the parameters of the self-dependent function are randomly initialized and no training is performed. <ref type="figure">Figure 5</ref> shows that, removing the variance loss does decrease the accuracies of our method on Cora and Reddit. For Citeseer, the effect of removing the variance reduction is not so significant. We conjecture that the average degree of Citeseer (i.e. 1.4) is smaller than Cora (i.e. 2.0) and Reddit (i.e. 492), and penalizing the variance is not so impeding due to the limited diversity of neighborhoods.</p><p>Comparisons with other state-of-the-art methods. We contrast the performance of our methods with the graph kernel method KLED <ref type="bibr" target="#b24">[25]</ref> and Diffusion Convolutional Network (DCN) <ref type="bibr" target="#b17">[18]</ref>. We use the reported results of KLED and DCN on Cora and Pubmed in <ref type="bibr" target="#b17">[18]</ref>. We also summarize the results of GraphSAGE and FastGCN by their original implementations. For GraphSAGE, we report the results by the mean aggregator with the default parameters. For FastGCN, we directly make use of the provided results by <ref type="bibr" target="#b20">[21]</ref>. For the baselines and our approach, we run the experiments with random seeds over 20 trials and record the mean accuracies and the standard variances. All results are organized in <ref type="table">Table 1</ref>. As expected, our method achieves the best performance among all datasets, which are consistent with the results in <ref type="figure">Figure 5</ref>. It is also observed that removing the variance reduction will decrease the performance of our method especially on Cora and Reddit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Evaluations of the Skip Connection</head><p>We evaluate the effectiveness of the skip connection on Cora. For the experiments on other datasets, we present the details in the supplementary material. The original network has two hidden layers. We further add a skip connection between the input and top layers, by using the computations in Eq. <ref type="bibr" target="#b11">(12)</ref> and Eq. <ref type="bibr" target="#b12">(13)</ref>. <ref type="figure">Figure 5</ref> displays the convergent curves of the original Adapt method and its variant with the skip connection, where the random seeds are shared and no early stopping is adapted. Although the improvement by our skip connection is not big regarding the final accuracy, it indeed speeds up the convergence significantly. This can be observed from <ref type="figure" target="#fig_1">Figure 3 (b)</ref> where adding the skip connection reduces the required epoches to converge from around 150 to 100.</p><p>We run experiments with different random seeds over 20 trials and report the mean results obtained by early stopping in <ref type="table" target="#tab_2">Table 2</ref>. It is observed that the skip connection slightly improves the performance. Adapt Adapt+sc Adapt+2-hop 0.8744 ± 0.0034 0.8774 ± 0.0032 0.8814 ± 0.0017</p><p>Besides, we explicitly involve the 2-hop neighborhood sampling in our method by replacing the re-normalization matrixÂ with its 2-order power expansion, i.e.Â +Â 2 . As displayed in <ref type="table" target="#tab_2">Table 2</ref>, the explicit 2-hop sampling further boosts the classification accuracy. Although the skip-connection method is slightly inferior to the explicit 2-hop sampling, it avoids the computation of (i.e.Â 2 ) and yields more computationally beneficial for large and dense graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We present a framework to accelerate the training of GCNs through developing a sampling method by constructing the network layer by layer. The developed layer-wise sampler is adaptive for variance reduction. Our method outperforms the other sampling-based counterparts: GraphSAGE and FastGCN in effectiveness and accuracy on extensive experiments. We also explore how to preserve the second-order proximity by using the skip connection. The experimental evaluations demonstrate that the skip connection further enhances our method in terms of the convergence speed and eventual classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Gradient Calculation</head><p>We prove that the gradient of the expectationμq(vi) in Eq. (5) with respect to the sampler q(uj) is equal to zero. To demonstrate this, we decompose the gradient as</p><formula xml:id="formula_18">∂ ∂q(uj)μ q (vi) = ∂ ∂q(uj) E q(u j ) [ p(uj|vi) q(uj) h (l) (uj)], = ∂ ∂q(uj) E p(u j |v i )) [h (l) (uj)], = 0.<label>(15)</label></formula><p>Hence, the gradient of the classification loss in Eq. (10) regarding the sampler is equal to zero. To perform the gradient calculation for the variance term, we first estimate it with the sampled instances by</p><p>Varq(μq(vi)) = 1 n 2 n j=1 p θ (ûj|vi)|h (l) (ûj)| −μq(vi)q(ûj)</p><formula xml:id="formula_19">q 2 (ûj) 2 ,<label>(16)</label></formula><p>whose gradient is given by</p><formula xml:id="formula_20">∂ ∂q(ûj)</formula><p>Varq(μq(vi))) = − 1 n 2 p θ (ûj|vi)|h (l) (ûj)| p θ (ûj|vi)|h (l) (ûj)| −μq(vi)q(ûj)</p><formula xml:id="formula_21">q 3 (ûj) ,<label>(17)</label></formula><p>where the samples {ûj} n j=1 generated from q(uj) independently.  <ref type="table" target="#tab_3">Table 3</ref>.</p><p>Further implementation details. The initial learning rates for the Adam optimizer are set to be 0.001 for Cora, Citeseer and Pubmed, and 0.01 for Reddit. The weight decays for all datasets are selected to be 0.0004. We apply ReLu function as the activation function and no dropout in our experiments. As presented in the paper, all models are implemented with 2-hidden-layer networks. For the Reddit dataset, we follow the suggestion by <ref type="bibr" target="#b20">[21]</ref> to fix the weight of the bottom layer and pre-compute the productÂH (0) given the input features for efficiency. All experiments are conducted on a single Tesla P40 GPU. We apply the early-stopping for the training with a window size of 30 and apply the model that achieves the best validation accuracy for testing.</p><p>More results on the variance reduction. As shown in <ref type="table">Table 1</ref>, it is sufficient to boost the performance by only reducing the variance of the top layer. Indeed, it is convenient to reduce the variances of all layers in our method, e.g., adding them all to the loss. To show this, we conduct an experiment on Cora by minimizing the variances of both the first and top hidden layers, where the experimental settings are the same as <ref type="table">Table 1</ref>. The result is 0.8780 ± 0.0014, which slightly outperforms the original accuracy in <ref type="table">Table 1</ref> (i.e. 0.8744 ± 0.0034).</p><p>Comparisons with FastGCN by using the official codes. We use the public code to re-run the experiments of FastGCN in <ref type="figure" target="#fig_0">Figure 2</ref> and <ref type="table">Table 1</ref>. The average accuracies of FastGCN for four datasets are 0.840 ± 0.005, 0.774 ± 0.004, 0.881 ± 0.002 and 0.920 ± 0.005. The running curves of <ref type="figure" target="#fig_0">Figure 2</ref> in the paper are updated by <ref type="figure">Figure 5</ref> here. Clearly, our method still outperforms FastGCN remarkably. We have observed the inconsistences between the official implementations of GraphSAGE and FastGraph including the adjacent matrix construction, hidden dimensions, mini-batch sizes, maximal training epoches and other engineering tricks not mentioned in their papers. For fair comparisons, we re-implements them and uses the same experimental settings as our method in the main text. More results on Pubmed. In the paper, <ref type="figure" target="#fig_0">Figure 2</ref> displays the accuracy curves of test data on Cora, Citeseer and Reddit, where the random seeds are fixed. For those on Pubmed, we provide results in <ref type="figure">Figure 5</ref>. Obviously, our method outperforms the IID and Node-Wise counterparts consistently. The Full model achieves the best accuracy around the 30-th epoch, but drops down after the 60-th epoch properly due to the overfitting. In contrast, our performance is more stable and it gives even better results in the end. Performing the variance reduction on this dataset is only helpful during the early stage, but contributes little when the model converges. <ref type="table" target="#tab_3">Table 3</ref> (b) reports the accuracy curve of the model with the skip connection on Cora. Here, we evaluate the effectiveness of the skip connection on Citeseer and Pubmed in <ref type="figure">Figure 6</ref>. It demonstrates that the skip connection is helpful to speed up the convergence on Citeseer. While on the Pubmed dataset, adding the skip connection boosts the performance only during early training epochs. For the Reddit dataset, we can not apply the skip connection in the network since the bottom layer is fixed and the output features are pre-computed. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The accuracy curves of test data on Cora, Citeseer and Reddit. Here, one training epoch means a complete pass of all training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Training time per epoch on Pubmed and Reddit. (b) Accuracy curves of testing data on Cora for our Adapt method and its variant by adding skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The accuracy curves of test data on Cora, Citeseer and Reddit. Here, one training epoch means a complete pass of all training samples. The sampling nodes of each hidden layer for FastGCN and our method on Cora, Citeseer, Pubmed and Reddit are selected as 128, 128, 256, and 512, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>The accuracy curves of test data on Pubmed. Here, a training epoch means a complete pass of all training samples. Accuracy curves of testing data on Citeseer and Pubmed for our Adapt method and its variant by adding skip connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>:1809.05343v3 [cs.CV] 19 Nov 2018</figDesc><table><row><cell></cell><cell>Skip Connection</cell><cell></cell></row><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell></row></table><note>32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.arXiv</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Testing Accuracies on Cora.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Dataset Statistics. The Cora, Citeseer and Pubmed datasets are downloaded from https://github.com/tkipf/ gcn. We follow the setting as<ref type="bibr" target="#b20">[21]</ref> by keeping the validation and test indexes unchanged but using all remaining samples for training. The Reddit dataset is from http://snap.stanford.edu/graphsage/. The statistics of four datasets are summarized in</figDesc><table><row><cell cols="2">Datasets Nodes</cell><cell>Edges</cell><cell cols="3">Classes Features Training/Validation/Testing</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>1, 208/500/1,000</cell></row><row><cell cols="2">Citeseer 3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>1, 812/500/1,000</cell></row><row><cell cols="2">Pubmed 19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell><cell>18, 217/500/1,000</cell></row><row><cell cols="3">Reddit 232,965 11,606,919</cell><cell>41</cell><cell>602</cell><cell>152,410/23,699/55,334</cell></row><row><cell cols="4">10 More Experimental Evaluations</cell><cell></cell><cell></cell></row><row><cell>Datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Here, lower layers denote the ones closer to the input.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">One can reduce the complexity of the node-wise sampling by removing the repeated nodes. Even so, for dense graphs, the sampling nodes will still quickly fills up the whole graph as the depth grows.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also perform experimental comparisons by using the public codes of FastGCN in the supplementary material.<ref type="bibr" target="#b3">4</ref> The results on Pubmed are provided in the supplementary material.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This supplementary material provides the gradient calculation of the loss function ( Eq. (10)) with respect to the sampler. It also contains more setting details and more results for the experiments.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6533" to="6542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust spatial filtering with graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagan</forename><surname>Sah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Alexander</forename><surname>Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhas</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="884" to="896" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust and scalable graph-based semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2624" to="2638" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Monte Carlo theory, methods and examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Art</forename><forename type="middle">B</forename><surname>Owen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An experimental investigation of graph kernels on a collaborative recommendation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Fran?ois Fouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luh</forename><surname>Fran?oisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pirotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Data Mining (ICDM 2006</title>
		<meeting>the 6th International Conference on Data Mining (ICDM 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="863" to="868" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
