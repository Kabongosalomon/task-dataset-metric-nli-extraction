<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
							<email>jielei@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
							<email>licheng@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
							<email>tlberg@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce TV show Retrieval (TVR), a new multimodal retrieval dataset. TVR requires systems to understand both videos and their associated subtitle (dialogue) texts, making it more realistic. The dataset contains 109K queries collected on 21.8K videos from 6 TV shows of diverse genres, where each query is associated with a tight temporal window. The queries are also labeled with query types that indicate whether each of them is more related to video or subtitle or both, allowing for in-depth analysis of the dataset and the methods that built on top of it. Strict qualification and post-annotation verification tests are applied to ensure the quality of the collected data. Further, we present several baselines and a novel Cross-modal Moment Localization (XML) network for multimodal moment retrieval tasks. The proposed XML model uses a late fusion design with a novel Convolutional Start-End detector (ConvSE), surpassing baselines by a large margin and with better efficiency, providing a strong starting point for future work. We have also collected additional descriptions for each annotated moment in TVR to form a new multimodal captioning dataset with 262K captions, named TV show Caption (TVC). 1 Query: Rachel explains to her dad on the phone why she can't marry her fiancé.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Enormous numbers of multimodal videos (with audio and/or text) are being uploaded to the web every day. To enable users to search through these videos and find relevant moments, an efficient and accurate method for retrieval of video data is crucial. Recent works <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b35">9]</ref> introduced the task of Single Video Moment Retrieval (SVMR), whose goal is to retrieve a moment from a single video via a natural language query. Escorcia et al. <ref type="bibr" target="#b34">[8]</ref> extended SVMR to Video Corpus Moment Retrieval (VCMR), where a system is required to retrieve the most relevant moments from a large video corpus instead of from a single video. However, these works rely on a single modality (visual) as the context source for retrieval, as existing moment retrieval datasets <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b58">32,</ref><ref type="bibr" target="#b35">9,</ref><ref type="bibr" target="#b48">22]</ref> are based on videos. In practice, videos are often associated with other modalities such as audio or text, e.g., subtitles for movie/TV-shows or audience discourse accompanying live  <ref type="figure">Fig. 1</ref>: A TVR example in the VCMR task. Ground truth moment is shown in green box. Colors in the query indicate whether the words are related to video (blue) or subtitle (magenta) or both (black ). To better retrieve relevant moments from the video corpus, a system needs to comprehend both videos and subtitles stream videos. These associated modalities could be equally important sources for retrieving user-relevant moments. <ref type="figure">Fig. 1</ref> shows a query example in the VCMR task, in which both videos and subtitles are vital to the retrieval process.</p><p>Hence, to study multimodal moment retrieval with both video and text contexts, we propose a new dataset -TV show Retrieval (TVR). Inspired by recent works <ref type="bibr" target="#b65">[39,</ref><ref type="bibr" target="#b47">21,</ref><ref type="bibr" target="#b50">24]</ref> that built multimodal datasets based on Movie/Cartoon/TV shows, we select TV shows as our data resource as they typically involve rich social interactions between actors, involving both activities and dialogues. During data collection, we present annotators with videos and associated subtitles to encourage them to write multimodal queries. A tight temporal timestamp is labeled for each video-query pair. We do not use predefined fixed segments (as in <ref type="bibr" target="#b40">[14]</ref>) but choose to freely annotate the timestamps for more accurate localization. Moreover, query types are collected for each query to indicate whether it is more related to the video, the subtitle, or both, allowing deeper analyses of systems. To ensure data quality, we set up strict qualification and post-annotation quality verification tests. In total, we have collected 108,965 high-quality queries on 21,793 videos from 6 TV shows, producing the largest dataset of this kind. Compared to existing datasets <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b58">32,</ref><ref type="bibr" target="#b35">9,</ref><ref type="bibr" target="#b48">22]</ref>, we show TVR has greater linguistic diversity ( <ref type="figure" target="#fig_1">Fig. 3</ref>) and involves more actions and people in its queries ( <ref type="table" target="#tab_1">Table 2)</ref>.</p><p>With the TVR dataset, we extend the moment retrieval task to a more realistic multimodal setup where both video and subtitle text need to be considered (i.e., 'Video-Subtitle Moment Retrieval'). In this paper, we focus on the corpuslevel task VCMR , as SVMR can be viewed as a simplified version of VCMR in which the ground-truth video is given beforehand. Prior works <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b35">9,</ref><ref type="bibr" target="#b41">15,</ref><ref type="bibr" target="#b69">43,</ref><ref type="bibr" target="#b36">10,</ref><ref type="bibr" target="#b34">8]</ref> explore the moment retrieval task as a ranking problem over a predefined set of moment proposals. These proposals are usually generated using handcrafted heuristics <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b41">15]</ref> or sliding windows <ref type="bibr" target="#b35">[9,</ref><ref type="bibr" target="#b69">43,</ref><ref type="bibr" target="#b36">10,</ref><ref type="bibr" target="#b34">8]</ref> and are usually not temporally precise, leading to suboptimal performance. Furthermore, these methods may not be easily scaled to long videos: the number of proposals often increase quadratically with video length, making computational costs infeasible. Recent methods <ref type="bibr" target="#b37">[11,</ref><ref type="bibr" target="#b51">25]</ref> adapt start-end span predictors <ref type="bibr" target="#b62">[36,</ref><ref type="bibr" target="#b29">3]</ref> from the reading comprehension task to moment retrieval, by early fusion of video and language (query) features, then applying neural networks on the fused features to predict start-end probabilities. It has been shown <ref type="bibr" target="#b37">[11]</ref> that using span predictors outperforms several proposalbased methods. Additionally, start-end predictors allow a hassle-free extension to long videos, with only linearly increased computational cost. While <ref type="bibr" target="#b37">[11]</ref> has shown promising results in SVMR, it is not scalable to VCMR as it uses expensive early fusion operation. Consider retrieving N queries in a corpus of M videos, the approach in <ref type="bibr" target="#b37">[11]</ref> requires running several layers of LSTM <ref type="bibr" target="#b43">[17]</ref> on M ·N early fused representations to generate the probabilities, which is computationally expensive for large values of M and N .</p><p>To address these challenges, we propose Cross-modal Moment Localization (XML), a late fusion approach for VCMR. In XML, videos (or subtitles) and queries are encoded independently, thus only M +N neural network operations are needed. Furthermore, videos can be pre-encoded and stored. At test time, one only needs to encode new user queries, which greatly reduces user waiting time. Late fusion then integrates video and query representations with highly optimized matrix multiplication to generate 1D query-clip similarity scores over the temporal dimension of the videos. To produce moment predictions from these similarity scores, a naive approach is to rank the aforementioned sliding window proposals with confidence scores computed as the average of the similarity scores inside each proposal region. Alternatively, one can use TAG <ref type="bibr" target="#b76">[50]</ref> to progressively group top-scored clips. However, these methods rely on handcrafted rules and are not end-to-end trainable. Inspired by image edge detectors <ref type="bibr" target="#b64">[38]</ref> in image processing, we propose Convolutional Start-End detector (ConvSE) that learns to detect start (up) and end (down) edges in the similarity signals with two trainable 1D convolution filters. Using the same backbone net, we show ConvSE has better performance than both approaches. With late fusion and ConvSE, we further show XML outperforms previous methods <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b34">8,</ref><ref type="bibr" target="#b37">11]</ref>, and does this with better computational efficiency.</p><p>To summarize, our contributions are three-fold: (i) We introduce TVR dataset, a large-scale multimodal moment retrieval dataset with 109K highquality queries of great linguistic diversity. (ii) We propose XML, an efficient approach that uses a late fusion design for the VCMR task. The core of XML is our novel ConvSE module which learns to detect start-end edges in 1D similarity signals with 2 convolution filters. Comprehensive experiments and analyses show XML surpasses all presented baselines by a large margin and runs with better efficiency. (iii) We have also collected additional descriptions for each annotated moment in TVR to form a new multimodal captioning dataset with 262K captions, named TV show Caption (TVC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The goal of natural language-based moment retrieval is to retrieve relevant moments from a single video <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b35">9]</ref> or from a large video corpus <ref type="bibr" target="#b34">[8]</ref>. In the following, we present a brief overview of the community efforts on these tasks and make distinctions between existing works and ours.</p><p>Datasets. Several datasets have been proposed for the task, e.g., DiDeMo <ref type="bibr" target="#b40">[14]</ref>, ActivityNet Captions <ref type="bibr" target="#b48">[22]</ref>, CharadesSTA <ref type="bibr" target="#b35">[9]</ref>, and TACoS <ref type="bibr" target="#b58">[32]</ref>, where queries can be localized solely from video. TVR differs from them by requiring additional text (subtitle) information in localizing the queries. Two types of data annotation have been explored in previous works: (i) uniformly chunking videos into segments and letting an annotator pick one (or more) and write an unambiguous description <ref type="bibr" target="#b40">[14]</ref>. For example, moments in DiDeMo <ref type="bibr" target="#b40">[14]</ref> are created from fixed 5-second segments. However, such coarse temporal annotations are not well aligned with natural moments. In TVR, temporal windows are freely selected to more accurately capture important moments. (ii) converting a paragraph written for a whole video into separate query sentences <ref type="bibr" target="#b58">[32,</ref><ref type="bibr" target="#b35">9,</ref><ref type="bibr" target="#b48">22]</ref>. While it is natural for people to use temporal connectives (e.g., 'first', 'then') and anaphora (e.g., pronouns) <ref type="bibr" target="#b60">[34]</ref> in a paragraph, these words make individual sentences less suitable as retrieval queries. In comparison, the TVR annotation process encourages annotators to write queries individually without requiring the context of a paragraph. Besides, TVR also has a larger size and greater linguistic diversity, see Sec. 3.2.</p><p>Methods. Existing works <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b35">9,</ref><ref type="bibr" target="#b41">15,</ref><ref type="bibr" target="#b69">43,</ref><ref type="bibr" target="#b36">10,</ref><ref type="bibr" target="#b34">8]</ref> pose moment retrieval as ranking a predefined set of moment proposals. These proposals are typically generated with handcrafted rules <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b41">15]</ref> or sliding windows <ref type="bibr" target="#b35">[9,</ref><ref type="bibr" target="#b69">43,</ref><ref type="bibr" target="#b36">10,</ref><ref type="bibr" target="#b34">8]</ref>. Typically, such proposals are not temporally precise and are not scalable to long videos due to high computational cost. <ref type="bibr" target="#b35">[9,</ref><ref type="bibr" target="#b69">43,</ref><ref type="bibr" target="#b36">10]</ref> alleviate the first with a regression branch that offsets the proposals. However, they are still restricted by the coarseness of the initial proposals. Inspired by span predictors in reading comprehension <ref type="bibr" target="#b62">[36,</ref><ref type="bibr" target="#b29">3]</ref> and action localization <ref type="bibr" target="#b53">[27]</ref>, we use start-end predictors to predict start-end probabilities from early fused query-video representations. Though these methods can be more flexibly applied to long videos and have shown promising performance on single video moment retrieval, the time cost of early fusion becomes unbearable when dealing with the corpus level moment retrieval problem: they require early fusing every possible query-video pair <ref type="bibr" target="#b34">[8]</ref>. Proposal based approaches MCN <ref type="bibr" target="#b40">[14]</ref> and CAL <ref type="bibr" target="#b34">[8]</ref> use a late fusion design, in which the video representations can be pre-computed and stored, making the retrieval more efficient. The final moment predictions are then made by ranking the Squared Euclidean Distances between the proposals w.r.t. a given query. However, as they rely on predefined proposals, MCN and CAL still suffer from the aforementioned drawbacks, leading to less precise predictions and higher costs (especially for long videos). Recent works <ref type="bibr" target="#b74">[48,</ref><ref type="bibr" target="#b30">4,</ref><ref type="bibr" target="#b75">49]</ref> consider word-level early fusion with the videos, which can be even more expensive. In contrast, XML uses a late fusion design with a novel Convolutional Start-End (ConvSE) detector, which produces more accurate moment predictions while reducing the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>Our TVR dataset is built on 21,793 videos from 6 long-running TV shows across 3 genres (sitcom, medical, crime), provided by TVQA <ref type="bibr" target="#b50">[24]</ref>. Videos are paired with subtitles and are on average 76.2 seconds in length. In the following, we describe how we collected TVR and provide a detailed analysis of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>We used Amazon Mechanical Turk (AMT) for TVR data collection. Each AMT worker was asked to write a query using information from the video and/or subtitle, then mark the start and end timestamps to define a moment that matches the written query. This query-moment pair is required to be a unique match within the given video, i.e., the query should be a referring expression <ref type="bibr" target="#b46">[20,</ref><ref type="bibr" target="#b40">14]</ref> that uniquely localizes the moment. We additionally ask workers to select a query type from three types: video-only -queries relevant to the visual content only, sub-only -queries relevant to the subtitles only, and video+sub -queries that involve both. In our pilot study, we found workers preferred to write sub-only queries. A similar phenomenon was observed in TVQA <ref type="bibr" target="#b50">[24]</ref>, where people can achieve 72.88% QA accuracy by reading the subtitles only. Therefore, to ensure that we collect a balance of queries requiring one or both modalities, we split the data annotation into two roundsvisual round and textual round. For the visual round, we encourage workers to write queries related to the visual content, including both video-only and video+sub queries. For the textual round, we encourage sub-only and video+sub queries. We ensure data quality with the following strategies: 2 Qualification Test. We designed a set of 12 multiple-choice questions as our qualification test and only let workers who correctly answer at least 9 questions participate in our annotation task, ensuring that workers understand our task requirements well. In total, 1,055 workers participated in the test, with a pass rate of 67%. Adding this qualification test greatly improved data quality. Automatic Check. During collection, we used an automatic tool checking that all required annotations (query, timestamps, etc) have been performed and each query contains at least 8 words and is not copied from the subtitle. Manual Check. Additional manual check of the collected data was done in house throughout the collection process. Those disqualified queries were re-annotated and workers with disqualified queries were removed from our worker list. Post-Annotation Verification. To verify the quality of the collected data, we performed a post-annotation verification experiment. We set up another AMT task where workers were required to rate the quality of the collected querymoment pairs based on relevance, is the query-moment pair a unique-match, etc. The rating was done in a likert-scale manner with 5 options: strongly agree, agree, neutral, disagree and strongly disagree. Results show that 92% of the pairs have a rating of at least neutral. We further analyzed the group of queries that were rated as strongly disagree, and found that 80% of them were still of acceptable  <ref type="bibr" target="#b58">[32,</ref><ref type="bibr" target="#b40">14,</ref><ref type="bibr" target="#b48">22,</ref><ref type="bibr" target="#b35">9]</ref>, TVR has relatively shorter moments (normalized) and longer queries. Best viewed digitally with zoom Right: CDF of queries ordered by frequency, to obtain this plot, we sampled 10K queries from each dataset, we consider two queries to be the same if they exact match, after tokenization and lemmatization, following <ref type="bibr" target="#b73">[47]</ref>. Compared to existing moment retrieval datasets <ref type="bibr" target="#b58">[32,</ref><ref type="bibr" target="#b40">14,</ref><ref type="bibr" target="#b48">22,</ref><ref type="bibr" target="#b35">9]</ref>, TVR has greater diversity, i.e., it has more unique 4-grams and almost every TVR query is unique. Best viewed digitally with zoom quality: e.g., slightly mismatched timestamps (≤1 sec.). This verification was conducted on 3,600 query-moment pairs. Details are presented in Sec. A.1.</p><p>Given the high quality demonstrated by this verification, we did not further annotate each query, instead prioritizing collection toward adding more TVR queries, and collecting additional captions for each annotated moment to form TVC, a large-scale multimodal video captioning dataset with 262K captions. See details in Sec. D  <ref type="table" target="#tab_0">Table 1</ref> shows an overview of TVR and its comparisons with existing moment retrieval datasets <ref type="bibr" target="#b58">[32,</ref><ref type="bibr" target="#b35">9,</ref><ref type="bibr" target="#b48">22,</ref><ref type="bibr" target="#b40">14]</ref>. TVR contains 109K human annotated query-moment pairs on 21.8K videos, making it the largest of its kind. Moments have an average length of 9.1 seconds, and are annotated with tight start and end timestamps, enabling training and evaluating on more precise localization. Compared to existing datasets, TVR has relatively shorter (video-length normalized) moments and longer queries ( <ref type="figure">Fig. 2</ref>). It also has greater linguistic diversity ( <ref type="figure" target="#fig_1">Fig. 3</ref>): it has more unique 4-grams and almost every query is unique, making the textual understanding of TVR more challenging. As TVR is collected on TV shows, query-moment matching often involves understanding rich interactions between characters. <ref type="table" target="#tab_1">Table 2</ref> shows a comparison of the percentages of queries that involve more than one action or person across different datasets. 66% of TVR queries involve at least two people and 67% involve at least two actions, both of which are significantly higher than those of other datasets. This makes TVR an interesting testbed for studying multimodal interactions between people. Additionally, each TVR query is labeled with a query type, indicating whether this query is based on video, subtitle or both, which can be used for deeper analyses of the systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Analysis and Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Cross-modal Moment Localization (XML)</head><p>In VCMR, the goal is to retrieve a moment from a large video corpus</p><formula xml:id="formula_0">V ={v i } n i=1</formula><p>given a query q j . Each video v i is represented as a list of consecutive short clips, i.e., v i =[c i,1 , c i,2 , ..., c i,l ]. In TVR, each short clip is also associated with temporally aligned subtitle sentences. The retrieved moment is denoted as (VR) in its shallower layers and more fine-grained moment retrieval in its deeper layers. It uses a late fusion design with a novel Convolutional Start-End (ConvSE) detector, making the moment predictions efficient and accurate.</p><formula xml:id="formula_1">v i [t st :t ed ]=[c i,tst , c i,tst+1 , ..., c i,t ed ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">XML Backbone Network</head><p>Input Representations. To represent videos, we consider both appearance and motion features. For appearance, we extract 2048D ResNet-152 <ref type="bibr" target="#b39">[13]</ref> features at 3FPS and max-pool the features every 1.5 seconds to get a clip-level feature. For motion, we extract 1024D I3D <ref type="bibr" target="#b28">[2]</ref> features every 1.5 seconds. The ResNet-152 model is pre-trained on ImageNet <ref type="bibr" target="#b31">[5]</ref> for image recognition, and the I3D model is pre-trained on Kinetics-600 <ref type="bibr" target="#b45">[19]</ref> for action recognition. The final video representation is the concatenation of the two features after L2-normalization, denoted as E v ∈ R l×3072 , where l is video length (#clips). We extract contextualized text features using a 12-layer RoBERTa <ref type="bibr" target="#b54">[28]</ref>. Specifically, we first fine-tune RoBERTa using the queries and subtitle sentences in TVR train-split with MLM objective <ref type="bibr" target="#b33">[7]</ref>, then fix the parameters to extract contextualized token embeddings from its second-to-last layer <ref type="bibr" target="#b51">[25]</ref>. For queries, we directly use the extracted token embeddings, denoted as E q ∈ R lq×768 , where l q is query length (#words). For subtitles, we first extract token-level embeddings, then max-pool them every 1.5 seconds to get a 768D clip-level feature vector. We use a 768D zero vector if encountering no subtitle. The final subtitle embedding is denoted as E s ∈ R l×768 . The extracted features are projected into a low-dimensional space via a linear layer with ReLU <ref type="bibr" target="#b38">[12]</ref>. We then add learned positional encoding <ref type="bibr" target="#b33">[7]</ref> to the projected features. Without ambiguity, we reuse the symbols by denoting the processed features as</p><formula xml:id="formula_2">E v ∈ R l×d , E s ∈ R l×d , E q ∈ R lq×d , where d is hidden size.</formula><p>Query Encoding. As TVR queries can be related to either video or subtitle, we adopt a modular design to dynamically decompose the query into two modularized vectors. Specifically, the query feature is encoded using a Self-Encoder, consisting of a self-attention <ref type="bibr" target="#b66">[40]</ref> layer and a linear layer, with a residual <ref type="bibr" target="#b39">[13]</ref> connection followed by layer normalization <ref type="bibr" target="#b27">[1]</ref>. We denote the encoded query as H q ∈ R lq×d . Then, we apply two trainable modular weight vectors w m ∈ R d , m ∈ {v, s} to compute the attention scores of each query word w.r.t. the video (v) or subtitle (s). The scores are used to aggregate the information of H q ={h q r } lq r=1 to generate modularized query vectors q m ∈ R d <ref type="bibr" target="#b72">[46]</ref>:</p><formula xml:id="formula_3">a m r = exp(w T m h q r ) lq k=1 exp(w T m h q k ) , q m = lq r=1 a m r h q r , where m ∈ {v, s}.<label>(1)</label></formula><p>Context Encoding. Given the video and subtitle features E v , E s , we use two Self-Encoders to compute their single-modal contextualized features H v 0 ∈ R l×d and H s 0 ∈ R l×d . Then, we encode their cross-modal representations via Cross-Encoder. which takes as input the self-modality and cross-modality features, and encodes the two via cross-attention <ref type="bibr" target="#b66">[40]</ref> followed by a linear layer, a residual connection, a layer normalization, and another Self-Encoder. We denote the final video and subtitle representations as H v 1 ∈ R l×d and H s 1 ∈ R l×d , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convolutional Start-End Detector</head><p>Given H v 1 , H s 1 and q v , q s , we compute query-clip similarity scores S query-clip ∈ R l :</p><formula xml:id="formula_4">S query-clip = 1 2 (H v 1 q v + H s 1 q s ).<label>(2)</label></formula><p>To produce moment predictions from S query-clip , one could rank sliding window proposals with confidence scores computed as the average of scores in each proposal region, or use TAG <ref type="bibr" target="#b76">[50]</ref> to progressively group top-scored regions. However, both methods require handcrafted rules and are not trainable. Inspired by edge detectors in image processing <ref type="bibr" target="#b64">[38]</ref>, we propose Convolutional Start-End detector (ConvSE) with two 1D convolution filters to learn to detect start (up) and end (down) edges in the score curves. Clips inside a semantically close span will have higher similarity to the query than those outside, naturally forming detectable edges around the span. <ref type="figure">Fig. 4</ref> (right) and <ref type="figure">Fig. 7</ref> show examples of the learned ConvSE filters applied to the similarity curves. Specifically, we use two trainable filters (no bias) to generate the start (st) and end (ed) scores:</p><formula xml:id="formula_5">S st = Conv1D st (S query-clip ), S ed = Conv1D ed (S query-clip ).<label>(3)</label></formula><p>The scores are normalized with softmax to output the probabilities P st , P ed ∈ R l . In Sec. 5.3, we show ConvSE outperforms the baselines and is also interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Inference</head><p>Video Retrieval. Given the modularized queries q v , q s and the encoded contexts H v 0 , H s 0 , we compute the video-level retrieval (VR) score as:</p><formula xml:id="formula_6">s vr = 1 2 m∈{v,s} max( H m 0 H m 0 q m q m ).<label>(4)</label></formula><p>This essentially computes the cosine similarity between each clip and query and picks the maximum. The final VR score is the average of the scores from the two modalities. During training, we sample two negative pairs (q i , v j ) and (q z , v i ) for each positive pair of (q i , v i ) to calculate a combined hinge loss as <ref type="bibr" target="#b72">[46]</ref>:</p><formula xml:id="formula_7">L vr = 1 n i [max(0, ∆ + s vr (v j |q i ) − s vr (v i |q i )) + max(0, ∆ + s vr (v i |q z ) − s vr (v i |q i ))].<label>(5)</label></formula><p>Single Video Moment Retrieval. Given the start, end probabilities P st , P ed , we define single video moment retrieval loss as:</p><formula xml:id="formula_8">L svmr = − 1 n i [log(P i,st (t i st )) + log(P i,ed (t i ed ))],<label>(6)</label></formula><p>where t i st and t i ed are the ground-truth indices. At inference, predictions can be generated from the probabilities in linear time using dynamic programming <ref type="bibr" target="#b62">[36]</ref>. The confidence score of a predicted moment [t st , t ed ] is computed as:</p><formula xml:id="formula_9">s svmr (t st , t ed ) = P st (t st )P ed (t ed ), t st ≤ t ed .<label>(7)</label></formula><p>To use length prior, we add an additional constraint L min ≤ t ed − t st + 1 ≤ L max . For TVR, we set L min =2 and L max =16 for clip length 1.5 seconds.</p><p>Video Corpus Moment Retrieval. Our final training loss combines both: L vcmr = L vr + λL svmr , where the hyperparameter λ is set as 0.01. At inference, we compute the VCMR score with the following aggregation function:</p><formula xml:id="formula_10">s vcmr (v j , t st , t ed |q i ) = s svmr (t st , t ed |v j , q i )exp(αs vr (v j |q i )),<label>(8)</label></formula><p>where s vcmr (v j , t st , t ed |q i ) is the retrieval score of moment v j [t st :t ed ] w.r.t. the query q i . The exponential term and the hyperparameter α are used to balance the importance of the two scores. A higher α encourages more moments from top retrieved videos. Empirically, we find α=20 works well. At inference, for each query, we first retrieve the top 100 videos based on s vr , then rank all the moments in the 100 videos by s vcmr to give the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data, Metrics and Implementation Details</head><p>Data. TVR contains 109K queries from 21.8K videos. We split TVR into 80% train, 10% val, 5% test-public and 5% test-private splits such that videos and their associated queries appear in only one split. test-public will be used for a public leaderboard, test-private is reserved for future challenges. Metrics. Following <ref type="bibr" target="#b34">[8,</ref><ref type="bibr" target="#b35">9]</ref>, we use average recall at K (R@K) over all queries as our metric. A prediction is correct if: (i) predicted video matches the ground truth; (ii) predicted span has high overlap with the ground truth where temporal intersection over union (IoU) is used to measure overlap.</p><p>Implementation Details. All baseline comparisons are configured to use the same hidden size as XML. We train the baselines following the original papers. We use the same features for all the models. To support retrieval using subtitle for the baselines, we add a separate subtitle stream and average the final predictions from both streams. Non-maximum suppression is not used as we do not observe consistent performance gain on the val set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines Comparison</head><p>In this section, we compare XML with baselines on TVR test-public set (5,445 queries and 1,089 videos). We report the runtime for top-performing methods, averaged across 3 runs on an RTX 2080Ti GPU. Time spent on data loading, preprocessing, backend model (i.e., ResNet-152, I3D, RoBERTa) feature extraction, etc, is ignored since they should be similar for all methods. We mainly focus on the VCMR task here. In Sec. B and Sec. C, we include additional experiments: (1) model performance on single video moment retrieval and video retrieval tasks; <ref type="bibr" target="#b28">(2)</ref> computation and storage cost comparison in a 1M videos corpus; (3) Temporal Endpoint Feature (TEF) <ref type="bibr" target="#b40">[14]</ref> model results; (4) feature and model architecture ablation studies; (5) VCMR results on DiDeMo <ref type="bibr" target="#b40">[14]</ref> dataset, etc.</p><p>Proposal based Methods. MCN <ref type="bibr" target="#b40">[14]</ref> and CAL [8] pose the moment retrieval task as a ranking problem in which all moment proposal candidates are ranked based on their squared Euclidean Distance with the queries. For VCMR, they require directly ranking all the proposals (95K in the following experiments) in the video corpus for each query, which can be costly and difficulty. In contrast, XML uses a hierarchical design that performs video retrieval in its shallow layers and moment retrieval on the retrieved videos in its deeper layers. In <ref type="table" target="#tab_3">Table 3</ref>, XML is showing to have significantly higher performance than MCN and CAL. Retrieval+Re-ranking Methods. We also compare to methods under the retrieval+re-ranking setting <ref type="bibr" target="#b34">[8]</ref> where we first retrieve a set of candidate videos using a given method and then re-rank the moment predictions in the candidate videos using another method. Specifically, we first use MEE <ref type="bibr" target="#b55">[29]</ref> to retrieve 100 videos for each query as candidates. Then, we use MCN and CAL to rank all of the proposals in the candidate videos. ExCL <ref type="bibr" target="#b37">[11]</ref> is an early fusion method designed for SVMR, with a start-end predictor. We adapt it to VCMR by combining MEE video-level scores with ExCL moment-level scores, using Eq. 8. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Compared to their purely proposal based counterparts (i.e., MCN and CAL), both MEE+MCN and MEE+CAL achieve significant performance gain, showing the benefit of reducing the number of proposals needed to rank (by reducing the number of videos). However, they are still far below XML as they use very coarse-grained, predefined proposals. In Sec. 5.3, we show our start-end detector performs consistently better than predefined proposals <ref type="bibr" target="#b34">[8,</ref><ref type="bibr" target="#b76">50]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Analysis</head><p>Video vs. Subtitle. In <ref type="figure" target="#fig_2">Fig. 5</ref>, we compare to XML variants that use only video or subtitle. We observe that the full video+subtitle model has better overall performance than single modality models (video and subtitle), demonstrating that both modalities are useful. We also see that a model trained on one modality does not perform well on the queries tagged by another modality, e.g., the video model performs much worse on sub-only queries compared to the subtitle model.  (2) TAG <ref type="bibr" target="#b76">[50]</ref> that progressively groups top-scored clips with the classical watershed algorithm <ref type="bibr" target="#b59">[33]</ref>. Since these two methods do not produce start-end probabilities, we cannot train the model with the objective in Eq. 6. Thus, we directly optimize the query-clip similarity scores in Eq.2 with Binary Cross Entropy loss: we assign a label of 1 if the clip falls into the ground-truth region, 0 otherwise. While both sliding window and TAG approaches rely on handcrafted rules, ConvSE learns from data. We show in <ref type="figure">Fig. 6 (left)</ref>, under the same XML backbone network, ConvSE has consistent better performance across all IoU thresholds on both VCMR and SVMR tasks. In <ref type="figure">Fig. 6 (right)</ref>, we vary the kernel size (k ) of ConvSE filters. While the performance is reasonable when k=3, 5 or 7, we observe a significant performance drop at k=1. In this case, the filters essentially degrade to scaling factors on the scores. This comparison demonstrates that neighboring information is important. <ref type="figure">Fig. 7</ref> shows examples of using the learned convolution filters: the filters output stronger responses to the up (Start) and down (End ) edges of the score <ref type="figure">Fig. 8</ref>: XML prediction examples for VCMR, on TVR val set. We show top-3 retrieved moments for each query. Top row shows modular attention scores for query words. Left column shows a correct prediction, right column shows a failure. Text inside dashed boxes is the subtitles associated with the predicted moments. Orange box shows the predictions, green bar shows the ground truth curves and thus detect them. Interestingly, the learned weights Conv1D st and Conv1D ed in <ref type="figure">Fig. 7</ref> are similar to the edge detectors in image processing <ref type="bibr" target="#b64">[38]</ref>.</p><p>Qualitative Analysis. <ref type="figure">Fig. 8</ref> shows XML example predictions on the TVR val set. In the top row, we also show the query word attention scores for video and subtitle, respectively. <ref type="figure">Fig. 8 (left)</ref> shows a correct prediction. The top-2 moments are from the same video and are both correct. The third moment is retrieved from a different video. While incorrect, it is still relevant as it also happens in a 'restaurant'. <ref type="figure">Fig. 8 (right)</ref> shows a failure. It is worth noting that the false moments are very close to the correct prediction with minor differences ('on the shoulder' vs. 'around the shoulder'). Besides, it is also interesting to see which words are important for video or subtitle. For example, the words 'waitress', 'restaurant', 'menu' and 'shoulder' get the most weight for video; while the words 'Rachel', 'menu', 'Barney', 'Ted' have higher attention scores for subtitle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we present TVR, a large-scale dataset designed for multimodal moment retrieval tasks. Detailed analyses show TVR is of high quality and is more challenging than previous datasets. We also propose Cross-modal Moment Localization (XML), an efficient model suitable for the VCMR task. 4031, DARPA KAIROS Grant #FA8750-19-2-1004, ARO-YIP Award #W911NF-18-1-0336, and Google Focused Research Award.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional TVR Data Details</head><p>A.1 Data Collection TVR Data Collection Procedure. In <ref type="figure">Fig. 9</ref> we show an overview of TVR data collection procedure. For details of each step, please refer to both Sec. 3.1 and the rest of this section. <ref type="bibr" target="#b28">2</ref> Step 2: Annotate Queries 4</p><p>Step 4: Manual Check  Step 5: Post-Annotation Verification 5 <ref type="figure">Fig. 9</ref>: TVR data collection procedure Qualification Test. We designed a qualification test with 12 multiple-choice questions and only let workers who correctly answer at least 9 questions participate in our annotation task, ensuring that workers understand our task requirements well. In total, 1,055 workers participated in the test, with a pass rate of 67%. Adding this qualification test greatly improved data quality. In <ref type="figure" target="#fig_5">Fig. 10</ref>, we show a question from our qualification test. This particular question is designed to make sure the annotators write relevant and correct descriptions (queries).</p><p>Post-Annotation Verification. To verify the quality of the collected data, we performed a post-annotation verification experiment. We set up another AMT task where workers were required to rate the quality of the collected querymoment pairs based on relevance, is the query-moment pair a unique-match, etc. The rating was done in a likert-scale manner with 5 options: strongly agree, agree, neutral, disagree and strongly disagree, as is shown in <ref type="figure">Fig. 11</ref>. Results show that 92% of the pairs have a rating of at least neutral. This verification was conducted on 3,600 query-moment pairs. Detailed rating distribution is shown in <ref type="figure">Fig. 12</ref>.</p><p>We further analyzed the group of queries that were rated as strongly disagree, and   found that 80% of them were still of acceptable quality: e.g., slightly mismatched timestamps (≤1 sec.). For the group of queries that were rated as disagree, this number is 90%. This verification demonstrates the high quality of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Data Analysis</head><p>Statistics by TV Show. TVR is built on 21,793 videos (provided by TVQA <ref type="bibr" target="#b50">[24]</ref>) from 6 long-running TV shows: The Big Bang Theory, Friends, How I Met You Mother, Grey's Anatomy, House, Castle. <ref type="table" target="#tab_7">Table 4</ref> shows detailed statistics.</p><p>Moments and Queries. <ref type="figure" target="#fig_1">Fig. 13 (left)</ref> shows TVR moment length distribution. The majority of the moments are relatively short, with an average length of 9.1 secs. As a comparison, the average length of the videos is 76.2 secs. <ref type="figure" target="#fig_1">Fig. 13 (right)</ref> shows the video-length normalized moment center distributions. More moments are located at the beginning of the videos. A similar phenomenon was observed in DiDeMo <ref type="bibr" target="#b40">[14]</ref>. <ref type="figure">Fig. 14</ref>   We notice that TVR covers a wide range of common objects/scenes and actions, while also has many genre-specific words such as 'patient' and 'hospital'.</p><p>Video Comparison. TVR videos are from 6 TV shows of 3 different genres, covering a diverse set of objects/scenes/activities. In <ref type="figure">Fig. 19</ref>, we compare TVR videos with videos from existing datasets <ref type="bibr" target="#b58">[32,</ref><ref type="bibr" target="#b35">9,</ref><ref type="bibr" target="#b48">22,</ref><ref type="bibr" target="#b40">14]</ref>. Each TVR video typically has more visual diversity, i.e., more camera viewpoints, activities and people, etc. Frequency Baseline. Following prior works <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b34">8]</ref>, we first discretize the videolength normalized start-end points, then use moments with most frequent startend points as predictions. For video retrieval, we randomly sample videos from the dataset. The results of this baseline is presented in <ref type="table" target="#tab_9">Table 5</ref>. We observe this baseline has slightly better performance than chance, we hypothesize it is mainly caused by the fact that the annotators tend to annotate the first few seconds of the video <ref type="bibr" target="#b40">[14]</ref>, as we have shown in <ref type="figure" target="#fig_1">Fig. 13 (Right)</ref>.</p><p>Models Trained with TEF. It is shown in <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b34">8]</ref> that adding Temporal Endpoint Feature (TEF) <ref type="bibr" target="#b40">[14]</ref> improves models' performance in moment retrieval tasks.</p><p>In <ref type="table" target="#tab_9">Table 5</ref>, we compare models trained with TEF. In most cases, adding TEF increases models' performance, which suggests there exists a certain degree of bias in the proposed dataset. This phenomenon is also observed by recent works <ref type="bibr" target="#b40">[14,</ref><ref type="bibr" target="#b34">8]</ref> in various moment retrieval datasets, i.e., DiDeMo <ref type="bibr" target="#b40">[14]</ref>, CharadesSTA <ref type="bibr" target="#b35">[9]</ref> and ActivityNet Captions <ref type="bibr" target="#b48">[22]</ref>. We attribute this phenomenon into two aspects: (1)moment distribution bias -the moments are not evenly distributed over the video, e.g., in TVR and DiDeMo <ref type="bibr" target="#b40">[14]</ref>, there are more moments appear at the beginning of the video.</p><p>(2)language timestamp correlation bias -some query words are highly indicative of the potential temporal location of the queries, e.g., temporal connectives like 'first' strongly indicate the associated query might be located around the beginning of the video and pronouns like 'He' may suggest this query should not be placed at the beginning of the video as people would usually not use pronouns when they first mention someone. The second bias commonly exists in datasets that are built by converting paragraphs into separate sentences, i.e., CharadesSTA <ref type="bibr" target="#b35">[9]</ref>, TACoS <ref type="bibr" target="#b58">[32]</ref> and ActivityNet Captions <ref type="bibr" target="#b48">[22]</ref>. TVR avoids this bias by explicitly ask annotators to write queries as individual sentences without requiring the context of a paragraph.</p><p>XML with Sliding Windows. In Sec. 5.3, we compared XML variants with different proposal generation strategies. In <ref type="table" target="#tab_9">Table 5</ref>, we further compare XML (sw, sliding window) with MCN/CAL models. For details of this variant, see Sec. 5.3. Compared to the best baseline (MEE+CAL), using the same set of sliding window proposals, we observe XML (sw) still perform much better (3.82 vs. 0.97, R@1 IoU=0.7). We hypothesize that the lower performance of MCN/CAL models compared to XML is mainly caused by the difficulties of training and ranking with a large pool of proposal candidates (1.5M proposals for TVR train). Both MCN and CAL are trained with a ranking objective, which relies on informative negatives to learn effectively. However, effective negative sampling in such a large pool of candidates can be challenging. In comparison, XML breaks the video corpus level moment retrieval problem into two sub-problems: video-level and moment-level retrieval. At video-level retrieval, XML performs ranking within a small set of videos (17.4K), which eases the aforementioned issue. At momentlevel, XML (sliding window) utilizes Binary Cross Entropy to maximize the similarity scores of each ground-truth clip, eliminating the need for manually designing a negative sampling strategy.</p><p>Model Architecture. <ref type="table" target="#tab_10">Table 6</ref> presents a model architecture ablation. We first compare with different self-encoder architectures, replacing our transformer style encoder with a bidirectional LSTM encoder <ref type="bibr" target="#b50">[24]</ref> or a CNN encoder <ref type="bibr" target="#b71">[45,</ref><ref type="bibr" target="#b51">25]</ref>. We observe worse performance after the change and attribute this performance drop to the ineffectiveness of LSTMs and CNNs to capture long-term dependencies <ref type="bibr" target="#b42">[16,</ref><ref type="bibr" target="#b66">40]</ref>. Next, we compare XML with a variant that uses a single max-pooled query instead of two modularized queries. Across all metrics, XML performs better than the variant without modular queries, showing the importance of considering different query representations in matching the context from different modalities.  Feature Ablation. We tested XML model with different visual features, the results are shown in <ref type="table" target="#tab_11">Table 7</ref>. The model that uses both static appearance features (ResNet <ref type="bibr" target="#b39">[13]</ref>) and action features (I3D <ref type="bibr" target="#b28">[2]</ref>) outperforms models using only one of the features, demonstrating the importance of recognizing both the objects and the actions in the VCMR task.</p><p>Retrieval Efficiency in 1M Videos. We consider Video Corpus Moment Retrieval in a video corpus containing 1M videos with 100 queries. Following <ref type="bibr" target="#b34">[8]</ref>, we conduct this experiment in a simulated setting with each video containing 20 clips with max moment length of 14 clips. Each query containing 15 words. We report the following metrics: (1) feature encoding time (feat time) -measures the time for encoding the context (video and subtitle) features offline. (2) encoded feature size (feat size) -measures the disk space needed to store the encoded context features. (3) retrieval time (retrieval time) -measures the time needed to retrieve relevant moments for 100 new queries. It includes time for encoding the queries and performing approximate nearest neighbor search <ref type="bibr" target="#b44">[18]</ref> or matrix multiplication. The time spent on data loading, pre-processing, feature extraction on backend models (i.e., ResNet-152, I3D, RoBERTa) are not considered as they should be similar if not the same for all the methods. Note that the retrieval time here is different from the runtime in <ref type="table" target="#tab_9">Table 5</ref>, which additional includes feat time. We do not report feat time and feat size for ExCL <ref type="bibr" target="#b37">[11]</ref> as it does not have the ability to pre-encode the features -its context encoding depends on the input queries. This experiment was conducted on an RTX 2080Ti GPU and an Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz × 40, with PyTorch <ref type="bibr" target="#b57">[31]</ref> and FAISS <ref type="bibr" target="#b44">[18]</ref>.</p><p>The results are shown in <ref type="table" target="#tab_12">Table 8</ref>. Our XML model is more efficient than all the baselines. Compared to the best baseline methods MEE+MCN, XML is 18×  faster in retrieval, 4.5× faster in feature encoding and needs 77% less disk space to store the encoded features. Besides, it also has 7.7× higher performance (3.25 vs. 0.42, IoU=0.7, R@1, on TVR test-public set). Note that MEE+ExCL has very poor retrieval time performance (287× slower than XML), as it requires early fusion of context and query features. In comparison, the other 3 methods are able to pre-encode the context features and only perform lightweight query encoding and highly optimized nearest neighbor search or matrix multiplication to obtain the moment predictions.</p><p>Impact of #Retrieved Videos. In previous experiments, we fix the number of videos retrieved by XML to be 100 for corpus level moment retrieval experiments.</p><p>To study the impact of this hyperparameter, we perform experiments when #videos ∈ <ref type="bibr" target="#b36">[10,</ref><ref type="bibr" target="#b76">50,</ref><ref type="bibr">100,</ref><ref type="bibr">200]</ref>, the results are shown in <ref type="table" target="#tab_13">Table 9</ref>. Overall, we notice XML is not sensitive to the number of retrieved videos in terms of R@1, R@5 and R@10 (IoU=0.5, 0.7) in the tested range. When we focus on R@100, IoU=0.5, we find that using more videos helps improve the retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SVMR and Video Retrieval Experiments</head><p>Single Video Moment Retrieval. <ref type="table" target="#tab_0">Table 10</ref> shows the Single Video Moment Retrieval (SVMR) results on TVR val set. The goal of the task is to retrieve relevant moments from a single video rather than from a video corpus as in VCMR. We observe XML achieves comparable performance with the state-ofthe-art method ExCL <ref type="bibr" target="#b37">[11]</ref>. However, note that XML significantly outperforms ExCL on the VCMR task with higher efficiency, as stated in Sec. 5.2 and Sec. B.1.</p><p>We also noticed that adding TEF has minimal impact on the performance of XML and ExCL, while greatly improves MCN's and CAL's performance. This is not surprising as XML and ExCL directly model the complete video where the temporal information could be acquired, while MCN and CAL break the video into separate proposals where the temporal information is lost in the process.</p><p>Video Retrieval. <ref type="table" target="#tab_0">Table 11</ref> shows the Video Retrieval results on TVR val set. The goal of the task is to retrieve relevant videos from a large corpus. As MCN and CAL do not perform whole-video retrieval, we approximate their video retrieval predictions using the videos associated with the top-retrieved moments, as in <ref type="bibr" target="#b34">[8]</ref>. MCN and CAL models perform rather poor (&gt;50x lower performance than XML, R@1) on the video retrieval task, we summarize some possible reasons here: (1) MCN and CAL's video retrieval results are only an approximation as they are trained to differentiate moments rather than videos; (2) they need to rank a large number of proposals (187K proposals in TVR val set), which has many drawbacks, e.g., inefficient negative sampling in training. MEE gets less than half of XML's performance as it uses global pooled context features instead of more fine-grained local context features as XML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 More Qualitative Examples</head><p>We show more qualitative examples from our XML model in <ref type="figure">Fig. 20</ref> and <ref type="figure">Fig. 21</ref>. We show top-3 predictions for the VCMR task, as well as associated predictions (with ConvSE filter responses) for the SVMR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C TVR DiDeMo Experiments</head><p>To show the effectiveness of XML for the VCMR task, we also tested it on the popular moment retrieval dataset DiDeMo <ref type="bibr" target="#b40">[14]</ref>. Different from TVR experiments, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D TVC Dataset and Experiments</head><p>After the TVR data collection, we extended TVR by collecting extra descriptions for each annotated moment. This dataset, named TV show Captions (TVC), is a large-scale multimodal video captioning dataset. <ref type="figure">Fig. 16</ref> shows two TVC examples. Similar to TVR, the TVC task requires systems to gather information from both video and subtitle to generate relevant descriptions. In the following, we present a brief analysis and initial baselines for TVC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Data Collection and Analysis</head><p>To promote better coverage of the video (subtitle) content, we encourage annotators to write descriptions that are of different types from existing ones, e.   <ref type="table" target="#tab_0">Table 13</ref> gives an overview of TVC and its comparison with recent video captioning datasets. In total, TVC contains 262K descriptions paired with 108K moments. TVC is unique as its captions may also describe dialogues/subtitles while the captions in the other datasets are only describing the visual content. TVC also has a description type annotation, which can be used for model training and analysis. <ref type="figure">Fig. 17</ref> compares the description type distribution between TVR and TVC. As we encouraged annotators to write different types of descriptions, the description type distribution is more balanced in TVC compared to that of TVR. As TVC is built on top of TVR, it shares many properties of TVR, e.g., </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtitle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Multimodal Transformer</head><p>To provide a strong initial baseline for the TVC multimodal video captioning task, we designed a MultiModal Transformer (MMT) captioning model which follows the classical encoder-decoder transformer architecture <ref type="bibr" target="#b66">[40]</ref>. It takes both video and subtitle as encoder inputs to generate the captions from the decoder. <ref type="figure">Fig. 18</ref> gives an overview of the designed model.</p><p>Input Representation. We use the concatenation of I3D <ref type="bibr" target="#b28">[2]</ref> feature and ResNet-152 <ref type="bibr" target="#b39">[13]</ref> feature to represent videos. The features are pre-processed in the same  way as our XML model for the TVR task, as in Sec. 4.1. To represent subtitles, we use trainable 300D word embeddings. Next, we project raw video features and subtitle word features into a common embedding space using linear layers and layernorm <ref type="bibr" target="#b27">[1]</ref> layers. The projected video embedding E v ∈ R lv×d and subtitle embedding E s ∈ R ls×d are then concatenated at length dimension <ref type="bibr" target="#b49">[23]</ref> as the input to the encoder: E ctx = [E v ; E s ], where E ctx ∈ R (lv+ls)×d stands for the context embedding, d is hidden size.</p><p>Encoder and Decoder. Both the encoder and decoder follows the standard design <ref type="bibr" target="#b66">[40]</ref> with 2 layers, i.e., N =2. The decoder access encoder outputs at each layer with a multi-head attention <ref type="bibr" target="#b66">[40]</ref>. We refer readers to <ref type="bibr" target="#b66">[40]</ref> for a more detailed explanation of the model architecture.</p><p>Training and Inference. We train the model using Maximum Likelihood Estimation (MLE), i.e., we maximize the likelihood of generating the ground truth words. At inference, we use greedy decoding instead of beam search as it performs better in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Experiments</head><p>We use the same video split for TVC as in TVR, see Sec. E for more details. We report numbers on standard metrics, inlcuding BLEU@4 <ref type="bibr" target="#b56">[30]</ref>, METEOR <ref type="bibr" target="#b32">[6]</ref>, Rouge-L <ref type="bibr" target="#b52">[26]</ref>, CIDEr-D <ref type="bibr" target="#b67">[41]</ref>. We first compare MMT models with different input modalities. The results are shown in <ref type="table" target="#tab_0">Table 14</ref>. Across all metrics, the model with both videos and subtitles performs better than the models with only one of them, which shows both videos and subtitles are important for describing the moments. Next, we compare models with different visual features. The results are shown in <ref type="table" target="#tab_0">Table 15</ref>. Models with both appearance features (ResNet-152 <ref type="bibr" target="#b39">[13]</ref>) and motion feature (I3D <ref type="bibr" target="#b28">[2]</ref>) performs better than only using one of them.</p><p>Rachel explains to her dad on the phone why she can't marry her fiancé. (video+subtitle) 00:00:07,786 --&gt; 00:00:13,156 Monica: Who wasn't invited to the wedding. The man in the hat briefly bends over the machine. <ref type="figure">Fig. 19</ref>: Comparison of TVR with existing moment retrieval datasets <ref type="bibr" target="#b58">[32,</ref><ref type="bibr" target="#b35">9,</ref><ref type="bibr" target="#b48">22,</ref><ref type="bibr" target="#b40">14]</ref>. Ground truth moment is shown in green box. TVR videos are typically more diverse, containing more camera viewpoints, activities and people, etc.  Text inside dashed boxes is the subtitles associated with the moments. Each ground-truth caption description is followed by a description type tag. We show comparison among models trained with only videos (video), subtitles (sub), or both (video + sub)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>…</head><label></label><figDesc>00:00:35,180 --&gt; 00:00:37,774 "Tuna or egg salad! Decide!"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Left: #unique 4-gram as a function of #queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Performance breakdown of XML models that use only video, subtitle, or both as inputs, by different query types (with percentage of queries shown in brackets). The performance is evaluated on TVR val set for VCMR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>ConvSE Analysis. Left: comparison of moment generation methods. Right: comparison of ConvSE filters with different kernel sizes (k ) Query-Clip Similarity Conv1Dst = [-0.1001, -0.1675, 0.3975, 0.5076, 0.2873] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j 7 X 5 U V M w 9 M b R r X r 8 8 m s z U R B H o j c = " &gt; A A A C R H i c b Z D L S g M x F I Y z 3 q 2 3 q k s 3 w S K 4 0 C G x 2 i o i i L p w q W B V a I e S S d M a z C R D k i m W Y R 7 O j Q / g z i d w 4 0 I R t 2 J a R / D 2 Q + D L f 8 4 h O X 8 Y C 2 4 s Q g / e 0 P D I 6 N j 4 x G R h a n p m d q 4 4 v 3 B u V K I p q 1 E l l L 4 M i W G C S 1 a z 3 A p 2 G W t G o l C w i / D 6 s F + / 6 D J t u J J n t h e z I C I d y d u c E u u s Z r H e i I i 9 0 l F 6 q G Q X H 2 X N 9 M s w N sv g H n Q N o b p J 1 z P k Y 4 T w G v x 2 r 1 S 3 1 h q 7 y C / v 5 L C F q p U B b G x X y 0 G z W E I + G g j + B Z x D C e Q 6 a R b v G y 1 F k 4 h J S w U x p o 5 R b I O U a M u p Y F m h k R g W E 3 p N O q z u U J K I m S A d h J D B F e e 0 Y F t p d 6 S F A / f 7 R E o i Y 3 p R 6 D r 7 G 5 r f t b 7 5 X 6 2 e 2 P Z 2 k H I Z J 5 Z J + v l Q O x H Q K t h P F L a 4 Z t S K n g N C N X d / h f S K a E K t y 7 3 g Q s C / V / 4 L 5 x s + L v v 4 d L O 0 f 5 D H M Q G W w D J Y B R h U w T 4 4 B i e g B i i 4 B Y / g G bx 4 d 9 6 T 9 + q 9 f b Y O e f n M I v g h 7 / 0 D D v C s Y A = = &lt; / l a t e x i t &gt; Conv1Ded = [ 0.6163, 0.2625, -0.0469, -0.1200, -0.0478] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c a c m k h P U S p b 5 U o A 2 T 5 A 3 Z W n h W 8 A = " &gt; A A A C S X i c b Z B L S w M x F I U z 9 V 1 f V Z d u g k V w o W V S a 6 u I I O r C p Y J V o T P U T J p q M I 8 h y Y h l m L / n x p 0 7 / 4 M b F 4 q 4 M l M r + L o Q + H L O v S T 3 R D F n x v r + o 1 c Y G h 4 Z H R u f K E 5 O T c / M l u b m T 4 1 K N K F N o r j S 5 x E 2 l D N J m 5 Z Z T s 9 j T b G I O D 2 L r v d z / + y G a s O U P L G 9 m I Y C X 0 r W Z Q R b J 7 V L F 4 H A 9 k q L d F / J G 3 S Q t d M v g X a y D O 7 A V r D t V + q o v r 6 a Q 7 V e 3 V g N R K R u 0 7 X M r / i 1 + t a 3 K 6 r 6 / g + 3 s R m 2 S 2 V H / Y J / A Q 2 g D A Z 1 1 C 4 9 B B 1 F E k G l J R w b 0 0 J + b M M U a 8 s I p 1 k x S A y N M b n G l 7 T l U G J B T Z j 2 k 8 j g s l M 6 s K u 0 O 9 L C v v p 9 I s X C m J 6 I X G e + p v n t 5 e J / X i u x 3 c 0 w Z T J O L J X k 8 6 F u w q F V M I 8 V d p i m x P K e A 0 w 0 c 3 + F 5 A p r T K w L v + h C Q L 9 X / g u n 1 Q p a r 6 D j W n l 3 b x D H O F g E S 2 A F I N A A u + A Q H I E m I O A O P I E X 8 O r d e 8 / e m / f + 2 V r w B j M L 4 E c V h j 4 A w S S t 6 g = = &lt; / l a t e x i t &gt; Examples of learned ConvSE filters applying on query-clip similarity scores. Ground truth span is indicated by the two arrows labeled by GT. Note the two filters output stronger responses on the up (Start) and down (End ) edges with proposal confidence scores calculated as the average of scores inside each proposal region. On average, it produces 87 proposals per video. The proposals used here are the same as the ones used for MCN and CAL in our previous experiments;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Example question from our qualification test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Post-Annotation quality rating interface Distribution of quality rating on 3,600 query-moment pairs. 92% of the pairs have a rating of at least neutral</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 14 :Fig. 15 :</head><label>1415</label><figDesc>Distribution of query types based on reasoning type. Text inside dashed boxes are query examples for each query type TVR query word clouds for nouns (left) and verbs (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>00:00:44,223 --&gt; 00:00:52,929 Rachel: Daddy, I just I can't marry him. I'm sorry. I just don't love him. 00:00:58,771 --&gt; 00:01:05,032 "If I let go of my hair, my head will fall off."… 00:00:35,180 --&gt; 00:00:37,774 "Tuna or egg salad! Decide!" … … ActivityNet Captions She continues dancing around the room and ends by laying on the floor. The man mixes up various ingredients and begins laying plaster on the floor. Another man running past. Person they take a mobile phone. DiDeMo She took out figs. She washes the pepper. CharadesSTA TACoS 00:00:00,327 --&gt; 00:00:04,320 Whitney: Dr. House? This is my fiancé, Geoff. 00:00:59,486 --&gt; 00:01:02,046 Whitney: We'll do the paternity test. 00:01:25,979 --&gt; 00:01:28,573 Kutner: You're in good spirits. You feeling better? … 00:00:32,192 --&gt; 00:00:34,626 House: Nine months later, a miracle child was born. … … TVR Kutner stands in front of Natalie as she has her back turned. (video) Camera stops panning right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>fault. I brought him up. Yeah. Sheldon: Not so much, huh? … Dave: Sorry. Amy: My fault. I brought him up. Yeah.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 20 :Fig. 21 :Fig. 22 :</head><label>202122</label><figDesc>Qualitative examples of XML. We show top-3 retrieved moments for VCMR (top) and SVMR results (bottom, with convolution filter responses) for each query. Text inside dashed boxes is the subtitles with the predicted moments. Orange box shows the predictions, green bar shows the ground truth. Best viewed in color MARK: Can I buy you a drink, Callie? … Best Man: Anyway, I wish you both a wonderful life together. Raj: That sounds really cool. Howard: Does it? Well, okay, if you like space stuff... Best Man: Anyway, I wish you both a wonderful life together. MARK: Can I buy you a drink, Callie? … MARK: Can I buy you a drink, Callie? Anyway, I wish you both a wonderful life together. MARK: Can I buy you a drink, Callie? … Qualitative examples of XML. We show top-3 retrieved moments for VCMR (top) and SVMR results (bottom, with convolution filter responses) for each query. Text inside dashed boxes is the subtitles with the predicted moments. Orange box shows the predictions, green bar shows the ground truth. Best viewed in color Qualitative comparison of MMT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of TVR with existing moment retrieval datasets. Q stands for query. Q context indicate which modality the queries are related. Free st-ed indicates whether the timestamps are freely annotated. Individual Q means the queries are collected as individual sentences, rather than sentences in paragraphs</figDesc><table><row><cell>Dataset</cell><cell cols="2">Domain #Q/#videos</cell><cell cols="7">Vocab. Avg. Avg. len. (s) Q context Free Q type Individual size Q len. moment/video video text st-ed anno. Q</cell></row><row><cell>TACoS [32]</cell><cell cols="2">Cooking 16.2K / 0.1K</cell><cell>2K</cell><cell>10.5</cell><cell>5.9 / 287</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>DiDeMo [14]</cell><cell>Flickr</cell><cell>41.2K / 10.6K</cell><cell>7.6K</cell><cell>8.0</cell><cell>6.5 / 29.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ActivityNet Captions [22] Activity</cell><cell>72K / 15K</cell><cell>12.5K</cell><cell>14.8</cell><cell>36.2 / 117.6</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>CharadesSTA [9]</cell><cell cols="2">Activity 16.1K / 6.7K</cell><cell>1.3K</cell><cell>7.2</cell><cell>8.1 / 30.6</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>TVR</cell><cell cols="2">TV show 109K / 21.8K</cell><cell>57.1K</cell><cell>13.4</cell><cell>9.1 / 76.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Fig. 2: Distributions of moment (left) and query (right) lengths. Compared</cell></row><row><cell cols="4">to existing moment retrieval datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Percentage of queries that have multiple actions or involve multiple people. Statistics is based on 100 manually labeled queries from each dataset. We also show query examples, with unique person mentions underlined and actions in bold. Compared to existing datasets, TVR queries typically have more people and actions and require both video and sub (subtitle) context</figDesc><table><row><cell>Dataset</cell><cell cols="3">#actions #people ≥2 (%) ≥2 (%)</cell><cell>Query examples (query type)</cell></row><row><cell>TACoS [32]</cell><cell></cell><cell>20</cell><cell>0</cell><cell>She rinses the peeled carrots off in the sink. (video) The person removes roots and outer leaves and rewashes the leek. (video)</cell></row><row><cell cols="2">CharadesSTA [9]</cell><cell>6</cell><cell>12</cell><cell>A person is eating food slowly. (video) A person is opening the door to a bedroom. (video)</cell></row><row><cell>ActivityNet Caption [22]</cell><cell></cell><cell>44</cell><cell>44</cell><cell>He then grabs a metal mask and positions himself correctly on the floor. (video) The same man comes back and lifts the weight over his head again. (video)</cell></row><row><cell>DiDeMo [14]</cell><cell></cell><cell>6</cell><cell>10</cell><cell>A dog shakes its body. (video) A lady in a cowboy hat claps and jumps excitedly. (video)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bert leans down and gives Amy a hug who is standing next to Penny. (video)</cell></row><row><cell>TVR</cell><cell></cell><cell>67</cell><cell>66</cell><cell>Taub argues with the patient that fighting in Hockey undermines the sport. (sub)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Chandler points at Joey while describing a woman who wants to date him. (video+sub)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>To address VCMR, we propose a hierarchical Cross-modal Moment Localization (XML) network. XML performs video retrievalFig. 4: Cross-modal Moment Localization (XML) model overview. Self =Self Encoder, Cross=Cross Encoder. We describe XML Backbone in Sec. 4.1, ConvSE module in Sec. 4.2 and show XML's training and inference procedure in Sec. 4.3</figDesc><table><row><cell>XML Backbone Sheldon and Leonard go downstairs side by side.</cell><cell>RoBERTa</cell><cell></cell><cell>FC</cell><cell>PE</cell><cell>Self</cell><cell></cell><cell>FC</cell><cell></cell><cell>[</cell><cell>;</cell><cell>]</cell><cell>ConvSE Query-Clip Similarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ConvSE Detector</cell></row><row><cell cols="2">... 00:26.568→ 00:27.818 Leonard: Sounds like a breakthrough ... PE Positional Encoding Element-wise Addition I3D+ResNet RoBERTa Matrix Multiplication</cell><cell>0</cell><cell>FC FC L2-Norm</cell><cell>PE PE</cell><cell>Self Self max</cell><cell>0 0</cell><cell>0</cell><cell>Cross Cross L2-Norm</cell><cell cols="2">FC FC max</cell><cell>Softmax Dynamic Programming Conv. Filter Response Start-End Probabilities SVMR Scores Video Retrieval Score Aggregation Function</cell><cell>Scores VCMR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Baseline comparison on TVR test-public set, VCMR task. Model references: MCN<ref type="bibr" target="#b40">[14]</ref>, CAL<ref type="bibr" target="#b34">[8]</ref>, MEE<ref type="bibr" target="#b55">[29]</ref>, ExCL<ref type="bibr" target="#b37">[11]</ref>. Results with TEF<ref type="bibr" target="#b40">[14]</ref> feature are presented inTable 5</figDesc><table><row><cell>Model</cell><cell cols="2">w/ video w/ sub.</cell><cell></cell><cell cols="2">IoU=0.5</cell><cell></cell><cell></cell><cell cols="2">IoU=0.7</cell><cell></cell><cell>Runtime ↓</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">R@1 R@5 R@10 R@100 R@1 R@5 R@10 R@100 (seconds)</cell></row><row><cell>Chance</cell><cell>-</cell><cell>-</cell><cell>0.00</cell><cell>0.02</cell><cell>0.04</cell><cell>0.33</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.07</cell></row><row><cell cols="2">Proposal based Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MCN</cell><cell></cell><cell></cell><cell>0.02</cell><cell>0.15</cell><cell>0.24</cell><cell>2.20</cell><cell>0.00</cell><cell>0.07</cell><cell>0.09</cell><cell>1.03</cell><cell>-</cell></row><row><cell>CAL</cell><cell></cell><cell></cell><cell>0.09</cell><cell>0.31</cell><cell>0.57</cell><cell>3.42</cell><cell>0.04</cell><cell>0.15</cell><cell>0.26</cell><cell>1.89</cell><cell>-</cell></row><row><cell cols="2">Retrieval + Re-ranking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MEE+MCN</cell><cell></cell><cell></cell><cell>0.92</cell><cell>3.69</cell><cell cols="2">5.58 17.91</cell><cell>0.42</cell><cell>1.89</cell><cell cols="2">2.98 10.84</cell><cell>66.8</cell></row><row><cell>MEE+CAL</cell><cell></cell><cell></cell><cell>0.97</cell><cell>3.75</cell><cell cols="2">5.80 18.66</cell><cell>0.39</cell><cell>1.69</cell><cell cols="2">2.98 11.52</cell><cell>161.5</cell></row><row><cell>MEE+ExCL</cell><cell></cell><cell></cell><cell>0.92</cell><cell>2.53</cell><cell>3.60</cell><cell>6.01</cell><cell>0.33</cell><cell>1.19</cell><cell>1.73</cell><cell>2.87</cell><cell>1307.2</cell></row><row><cell>XML</cell><cell></cell><cell></cell><cell cols="8">7.25 16.24 21.65 44.44 3.25 8.71 12.49 29.51</cell><cell>25.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>ConvSE: Comparison and Analysis. To produce moment predictions from the query-clip similarity signals, we proposed ConvSE that learns to detect start (up) and end (down) edges in the 1D similarity signals. To show its effectiveness, we compare ConvSE with two baselines under our XML backbone network:<ref type="bibr" target="#b27">(1)</ref> sliding window, where we rank proposals generated by multi-scale sliding windows,</figDesc><table><row><cell>Comparison of moment generation methods</cell><cell>Comparison of ConvSE filters with different kernel size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Data Statistics for each TV show. BBT=The Big Bang Theory, HIMYM=How I Met You Mother, Grey=Grey's Anatomy, Epi=Episode, Sea.=Season</figDesc><table><row><cell>Show</cell><cell cols="2">Genre #Sea.</cell><cell>#Epi.</cell><cell cols="2">#Clip #Query</cell></row><row><cell>BBT</cell><cell>sitcom</cell><cell>10</cell><cell>220</cell><cell>4,198</cell><cell>20,990</cell></row><row><cell cols="2">Friends sitcom</cell><cell>10</cell><cell>226</cell><cell>5,337</cell><cell>26,685</cell></row><row><cell cols="2">HIMYM sitcom</cell><cell>5</cell><cell>72</cell><cell>1,512</cell><cell>7,560</cell></row><row><cell>Grey</cell><cell>medical</cell><cell>3</cell><cell>58</cell><cell>1,427</cell><cell>7,135</cell></row><row><cell>House</cell><cell>medical</cell><cell>8</cell><cell>176</cell><cell>4,621</cell><cell>23,105</cell></row><row><cell>Castle</cell><cell>crime</cell><cell>8</cell><cell>173</cell><cell>4,698</cell><cell>23,490</cell></row><row><cell>Total</cell><cell>-</cell><cell>44</cell><cell>925</cell><cell cols="2">21,793 108,965</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>shows TVR query type distribution, around 91% of the queries need video context, while 26% of the queries need subtitle context.Frequent Words in Queries. InFig. 15we show frequent nouns (left) and verbs (right) in TVR queries. The words are lemmatized, stop words are removed.</figDesc><table><row><cell cols="4">Fig. 13: Distribution of TVR moment lengths (left) and moment center locations</cell></row><row><cell>(right)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Castle is crying as he pleads</cell></row><row><cell></cell><cell></cell><cell cols="2">with Mason to not go</cell></row><row><cell></cell><cell></cell><cell cols="2">forward with his plans.</cell></row><row><cell></cell><cell></cell><cell cols="2">Video + Subtitle</cell></row><row><cell></cell><cell></cell><cell>16.6%</cell><cell></cell></row><row><cell>Howard drops his food, picks up a remote control and mutes the TV.</cell><cell>74.2%</cell><cell>9.1%</cell><cell>Subtitle-only Monica is excited when she says the name of a</cell></row><row><cell>Video-only</cell><cell></cell><cell></cell><cell>famous dancer.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Baseline comparison on TVR test-public set, VCMR task. Model references: MCN [14], CAL [8], MEE [29], ExCL [11]. This table includes models trained with Temporal Endpoint Feature (TEF)<ref type="bibr" target="#b40">[14]</ref> </figDesc><table><row><cell>Model</cell><cell cols="2">w/ video w/ sub.</cell><cell></cell><cell cols="2">IoU=0.5</cell><cell></cell><cell></cell><cell cols="2">IoU=0.7</cell><cell></cell><cell>Runtime ↓</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">R@1 R@5 R@10 R@100 R@1 R@5 R@10 R@100 (seconds)</cell></row><row><cell>Chance</cell><cell>-</cell><cell>-</cell><cell>0.00</cell><cell>0.02</cell><cell>0.04</cell><cell>0.33</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.07</cell><cell>-</cell></row><row><cell>Frequency</cell><cell>-</cell><cell>-</cell><cell>0.06</cell><cell>0.07</cell><cell>0.11</cell><cell>0.28</cell><cell>0.02</cell><cell>0.04</cell><cell>0.06</cell><cell>0.11</cell><cell>-</cell></row><row><cell cols="2">Proposal based Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TEF-only</cell><cell>-</cell><cell>-</cell><cell>0.00</cell><cell>0.09</cell><cell>0.15</cell><cell>0.79</cell><cell>0.00</cell><cell>0.07</cell><cell>0.09</cell><cell>0.48</cell><cell>-</cell></row><row><cell>MCN</cell><cell></cell><cell></cell><cell>0.02</cell><cell>0.15</cell><cell>0.24</cell><cell>2.20</cell><cell>0.00</cell><cell>0.07</cell><cell>0.09</cell><cell>1.03</cell><cell>-</cell></row><row><cell>MCN (TEF)</cell><cell></cell><cell></cell><cell>0.04</cell><cell>0.11</cell><cell>0.17</cell><cell>1.84</cell><cell>0.02</cell><cell>0.06</cell><cell>0.07</cell><cell>1.10</cell><cell>-</cell></row><row><cell>CAL</cell><cell></cell><cell></cell><cell>0.09</cell><cell>0.31</cell><cell>0.57</cell><cell>3.42</cell><cell>0.04</cell><cell>0.15</cell><cell>0.26</cell><cell>1.89</cell><cell>-</cell></row><row><cell>CAL (TEF)</cell><cell></cell><cell></cell><cell>0.04</cell><cell>0.17</cell><cell>0.31</cell><cell>2.48</cell><cell>0.02</cell><cell>0.15</cell><cell>0.22</cell><cell>1.30</cell><cell>-</cell></row><row><cell cols="2">Retrieval + Re-ranking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MEE+MCN</cell><cell></cell><cell></cell><cell>0.92</cell><cell>3.69</cell><cell cols="2">5.58 17.91</cell><cell>0.42</cell><cell>1.89</cell><cell cols="2">2.98 10.84</cell><cell>-</cell></row><row><cell>MEE+MCN (TEF)</cell><cell></cell><cell></cell><cell>1.36</cell><cell>3.89</cell><cell cols="2">5.79 19.34</cell><cell>0.62</cell><cell>2.04</cell><cell cols="2">3.21 11.66</cell><cell>66.8</cell></row><row><cell>MEE+CAL</cell><cell></cell><cell></cell><cell>0.97</cell><cell>3.75</cell><cell cols="2">5.80 18.66</cell><cell>0.39</cell><cell>1.69</cell><cell cols="2">2.98 11.52</cell><cell>-</cell></row><row><cell>MEE+CAL (TEF)</cell><cell></cell><cell></cell><cell>1.23</cell><cell>4.00</cell><cell cols="2">6.52 20.07</cell><cell>0.66</cell><cell>1.93</cell><cell cols="2">3.09 12.03</cell><cell>161.5</cell></row><row><cell>MEE+ExCL</cell><cell></cell><cell></cell><cell>0.92</cell><cell>2.53</cell><cell>3.60</cell><cell>6.01</cell><cell>0.33</cell><cell>1.19</cell><cell>1.73</cell><cell>2.87</cell><cell>-</cell></row><row><cell>MEE+ExCL (TEF)</cell><cell></cell><cell></cell><cell>1.01</cell><cell>2.50</cell><cell>3.60</cell><cell>5.77</cell><cell>0.40</cell><cell>1.21</cell><cell>1.73</cell><cell>2.96</cell><cell>1307.2</cell></row><row><cell>XML (sw)</cell><cell></cell><cell></cell><cell cols="4">3.82 10.38 14.20 35.89</cell><cell>1.91</cell><cell>5.25</cell><cell cols="2">8.12 23.47</cell><cell>-</cell></row><row><cell>XML</cell><cell></cell><cell></cell><cell cols="8">7.25 16.24 21.65 44.44 3.25 8.71 12.49 29.51</cell><cell>-</cell></row><row><cell>XML (TEF)</cell><cell></cell><cell></cell><cell cols="8">7.88 16.53 21.84 45.51 3.32 9.46 13.41 30.52</cell><cell>25.5</cell></row><row><cell cols="6">B Additional TVR Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>B.1 More VCMR Experiments</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Model architecture ablation on TVR val set, VCMR task. Our full XML model in the last row is configured with transformer encoder and modular query. All models use both videos and subtitles</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">IoU=0.7</cell></row><row><cell></cell><cell cols="3">R@1 R@5 R@10 R@100</cell></row><row><cell>Self-Encoder Type</cell><cell></cell><cell></cell><cell></cell></row><row><cell>XML (LSTM)</cell><cell>2.12</cell><cell>4.97</cell><cell>6.86 18.06</cell></row><row><cell>XML (CNN)</cell><cell>2.45</cell><cell>5.53</cell><cell>7.77 19.88</cell></row><row><cell>Modular Query</cell><cell></cell><cell></cell><cell></cell></row><row><cell>XML (No modular query)</cell><cell>2.46</cell><cell>5.87</cell><cell>8.56 22.00</cell></row><row><cell>XML</cell><cell cols="3">2.62 6.39 9.05 22.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Feature ablation on TVR val set, VCMR task. All models use both videos and subtitles</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">IoU=0.7</cell></row><row><cell></cell><cell cols="3">R@1 R@5 R@10 R@100</cell></row><row><cell>XML (ResNet)</cell><cell>2.28</cell><cell>5.40</cell><cell>7.33 20.28</cell></row><row><cell>XML (I3D)</cell><cell>2.22</cell><cell>5.75</cell><cell>8.37 21.20</cell></row><row><cell>XML (ResNet+I3D)</cell><cell cols="3">2.62 6.39 9.05 22.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>VCMR on 1M videos with 100 queries. TVR test-public set results are included as reference.Model references: MCN [14], CAL [8], MEE [29], ExCL [11]</figDesc><table><row><cell>Model</cell><cell cols="2">IoU=0.7</cell><cell cols="2">Search 100 queries in 1M videos ↓</cell><cell></cell></row><row><cell></cell><cell cols="2">R@1 R@5</cell><cell cols="3">feat time (s) feat size (GB) retrieval time (s)</cell></row><row><cell>Retrieval + Re-ranking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MEE+MCN</cell><cell>0.42</cell><cell>1.89</cell><cell>131</cell><cell>326</cell><cell>0.090</cell></row><row><cell>MEE+CAL</cell><cell>0.39</cell><cell>1.69</cell><cell>841</cell><cell>2,235</cell><cell>0.166</cell></row><row><cell>MEE+ExCL</cell><cell>0.33</cell><cell>1.19</cell><cell>-</cell><cell>-</cell><cell>1.435</cell></row><row><cell>XML</cell><cell cols="2">3.25 8.71</cell><cell>29</cell><cell>76</cell><cell>0.005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Impact of #retrieved videos on TVR val set, VCMR task.</figDesc><table><row><cell cols="2">Model #retrieved</cell><cell>IoU=0.5</cell><cell></cell><cell cols="2">IoU=0.7</cell></row><row><cell></cell><cell>videos</cell><cell cols="4">R@1 R@5 R@10 R@100 R@1 R@5 R@10 R@100</cell></row><row><cell></cell><cell>10</cell><cell>5.29 11.82 15.83 31.05</cell><cell>2.62</cell><cell>6.54</cell><cell>9.14 21.19</cell></row><row><cell>XML</cell><cell>50 100</cell><cell>5.29 11.74 15.92 35.95 5.28 11.73 15.90 36.16</cell><cell>2.63 2.62</cell><cell>6.40 6.39</cell><cell>9.07 22.55 9.05 22.47</cell></row><row><cell></cell><cell>200</cell><cell>5.28 11.73 15.90 36.20</cell><cell>2.62</cell><cell>6.39</cell><cell>9.05 22.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>SVMR results on TVR val set. Model references: MCN [14], CAL [8], MEE<ref type="bibr" target="#b55">[29]</ref>, ExCL<ref type="bibr" target="#b37">[11]</ref>. We show top-2 scores in each column in bold</figDesc><table><row><cell>Model</cell><cell cols="2">w/ video w/ sub.</cell><cell>IoU=0.5</cell><cell cols="2">IoU=0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">R@1 R@5 R@1 R@5</cell></row><row><cell>Chance</cell><cell>-</cell><cell>-</cell><cell>3.24 12.79</cell><cell>0.94</cell><cell>4.41</cell></row><row><cell>Moment Frequency</cell><cell>-</cell><cell>-</cell><cell>7.72 18.93</cell><cell cols="2">4.19 12.27</cell></row><row><cell>TEF-only</cell><cell>-</cell><cell>-</cell><cell>9.63 24.86</cell><cell cols="2">5.14 14.92</cell></row><row><cell>MCN</cell><cell></cell><cell></cell><cell>13.08 39.61</cell><cell cols="2">5.06 20.37</cell></row><row><cell>MCN (TEF)</cell><cell></cell><cell></cell><cell>16.86 40.55</cell><cell cols="2">7.96 21.45</cell></row><row><cell>CAL</cell><cell></cell><cell></cell><cell>12.07 39.52</cell><cell cols="2">4.68 20.17</cell></row><row><cell>CAL (TEF)</cell><cell></cell><cell></cell><cell>17.61 42.08</cell><cell cols="2">8.07 21.40</cell></row><row><cell>ExCL</cell><cell></cell><cell></cell><cell cols="3">31.34 47.40 14.19 28.01</cell></row><row><cell>ExCL (TEF)</cell><cell></cell><cell></cell><cell cols="3">31.31 48.54 14.34 28.89</cell></row><row><cell>XML</cell><cell></cell><cell></cell><cell cols="3">30.75 51.20 13.41 31.11</cell></row><row><cell>XML (TEF)</cell><cell></cell><cell></cell><cell cols="3">31.43 51.66 13.89 31.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Video retrieval results on TVR val set. Model references: MCN [14], CAL [8], MEE [29]</figDesc><table><row><cell>Model</cell><cell cols="2">w/ video w/ sub.</cell><cell cols="4">R@1 R@5 R@10 R@100</cell></row><row><cell>Chance</cell><cell>-</cell><cell>-</cell><cell>0.03</cell><cell>0.22</cell><cell>0.47</cell><cell>4.61</cell></row><row><cell>MCN</cell><cell></cell><cell></cell><cell>0.05</cell><cell>0.38</cell><cell>0.66</cell><cell>3.59</cell></row><row><cell>MCN (TEF)</cell><cell></cell><cell></cell><cell>0.07</cell><cell>0.28</cell><cell>0.51</cell><cell>3.93</cell></row><row><cell>CAL</cell><cell></cell><cell></cell><cell>0.28</cell><cell>1.02</cell><cell>1.68</cell><cell>8.55</cell></row><row><cell>CAL (TEF)</cell><cell></cell><cell></cell><cell>0.06</cell><cell>0.34</cell><cell>0.63</cell><cell>5.26</cell></row><row><cell>MEE</cell><cell></cell><cell></cell><cell cols="4">7.56 20.78 29.88 73.07</cell></row><row><cell>XML</cell><cell></cell><cell></cell><cell cols="4">16.54 38.11 50.41 88.22</cell></row><row><cell>XML (TEF)</cell><cell></cell><cell></cell><cell cols="4">16.08 37.92 50.38 88.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>VCMR results on DiDeMo<ref type="bibr" target="#b40">[14]</ref> test set. Model references: MCN<ref type="bibr" target="#b40">[14]</ref>, CAL<ref type="bibr" target="#b34">[8]</ref>, MEE<ref type="bibr" target="#b55">[29]</ref>. This table includes models trained with Temporal Endpoint Feature (TEF)<ref type="bibr" target="#b40">[14]</ref>. We show top scores in each column in bold 10.42 34.49 1.59 6.7125.44    we only use ResNet features for DiDeMo. Besides, we also switch off the subtitle stream as DiDeMo has only video context. The results are shown inTable 12. The baseline results are directly taken from<ref type="bibr" target="#b34">[8]</ref>. We observe XML outperforms all the baseline methods on DiDeMo dataset by a large margin, showing XML is able to generalize well to datasets where only video is available.</figDesc><table><row><cell>Model</cell><cell>w/ video</cell><cell></cell><cell>IoU=0.5</cell><cell></cell><cell></cell><cell>IoU=0.7</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">R@1 R@10 R@100 R@1 R@10 R@100</cell></row><row><cell>Chance</cell><cell>-</cell><cell>0.00</cell><cell>0.10</cell><cell>1.99</cell><cell>0.00</cell><cell>0.02</cell><cell>0.64</cell></row><row><cell>Frequency</cell><cell>-</cell><cell>0.02</cell><cell>0.22</cell><cell>2.34</cell><cell>0.02</cell><cell>0.17</cell><cell>1.99</cell></row><row><cell>Proposal based Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TEF-only</cell><cell>-</cell><cell>0.05</cell><cell>0.32</cell><cell>2.58</cell><cell>0.03</cell><cell>0.27</cell><cell>2.12</cell></row><row><cell>MCN (TEF)</cell><cell></cell><cell>0.88</cell><cell cols="2">5.16 26.23</cell><cell>0.58</cell><cell cols="2">4.12 21.03</cell></row><row><cell>CAL (TEF)</cell><cell></cell><cell>0.97</cell><cell cols="2">6.15 28.06</cell><cell>0.66</cell><cell cols="2">4.69 22.89</cell></row><row><cell>Retrieval + Re-ranking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MEE+MCN (TEF)</cell><cell></cell><cell>0.53</cell><cell>3.00</cell><cell>6.52</cell><cell>0.46</cell><cell>2.64</cell><cell>6.37</cell></row><row><cell>MCN+MCN (TEF)</cell><cell></cell><cell>0.92</cell><cell cols="2">4.83 17.50</cell><cell>0.64</cell><cell cols="2">3.67 13.12</cell></row><row><cell>CAL+CAL (TEF)</cell><cell></cell><cell>1.07</cell><cell cols="2">6.45 22.60</cell><cell>0.72</cell><cell cols="2">4.86 17.60</cell></row><row><cell>CAL+CAL (TEF,re-train)</cell><cell></cell><cell>1.29</cell><cell cols="2">6.71 22.51</cell><cell>0.85</cell><cell cols="2">4.95 17.73</cell></row><row><cell>Approx. CAL+CAL (TEF,re-train)</cell><cell></cell><cell>1.27</cell><cell cols="2">6.39 15.82</cell><cell>0.80</cell><cell cols="2">4.95 11.59</cell></row><row><cell>XML (TEF)</cell><cell></cell><cell>2.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>g., we encourage annotators to write video-only and video+sub type descriptions if there already exists a sub-only description. For each moment in the TVR training set, we collect one extra description, together with the original description forms the TVC training set with 2 descriptions for each moment. For each moment in TVR val/test sets, we collect 4 extra descriptions as the TVC val/test sets. The original val/test descriptions in TVR are not used to ensure data integrity. Details regarding data split are presented in Sec. E. The Captain says its ok if Ted will not be on the ship. (sub-only) • The Captain agrees and points at Ted with a glass in his hand. (video+sub) Fig. 16: TVC caption description examples. Each caption description is followed by a description type tag. Text inside dashed boxes is the subtitles associated with the moments. For brevity, here we only show sampled frames from the moments</figDesc><table><row><cell>Castle : I'm so sorry for everything.</cell></row><row><cell>Mia: Come on, I did some pretty extraordinary things yesterday.</cell></row><row><cell>Captions</cell></row><row><cell>• Castle passes the flowers to Mia and Mia takes them. (video-only)</cell></row><row><cell>• Castle apologizes to the woman while handing her flowers. (video+sub)</cell></row><row><cell>Ted: Just not on a boat.</cell></row><row><cell>Captain: Fair enough.</cell></row><row><cell>Captions</cell></row><row><cell>•</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Comparison of TVC with existing video captioning datasets. Desc. context = Description context, it indicates which modality the descriptions are related to</figDesc><table><row><cell>Dataset</cell><cell cols="3">Domain #Moment #Desc.</cell><cell cols="3">#Desc. per Desc. context Desc. type moment video text anno.</cell></row><row><cell cols="2">TACoS-MLevel [34] Cooking</cell><cell>25K</cell><cell>75K</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell>YouCook II [51]</cell><cell>Cooking</cell><cell>15.4K</cell><cell>15.4K</cell><cell>11</cell><cell>-</cell><cell>-</cell></row><row><cell>ANetCap [22]</cell><cell>Activity</cell><cell>100K</cell><cell>100K</cell><cell>1</cell><cell>-</cell><cell>-</cell></row><row><cell>Charades [37]</cell><cell>Indoor</cell><cell>10K</cell><cell>27.8K</cell><cell>2-3</cell><cell>-</cell><cell>-</cell></row><row><cell>VATEX [42]</cell><cell>Activity</cell><cell>41K</cell><cell>826K</cell><cell>20</cell><cell>-</cell><cell>-</cell></row><row><cell>LSMDC [35]</cell><cell>Movie</cell><cell>128K</cell><cell>128K</cell><cell>1</cell><cell>-</cell><cell>-</cell></row><row><cell>MST-VTT [44]</cell><cell>Open</cell><cell>10k</cell><cell>200k</cell><cell>20</cell><cell>-</cell><cell>-</cell></row><row><cell>TVC</cell><cell>TV show</cell><cell>108K</cell><cell>262K</cell><cell>2-4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Overview of the MultiModal Transformer (MMT) model for the TVC task. PE stands for Positional Encoding great linguistic diversity, rich inter-human interactions, more actions and people in a single description, etc. See Sec. 3 for more details.</figDesc><table><row><cell></cell><cell cols="2">Video + Subtitle</cell><cell></cell><cell></cell></row><row><cell></cell><cell>9.1% 16.6%</cell><cell>-only</cell><cell>50.0%</cell><cell>31.8%</cell><cell>Video + Subtitle</cell></row><row><cell>Video-only</cell><cell>74.2%</cell><cell cols="2">Video-only</cell><cell>18.1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Subtitle-only</cell></row><row><cell cols="3">TVR description type distribution</cell><cell cols="3">TVC description type distribution</cell></row><row><cell cols="6">Fig. 17: Description type distributions of TVR and TVC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Softmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Linear</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feed Forward</cell></row><row><cell></cell><cell>Add &amp; Norm</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Feed Forward</cell><cell></cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multi-Head</cell></row><row><cell></cell><cell>Add &amp; Norm</cell><cell></cell><cell></cell><cell>Attention</cell></row><row><cell></cell><cell>Multi-Head Attention</cell><cell></cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Masked</cell></row><row><cell></cell><cell>x N</cell><cell></cell><cell></cell><cell>Multi-Head</cell></row><row><cell></cell><cell>PE</cell><cell></cell><cell></cell><cell>Attention</cell></row><row><cell></cell><cell>Concat</cell><cell></cell><cell></cell><cell></cell><cell>x N</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PE</cell></row><row><cell></cell><cell>Norm</cell><cell>Norm</cell><cell></cell><cell>Norm</cell></row><row><cell cols="2">Linear</cell><cell>Linear</cell><cell></cell><cell>Linear</cell></row><row><cell cols="2">I3D+ResNet</cell><cell>Enbedding</cell><cell></cell><cell>Enbedding</cell></row><row><cell></cell><cell cols="2">Alex: There were two donors,</cell><cell cols="3">Alex is on the phone with Izzie and</cell></row><row><cell></cell><cell cols="2">Izzie. Our heart flatlined...</cell><cell cols="3">he is updating her on the heart situation.</cell></row><row><cell></cell><cell></cell><cell>Subtitles</cell><cell cols="3">Caption (shifted right)</cell></row><row><cell>Fig. 18:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 :</head><label>14</label><figDesc>Model comparison on TVC test-public set, with different input context</figDesc><table><row><cell>Model</cell><cell cols="2">B@4 METEOR</cell><cell cols="2">Rouge-L CIDEr-D</cell></row><row><cell>MMT (sub)</cell><cell>6.33</cell><cell>13.92</cell><cell>7.73</cell><cell>33.76</cell></row><row><cell>MMT (video)</cell><cell>9.98</cell><cell>15.23</cell><cell>30.44</cell><cell>36.07</cell></row><row><cell>MMT (video+sub)</cell><cell>10.87</cell><cell>16.91</cell><cell>32.81</cell><cell>45.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 :</head><label>15</label><figDesc>Feature ablation on TVC val set. All the models use both videos and subtitles</figDesc><table><row><cell>Model</cell><cell cols="2">B@4 METEOR</cell><cell cols="2">Rouge-L CIDEr-D</cell></row><row><cell>MMT (ResNet)</cell><cell>9.92</cell><cell>16.24</cell><cell>31.76</cell><cell>43.94</cell></row><row><cell>MMT (I3D)</cell><cell>10.25</cell><cell>16.48</cell><cell>31.98</cell><cell>43.70</cell></row><row><cell>MMT (ResNet+I3D)</cell><cell>10.53</cell><cell>16.61</cell><cell>32.35</cell><cell>44.39</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Published in ECCV 2020. Both datasets are publicly available. TVR: https://tvr. cs.unc.edu, TVC: https://tvr.cs.unc.edu/tvc.html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We present a pipeline figure of our data collection procedure inFig. 9.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">descriptions while each of the moments in other splits are paired with 4 descriptions. Details are presented inTable 17. The rules on split usage are also the same as TVR.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We thank the reviewers for their helpful feedback. This research is supported by NSF Award #1562098, DARPA MCS Grant #N66001-19-2-</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Qualitative Examples</head><p>We show qualitative examples of MMT in <ref type="figure">Fig. 22</ref>, with generated captions by the three MMT models trained with different input context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Data Release and Public Leaderboards</head><p>Both TVR and TVC are publicly available at their websites: https://tvr. cs.unc.edu, https://tvr.cs.unc.edu/tvc.html. With the datasets, we host public leaderboards to better compare the systems. In the following, we describe data split and usage in detail. We split TVR into 80% train, 10% val, 5% test-public and 5% test-private such that videos and their associated queries appear in only one split. This setup is the same as TVQA <ref type="bibr" target="#b50">[24]</ref>. Details of the splits are presented in <ref type="table">Table 16</ref>. test-public will be used for a public leaderboard, test-private is reserved for future challenges. val set should only be used for parameter tuning, it should not be used in the training process, including but not limited to pre-train the language features. TVC follows the same data split as TVR, but with a different number of descriptions per moment, i.e., each of the training moments are paired with </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">But still, you think, this is different. Barney: The platinum rule doesn&apos;t apply to me. And that&apos;s step 2. Barney: He is our Neil Armstrong. Spacesuit up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barney</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ted, Barney</pubPlace>
		</imprint>
	</monogr>
	<note>cause you&apos;re going to the moon</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Not that I don&apos;t enjoy talking about high school... because I do. Maybe we can talk about something else. Cameron: or should we just start running a thousand different tox screens? … Ground-Truth Captions • House grabs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The platinum rule doesn&apos;t apply to me. And that&apos;s step 2. Rachel: Three-pound lobster? Joey: You know what? Bring her both. And I&apos;ll have the same. Rachel: Three-pound lobster? Joey: You know what? Bring her both. And I&apos;ll have the same</title>
		<imprint/>
	</monogr>
	<note>But still, you think, this is different. a file and opens it up. (video-only</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">• House opens a file and says he won&apos;t read it</title>
		<imprint/>
	</monogr>
	<note>video-text</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">• House jokes about tox screens when Foreman suggests it&apos;s something different</title>
		<imprint/>
	</monogr>
	<note>video-text</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generated Captions: • House and Cameron are having a conversation with each other</title>
		<imprint/>
	</monogr>
	<note>model: sub</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">• House puts a red box on the table and takes a red coffee cup. (model: video) • House picks up a red mug and takes it off</title>
		<imprint/>
	</monogr>
	<note>model: video + sub</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Beckett: No, Castle, I&apos;m talking about my life. I don&apos;t know what to do about my life</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Ground-Truth Captions • Beckett rolls over to lay on her back</title>
		<imprint/>
	</monogr>
	<note>video-only</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">• Beckett is visibly worried and speaks to Castle while they are in bed</title>
		<imprint/>
	</monogr>
	<note>videoonly</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Beckett discuss her vacation when they&apos;re in bed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Castle</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>video-text</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">• Beckett is talking to Castle about losing her job. (text-only) Generated Captions: • Beckett tells Castle that she is going to be honest</title>
		<imprint/>
	</monogr>
	<note>model: sub</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Castle are in bed and they are in bed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Beckett</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>model: video</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Castle are in bed together, and Beckett is sleeping in bed. (model: video + sub) Sheldon: ...with Adamantium like Wolverine. Penny: Are they working on that</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Beckett</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ground-Truth Captions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">• Sheldon holds out a large pile of cash with his right hand in front of Sheldon</title>
		<imprint/>
	</monogr>
	<note>video-only</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">• Sheldon is holding something in his hand out to Penny</title>
		<imprint/>
	</monogr>
	<note>video-only</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Penny makes a question, to which Sheldon gives a serious answer while presenting her with money</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Astonished</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>video-text</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">• Penny questions Sheldon as to whether somebody is trying something</title>
		<imprint/>
	</monogr>
	<note>textonly</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generated Captions: • Sheldon asks penny if she is feeling like a certain way</title>
		<imprint/>
	</monogr>
	<note>model: sub</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">• Sheldon is standing in front of penny as he speaks to her. (model: video + sub) Rachel: You&apos;ve been here for two months now. And your boss is required to hand in a performance evaluation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ground-Truth Captions • Tag prepares to leave before being reeled back in by Rachel</title>
		<imprint/>
	</monogr>
	<note>video-only</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">• As Tag is leaving Rachel tells him about his evaluation because of how long he has been there</title>
		<imprint/>
	</monogr>
	<note>video-text</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">• Tag is surprised to learn that Rachel will evaluate him</title>
		<imprint/>
	</monogr>
	<note>video-text</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">• Rachel tells Tag that he will put his performance into his evaluation</title>
		<imprint/>
	</monogr>
	<note>textonly</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generated Captions: • Tag tells Rachel that he has been in his office</title>
		<imprint/>
	</monogr>
	<note>model: sub</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">• Rachel walks into the office and picks up a book</title>
		<imprint/>
	</monogr>
	<note>model: video</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">• Tag walks into Rachel&apos;s office and hands her a file. (model: video + sub) Chandler: We could trade later</title>
	</analytic>
	<monogr>
		<title level="m">Yeah, I&apos;m good…</title>
		<meeting><address><addrLine>Monica</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Temporally grounding natural sentence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12763</idno>
		<title level="m">Temporal localization of moments in video collections with natural language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Mac: Mining activity concepts for languagebased temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Excl: Extractive clip localization using natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Localizing moments in video with temporal language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deepstory: Video story qa by deep embedded memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mart: Memoryaugmented recurrent transformer for coherent video paragraph captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Tvqa+: Spatio-temporal grounding for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Grounding action descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The watershed transform: Definitions, algorithms and parallelization strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Roerdink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meijster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamenta informaticae</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">Movie description. IJCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Multilevel language and vision integration for text-to-clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Cross-modal interaction networks for querybased moment retrieval in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>SIGIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
