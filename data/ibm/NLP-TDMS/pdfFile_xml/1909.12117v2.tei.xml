<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BALANCED BINARY NEURAL NETWORKS WITH GATED RESIDUAL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhu</forename><surname>Shen</surname></persName>
							<email>shenmingzhu@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
							<email>gongruihao@nlsde.buaa.edu.cnhankai@ios.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institude of Software</orgName>
								<orgName type="laboratory">State Key Lab of Computer Science</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<country>UCAS</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BALANCED BINARY NEURAL NETWORKS WITH GATED RESIDUAL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-model compression</term>
					<term>binary neural net- works</term>
					<term>energy-efficient models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Binary neural networks have attracted numerous attention in recent years. However, mainly due to the information loss stemming from the biased binarization, how to preserve the accuracy of networks still remains a critical issue. In this paper, we attempt to maintain the information propagated in the forward process and propose a Balanced Binary Neural Networks with Gated Residual (BBG for short). First, a weight balanced binarization is introduced and thus the informative binary weights can capture more information contained in the activations. Second, for binary activations, a gated residual is further appended to compensate their information loss during the forward process, with a slight overhead. Both techniques can be wrapped as a generic network module that supports various network architectures for different tasks including classification and detection. The experimental results show that BBG-Net performs remarkably well across various network architectures such as VGG, ResNet and SSD with the superior performance over state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Deep neural networks (DNNs), especially deep convolution neural networks (CNNs), have been well demonstrated in a wide variety of computer vision applications. However, most of these advanced deep CNN models requires expensive storage and computing resources, and cannot be easily deployed on portable devices such as mobile phones, cameras, etc.</p><p>In recent years, a number of approaches have been proposed to learn portable deep neural networks, including multibit quantization <ref type="bibr" target="#b0">[1]</ref>, pruning <ref type="bibr" target="#b1">[2]</ref>, and lightweight architecture design <ref type="bibr" target="#b2">[3]</ref>, knowledge distillation <ref type="bibr" target="#b3">[4]</ref>. Among them, quantization based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> represent the weights and activations in the network with a very low precision, and thus can yield highly more compact DNN models compared to Corresponding author those floating-point counterparts. In an extreme case where both the weights and activations are quantized into one-bit values, the conventional convolution operations can be efficiently achieved via bitwise operations <ref type="bibr" target="#b5">[6]</ref>. Ihe resulting decrease in the storage and acceleration in the inference are therefore appealing to the community. Since the proposition of binary neural network, many works have been done to address the performance drop and improve expression ability of binarized networks. Bireal-Net <ref type="bibr" target="#b6">[7]</ref> proposes using additional shortcut and different approximation of sign function in the backward pass. PCNN <ref type="bibr" target="#b7">[8]</ref> employs a projection matrix to help with the network training. CircConv <ref type="bibr" target="#b8">[9]</ref> rotate binary weight by three times and calculate feature map with four binary weights and merge them together. BENN <ref type="bibr" target="#b9">[10]</ref> ensembles multiple standard binary networks to improve performance. AutoBNN <ref type="bibr" target="#b10">[11]</ref> employs genetic algorithm to search for binary neural network architectures. Most of these methods have complicated training pipeline and even increase the FLOPs to achieve better results. Although much progress has been made on binarizing DNNs, the existing quantization methods still cause a significantly large accuracy drops, compared with the full-precision models. This observation indicates that the information propagates through the full-precision network are largely lost, when using the binary representation. To address this problem and maximally preserve the information, in this paper we propose the Balanced Binary Network with Gated Residual (BBG for short), which learns the balanced binary weights, and reduce binarization loss for activations by a gated residual path. <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrates the whole structure of our BBG. We re-parameterize the weight and devise a linear transformation to replace the standard one, which can be easily implemented and learnt. To compensate the information loss when binarizing activations, the gated residual path employs a lightweight operation to use the channel attention information of floatingpoint activations to further reconstruct the information flow across layers. We evaluate our BBG method on the image classification and object detection benchmarks, and the experimental results show that it performs remarkably well across various network architectures, outperforming state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Maximizing Entropy with Balanced Binary Weights</head><p>In Binary Neural Networks, the most important training parameter is the non-differentiable discrete binary weights. This discrete property of binary weights brings troublesome problems to the training of the network. To preserve the information of binary weights, we propose to maximize entropy of binary weight in the training process. Directly optimize the above regularization is hard. Instead, we approximate the optimal solution for it by making the expectation of binary weights to be zero to, i.e. w b 1 = 0.</p><p>Hence, we first center the real-valued weights and then quantize them into binary codes. More specifically, we use a proxy parameter v ∈ R d×d to get w and then quantize it into binary codes w b . In the convolutional layer, we express the weight w in terms of the proxy parameter v using:</p><formula xml:id="formula_0">w = v − 1 d 1(1 v),<label>(1)</label></formula><p>After balanced weight normalization, we can perform binary quantization on the floating-point weight w. Subsequently, the forward pass and backward pass of binary weights are as follows:</p><formula xml:id="formula_1">Forward:w b = sign(w) × E(|w|), Backward: ∂L ∂w b = ∂L ∂w ,<label>(2)</label></formula><p>where sign(·) is sign function that outputs +1 for positive numbers and −1 otherwise and E(| · |) calculates the mean of absolute value. In our balanced weight quantization, the parameter updating is completed based on the proxy parameters v, and thus the gradient signal can back-propagate through the normalization process. In the whole process, v and w are floating-point, and we update v on training process and only w b is needed on inference.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Reconstructing Information Flow with Gated Residual</head><p>In the binarization of activation, we first clip the value range of activations x into [0, 1] and use a round function to binarize activations. Therefore, the forward pass and backward pass for binary activations are as follows:</p><formula xml:id="formula_2">Forward:x b = round(clip(x, 0, 1)), Backward: ∂L ∂x b = ∂L ∂x I 0&lt;x&lt;1 ,<label>(3)</label></formula><p>where I 0&lt;x&lt;1 means if elements of x is in the range of [0, 1], then it is 1, otherwise 0. Binarizing activations results in much larger loss of precision than binary weights. All activation values of different channels are quantized to 0 or 1, without considering the differences among the channels. The quantization error caused by binary layers is accumulated layer by layer. To address this problem, we further propose a new module named gated residual to reconstruct information flow in channel-wise manner, during the forward propagation. Our gated residual employs the floating-point activations to reduce quantization error and recalibrate features.</p><p>Subsequently, we propose a new designed layer with gated weights s = [s 1 , ..., s c ] ∈ R c that learn the channel attention information of floating-point input feature map x = [x 1 , ..., x c ] ∈ R c×h×w in a binary convolution layer (c, h, w means channels, height and width respectively). The operation on the ith channel of the input feature map x i is defined as follows:</p><formula xml:id="formula_3">r(x i , s i ) = s i x i<label>(4)</label></formula><p>Based on the gated residual, the output feature map y ∈ R c×h×w can be recalibrated, enhancing the representation power of the activations. The operation in the gated module can be written in the following form:</p><formula xml:id="formula_4">y = F(x) + r(x, s)<label>(5)</label></formula><p>where F(x) means the operation in the main path including activation binarization, balanced convolution and BatchNorm in total.</p><p>With gated residual layer, the overall structure of gated module is shown in the <ref type="figure" target="#fig_2">Figure 2(d)</ref>. We initialize weight of gated residual with 1 which is the same with identity shortcut of vanilla ResNet. In training process, the gated residual learns to distinguish the channels and eliminate the less useful channels. Beside reconstructing the information flow in the forward process, this path also acts as an auxiliary gradient path for activations in the backward propagation. Usually, the STE used in the backward pass to approximate discrete binary functions results in severe gradient mismatch problem. Fortunately, with learnable gated weights, our gated residual can also reconstruct gradient in the following way:</p><formula xml:id="formula_5">∂y ∂x = ∂F(x) ∂x + s<label>(6)</label></formula><p>In terms of computational complexity and memory limitation in the nature of the binary networks, the additional operations for designing a new module need to be as small as possible. The structure of HighwayNet <ref type="bibr" target="#b11">[12]</ref> requires a fullprecision weights that is as large as the weight in the convolutional layer, which is unacceptable. Compared to SENet <ref type="bibr" target="#b12">[13]</ref>, SE module is a correction to the output after the convolution layer, and we argue that it is better to make use of the unquantified information. Similarly, our FLOPs is only c × w × h and is much smaller than the SE module. When the number of channels increases and the reduction decreases, the amount of FLOPs required by the SE module will be much more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>In this section, to verify the effectiveness of our proposed BBG-Net, we conduct experiments on both image classification and object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Implementation Details</head><p>Datasets. In our experiments, we adopt three common image classification datasets: CIFAR-10/100, and ILSVRC-2012 ImageNet. We also evaluate our proposed method on the object detection task with a standard detection benchmark, Pascal VOC datasets named VOC2007, and VOC2012.</p><p>Network Architectures and Setup.We conduct our experiments on popular and powerful network architectures ResNet <ref type="bibr" target="#b13">[14]</ref> and Single Shot Detector(SSD <ref type="bibr" target="#b14">[15]</ref>). For fair comparison with the existing methods, we respectively choose ResNet-20, ResNet-18 as the baseline model on CIFAR-10/100 and ImageNet datasets. And we verify SSD with different backbones, i.e. VGG-16 <ref type="bibr" target="#b15">[16]</ref> and ResNet-34 in Pascal VOC datasets. As for hyper-parameter, we mostly follow the same setup in the original papers and all our models are trained from scratch. Note that we do not quantize the first and last layer and we also do not quantize down-sample layers as suggested by many previous work <ref type="bibr" target="#b16">[17]</ref>. And we only quantize backbone network in SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation Study</head><p>Now we study how our balanced weight quantization and gated residual module affects the network's performance. In <ref type="table" target="#tab_0">Table 1</ref>, we report the results of ResNet-20 on CIFAR-10, with and without balanced quantization or gated residual. In the performance comparison from the first two rows, the network with balanced quantization can obtain 0.6% accuracy than that without this operation. From the whole table, we can easily observe that, balanced weights or gated residual brings accuracy improvement and together they work even better with 1.2% accuracy improvement. It reveals that our proposed method faithfully helps pursue a highly accurate binary network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with the State-of-the-Art</head><p>CIFAR-10/100 Dataset. In ResNet-20 on CIFAR-10/100 datasets, we further compare four different kinds of modules in <ref type="figure" target="#fig_2">Fig. 2</ref>. As shown in <ref type="table">Table 2</ref>, in CIFAR-10 datasets, the accuracy improvement caused by Gated or module is less than 1% while in a more challenging CIFAR-100 dataset, it adds up to over 2%. Through the whole experiments, we can conclude that Vanilla Gated and Gated show superiority ResNet-18 on ImageNet Dataset.In <ref type="figure" target="#fig_3">Fig. 3</ref>, we compare our method with many binarization methods of recent years including XNOR-Net <ref type="bibr" target="#b5">[6]</ref>, DoReFa-Net <ref type="bibr" target="#b17">[18]</ref>, Bireal-Net <ref type="bibr" target="#b6">[7]</ref>, PCNN <ref type="bibr" target="#b7">[8]</ref>, and it further reveals stability of our proposed BBG-Net on larger datasets. Our method significantly outperforms all the other methods, with 1.3% performance gain over the state-of-the-art ResNetE <ref type="bibr" target="#b16">[17]</ref>.</p><p>We also improve the accuracy of binary networks by exploring network width and resolution in a simple but effective way. In the calculation of FLOPs, the binary layer is divided by 64 following Bireal-Net <ref type="bibr" target="#b6">[7]</ref>. In the network width experiments, we expand all channels of the original ResNet-18 by 2× and 3×. Compared with the full-precision networks, our 3× width models can achieve almost the same accuracy. We also compare with ABC-Net where 5/3 means 5 binary bases for weight and 3 bases for activations, our methods consistently perform better than ABC-Net by a large margin. In resolution exploration, we employ a simple strategy that we remove the max pooling layer after the first convolution, which makes all hidden layers have 2× feature maps compared with the original ones. Compared to CircConv <ref type="bibr" target="#b8">[9]</ref> which employs four times binary weights, we have better results with 1.6% accuracy improvement, while we have slightly higher FLOPs but the same memory. Our resolution accuracy is 2.3% higher while the FLOPs is only 1.1× of the FLOPs of DenseNet-28 <ref type="bibr" target="#b16">[17]</ref>. BENN <ref type="bibr" target="#b9">[10]</ref> ensembles 6 standard binary ResNet-18 which has nearly three times FLOPs of our resolution while the accuracy declines by 2%.</p><p>SSD on Pascal VOC Dataset. In object detection task, we compare our method with XNOR-Net <ref type="bibr" target="#b5">[6]</ref>, TBN <ref type="bibr" target="#b18">[19]</ref>, and BDN <ref type="bibr" target="#b16">[17]</ref> which includes DenseNet37 and DenseNet-45.In the comparison of ResNet-34 as its backbone, we outperform XNOR and TBN by 6.9% and 2.5%. It proves that our solution using 1-bit can preserve the stronger feature representation and maintain the better generalization ability than ternary neural networks. As for VGG-16, we only need half of FLOPs of binary DenseNet-45 to achieve slightly higher results and our model outperforms DenseNet-37 by 2% with one thirds fewer FLOPs. The accuracy of our VGG-16 is only 5.8% less than full-precision counterparts. The significant performance gain further demonstrates that our method can better preserve the information propagated in the network and help extract the most discriminative features for detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Deploying Efficiency</head><p>Finally, we implement our method on mobile devices using a framework named daBNN <ref type="bibr" target="#b19">[20]</ref>. The mobile device we use is Rasberry Pi 3B, which has a 1.2GHz 64-bit quad-core ARM Cortex-A53. As shown in <ref type="table" target="#tab_2">Table 4</ref>, we implement DoReFa which binarizes downsample layers while we do not, the difference in inference time is only 2ms which can be ignored. Our proposed method can run 5.8× faster than full-precision counterparts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>Binarization methods for establishing portable neural networks are urgently required so that these networks with massive parameters and complex architectures can be launched efficiently. In this work, we proposed a novel Balanced Binary Neural Network with Gated Residual, namely BBG-Net. Experiments conducted on benchmark datasets and architectures demonstrate the effectiveness of the proposed balanced weight quantization and gated residual for learning binary neural networks with higher performance but lower memory and computation consumption than the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The diagram of the proposed method for learning binary neural networks by exploiting balanced weights binarization and gated residual to reconstruct information loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The four different kinds of modules are utilized to construct ResNet networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 .</head><label>3</label><figDesc>Performance comparison with state-of-the-art methods and our network width and resolution exploration experiments. over Bireal and Vanilla modules. Especially when the network grows wider, Bireal Module even performs worse than Vanilla while Vanilla Gated and Gated consistently performs better. With the kernel stage of 48-96-192, our binary network matches the accuracy of full-precision networks in both CIFAR-10 and CIFAR-100 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of our proposed methods with vanilla binary networks on ResNet-20 networks on CIFAR-10 validation set.</figDesc><table><row><cell>W/A</cell><cell>Weight</cell><cell cols="2">Residual Acc.(%)</cell></row><row><cell>32/32</cell><cell>FP</cell><cell>FP</cell><cell>92.1</cell></row><row><cell>1/1</cell><cell>Vanilla</cell><cell>Identity</cell><cell>84.13</cell></row><row><cell>1/1</cell><cell>Balanced</cell><cell>Identity</cell><cell>84.71</cell></row><row><cell>1/1</cell><cell>Vanilla</cell><cell>Gated</cell><cell>84.89</cell></row><row><cell>1/1</cell><cell>Balanced</cell><cell>Gated</cell><cell>85.34</cell></row><row><cell cols="4">Table 2. Performance comparison of 4 different modules on</cell></row><row><cell cols="4">ResNet-20 models on CIFAR-10/100 validation set.</cell></row><row><cell>Method</cell><cell cols="3">Kernel Stage CIFAR-10 CIFAR-100</cell></row><row><cell>FP</cell><cell>16-32-64</cell><cell>92.1</cell><cell>68.1</cell></row><row><cell>Vanilla</cell><cell>16-32-64</cell><cell>84.71</cell><cell>53.37</cell></row><row><cell>Vanilla Gated</cell><cell>16-32-64</cell><cell>84.96</cell><cell>55.24</cell></row><row><cell>Bireal</cell><cell>16-32-64</cell><cell>85.54</cell><cell>55.07</cell></row><row><cell>Gated</cell><cell>16-32-64</cell><cell>85.34</cell><cell>55.62</cell></row><row><cell>Vanilla</cell><cell>32-64-128</cell><cell>90.22</cell><cell>65.06</cell></row><row><cell>Vanilla Gated</cell><cell>32-64-128</cell><cell>90.71</cell><cell>66.15</cell></row><row><cell>Bireal</cell><cell>32-64-128</cell><cell>90.27</cell><cell>65.6</cell></row><row><cell>Gated</cell><cell>32-64-128</cell><cell>90.68</cell><cell>66.47</cell></row><row><cell>Vanilla</cell><cell>48-96-192</cell><cell>92.01</cell><cell>68.66</cell></row><row><cell>Vanilla Gated</cell><cell>48-96-192</cell><cell>92.31</cell><cell>69.11</cell></row><row><cell>Bireal</cell><cell>48-96-192</cell><cell>91.78</cell><cell>68.5</cell></row><row><cell>Gated</cell><cell>48-96-192</cell><cell>92.46</cell><cell>69.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Performance of binarized SSD on Pascal VOC dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Resolution FLOPs mAP(%)</cell></row><row><cell>FP</cell><cell>VGG-16</cell><cell>300</cell><cell>29986</cell><cell>74.3</cell></row><row><cell>FP</cell><cell>ResNet-34</cell><cell>300</cell><cell>6850</cell><cell>75.5</cell></row><row><cell>XNOR</cell><cell>ResNet-34</cell><cell>300</cell><cell>362</cell><cell>55.1</cell></row><row><cell>TBN</cell><cell>ResNet-34</cell><cell>300</cell><cell>464</cell><cell>59.5</cell></row><row><cell>BDN</cell><cell>DenseNet-37</cell><cell>512</cell><cell>1530</cell><cell>66.4</cell></row><row><cell>BDN</cell><cell>DenseNet-45</cell><cell>512</cell><cell>1960</cell><cell>68.2</cell></row><row><cell>BBG</cell><cell>ResNet-34</cell><cell>300</cell><cell>362</cell><cell>62.8</cell></row><row><cell>BBG</cell><cell>VGG-16</cell><cell>300</cell><cell>1062</cell><cell>68.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison of inference time of full-precision ResNet-18 and binary ResNet-18.</figDesc><table><row><cell>Model</cell><cell cols="3">Full-Precision DoReFa BBG</cell></row><row><cell>time(ms)</cell><cell>1457</cell><cell>249</cell><cell>251</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kligys</forename><surname>Skirmantas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Menglong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tang</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Hartwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalenichenko</forename><surname>Dmitry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2704" to="2713" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards evolutional compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08005</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Projection convolutional neural networks for 1-bit cnns via discrete back propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8344" to="8351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Binary ensemble neural network: More bits per network or more networks per bit?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4923" to="4932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Searching for accurate binary neural architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yunhe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07378</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note type="report_type">CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Back to simplicity: How to train accurate bnns from scratch?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Bornstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08637</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tbn: Convolutional neural network with ternary inputs and binary weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diwen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="315" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">dabnn: A super fast inference framework for binary neural networks on arm devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05858</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
