<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
							<email>xyfu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
							<email>jnzhang@cse.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
							<email>zqmeng@cse.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
							<email>king@cse.cuhk.edu.hk</email>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong Kong Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3366423.3380297</idno>
					<note>ACM Reference Format: Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020. MAGNN: Meta-path Aggregated Graph Neural Network for Heterogeneous Graph Em-bedding. In Proceedings of The Web Conference 2020 (WWW &apos;20), April 20-24, 2020, Taipei, Taiwan. ACM, New York, NY, USA, 11 pages. https: // This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. WWW &apos;20, April 20-24, 2020, Taipei, Taiwan © 2020 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License. ACM ISBN 978-1-4503-7023-3/20/04. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Neural networks</term>
					<term>Learning latent representations</term>
					<term>• Information systems → Social net- works KEYWORDS Heterogeneous graph</term>
					<term>Graph neural network</term>
					<term>Graph embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A large number of real-world graphs or networks are inherently heterogeneous, involving a diversity of node types and relation types. Heterogeneous graph embedding is to embed rich structural and semantic information of a heterogeneous graph into low-dimensional node representations. Existing models usually define multiple metapaths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models either omit node content features, discard intermediate nodes along the metapath, or only consider one metapath. To address these three limitations, we propose a new model named Metapath Aggregated Graph Neural Network (MAGNN) to boost the final performance. Specifically, MAGNN employs three major components, i.e., the node content transformation to encapsulate input node attributes, the intra-metapath aggregation to incorporate intermediate semantic nodes, and the inter-metapath aggregation to combine messages from multiple metapaths. Extensive experiments on three real-world heterogeneous graph datasets for node classification, node clustering, and link prediction show that MAGNN achieves more accurate prediction results than state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Many real-world datasets are naturally represented in a graph data structure, where objects and the relationships among them are embodied by nodes and edges, respectively. Examples include social networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>, physical systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>, traffic networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>, citation networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>, recommender systems <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b34">35]</ref>, knowledge graphs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, and so on. The unique non-Euclidean nature of graphs renders them difficult to be modeled by traditional machine learning models. For the neighborhood set of each node, there is no order or size limit. However, most statistical models assume an ordered and fixed-size input lying in the Euclidean space. Therefore, it would be beneficial if nodes could be represented by meaningful low-dimensional vectors in the Euclidean space and then be taken as the input for other machine learning models.</p><p>Different graph embedding techniques have been proposed for the graph structure. LINE <ref type="bibr" target="#b24">[25]</ref> generates node embeddings by exploiting the first-order and second-order proximity between nodes. Random-walk-based methods including DeepWalk <ref type="bibr" target="#b20">[21]</ref>, node2vec <ref type="bibr" target="#b12">[13]</ref>, and TADW <ref type="bibr" target="#b31">[32]</ref> feed node sequences generated by random walks to a skip-gram model <ref type="bibr" target="#b18">[19]</ref> to learn node embeddings. With the rapid development of deep learning, graph neural networks (GNNs) have been proposed, which learn the graph representations using specially designed neural layers. Spectral-based GNNs, including ChebNet <ref type="bibr" target="#b7">[8]</ref> and GCN <ref type="bibr" target="#b15">[16]</ref>, perform graph convolution operations in the Fourier domain of an entire graph. Recent spatial-based GNNs, including GraphSAGE <ref type="bibr" target="#b13">[14]</ref>, GAT <ref type="bibr" target="#b27">[28]</ref>, and many other variants <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, address the issues around scalability and generalization ability of the spectral-based models by performing graph convolution operations directly in the graph domain. An increasing number of researchers have paid attention to this promising area.</p><p>Although GNNs have achieved state-of-the-art results in many tasks, most GNN-based models assume that the input is a homogeneous graph with only one node type and one edge type. Most real-world graphs consist of various types of nodes and edges associated with attributes in different feature spaces. For example, a co-authorship network contains at least two types of nodes, namely authors and papers. Author attributes may include affiliations, citations, and research fields. Paper attributes may consist of keywords, venue, year, and so on. We refer to graphs of this kind as heterogeneous information networks (HINs) or heterogeneous graphs. The heterogeneity in both graph structure and node content makes it arXiv:2002.01680v2 [cs.SI] 31 Mar 2020 challenging for GNNs to encode their rich and diverse information into a low-dimensional vector space.</p><p>Most existing heterogeneous graph embedding methods are based on the idea of metapaths. A metapath is an ordered sequence of node types and edge types defined on the network schema, which describes a composite relation between the nodes types involved. For example, in a scholar network with authors, papers, and venues, Author-Paper-Author (APA) and Author-Paper-Venue-Paper-Author (APVPA) are metapaths describing two different relations among authors. The APA metapath associates two co-authors, while the APVPA metapath associates two authors who published papers in the same venue. Therefore, we can view a metapath as high-order proximity between two nodes. Because traditional GNNs treat all nodes equally, they are unable to model the complex structural and semantic information in heterogeneous graphs.</p><p>Although these metapath-based embedding methods outperform traditional network embedding methods on various tasks, such as node classification and link prediction, they still suffer from at least one of the following limitations. (1) The model does not leverage node content features, so it rarely performs well on heterogeneous graphs with rich node content features (e.g., metapath2vec <ref type="bibr" target="#b8">[9]</ref>, ESim <ref type="bibr" target="#b21">[22]</ref>, HIN2vec <ref type="bibr" target="#b10">[11]</ref>, and HERec <ref type="bibr" target="#b22">[23]</ref>).</p><p>(2) The model discards all intermediate nodes along the metapath by only considering two end nodes, which results in information loss (e.g., HERec <ref type="bibr" target="#b22">[23]</ref> and HAN <ref type="bibr" target="#b30">[31]</ref>). (3) The model relies on a single metapath to embed the heterogeneous graph. Hence, the model requires a manual metapath selection process and loses aspects of information from other metapaths, leading to suboptimal performance (e.g., metapath2vec <ref type="bibr" target="#b8">[9]</ref>).</p><p>To address these limitations, we propose a novel Metapath Aggregated Graph Neural Network (MAGNN) for heterogeneous graph embedding. MAGNN addresses all the issues described above by applying node content transformation, intra-metapath aggregation, and inter-metapath aggregation to generate node embeddings. Specifically, MAGNN first applies type-specific linear transformations to project heterogeneous node attributes, with possibly unequal dimensions for different node types, to the same latent vector space. Next, MAGNN applies intra-metapath aggregation with the attention mechanism <ref type="bibr" target="#b27">[28]</ref> for every metapath. During this intrametapath aggregation, each target node extracts and combines information from the metapath instances connecting the node with its metapath-based neighbors. In this way, MAGNN captures the structural and semantic information of heterogeneous graphs from both neighbor nodes and the metapath context in between. Following intra-metapath aggregation, MAGNN further conducts intermetapath aggregation using the attention mechanism to fuse latent vectors obtained from multiple metapaths into final node embeddings. By integrating multiple metapaths, our model can learn the comprehensive semantics ingrained in the heterogeneous graph.</p><p>In summary, this work makes several major contributions:</p><p>(1) We propose a novel metapath aggregated graph neural network for heterogeneous graph embedding. (2) We design several candidate encoder functions for distilling information from metapath instances, including one based on the idea of relational rotation in complex space <ref type="bibr" target="#b23">[24]</ref>. (3) We conduct extensive experiments on the IMDb and the DBLP datasets for node classification and node clustering, as The set of edges in a graph</p><formula xml:id="formula_0">G A graph G = (V, E) v A node v ∈ V P A metapath P (v, u) A metapath instance connecting node v and u N v</formula><p>The set of neighbors of node v N P v The set of metapath-P-based neighbors of node v</p><formula xml:id="formula_1">x v Raw (content) feature vector of node v h v Hidden state (embedding) of node v W Weight matrix α, β Normalized attention weight σ (·) Activation function ⊙ Element-wise multiplication | · |</formula><p>The cardinality of a set ∥ Vector concatenation well as on the Last.fm dataset for link prediction to evaluate the performance of our proposed model. Experiments on all of these datasets and tasks show that the node embeddings learned by MAGNN are consistently better than those generated by other state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>In this section, we give formal definitions of some important terminologies related to heterogeneous graphs. Graphical illustrations are provided in <ref type="figure" target="#fig_0">Figure 1</ref>. Besides, <ref type="table" target="#tab_0">Table 1</ref> summarizes frequently used notations in this paper for quick reference. </p><formula xml:id="formula_2">form of A 1 R 1 −→ A 2 R 2 −→ · · · R l −→ A l +1 (abbreviated as A 1 A 2 · · · A l +1 )</formula><p>, which describes a composite relation R = R 1 • R 2 • · · · • R l between node types A 1 and A l +1 , where • denotes the composition operator on relations. Definition 2.3. Metapath Instance. Given a metapath P of a heterogeneous graph, a metapath instance p of P is defined as a node sequence in the graph following the schema defined by P. Definition 2.4. Metapath-based Neighbor. Given a metapath P of a heterogeneous graph, the metapath-based neighbors N P v of a node v is defined as the set of nodes that connect with node v via metapath instances of P. A neighbor connected by two different metapath instances is regarded as two different nodes in N P v . Note that N P v includes v itself if P is symmetric. For example, considering the metapath UATA in <ref type="figure" target="#fig_0">Figure 1</ref>, artist Queen is a metapath-based neighbor of user Bob. These two nodes are connected via the metapath instance Bob-Beatles-Rock-Queen. Moreover, we may refer to Beatles and Rock as the intermediate nodes along this metapath instance. Definition 2.5. Metapath-based Graph. Given a metapath P of a heterogeneous graph G, the metapath-based graph G P is a graph constructed by all the metapath-P-based neighbor pairs in graph G. Note that G P is homogeneous if P is symmetric. Definition 2.6. Heterogeneous Graph Embedding. Given a heterogeneous graph G = (V, E), with node attribute matrices X A i ∈ R | V A i |×d A i for node types A i ∈ A, heterogeneous graph embedding is the task to learn the d-dimensional node representations h v ∈ R d for all v ∈ V with d ≪ |V | that are able to capture rich structural and semantic information involved in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>In this section, we review studies on graph representation learning that are related to our model. They are organized into two subsections: Section 3.1 summarizes research efforts on GNNs for general graph embedding, while Section 3.2 introduces graph embedding methods designed for heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Neural Networks</head><p>The goal of a GNN is to learn a low-dimensional vector representation h v for every node v, which can be used for many downstream tasks, e.g., node classification, node clustering, and link prediction. The rationale behind this is that each node is naturally defined by its own features and its neighborhood. Following this idea and based on graph signal processing, spectral-based GNNs were first developed to perform graph convolution in the Fourier domain of a graph. ChebNet <ref type="bibr" target="#b7">[8]</ref> utilizes Chebyshev polynomials to filter graph signals (node features) in the graph Fourier domain. Another influential model of this kind is GCN <ref type="bibr" target="#b15">[16]</ref>, which constrains and simplifies the parameters of ChebNet to alleviate the overfitting problem and improve the performance. However, spectral-based GNNs suffer from poor scalability and generalization ability, because they require the entire graph as input for every layer, and their learned filters depend on the eigenbasis of the graph Laplacian, which is closely related to the specific graph structure.</p><p>Spatial-based GNNs have been proposed to address these two limitations. GNNs of this kind define convolutions directly in the graph domain by aggregating feature information from neighbors for each node, thus imitating the convolution operations of convolutional neural networks for image data. GraphSAGE <ref type="bibr" target="#b13">[14]</ref>, the seminal spatial-based GNN framework, is founded upon the general notion of aggregator functions for efficient generation of node embeddings. The aggregator function samples, extracts, and transforms a target node's local neighborhood, and thus facilitates parallel training and generalization to unseen nodes or graphs. Many other spatial-based GNN variants have been proposed based on this idea. Inspired by the Transformer <ref type="bibr" target="#b26">[27]</ref>, GAT <ref type="bibr" target="#b27">[28]</ref> incorporates the attention mechanism into the aggregator function to take into account the relative importance of each neighbor's information from the target node's perspective. GGNN <ref type="bibr" target="#b16">[17]</ref> adds a gated recurrent unit (GRU) <ref type="bibr" target="#b6">[7]</ref> to the aggregator function by treating the aggregated neighborhood information as the input to the GRU of the current time step. GaAN <ref type="bibr" target="#b33">[34]</ref> combines GRU with the gated multi-head attention mechanism for dealing with spatiotemporal graphs. STAR-GCN <ref type="bibr" target="#b34">[35]</ref> stacks multiple GCN encoder-decoders to boost the rating prediction performance.</p><p>All of the GNNs mentioned above are either built for homogeneous graphs, or designed for graphs with a special structure, as in user-item recommender systems. Because most existing GNNs operate on features of nodes in the same shared embedding space, they cannot be naturally adapted to heterogeneous graphs with node features lying in different spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heterogeneous Graph Embedding</head><p>Heterogeneous graph embedding aims to project nodes in a heterogeneous graph into a low-dimensional vector space. This challenging topic has been addressed by a number of studies. For example, metapath2vec <ref type="bibr" target="#b8">[9]</ref> generates random walks guided by a single metapath, which are then fed to a skip-gram model <ref type="bibr" target="#b18">[19]</ref> to generate node embeddings. Given user-defined metapaths, ESim <ref type="bibr" target="#b21">[22]</ref> generates node embeddings by learning from sampled positive and negative metapath instances. HIN2vec <ref type="bibr" target="#b10">[11]</ref> carries out multiple prediction training tasks to learn representations of nodes and metapaths of a heterogeneous graph. Given a metapath, HERec <ref type="bibr" target="#b22">[23]</ref> converts a heterogeneous graph into a homogeneous graph based on metapathbased neighbors and applies the DeepWalk model to learn the node embeddings of the target type. Like HERec, HAN <ref type="bibr" target="#b30">[31]</ref> converts a heterogeneous graph into multiple metapath-based homogeneous graphs in a similar way, but uses a graph attention network architecture to aggregate information from the neighbors and leverages the attention mechanism to combine various metapaths. Another model, PME <ref type="bibr" target="#b5">[6]</ref>, learns node embeddings by projecting them into the corresponding relation spaces and optimizing the proximity between the projected nodes.</p><p>However, all of the heterogeneous graph embedding methods introduced above have the limitations of either ignoring node content features, discarding all intermediate nodes along the metapath, or utilizing only a single metapath. Although they might have improved upon the performance of homogeneous graph embedding methods for some heterogeneous graph datasets, there is still room for improvement by exploiting more comprehensively the information embedded in heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we describe a new metapath aggregated graph neural network (MAGNN) for heterogeneous graph embedding. MAGNN is constructed by three major components: node content transformation, intra-metapath aggregation, and inter-metapath aggregation. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the embedding generation of a single node. The overall forward propagation process is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Node Content Transformation</head><p>For a heterogeneous graph associated with node attributes, different node types may have unequal dimensions of feature vectors. Even if they happen to be the same dimension, they may lie in different feature spaces. For example, n 1 -dimensional bag-of-words vectors of texts and n 2 -dimensional intensity histogram vectors of images cannot directly operate together even if n 1 = n 2 . Feature vectors of different dimensions are troublesome when we process them in a unified framework. Therefore, we need to project different types of node features into the same latent vector space before all else.</p><p>So before feeding node vectors into MAGNN, we apply a typespecific linear transformation for each type of nodes by projecting feature vectors into the same latent factor space. For a node v ∈ V A of type A ∈ A, we have</p><formula xml:id="formula_3">h ′ v = W A · x A v ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_4">x v ∈ R d A is the original feature vector, and h ′ v ∈ R d ′ is the projected latent vector of node v. W A ∈ R d ′ ×d A is the parametric weight matrix for type A's nodes.</formula><p>The node content transformation addresses the heterogeneity of a graph that originates from the node content features. After applying this operation, all nodes' projected features share the same dimension, which facilitates the aggregation process of the next model component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intra-metapath Aggregation</head><p>Given a metapath P, the intra-metapath aggregation layer learns the structural and semantic information embedded in the target node, the metapath-based neighbors, and the context in between, by encoding the metapath instances of P. Let P(v, u) be a metapath instance connecting the target node v and the metapath-based neighbor u ∈ N P v , we further define the intermediate nodes of</p><formula xml:id="formula_5">P(v, u) as {m P (v,u) } = P(v, u) \ {u, v}.</formula><p>Intra-metapath aggregation employs a special metapath instance encoder to transform all the node features along a metapath instance into a single vector,</p><formula xml:id="formula_6">h P (v,u) = f θ (P(v, u)) = f θ h ′ v , h ′ u , h ′ t , ∀t ∈ {m P (v,u) } , (2) where h P (v,u) ∈ R d ′ has a dimension of d ′ .</formula><p>For simplicity, here we use P (v, u) to represent a single instance, although there might be multiple instances connecting the two nodes. Section 4.4 introduces several choices of a qualified metapath instance encoder.</p><p>After encoding the metapath instances into vector representations, we adopt a graph attention layer <ref type="bibr" target="#b27">[28]</ref> to weighted sum the metapath instances of P related to target node v. The key idea is that different metapath instances would contribute to the target node's representation in different degrees. We can model this by learning a normalized importance weight α P vu for each metapath instance and then weighted summing all instances:</p><formula xml:id="formula_7">e P vu = LeakyReLU a ⊺ P · h ′ v ∥h P (v,u) , α P vu = exp e P vu s ∈N P v exp e P vs , h P v = σ u ∈N P v α P vu · h P (v,u) .<label>(3)</label></formula><p>Here a P ∈ R 2d ′ is the parameterized attention vector for metapath P, and ∥ denotes the vector concatenation operator. e P vu indicates the importance of metapath instance P (v, u) to node v, which is then normalized across all choices of u ∈ N P v using the softmax function. Once the normalized importance weight α P vu is obtained for all u ∈ N P v , they are used to compute a weighted combination of the representations of the metapath instances about node v. Finally, the output goes through an activation function σ (·).</p><p>This attention mechanism can also be extended to multiple heads, which helps to stabilize the learning process and reduce the high variance introduced by the heterogeneity of graphs. That is, we execute K independent attention mechanisms, and then concatenate their outputs, resulting in the following formulation:</p><formula xml:id="formula_8">h P v = K ∥ k=1 σ u ∈N P v α P vu k · h P (v,u) ,<label>(4)</label></formula><p>where α P vu k is the normalized importance of metapath instance P (v, u) to node v at the k-th attention head.</p><p>To sum up, given the projected feature vectors h ′ u ∈ R d ′ ∀u ∈ V and the set of metapaths P A = {P 1 , P 2 , . . . , P M } which start or end with node type A ∈ A, the intra-metapath aggregation of MAGNN generates M metapath-specific vector representations of the target</p><formula xml:id="formula_9">node v ∈ V A , denoted as h P 1 v , h P 2 v , . . . , h P M v . Each h P i v ∈ R d ′ (assuming K = 1)</formula><p>can be interpreted as a summarization of the P imetapath instances about node v, exhibiting one aspect of semantic information contained in node v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inter-metapath Aggregation</head><p>After aggregating the node and edge data within each metapath, we need to combine the semantic information revealed by all metapaths using an inter-metapath aggregation layer. Now for a node type A, we have |V A | sets of latent vectors:</p><formula xml:id="formula_10">h P 1 v , h P 2 v , . . . , h P M v for v ∈ V A ,</formula><p>where M is the number of metapaths for type A. One straightforward inter-metapath aggregation approach is to take the element-wise mean of these node vectors. We extend this approach by exploiting the attention mechanism to assign different weights to different metapaths. This operation is reasonable because metapaths are not equally important in a heterogeneous graph. First, we summarize each metapath P i ∈ P A by averaging the transformed metapath-specific node vectors for all nodes v ∈ V A ,</p><formula xml:id="formula_11">s P i = 1 |V A | v ∈V A tanh M A · h P i v + b A ,<label>(5)</label></formula><p>where M A ∈ R d m ×d ′ and b A ∈ R d m are learnable parameters.</p><p>Then we use the attention mechanism to fuse the metapathspecific node vectors of v as follows:</p><formula xml:id="formula_12">e P i = q ⊺ A · s P i , β P i = exp e P i P ∈ P A exp (e P )</formula><p>,</p><formula xml:id="formula_13">h P A v = P ∈ P A β P · h P v ,<label>(6)</label></formula><p>where q A ∈ R d m is the parameterized attention vector for node type A. β P i can be interpreted as the relative importance of metapath P i to type A's nodes. Once β P i is computed for each P i ∈ P A , we weighted sum all the metapath-specific node vectors of v. At last, MAGNN employs an additional linear transformation with a nonlinear function to project the node embeddings to the vector space with the desired output dimension:</p><formula xml:id="formula_14">h v = σ W o · h P A v ,<label>(7)</label></formula><p>where σ (·) is an activation function, and W o ∈ R d o ×d ′ is a weight matrix. This projection is task-specific. It can be interpreted as a linear classifier for node classification or regarded as a projection to the space with node similarity measures for link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Metapath Instance Encoders</head><p>To encode each metapath instance in Section 4.2, we examine three candidate encoder functions:</p><p>• Mean encoder. This function takes the element-wise mean of the node vectors along the metapath instance P (v, u):</p><formula xml:id="formula_15">h P (v,u) = MEAN h ′ t , ∀t ∈ P (v, u) .<label>(8)</label></formula><p>• Linear encoder. This function is an extension to the mean encoder by appending it with a linear transformation:</p><formula xml:id="formula_16">h P (v,u) = W P · MEAN h ′ t , ∀t ∈ P (v, u) .<label>(9)</label></formula><p>• Relational rotation encoder. We also examine a metapath instance encoder based on relational rotation in complex space, an operation proposed by RotatE <ref type="bibr" target="#b23">[24]</ref> for knowledge graph embedding. The mean and linear encoders introduced above treat the metapath instance essentially as a set, and thus ignore the information embedded in the sequential structure of the metapath. Relational rotation provides a way to model this kind of knowledge. Given P (v, u) = (t 0 , t 1 , . . . , t n ) with t 0 = u and t n = v, let R i be the relation between node t i−1 and node t i , let r i be the relation vector of R i , the relational rotation encoder is formulated as:</p><formula xml:id="formula_17">o 0 = h ′ t 0 = h ′ u , o i = h ′ t i + o i−1 ⊙ r i , h P (v,u) = o n n + 1 ,<label>(10)</label></formula><p>where h ′ t i and r i are both complex vectors, ⊙ is the elementwise product. We can easily interpret a real vector of dimension d ′ as a complex vector of dimension d ′ /2 by treating the first half of the vector as the real part, and the second half as the imaginary part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training</head><p>After applying components introduced in the previous sections, we obtain the final node representations, which can then be used in different downstream tasks. Depending on the characteristics of different tasks and the availability of node labels, we can train MAGNN in two major learning paradigms, i.e., semi-supervised learning and unsupervised learning.</p><p>For semi-supervised learning, with the guide of a small fraction of labeled nodes, we can optimize the model weights by minimizing the cross entropy via backpropagation and gradient descent, and thereby learn meaningful node embeddings for heterogeneous graphs. The cross entropy loss for this semi-supervised learning is formulated as:</p><formula xml:id="formula_18">L = − v ∈V L C c=1 y v [c] · log h v [c],<label>(11)</label></formula><p>where V L is the set of nodes that have labels, C is the number of classes, y v is the one-hot label vector of node v, and h v is the predicted probability vector of node v. For unsupervised learning, without any node labels, we can optimize the model weights by minimizing the following loss function through negative sampling <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_19">L = − (u,v)∈Ω log σ h ⊺ u · h v − (u ′ ,v ′ )∈Ω − log σ −h ⊺ u ′ · h v ′ ,<label>(12)</label></formula><p>where σ (·) is the sigmoid function, Ω is the set of observed (positive) node pairs, Ω − is the set of negative node pairs sampled from all unobserved node pairs (the complement of Ω).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we present experiments to demonstrate the efficacy of MAGNN for heterogeneous graph embedding. The experiments aim to address the following research questions:</p><p>• RQ1. How does MAGNN perform in classifying nodes? </p><formula xml:id="formula_20">v ← W A · x v , ∀v ∈ V A ; 3 end 4 for l = 1 . . . L do 5</formula><p>for node type A ∈ A do <ref type="bibr" target="#b5">6</ref> for metapath P ∈ P A do <ref type="bibr" target="#b6">7</ref> for v ∈ V A do <ref type="bibr" target="#b7">8</ref> Calculate h l P (v,u) for all u ∈ N P v using the metapath instance encoder function; <ref type="bibr" target="#b8">9</ref> Combine extracted metapath instances</p><formula xml:id="formula_21">h P v l ← K ∥ k =1 σ u ∈N P v α P vu k · h l P (v,u) ; 10 end 11 end 12</formula><p>Calculate the weight β P for each metapath P ∈ P A ; <ref type="bibr" target="#b12">13</ref> Fuse the embeddings from different metapaths</p><formula xml:id="formula_22">h P A v l ← P ∈ P A β P · h P v l , ∀v ∈ V A ; 14 end 15</formula><p>Layer output projection</p><formula xml:id="formula_23">h l v = σ W l o · h P A v l , ∀v ∈ V A , ∀A ∈ A; 16 end 17 z v ← h L v , ∀v ∈ V;</formula><p>• RQ2. How does MAGNN perform in clustering nodes?</p><p>• RQ3. How does MAGNN perform in predicting plausible links between node pairs? • RQ4. What is the impact of the three major components of MAGNN described in the previous section? • RQ5. How do we understand the representation capability of different graph embedding methods?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We adopt three widely used heterogeneous graph datasets from different domains to evaluate the performance of MAGNN as compared to state-of-the-art baselines. Specifically, the IMDb and DBLP datasets are used in the experiments for node classification and node clustering. The Last.fm dataset is used in the experiments for link prediction. Simple statistics of the three datasets are summarized in <ref type="table" target="#tab_3">Table 2</ref>, and network schemas are illustrated in <ref type="figure">Figure 3</ref>. We assign one-hot id vectors to nodes with no attributes as their dummy input features.  <ref type="figure">Figure 3</ref>: Network schemas of the three heterogeneous graph datasets used in this paper.</p><p>• IMDb 1 is an online database about movies and television programs, including information such as cast, production crew, and plot summaries. We use a subset of IMDb scraped from online, containing 4278 movies, 2081 directors, and 5257 actors after data preprocessing. Movies are labeled as one of three classes (Action, Comedy, and Drama) based on their genre information. Each movie is also described by a bag-of-words representation of its plot keywords. For semisupervised learning models, the movie nodes are divided into training, validation, and testing sets of 400 (9.35%), 400 (9.35%), and 3478 (81.30%) nodes, respectively. • DBLP 2 is a computer science bibliography website. We adopt a subset of DBLP extracted by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>, containing 4057 authors, 14328 papers, 7723 terms, and 20 publication venues after data preprocessing. The authors are divided into four research areas (Database, Data Mining, Artificial Intelligence, and Information Retrieval). Each author is described by a bag-of-words representation of their paper keywords. For semi-supervised learning models, the author nodes are divided into training, validation, and testing sets of 400 (9.86%), 400 (9.86%), and 3257 (80.28%) nodes, respectively. • Last.fm 3 is a music website keeping track of users' listening information from various sources. We adopt a dataset released by HetRec 2011 <ref type="bibr" target="#b3">[4]</ref>, consisting of 1892 users, 17632 artists, and 1088 artist tags after data preprocessing. This dataset is used for the link prediction task, and no label or feature is included in this dataset. For semi-supervised learning models, the user-artist pairs are divided into training, validation, and testing sets of 64984 (70%), 9283 (10%), and 18567 (20%) pairs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare MAGNN against different kinds of graph embedding models, including traditional (as opposed to GNNs) homogeneous graph embedding models, traditional heterogeneous graph embedding models, GNNs for homogeneous graphs, and GNNs for heterogeneous graphs. We denote them as traditional homogeneous models, traditional heterogeneous models, homogeneous GNNs, and heterogeneous GNNs, respectively. The list of baseline models is shown as follows.  • LINE <ref type="bibr" target="#b24">[25]</ref> is a traditional homogeneous model exploiting the first-order and second-order proximity between nodes. We apply it to the heterogeneous graphs by ignoring the heterogeneity of graph structure and dropping all node content features. The LINE variant using second-order proximity is applied in our experiments. • node2vec <ref type="bibr" target="#b12">[13]</ref> is a traditional homogeneous model serving as a generalized version of DeepWalk <ref type="bibr" target="#b20">[21]</ref>. We apply it to the heterogeneous graphs in the same way as LINE. • ESim <ref type="bibr" target="#b21">[22]</ref> is a traditional heterogeneous model that learns node embeddings from sampled metapath instances. ESim requires a predefined weight for each metapath. Here we assign equal weights to all metapaths because searching for the optimal weights of metapaths is difficult, and does not provide a significant performance gain over equal weights according to the authorsâĂŹ experiments. • metapath2vec <ref type="bibr" target="#b8">[9]</ref> is a traditional heterogeneous model that generates node embeddings by feeding metapath-guided random walks to a skip-gram model. This model relies on a single user-specified metapath, so we test on all metapaths separately and report the one with the best results. We use the metapath2vec++ model variant in our experiments. • HERec <ref type="bibr" target="#b22">[23]</ref> is a traditional heterogeneous model that learns node embeddings by applying DeepWalk to the metapathbased homogeneous graphs converted from the original heterogeneous graph. This model comes with an embedding fusion algorithm designed for rating prediction, which can be adapted to link prediction. For node classification/clustering, we select and report the metapath with the best performance. <ref type="figure" target="#fig_0">• GCN [16]</ref> is a homogeneous GNN. This model performs convolutional operations in the graph Fourier domain. Here we test GCN on metapath-based homogeneous graphs and report the results from the best metapath. • GAT [28] is a homogeneous GNN. This model performs convolutional operations in the graph spatial domain with the attention mechanism incorporated. Similarly, here we test GAT on metapath-based homogeneous graphs and report the results from the best metapath. • GATNE [5] is a heterogeneous GNN. It generates a node's representation from the base embedding and the edge embeddings, with a focus on the link prediction task. Here we report the results from the best-performing GATNE variant. <ref type="figure" target="#fig_0">• HAN [31]</ref> is a heterogeneous GNN. It learns metapath-specific node embeddings from different metapath-based homogeneous graphs, and leverages the attention mechanism to combine them into one vector representation for each node.</p><p>For traditional models, including LINE, node2vec, ESim, meta-path2vec, and HERec, we set the window size to 5, walk length to 100, walks per node to 40, and number of negative samples to 5, if applicable. For GNNs, including GCN, GAT, HAN, and our proposed MAGNN, we set the dropout rate to 0.5; we use the same splits of training, validation, and testing sets; we employ the Adam optimizer with the learning rate set to 0.005 and the weight decay (L2 penalty) set to 0.001; we train the GNNs for 100 epochs and apply early stopping with a patience of 30. For node classification and node clustering, the GNNs are trained in a semi-supervised fashion with a small fraction of nodes labeled as guidance. For GAT, HAN, and MAGNN, we set the number of attention heads to 8. For HAN and MAGNN, we set the dimension of the attention vector in inter-metapath aggregation to 128. For a fair comparison, we set the embedding dimension of all the models mentioned above to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Node Classification (RQ1)</head><p>We conduct experiments on the IMDb and DBLP datasets to compare the performance of different models on the node classification task. We feed the embeddings of labeled nodes (movies in IMDb and authors in DBLP) generated by each learning model to a linear support vector machine (SVM) classifier with varying training proportions. Note that for a fair comparison, only the nodes in the testing set are fed to the linear SVM, because semi-supervised models have already "seen" the nodes in the training and validation sets, as shown in <ref type="bibr">Equation 11</ref>. Hence, the training and testing proportions of the linear SVM here only concern the testing set (i.e., 3478 nodes for IMDb and 3257 nodes for DBLP). Again, the train/test splits for the linear SVM are also the same across embedding models. Similar strategies are also applied to the experiments of node clustering and link prediction. We report the average Macro-F1 and Micro-F1 of 10 runs of each embedding model in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>As shown in the table, MAGNN performs consistently better than other baselines across different training proportions and datasets. On IMDb, it is interesting to see that node2vec performs better than traditional heterogeneous models. That said, GNNs, especially heterogeneous GNNs, obtain even better results, demonstrating that the GNN architecture, which judiciously utilizes the heterogeneous node features, helps improve the embedding performance. The performance gain obtained by MAGNN over the best baseline (HAN) is around 4-7%, which indicates that metapath instances contain richer information than metapath-based neighbors. On DBLP, the node classification task is trivial, as evident from the high scores of all models. Even so, MAGNN still outperforms the strongest baseline by 1-2%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Node Clustering (RQ2)</head><p>We conduct experiments on the IMDb and DBLP datasets to compare the performance of different models on the node clustering task. We feed the embeddings of labeled nodes (movies in IMDb and authors in DBLP) generated by each learning model to the K-Means algorithm. The number of clusters in K-Means is set to the number of classes for each dataset, i.e., 3 for IMDb and 4 for DBLP. We employ the normalized mutual information (NMI) and adjusted Rand index (ARI) as the evaluation metrics. Since the clustering result of the K-Means algorithm is highly dependent on the initialization of the centroids, we repeat K-Means 10 times for each run of the embedding model, and each embedding model is tested for 10 runs. We report the averaged results in <ref type="table" target="#tab_5">Table 4</ref>.</p><p>From <ref type="table" target="#tab_5">Table 4</ref>, we can see that MAGNN is consistently superior to all other baselines in node clustering. Note that all models have much poorer performance on IMDb than on DBLP. This is presumably because of the dirty labels of movies in IMDb: every movie node in the original IMDb dataset has multiple genres, and we only choose the very first one as its class label. We can see that the traditional heterogeneous models do not have many advantages over the traditional homogeneous models in node clustering. Node2vec is expected to perform strongly in the node clustering task because, being a random-walk-based approach, it forces nodes that are close in the graph also to be close in the embedding space <ref type="bibr" target="#b32">[33]</ref>, and thereby encodes node positional information. This property implicitly facilitates the K-Means algorithm as it clusters nodes based on the Euclidean distances between embeddings. Despite this, the heterogeneity-aware GNNs (i.e., HAN and MAGNN) still rank the first in node clustering on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Link Prediction (RQ3)</head><p>We also conduct experiments on the Last.fm dataset to evaluate the performance of MAGNN and other baselines in the link prediction task. For the GNNs, we treat the connected user-artist pair as positive node pairs, and consider all unconnected user-artist links as negative node pairs. We add the same number of randomly sampled negative node pairs to the validation and testing sets. During the GNNs' training, negative node pairs are also uniformly sampled on the fly. The GNNs are then optimized by minimizing the objective function described in Equation 12.</p><p>Given the user embedding h u and the artist embedding h a generated by the trained model, we calculate the probability that u and v link together as follows:</p><formula xml:id="formula_24">p ua = σ h ⊺ u · h a ,<label>(13)</label></formula><p>where σ (·) is the sigmoid function. The embedding models for link prediction are evaluated by the area under the ROC curve (AUC) and average precision (AP) scores. We report the averaged results of 10 runs of each embedding model in <ref type="table" target="#tab_6">Table 5</ref>. From <ref type="table" target="#tab_6">Table 5</ref>, MAGNN outperforms other baseline models by a large margin. The strongest traditional model here is metapath2vec, which learns from node sequences generated from random walks guided by a single metapath. MAGNN achieves better scores than metapath2vec, showing that considering a single metapath is suboptimal. Among GNN baselines, HAN obtains the best results because it is heterogeneity-aware and combines multiple metapaths. Our MAGNN achieves a relative improvement of around 6% over HAN. This result supports our claim that the metapath contexts of nodes are critical to the node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study (RQ4)</head><p>To validate the effectiveness of each component of our model, we further conduct experiments on different MAGNN variants. Here we report the results obtained from the three datasets on all three tasks in <ref type="table" target="#tab_7">Table 6</ref>. Note that every presented score of the node classification task (i.e., Macro-F1 and Micro-F1) is an average of the scores in different training proportions (explained in Section 5.3). Here MAGNN rot is our proposed model using the relational rotation encoder, i.e., the one used to compete with other baselines in <ref type="table" target="#tab_4">Table 3</ref>, 4, and 5. Let MAGNN rot be the reference model, MAGNN feat is the equivalent model without utilizing node content features; MAGNN nb considers only the metapathbased neighbors; MAGNN sm considers the single best metapath; MAGNN avg switches to using the mean metapath instance encoder; MAGNN linear switches to using the linear metapath instance encoder. Except for the above-mentioned differences, all other settings are the same for these MAGNN variants. Note that MAGNN feat on Last.fm is equivalent to MAGNN rot because this dataset does not contain node attributes.</p><p>As can be seen, by utilizing the node content features, MAGNN rot obtains a significant performance improvement over MAGNN feat , which shows the necessity of applying node content transformation to incorporate node features. Comparing MAGNN nb with MAGNN avg , MAGNN linear , and MAGNN rot , we see that aggregating metapath instances rather than metapath-based neighbors brings about a boost in performance, which validates the efficacy of intrametapath aggregation. Next, the difference between the results of MAGNN sm and MAGNN rot reveals that the model performance is improved considerably by combining multiple metapaths in intermetapath aggregation. Finally, the results of MAGNN avg , MAGNN linear , and MAGNN rot suggest that the relational rotation encoder does help to improve MAGNN by a small margin. It is interesting to see that MAGNN linear performs worse than MAGNN avg . Nonetheless, all three MAGNN variants using different encoders still consistently outperform the best baseline, HAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Visualization (RQ5)</head><p>In addition to the quantitative evaluations of embedding models, we also visualize node embeddings to conduct a qualitative assessment of the embedding results. We randomly select 30 user-artist pairs from the positive testing set of the Last.fm dataset, and then project the embeddings of these nodes into a 2-dimensional space using t-SNE. Here we illustrate the visualization results of LINE, ESim, GCN, and MAGNN in <ref type="figure" target="#fig_4">Figure 4</ref>, where red points and green points indicate users and artists, respectively. Based on this visualization, one can quickly tell the differences among graph embedding models in terms of their learning ability towards heterogeneous graphs. As a traditional homogeneous graph embedding model, LINE cannot effectively divide user nodes and artist nodes into two different groups. In contrast, ESim, a traditional heterogeneous model, can roughly partition the two types of nodes. Thanks to the powerful GNN architecture and by choosing appropriate metapaths, a homogeneous GNN such as GCN can isolate different types of nodes and encode the correlation information of the user-artist pairs into the node embeddings. From <ref type="figure" target="#fig_4">Figure 4</ref>, we can see that our proposed MAGNN obtains the best embedding results, with two well-separated user and artist groups, and an aligned correlation of user-artist pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a novel metapath aggregated graph neural network (MAGNN) to address the three characteristic limitations of existing heterogeneous graph embedding methods, namely (1) dropping node content features, (2) discarding intermediate nodes along metapaths, and (3) considering only a single metapath. To be specific, MAGNN applies three building block components: (1) node content transformation, (2) intra-metapath aggregation, and <ref type="formula" target="#formula_7">(3)</ref> inter-metapath aggregation to deal with each of the limitations, respectively. Additionally, we define the notion of metapath instance encoders, which are used to extract the structural and semantic information ingrained in metapath instances. We propose several candidate encoder functions, including one inspired by the RotatE knowledge graph embedding model <ref type="bibr" target="#b23">[24]</ref>. In experiments, MAGNN achieves state-of-the-art results on three real-world datasets in the node classification, node clustering, and link prediction tasks. Ablation studies also demonstrate the efficacy of the three major components of MAGNN in boosting embedding performance. We plan to adapt this heterogeneous graph embedding framework to the rating prediction (recommendation) task with the user-item data assisted by the heterogeneous knowledge graph <ref type="bibr" target="#b29">[30]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of the terms defined in Section 2. (a) An example heterogeneous graph with three types of nodes (i.e., users, artists, and tags). (b) The User-Artist-Tag-Artist (UATA) metapath and the User-Artist-Tag-Artist-User (UATAU) metapath. (c) Example metapath instances of the UATA and UATAU metapaths, respectively. (d) The metapath-based graphs for the UATA and UATAU metapaths, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Node Content Transformation (b) Intra-metapath Aggregation (c) Inter-metapath Aggregation The overall architecture of MAGNN (path instances that start and end with the target node are omitted for clarity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Embedding visualization of node pairs in Last.fm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notations used in this paper.</figDesc><table><row><cell cols="2">Notations Definitions</cell></row><row><cell>R n</cell><cell>n-dimensional Euclidean space</cell></row><row><cell>a, a, A</cell><cell>Scalar, vector, matrix</cell></row><row><cell>A ⊺</cell><cell>Matrix/vector transpose</cell></row><row><cell>V</cell><cell>The set of nodes in a graph</cell></row><row><cell>E</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1: MAGNN forward propagation.Input: The heterogeneous graph G = (V, E), node types A = A 1 , A 2 , . . . , A | A | , metapaths P = P 1 , P 2 , . . . P | P | , node features {x v , ∀v ∈ V}, the number of attention heads K, the number of layers L Output: The node embeddings {z v , ∀v ∈ V} 1 for node type A ∈ A do</figDesc><table /><note>2 Node content transformation h 0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experiment results (%) on the IMDb and DBLP datasets for the node classification task.</figDesc><table><row><cell>Dataset</cell><cell>Metrics</cell><cell>Train %</cell><cell cols="6">Unsupervised LINE node2vec ESim metapath2vec HERec GCN GAT HAN MAGNN Semi-supervised</cell></row><row><cell></cell><cell></cell><cell>20%</cell><cell>44.04</cell><cell>49.00</cell><cell>48.37</cell><cell>46.05</cell><cell>45.61 52.73 53.64 56.19</cell><cell>59.35</cell></row><row><cell></cell><cell>Macro-F1</cell><cell>40% 60%</cell><cell>45.45 47.09</cell><cell>50.63 51.65</cell><cell>50.09 51.45</cell><cell>47.57 48.17</cell><cell>46.80 53.67 55.50 56.15 46.84 54.24 56.46 57.29</cell><cell>60.27 60.66</cell></row><row><cell>IMDb</cell><cell></cell><cell>80% 20%</cell><cell>47.49 45.21</cell><cell>51.49 49.94</cell><cell>51.37 49.32</cell><cell>49.99 47.22</cell><cell>47.73 54.77 57.43 58.51 46.23 52.80 53.64 56.32</cell><cell>61.44 59.60</cell></row><row><cell></cell><cell>Micro-F1</cell><cell>40% 60%</cell><cell>46.92 48.35</cell><cell>51.77 52.79</cell><cell>51.21 52.53</cell><cell>48.17 49.87</cell><cell>47.89 53.76 55.56 57.32 48.19 54.23 56.47 58.42</cell><cell>60.50 60.88</cell></row><row><cell></cell><cell></cell><cell>80%</cell><cell>48.98</cell><cell>52.72</cell><cell>52.54</cell><cell>50.50</cell><cell>49.11 54.63 57.40 59.24</cell><cell>61.53</cell></row><row><cell></cell><cell></cell><cell>20%</cell><cell>87.16</cell><cell>86.70</cell><cell>90.68</cell><cell>88.47</cell><cell>90.82 88.00 91.05 91.69</cell><cell>93.13</cell></row><row><cell></cell><cell>Macro-F1</cell><cell>40% 60%</cell><cell>88.85 88.93</cell><cell>88.07 88.69</cell><cell>91.61 91.84</cell><cell>89.91 90.50</cell><cell>91.44 89.00 91.24 91.96 92.08 89.43 91.42 92.14</cell><cell>93.23 93.57</cell></row><row><cell>DBLP</cell><cell></cell><cell>80% 20%</cell><cell>89.51 87.68</cell><cell>88.93 87.21</cell><cell>92.27 91.21</cell><cell>90.86 89.02</cell><cell>92.25 89.98 91.73 92.50 91.49 88.51 91.61 92.33</cell><cell>94.10 93.61</cell></row><row><cell></cell><cell>Micro-F1</cell><cell>40% 60%</cell><cell>89.25 89.34</cell><cell>88.51 89.09</cell><cell>92.05 92.28</cell><cell>90.36 90.94</cell><cell>92.05 89.22 91.77 92.57 92.66 89.57 91.97 92.72</cell><cell>93.68 93.99</cell></row><row><cell></cell><cell></cell><cell>80%</cell><cell>89.96</cell><cell>89.37</cell><cell>92.68</cell><cell>91.31</cell><cell>92.78 90.33 92.24 93.23</cell><cell>94.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experiment results (%) on the IMDb and DBLP datasets for the node clustering task.</figDesc><table><row><cell cols="2">Dataset Metrics</cell><cell cols="8">Unsupervised LINE node2vec ESim metapath2vec HERec GCN GAT HAN MAGNN Semi-supervised</cell></row><row><cell>IMDb</cell><cell>NMI ARI</cell><cell>1.13 1.20</cell><cell>5.22 6.02</cell><cell>1.07 1.01</cell><cell>0.89 0.22</cell><cell>0.39 0.11</cell><cell>7.46 7.69</cell><cell>7.84 10.79 8.87 11.11</cell><cell>15.58 16.74</cell></row><row><cell>DBLP</cell><cell>NMI ARI</cell><cell>71.02 76.52</cell><cell>77.01 81.37</cell><cell>68.33 72.22</cell><cell>74.18 78.11</cell><cell cols="3">69.03 73.45 70.73 77.49 72.45 77.50 76.04 82.95</cell><cell>80.81 85.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Experiment results (%) on the Last.fm dataset for the link prediction task.</figDesc><table><row><cell cols="10">Dataset Metrics LINE node2vec ESim metapath2vec HERec GCN GAT GATNE HAN MAGNN</cell></row><row><cell>Last.fm</cell><cell>AUC AP</cell><cell>85.76 88.07</cell><cell>67.14 64.11</cell><cell>82.00 82.19</cell><cell>92.20 90.11</cell><cell>91.52 90.97 92.36 89.47 91.65 91.55</cell><cell>89.21 88.86</cell><cell>93.40 92.44</cell><cell>98.91 98.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results (%) for ablation study.</figDesc><table><row><cell>Variant</cell><cell cols="3">IMDb Macro-F1 Micro-F1 NMI</cell><cell cols="4">DBLP ARI Macro-F1 Micro-F1 NMI</cell><cell>ARI</cell><cell cols="2">Last.fm AUC AP</cell></row><row><cell>MAGNN feat</cell><cell>48.87</cell><cell>50.36</cell><cell>5.82</cell><cell>5.30</cell><cell>92.80</cell><cell>93.32</cell><cell cols="2">77.17 82.15</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>MAGNN nb</cell><cell>58.45</cell><cell>58.84</cell><cell cols="2">12.87 11.98</cell><cell>92.61</cell><cell>93.15</cell><cell cols="4">77.64 82.60 93.68 92.95</cell></row><row><cell>MAGNN sm</cell><cell>56.77</cell><cell>56.64</cell><cell cols="2">11.90 11.84</cell><cell>93.19</cell><cell>93.69</cell><cell cols="4">79.48 84.39 92.54 91.52</cell></row><row><cell>MAGNN avg</cell><cell>59.66</cell><cell>59.78</cell><cell cols="2">13.64 15.27</cell><cell>93.13</cell><cell>93.44</cell><cell cols="4">79.31 84.30 98.63 98.57</cell></row><row><cell>MAGNN linear</cell><cell>57.80</cell><cell>57.96</cell><cell>9.80</cell><cell>8.49</cell><cell>93.21</cell><cell>93.52</cell><cell cols="4">78.95 83.89 98.56 98.48</cell></row><row><cell>MAGNN rot</cell><cell>60.43</cell><cell>60.63</cell><cell cols="2">15.58 16.74</cell><cell>93.51</cell><cell>93.94</cell><cell cols="4">80.81 85.54 98.91 98.93</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.imdb.com/ 2 https://dblp.uni-trier.de/ 3 https://www.last.fm/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 2300174 (Collaborative Research Fund, No. C5026-18GF) and CUHK 3133238 (Research Sustainability of Major RGC Funding Schemes)).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interaction Networks for Learning about Objects, Relations and Physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iván</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsvi</forename><surname>Kuflik</surname></persName>
		</author>
		<title level="m">2nd Workshop on Information Heterogeneity and Fusion in Recommender Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>RecSys</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation Learning for Attributed Multiplex Heterogeneous Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PME: Projected Metric Embedding on Heterogeneous Networks for Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1177" to="1186" />
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, and Xue Li</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Metapath2Vec: Scalable Representation Learning for Heterogeneous Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein Interface Prediction using Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph-based Consensus Maximization Among Multiple Supervised and Unsupervised Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Node2Vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph Regularized Transductive Classification on Heterogeneous Information Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="570" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09769</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heterogeneous Information Network Embedding for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is All you Need. In NIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structural Deep Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Knowledge Graph Convolutional Networks for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3307" to="3313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Network Representation Learning with Rich Text Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Position-aware Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<title level="m">GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs. In UAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">STAR-GCN: Stacked and Reconstructed Graph Convolutional Networks for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenglin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4264" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
