<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Video Object Segmentation using Motion Saliency-Guided Spatio-Temporal Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
							<email>jbhuang@vt.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<email>aschwing@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Video Object Segmentation using Motion Saliency-Guided Spatio-Temporal Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised video segmentation plays an important role in a wide variety of applications from object identification to compression. However, to date, fast motion, motion blur and occlusions pose significant challenges. To address these challenges for unsupervised video segmentation, we develop a novel saliency estimation technique as well as a novel neighborhood graph, based on optical flow and edge cues. Our approach leads to significantly better initial foreground-background estimates and their robust as well as accurate diffusion across time. We evaluate our proposed algorithm on the challenging DAVIS, SegTrack v2 and FBMS-59 datasets. Despite the usage of only a standard edge detector trained on 200 images, our method achieves state-of-the-art results outperforming deep learning based methods in the unsupervised setting. We even demonstrate competitive results comparable to deep learning based methods in the semi-supervised setting on the DAVIS dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised foreground-background video object segmentation of complex scenes is a challenging problem which has many applications in areas such as object identification, security, and video compression. It is therefore not surprising that many efforts have been devoted to developing efficient techniques that are able to effectively separate foreground from background, even in complex videos.</p><p>In complex videos, cluttered backgrounds, deforming shapes, and fast motion are major challenges. In addition, in the unsupervised setting, algorithms have to automatically discover foreground regions in the video. To this end, classical video object segmentation techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">58]</ref> often assume rigid background motion models and incorporate a scene prior, two assumptions which are restrictive in practice. Trajectory based methods, such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15]</ref>, require selection of clusters or a matrix rank, which may not be intuitive. Graphical model based approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b51">52]</ref> estimate the foreground regions using a probabilistic formulation. However, for computational efficiency, the constructed graph usually contains only local connections, both spatially and temporally, reducing the ability to consider long-term spatial and temporal coherence patterns. To address this concern, diffusion based methods <ref type="bibr" target="#b34">[35]</ref>, e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b54">55]</ref>, propagate an initial foreground-background estimate more globally. While promising results are shown, diffusion based formulations rely <ref type="figure">Fig. 1</ref>: Video object segmentation in challenging scenarios. Given an input video, our algorithm produces accurate segmentation of the foreground object without any manual annotations. Our method is capable of handling unconstrained videos that span a wide variety of situations including occlusion (BUS), non-ridge deformation (DANCE-JUMP), and dynamic background (KITE-SURF).</p><p>heavily on the initialization as well as an accurate neighborhood graph encoding the semantic distance between pixels or superpixels. Therefore, in this paper, we develop (1) a new initialization technique and (2) a more robust neighborhood graph. Our initialization technique is based on the intuition that the optical flow on the boundary of an image differs significantly from the moving direction of the object of interest. Our robust neighborhood graph is built upon accurate edge detection and flow cues.</p><p>We highlight the performance of our proposed approach in <ref type="figure">Figure 1</ref> using three challenging video sequences. Note the fine details that our approach is able to segment despite the fact that our method is unsupervised. Due to accurate initial estimates and a more consistent neighborhood graph, we found our method to be robust to different parameter choices. Quantitatively, our initialization technique and neighborhood graph result in significant improvements for unsupervised foreground-background video segmentation when compared to the current state-of-the-art. On the recently released DAVIS dataset <ref type="bibr" target="#b41">[42]</ref>, our unsupervised non-deep learning based segmentation technique outperforms current state-of-the-art methods by more than 1.3% in the unsupervised setting. Our method also achieves competitive performance compared with deep net based techniques in the semi-supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The past decade has seen the rapid development in video object segmentation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Given different degrees of human interaction, these methods model inter-and intra-frame relationship of the pixels or superpixels to determine the foreground-background labeling of the observed scene. Subsequently, we classify the literature into four areas based on the degree of human involvement and discuss the relationship between video object and video motion segmentation. Unsupervised video object segmentation: Fully automatic approaches for video object segmentation have been explored recently <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b29">30]</ref>, and no manual annotation is required in this setting. Unsupervised foreground segmentation discovery can be achieved by motion analysis <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b12">13]</ref>, trajectory clustering <ref type="bibr" target="#b38">[39]</ref>, or object proposal ranking <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b56">57]</ref>. Our approach computes motion saliency in a given video based on boundary similarity of motion cues. In contrast, Faktor and Irani <ref type="bibr" target="#b12">[13]</ref> find motion salient regions by extracting dominant motion. Subsequently they obtain the saliency scores by computing the motion difference with respect to the detected dominant motion. Papazoglou and Ferrari <ref type="bibr" target="#b39">[40]</ref> identify salient regions by finding the motion boundary based on optical flow and computing inside-outside maps to detect the object of interest.</p><p>Recently, deep learning based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref> were also used to address unsupervised video segmentation. Although these methods do not require the ground truth of the first frame of the video (unsupervised as opposed to semi-supervised), they need a sufficient amount of labeled data to train the models. In contrast, our approach works effectively in the unsupervised setting and does not require training data beyond the one used to obtain an accurate edge detector. Tracking-based video object segmentation: In this setting, the user annotation is reduced to only one mask for the first frame of the video <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. These approaches track the foreground object and propagate the segmentation results to successive frames by incorporating cues such as motion <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> and supervoxel consistency <ref type="bibr" target="#b23">[24]</ref>. Again, our approach differs in that we don't consider any human labels. Interactive video object segmentation: Interactive video object segmentation allows users to annotate the foreground segments in key frames to generate impressive results by propagating the user-specified masks across the entire video <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24]</ref>. Price et al. <ref type="bibr" target="#b43">[44]</ref> further combine multiple features, of which the weights are automatically selected and learned from user inputs. Fan et al. <ref type="bibr" target="#b13">[14]</ref> tackle interactive segmentation by enabling bi-directional propagation of the masks between non-successive frames. Our approach differs in that the proposed method does not require any human interaction. Video motion segmentation: Video motion segmentation <ref type="bibr" target="#b4">[5]</ref> aims to segment a video based on motion cues, while video object segmentation aims at segmenting the foreground based on objects. The objective function differs: for motion segmentation, clustering based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref> are predominant and group point trajectories. In contrast, for video object segmentation, a binary labeling formulation is typically applied as we show next by describing our approach. Given an input video, we compute the flow field for each frame. We detect the saliency score based on the flow vector by calculating a boundary dissimilarity map u (0) and a distance map u <ref type="bibr" target="#b0">(1)</ref> indicating the distance of each pixel to the boundaries. We use minimum barrier distance to measure the distance. The motion saliency estimation is computed by averaging the boundary dissimilarity map and the distance map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Video Object Segmentation</head><p>The two most important ingredients for unsupervised video object segmentation are the initial saliency estimate as well as a good assessment of the neighborhood relation of pixels or superpixels. For initial saliency prediction in unsupervised video object segmentation we describe a novel method comparing the motion at a pixel to the boundary motion. Intuitively, boundary pixels largely correspond to background and pixels with a similar motion are likely background too. To construct a meaningful neighborhood relation between pixels we assess flow and appearance cues. We provide details for both contributions after describing an overview of our unsupervised video object segmentation approach. Method overview: Our method uses a diffusion mechanism for unsupervised video segmentation. Hence, the approach distributes an initial foreground saliency estimate over the F frames x i , i ∈ {1, . . . , F}, of a video x = (x 1 , . . . , x F ). To this end, we partition each frame into a set of nodes using superpixels, and estimate and encode their semantic relationship within and across frames using a global neighborhood graph. Specifically, we represent the global neighborhood graph by a weighted row-stochastic adjacency matrix G ∈ R N×N , where N is the total number of nodes in the video. Diffusion of the initial foreground saliency estimates v 0 ∈ R N for each node is performed by repeated matrix multiplication of the current node estimate with the adjacency matrix G, i.e., for the t-th diffusion step v t = Gv t−1 .</p><p>With the adjacency matrix G and initialization v 0 being the only inputs to the algorithm, it is obvious that they are of crucial importance for diffusion based unsupervised video segmentation. We focus on both points in the following and develop first a new saliency estimation of v 0 before discussing construction of the neighborhood graph G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Saliency estimation</head><p>For unsupervised video object segmentation, we propose to estimate the motion saliency by leveraging a boundary condition. Since we are dealing with video, motion is one of the most important cues for identifying moving foreground objects. In general, the motion of the foreground object differs from background motion. But importantly, the background region is often connected to the boundary of the image. While the latter assumption is commonly employed for image saliency detection, it has not been exploited for motion saliency estimation. To obtain the initial saliency estimate v 0 defined over superpixels, we average the pixelwise motion saliency results u over the spatial support of each superpixel. We subsequently describe our developed procedure for foreground saliency estimation, taking advantage of the boundary condition. The proposed motion saliency detection is summarized in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Conventional motion saliency estimation techniques for video object segmentation are based on either background subtraction <ref type="bibr" target="#b5">[6]</ref>, trajectory clustering <ref type="bibr" target="#b4">[5]</ref>, or motion separation <ref type="bibr" target="#b12">[13]</ref>. Background subtraction techniques typically assume a static camera, which is not applicable for complex videos. Trajectory clustering groups points with similar trajectories, which is sensitive to non-rigid transformation. Motion separation detects background by finding the dominant motion and subsequently calculates the difference in magnitude and/or orientation between the motion at each pixel, and the dominant motion. The larger the difference, the more likely the pixel to be foreground. Again, complex motion poses challenges, making it hard to separate foreground from background.</p><p>In contrast, we propose to use the boundary condition that is commonly used for image saliency detection <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b52">53]</ref> to support motion saliency estimation for unsupervised video segmentation. Our approach is based on the intuition that the background region is connected to image boundaries in some way. Therefore we calculate a distance metric for every pixel to the boundary. Compared to the aforementioned techniques, we will show that our method can better deal with complex, non-rigid motion.</p><p>We use u to denote the foreground motion saliency of the video. Moreover, u i and u i (p i ) denote the foreground saliency for frame i and for pixel p i in frame i respectively. To compute the motion saliency estimate, we treat every frame x i , i ∈ {1, . . . , F} independently. Given a frame x i , let x i (p i ) refer to the intensity values of pixel p i , and let f i (p i ) ∈ R 2 denote the optical flow vector measuring the motion of the object illustrated at pixel p i between frame i and frame i + 1. In addition, let B i denote the set of boundary pixels of frame i.</p><p>We compute the foreground motion saliency u i of frame i based on two terms u (0) i and u <ref type="bibr" target="#b0">(1)</ref> i , each of which measures a distance between any pixel p i of the i-th frame and the boundary B i . For the first distance u (0) i , we compute the smallest flow direction difference observed between a pixel p i and common flow directions on the boundary. For the second distance u <ref type="bibr" target="#b0">(1)</ref> i , we measure the smallest barrier distance between pixel p i and boundary pixels. Both of the terms capture the similarity between the motion at pixel p i and the background motion. Subsequently, we explain both terms in greater detail.</p><p>Computing flow direction difference: More formally, to compute u (0) i (p i ), the flow direction difference between pixel p i in frame i and common flow directions on the boundary B i of frame i, we first cluster the boundary flow directions into a set of K clusters k ∈ {1, . . . , K} using k-means. We subsume the cluster centers in the set</p><formula xml:id="formula_0">K i = µ i,k : µ i,k = arg min µ i,k min r∈{0,1} |B i |K 1 2 ∑ p i ∈B i ,k r p i ,k f i (p i ) −μ i,k 2 2 .<label>(1)</label></formula><p>Hereby, r p i ,k ∈ {0, 1} is an indicator variable which assigns pixel p i to cluster k, and r is the concatenation of all those indicator variables. We update K i to only contain centers with more than 1/6 of the boundary pixels assigned. Given those cluster centers, we then obtain a first distance measure capturing the difference of flow between pixel p i in frame i and the major flow directions observed at the boundary of frame i via</p><formula xml:id="formula_1">u (0) i (p i ) = min µ i,k ∈K i f i (p i ) − µ i,k 2 2 .<label>(2)</label></formula><p>Computing smallest barrier distance: When computing the smallest barrier distance D bd,i between pixel p i in frame i and boundary pixels, i.e., to obtain</p><formula xml:id="formula_2">u (1) i (p i ) = min s∈B i D bd,i (p i , s),<label>(3)</label></formula><p>we use the following barrier distance:</p><formula xml:id="formula_3">D bd,i (p i , s) = max e∈Π i,p i ,s w i (e) − min e∈Π i,p i ,s w i (e).<label>(4)</label></formula><p>Hereby, Π i,p i ,s denotes the path, i.e., a set of edges connecting pixel p i to boundary pixel s ∈ B i , obtained by computing a minimum spanning tree on frame i. The edge weights w i (e), which are used to compute both the minimum spanning tree as well as the barrier distance given in Eq. <ref type="formula" target="#formula_3">(4)</ref>, are obtained as the maximum flow direction difference between two neighboring pixels, i.e., w i</p><formula xml:id="formula_4">(e) = max { f i (p i ) − f i (q i )} ∈ R where the max is taken across the two components of f i (p i ) − f i (q i ) ∈ R 2 .</formula><p>Note that e = (p i , q i ) refers to an edge connecting the two pixels p i and q i . To compute the minimum spanning tree we use the classical 4-connected neighborhood. Intuitively, we compute the barrier distance between 2 points as the difference between the maximum edge weight and minimum edge weight on the path of the minimum spanning tree between the 2 points. We then compute the smallest barrier distance of a point as the minimum of the barrier distances between the point and any point on the boundary. Computing foreground motion saliency: We obtain the pixelwise foreground motion saliency u i of frame i when adding the two distance metrics u</p><p>i and u</p><p>i after having normalized each of them to a range of [0, 1] by subtracting the minimum entry in u (·) i and dividing by the difference between the maximum and minimum entry. Examples for u</p><formula xml:id="formula_7">(0) i , u<label>(1)</label></formula><p>i and the combined motion saliency are visualized in <ref type="figure" target="#fig_0">Figure 2</ref>. We found the proposed changes to result in significant improvements for saliency estimation of video data. We present a careful assessment in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neighborhood construction</head><p>The second important term for diffusion based video segmentation beyond initial estimates is the neighborhood graph G. Classical techniques construct the adjacency matrix <ref type="figure">Fig. 3</ref>: Graph construnction. In our method, we construct a graph for diffusing the initial motion saliency estimation. Our graph contains 1) edge-aware spatial connections (intra-frame connections), 2) flow-based temporal connections (inter-frame connections and 3) non-local long range connections. We show the initial motion saliency and the diffused saliency map using the constructed graph. We found these three types of connections to help propagate the initial saliency estimation effectively.</p><p>using local information, such as connecting a node with its spatial and temporal neighbors, and non-local connections. These methods establish a connection between two nodes as long as their visual appearance is similar.</p><p>In contrast, we compute the neighborhood graph, i.e., the adjacency matrix for graph diffusion, G = T × E ×V as the product of three components, based on inter-frame information T , intra-frame signals E, and long-range components V , as shown in <ref type="figure">Figure 3</ref>, and use a variety of cues for robustness. We formally discuss each of the components in the following. Inter-frame temporal information is extracted from optical flow cues. We connect superpixels between adjacent frames following flow vectors while checking the forward/backward consistency in order to prevent inaccurate flow estimation at motion boundaries.</p><p>More formally, to compute the flow adjacency matrix T , consider two successive video frames x i and x i+1 each containing pixel p i and p i+1 , respectively. We compute a forward flow field f i (p i ) and a backward flow field b i+1 (p i+1 ) densely for every pixel p using <ref type="bibr" target="#b20">[21]</ref>. Using those flow fields, we define the forward confidence score c F i (p i ) at pixel p i of frame x i via</p><formula xml:id="formula_8">c F i (p i ) = exp − − f i (p i ) − b i+1 (p i + f i (p i )) 2 2 σ 2 ,<label>(5)</label></formula><p>and the backward confidence score c B i (p i ) at pixel p i of frame x i via</p><formula xml:id="formula_9">c B i (p i ) = exp − − b i (p i ) − f i−1 (p i + b i (p i )) 2 2 σ 2 ,<label>(6)</label></formula><p>where σ 2 is a hyper-parameter. Intuitively, this confidence score measures the distance between the pixel p i and the result obtained after following the flow field into frame x i+1 via p i + f i (p i ) and back into frame</p><formula xml:id="formula_10">x i via p i + f i (p i ) + b i+1 (p i + f i (p i )).</formula><p>Taking the difference between pixel p i and the obtained reprojection results in the term given in Eq. (5) and Eq. (6). We use the confidence scores to compute the connection strength between two superpixels s i,k and s i+1,m in frame i and i + 1 via</p><formula xml:id="formula_11">T (s i,k , s i+1,m ) = ∑ p∈s i,k δ (p + f i (p) ∈ s i+1,m )c F i (p) |s i,k | + |s i+1,m | + ∑ p ∈s i+1,m δ (p + b i+1 (p ) ∈ s i,k )c B i+1 (p ) |s i,k | + |s i+1,m | .<label>(7)</label></formula><p>Hereby δ (·) denotes the indicator function and |s i,k | and |s i+1,m | represent the number of pixels in s i,k and s i+1,m , respectively. Intuitively, the first term compares the strength of the connections that start in superpixel s i,k and end up in superpixel s i+1,m with the total amount of strength originating from both s i,k and s i+1,m . Similarly for the second term.</p><p>Intra-frame spatial information prevents diffusion across visual edges within a frame, while allowing information to be propagated between adjacent superpixels in the same frame if they aren't separated by a strong edge. More formally, to find the edge aware spatial connections E, we first detect the edge responses frame-by-frame using the training based method discussed in <ref type="bibr" target="#b9">[10]</ref>. Given edge responses, we calculate the confidence scores A(s) for all superpixel s by summing over the decay function, i.e.,</p><formula xml:id="formula_12">A(s) = 1 |s| ∑ p∈s 1 1 + exp(σ w · (G(p) − ε)) .<label>(8)</label></formula><p>Hereby, G(p) ∈ [0, 1] is the edge response at pixel p. σ w and ε are hyper-parameters, which we fix at σ w = 50 and ε = 0.05 for all our experiments. We calculate the edge-aware adjacency matrix E by exploiting the above edge information. Specifically,</p><formula xml:id="formula_13">E(s i,k , s i,m ) = 1 2 A(s i,k ) + A(s i,m ) ,<label>(9)</label></formula><p>if s i,k is spatially close to s i,m , i.e., if the distance between the centers of the two superpixels is less than 1.5 times the square root of the size of the superpixel. Long range connections based on visual similarity allow propagating information between superpixels that are far away either temporally or spatially as long as the two are visually similar. These long-range connections enable the information to propagate more efficiently through the neighborhood graph. More formally, to compute the visual similarity matrix V , we find those superpixels that are most closely related to a superpixel s i,m . To this end, we first perform a k nearest neighbor search. More specifically, for each superpixel s i,m we find its k nearest neighbors that are within a range of r frames temporally. To compute the distance between two superpixels we use the Euclidean distance in the feature space.</p><p>We compute features f (s) of a superpixel s by concatenating the LAB and RGB histograms computed over the pixels within a superpixel. We also include the HOG feature, and the x and y coordinate of the center of the superpixel.</p><p>Let the k nearest neighbors of the superpixel s i,m be referred to via N(s i,m ). The visual similarity matrix is then defined via</p><formula xml:id="formula_14">V (s i,m , s) = exp − f (s i,m ) − f (s) 2 2 σ ∀s ∈ N(s i,m ),<label>(10)</label></formula><p>where σ is a hyper-parameter and f (s) denotes the feature representation of the superpixel s. Note that we use the same features to find k nearest neighbors and to compute the visual similarity matrix V . In this work, we refrain from using deep net based information even though we could easily augment our technique with more features.</p><p>To address the computational complexity, we use an approximate k nearest neighbor search. Specifically, we use the fast implementation of ANN search utilizing the randomized k-d forest provided in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the following, we present the implementation details, describe the datasets and metrics used for evaluation, followed by ablation study highlighting the influences of the proposed design choices and comparisons with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>For the proposed saliency estimation algorithm, we set the number of clusters K = 3 for modeling the background. For neighborhood graph construction described in Section 3.2, we found k = 40, r = 15, σ = 0.1, σ 2 = 2 −6 , σ w = 50 to work well across datasets. The number of diffusion iterations is set to 25. In the supplementary material, we show that the performance of our method is reasonably robust to parameter choices.</p><p>The average running time of our approach on the DAVIS dataset, including the graph construction and diffusion is about 8.5 seconds per frame when using a single PC with Intel i7-4770 CPU and 32 GB memory. Extracting superpixels and feature descriptors takes about 1.5 and 0.8 seconds per frame, respectively. We use the implementation by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47]</ref> for computing optical flow, which takes about 10.7 seconds per frame, including both forward flow and backward flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>We extensively compare our proposed technique to a series of baselines using the DAVIS dataset <ref type="bibr" target="#b41">[42]</ref> (50 video sequences), the SegTrack v2 dataset <ref type="bibr" target="#b32">[33]</ref> (14 video sequences), and the FBMS-59 dataset <ref type="bibr" target="#b38">[39]</ref> (22 video sequences in the test set). These datasets are challenging as they contain nonrigid deformation, drastic illumination changes, cluttered background, rapid object motion, and occlusion. All three datasets provide pixel-level ground-truth annotations for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation metrics</head><p>Intersection over union (J ): The intersection over union (IoU) metric, also called the Jaccard index, computes the average over the dataset. The IoU metric has been widely used for evaluating the quality of the segmentation. Contour accuracy (F ) <ref type="bibr" target="#b41">[42]</ref>: To assess the segmentation quality, we compute the contour accuracy as F = 2PR P+R , where P and R are the matching precision and recall of the two sets of points on the contours of the ground truth segment and the output segment, calculated via a bipartite graph matching.</p><p>Temporal stability (T ) <ref type="bibr" target="#b41">[42]</ref>: The temporal stability is measured by computing the distance between the shape context descriptors <ref type="bibr" target="#b2">[3]</ref> describing the shape of the boundary of the segmentations between two successive frames. Intuitively, the metric indicates the degree of deformation required to transform the segmentation mask from one frame to its adjacent frames. Subsequently we first present an ablation study where we assess the contributions of our technique. Afterwards we perform a quantitative evaluation where we compare the accuracy of our approach to baseline video segmentation approaches. Finally we present qualitative results to illustrate the success and failure cases of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>We assess the resulting performance of the individual components of our adjacency defined neighborhood in <ref type="table" target="#tab_0">Table 1</ref>. The performance in IoU of the motion saliency estimation in our approach (with all the connections disabled) is 57.52%. We analyze the effect of the three main components in the adjacency graph: (1) inter-frame flow based temporal connections T , (2) intra-frame edge based spatial connections E and (3) long range connections V .</p><p>The improvements reported for saliency estimation and neighborhood construction motivate their use for unsupervised video segmentation. Besides, we apply a second round of 'focused diffusion,' restricted to the region which focuses primarily on the foreground object, to improve the results. The effects of the focused diffusion (denoted 'FDiff') can be found in <ref type="table" target="#tab_0">Table 1</ref> as well, showing significant improvements.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, the checkmark ' ' indicates the enabled components. We observe consistent improvements when including additional components, which improve the robustness of the proposed method.  <ref type="bibr" target="#b41">[42]</ref>. Evaluation metrics are the IoU measurement J , boundary precision F , and time stability T . Following <ref type="bibr" target="#b41">[42]</ref>, we also report the recall and the decay of performance over time for J and F measurements.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Quantitative evaluation</head><p>Evaluation on the DAVIS dataset: We compare the performance of our approach to several baselines using the DAVIS dataset. The results are summarized in <ref type="table" target="#tab_1">Table 2</ref>, where we report the IoU, the contour accuracy, and the time stability metrics. The best method is emphasized in bold font and the second best is underlined. We observe our approach to be quite competitive, outperforming a wide variety of existing unsupervised video segmentation techniques, e.g., NLC <ref type="bibr" target="#b12">[13]</ref>, MSG <ref type="bibr" target="#b4">[5]</ref>, KEY <ref type="bibr" target="#b30">[31]</ref>, FST <ref type="bibr" target="#b39">[40]</ref>, FSG <ref type="bibr" target="#b24">[25]</ref>, LMP <ref type="bibr" target="#b48">[49]</ref>, ARP <ref type="bibr" target="#b29">[30]</ref>. We also evaluate our method in the semi-supervised setting by simply replacing the saliency initialization of the first frame with the ground truth. Note that it is common to refer to usage of the first frame as 'semi-supervised.' Our unsupervised version is denoted as OURS-U and the semi-supervised version is referred to via OURS-S in <ref type="table" target="#tab_1">Table 2</ref>. Semi-supervised baselines are SEA <ref type="bibr" target="#b0">[1]</ref>, HVS <ref type="bibr" target="#b16">[17]</ref>, JMP <ref type="bibr" target="#b13">[14]</ref>, FCP <ref type="bibr" target="#b42">[43]</ref>, BVS <ref type="bibr" target="#b35">[36]</ref>, OFL <ref type="bibr" target="#b51">[52]</ref>, CTN <ref type="bibr" target="#b26">[27]</ref>, VPN <ref type="bibr" target="#b25">[26]</ref>, and MSK <ref type="bibr" target="#b40">[41]</ref>. Note that OFL uses deep features, and CTN, VPN, MSK, FSG, and LMP are deep learning based approaches. We observe our method to improve the state-of-the-art performance in IoU metric by 1.3% in the unsupervised setting and by 0.7% in the semi-supervised case. Note that beyond training of edge detectors, no learning is performed in our approach.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, we compare the average IoU of all DAVIS sequences, clustered by attributes, e.g., appearance change, dynamic blur, fast motion, motion blur, and occlusion.  Our method is more robust and outperforms the baselines for fast motion, motion blur and occlusion. In particular, our method performs well for objects with occlusion, outperforming other methods by 10% for this attribute.</p><p>Evaluation on the SegTrack v2 dataset: We assess our approach on the SegTrack v2 dataset using identical choice of parameters. We show the results in <ref type="table" target="#tab_3">Table 4</ref>. We observe our method to be competitive on SegTrack v2. Note that the reported performance of NLC differs from <ref type="bibr" target="#b12">[13]</ref> as in the evaluation in <ref type="bibr" target="#b12">[13]</ref> only a subset of the 12 video sequences were used. We ran the code released by <ref type="bibr" target="#b12">[13]</ref> and report the results on the full SegTrack v2 dataset with 14 video sequences. The results we report here are similar to the ones reported in <ref type="bibr" target="#b47">[48]</ref>.</p><p>Evaluation on the FBMS dataset: We evaluate our method on the FBMS <ref type="bibr" target="#b38">[39]</ref> test set which consists of 22 video sequences. The results are presented in <ref type="table" target="#tab_4">Table 5</ref>. We observe our approach to outperform the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons of the saliency estimation:</head><p>To illustrate the benefits of the proposed motion saliency estimation, we compare the performance of the proposed initialization with other approaches in <ref type="table" target="#tab_5">Table 6</ref> and observe that the proposed saliency estimation performs very well. Note that the saliency estimation in our approach is unsupervised as opposed to FSG and LMP which are trained on more than 10,000 images and 2,250 videos, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative evaluation</head><p>Side-by-side comparison: Next we present qualitative results comparing our algorithm to competing methods on challenging parts of the DAVIS dataset. In <ref type="figure" target="#fig_2">Figure 4</ref> we provide side-by-side comparisons to existing methods, i.e., APR <ref type="bibr" target="#b29">[30]</ref>, LMP <ref type="bibr" target="#b48">[49]</ref>, FSG <ref type="bibr" target="#b24">[25]</ref>, and NLC <ref type="bibr" target="#b12">[13]</ref>. We observe our approach to yield encouraging results even in challenging situations such as frames in BMX-TREES <ref type="figure" target="#fig_2">(Figure 4</ref>, first row), where the foreground object is very small and occluded, and the background is very colorful, and in FLAMINGO <ref type="figure" target="#fig_2">(Figure 4</ref>, second row), where there is non-rigid deformation, and the background object is similar to the foreground object. We refer the interested reader to the supplementary material for additional results and videos. Success cases: In <ref type="figure">Figure 5</ref>, we provide success cases of our algorithm, i.e., frames where our designed technique delineates the foreground object accurately. We want to highlight that our approach is more robust to challenges such as occlusions, motion blur and fast moving objects as the attribute-based aggregate performance in <ref type="table" target="#tab_2">Table 3</ref> suggests. Failure modes: In <ref type="figure">Figure 6</ref>, we also present failure modes of our approach. We observe our technique to be challenged by complex motion. Since our method mainly relies on motion and appearance, water is classified as foreground due to its complex motion (MALLARD-WATER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a saliency estimation and a graph neighborhood for effective unsupervised foreground-background video segmentation. Our key novelty is a motion saliency estimation and an informative neighborhood structure. Our unsupervised method demonstrates how to effectively exploit the structure of video data, i.e., taking advantage of flow and edges, and achieves state-of-the-art performance in the unsupervised setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Motion saliency estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Semi-supervised Unsupervised SEA HVS JMP FCP BVS OFL CTN VPN MSK OURS-S NLC MSG KEY FST FSG LMP ARP OURS-M ↑ 0.556 0.596 0.607 0.631 0.665 0.711 0.755 0.750 0.803 0.810 0.641 0.543 0.569 0.575 0.716 0.697 0.763 0.776 J Recall O ↑ 0.606 0.698 0.693 0.778 0.764 0.800 0.890 0.901 0.935 0.946 0.731 0.636 0.671 0.652 0.877 0.829 0.892 0.886 Decay D ↓ 0.355 0.197 0.372 0.031 0.260 0.227 0.144 0.093 0.089 0.102 0.086 0.028 0.075 0.044 0.017 0.056 0.036 0.044 Mean M ↑ 0.533 0.576 0.586 0.546 0.656 0.679 0.714 0.724 0.758 0.783 0.593 0.525 0.503 0.536 0.658 0.663 0.711 0.750 F Recall O ↑ 0.559 0.712 0.656 0.604 0.774 0.780 0.848 0.842 0.882 0.928 0.658 0.613 0.534 0.579 0.790 0.783 0.828 0.869 Decay D ↓ 0.339 0.202 0.373 0.039 0.236 0.240 0.140 0.136 0.095 0.115 0.086 0.057 0.079 0.065 0.043 0.067 0.073 0.042 T Mean M ↓ 0.137 0.296 0.131 0.285 0.316 0.239 0.198 0.300 0.189 0.212 0.356 0.250 0.190 0.276 0.286 0.689 0.352 0.243</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Comparison of our algorithm and other unsupervised methods on sequence BMX-TREES (1st row), FLAMINGO (2nd row), LIBBY (3rd row), RHINO (4th row), and DANCE-JUMP (5th row) of the DAVIS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Visual results of our approach on the sequences SWING (1st row), SOAPBOX (2nd row), DRIFT-STRAIGHT (3rd row), and DANCE-TWIRL (4th row) of the DAVIS dataset. Failure case. Groundtruth vs. our result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Contribution of different components of our algorithm evaluated on the DAVIS dataset. Our algorithm with inter-frame, intra-frame connections, long range connections, and focused diffusion (denoted as FDiff) enabled performs best and achieves an IoU of 77.56%.</figDesc><table><row><cell cols="3">Connections Inter-frame Intra-frame Long range</cell><cell cols="2">FDiff IoU (%)</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.52</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.75</cell></row><row><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>62.13</cell></row><row><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>72.38</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>65.01</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>72.70</cell></row><row><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>74.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>74.34</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The quantitative evaluation on the DAVIS dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The attribute-based aggregate performance comparing unsupervised methods on the DAVIS dataset<ref type="bibr" target="#b41">[42]</ref>. We calculate the average IoU of all sequences with the specific attribute: appearance change (AC), dynamic background (DB), fast motion (FM), motion blur (MB), and occlusion (OCC). The right column with small font indicates the performance change for the method on the remaining sequences if the sequences possessing the corresponding attribute are not taken into account.+0.13 0.48 +0.08 0.42 +0.19 0.55 +0.04 0.73 -0.02 0.67 +0.03 0.73 +0.04 0.72 +0.07 DB 0.53 +0.15 0.43 +0.15 0.52 +0.07 0.53 +0.06 0.67 +0.05 0.57 +0.16 0.70 +0.08 0.66 +0.15 FM 0.64 +0.00 0.46 +0.14 0.50 +0.12 0.50 +0.12 0.69 +0.04 0.67 +0.05 0.73 +0.05 0.75 +0.04 MB 0.61 +0.04 0.35 +0.29 0.51 +0.08 0.48 +0.14 0.65 +0.10 0.64 +0.08 0.69 +0.11 0.74 +0.06 OCC 0.70 -0.09 0.48 +0.10 0.52 +0.08 0.53 +0.07 0.65 +0.10 0.70 -0.01 0.71 +0.08 0.81 -0.05</figDesc><table><row><cell cols="2">Attribute NLC [13] MSG [5] KEY [31] FST [40] FSG [25] LMP [49] ARP [30] OURS-U</cell></row><row><cell>AC</cell><cell>0.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance in IoU on SegTrack v2 dataset<ref type="bibr" target="#b32">[33]</ref>.</figDesc><table><row><cell>Sequence</cell><cell cols="4">KEY [31] FST [40] NLC [13] FSG [25] Ours</cell></row><row><cell>BIRDFALL</cell><cell>0.490</cell><cell>0.014</cell><cell>0.565</cell><cell>0.380 0.649</cell></row><row><cell>BIRD OF PARADISE</cell><cell>0.922</cell><cell>0.837</cell><cell>0.814</cell><cell>0.699 0.937</cell></row><row><cell>BMX</cell><cell>0.630</cell><cell>0.621</cell><cell>0.754</cell><cell>0.591 0.847</cell></row><row><cell>CHEETAH</cell><cell>0.281</cell><cell>0.396</cell><cell>0.518</cell><cell>0.596 0.518</cell></row><row><cell>DRIFT</cell><cell>0.469</cell><cell>0.811</cell><cell>0.741</cell><cell>0.876 0.829</cell></row><row><cell>FROG</cell><cell>0.000</cell><cell>0.629</cell><cell>0.713</cell><cell>0.570 0.832</cell></row><row><cell>GIRL</cell><cell>0.877</cell><cell>0.441</cell><cell>0.860</cell><cell>0.667 0.846</cell></row><row><cell>HUMMINGBIRD</cell><cell>0.602</cell><cell>0.335</cell><cell>0.624</cell><cell>0.652 0.464</cell></row><row><cell>MONKEY</cell><cell>0.790</cell><cell>0.699</cell><cell>0.823</cell><cell>0.805 0.739</cell></row><row><cell>MONKEYDOG</cell><cell>0.396</cell><cell>0.523</cell><cell>0.525</cell><cell>0.328 0.381</cell></row><row><cell>PARACHUTE</cell><cell>0.963</cell><cell>0.839</cell><cell>0.859</cell><cell>0.516 0.937</cell></row><row><cell>PENGUIN</cell><cell>0.093</cell><cell>0.074</cell><cell>0.139</cell><cell>0.713 0.240</cell></row><row><cell>SOLDIER</cell><cell>0.666</cell><cell>0.453</cell><cell>0.692</cell><cell>0.698 0.800</cell></row><row><cell>WORM</cell><cell>0.844</cell><cell>0.705</cell><cell>0.782</cell><cell>0.506 0.800</cell></row><row><cell>Average IoU</cell><cell>0.573</cell><cell>0.527</cell><cell>0.672</cell><cell>0.614 0.701</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance in IoU on FBMS-59 test set<ref type="bibr" target="#b38">[39]</ref>.</figDesc><table><row><cell>Average IoU 0.445</cell><cell>0.473</cell><cell>0.542</cell><cell>0.555</cell><cell>0.598</cell><cell>0.608</cell></row></table><note>NLC [13] POR [59] POS [28] FST [40] ARP [30] OURS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons in IoU on the initialization on the DAVIS and Seg-Track v2 datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DAVIS</cell><cell></cell><cell></cell><cell cols="2">Segtrack v2</cell></row><row><cell></cell><cell cols="2">NLC FST</cell><cell cols="4">FSG LMP Ours NLC FST</cell><cell>FSG Ours</cell></row><row><cell>Training?</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="8">Initial saliency 0.402 0.456 0.602 0.569 0.575 0.419 0.389 0.530 0.424</cell></row><row><cell>Ground truth</cell><cell>APR [30]</cell><cell></cell><cell>LMP [49]</cell><cell>FSG [25]</cell><cell></cell><cell>NLC [13]</cell><cell>Ours</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221, 1755785, Samsung, and 3M. We thank NVIDIA for providing the GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SeamSeg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avinash Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video object segmentation by tracking regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation of background subtraction techniques for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hoeferlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting nonlocal spatiotemporal structure for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multi-body factorization method for motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilayer segmentation of live video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Background and foreground modeling using nonparametric kernel density estimation for visual surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">JumpCut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical background subtraction for a mobile observer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haymanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MaskRNN: Instance Level Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VideoMatch: Matching based Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient coarse-to-fine patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A unified approach to moving object detection in 2d and 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computing occluding and transparent motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rousso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos via alternate convex optimization of foreground and background distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Track to the future: Spatio-temporal video segmentation with long-range motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<title level="m">Roto++: Accelerating professional rotoscoping using shape manifolds. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovsz</surname></persName>
		</author>
		<title level="m">Random walks on graphs: A survey</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maerki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Application</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sorkine-Hornung: Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">LIVEcut: Learning-based interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Statistical background modeling for non-stationary camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Concerning bayesian motion segmentation, model averaging, matching and the trifocal tensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Motion coherent tracking with multi-label mrf optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video Segmentation via Object Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Real-time salient object detection with a minimum spanning tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Probabilistic motion diffusion of labeling priors for coherent video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Detecting motion regions in the presence of a strong parallax from a moving camera by multiview geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
