<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Style Transfer in Text: Exploration and Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
							<email>npeng@isi.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Information Science Institute</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Big Data Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<email>ruiyan@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Big Data Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Style Transfer in Text: Exploration and Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to transfer styles of texts or images, is an important measurement of the advancement of artificial intelligence (AI). However, the progress in language style transfer is lagged behind other domains, such as computer vision, mainly because of the lack of parallel data and reliable evaluation metrics. In response to the challenge of lacking parallel data, we explore learning style transfer from non-parallel data. We propose two models to achieve this goal. The key idea behind the proposed models is to learn separate content representations and style representations using adversarial networks. Considering the problem of lacking principle evaluation metrics, we propose two novel evaluation metrics that measure two aspects of style transfer: transfer strength and content preservation. We benchmark our models and the evaluation metrics on two style transfer tasks: paper-news title transfer, and positive-negative review transfer. Results show that the proposed content preservation metric is highly correlate to human judgments, and the proposed models are able to generate sentences with similar content preservation score but higher style transfer strength comparing to autoencoder.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Style transfer is an important problem in many subfields of artificial intelligence (AI), such as natural language processing (NLP) and computer vision (Gatys, Ecker, and Bethge 2016; Gatys et al. <ref type="bibr" target="#b29">2016;</ref><ref type="bibr" target="#b32">Zhu et al. 2017;</ref><ref type="bibr" target="#b16">Li et al. 2017)</ref>, as it reflects the ability of intelligence systems to generate novel contents. Specifically, style transfer of natural language texts is an important component of natural language generation. It facilitates many NLP applications, such as automatic conversion of paper title to news title, which reduces the human effort in academic news report. For tasks like poetry generation <ref type="bibr" target="#b28">(Yan et al. 2013;</ref><ref type="bibr" target="#b29">Yan 2016;</ref><ref type="bibr" target="#b11">Ghazvininejad et al. 2016)</ref>, style transfer can be applied to generate poetry in different styles. Nevertheless, the progress in style transfer of language is lagged behind other domains such as computer vision, largely because of the lack of parallel corpus and reliable evaluation metrics.</p><p>Sequence to sequence (seq2seq) neural network models <ref type="bibr" target="#b25">(Sutskever, Vinyals, and Le 2014)</ref> have demonstrated great success in many generation tasks, such as machine translation, dialog system and image caption, with the requirement of a large amount of parallel data. However, it is hard to get parallel data for tasks such as style transfer. For instance, there is only a small number of academic news reports which have corresponding papers. Therefore, we need algorithms that perform style transfer without parallel data.</p><p>Another major challenge of style transfer is to separate style from the content. In computer vision, Li et al. <ref type="bibr">(2017)</ref> proposes an expression to distinguish style and content of a picture. However, this is under-explored in the NLP community. How to separate content from style in text remains an open research problem in text style transfer.</p><p>Evaluation is also a key challenge in style transfer. In machine translation and summarization, researchers use BLEU <ref type="bibr" target="#b21">(Papineni et al. 2002)</ref> and ROUGE <ref type="bibr" target="#b18">(Lin 2004)</ref> to compute the similarity between model outputs and the ground truth. However, we lack parallel data for style transfer to provide ground truth references for evaluation. The same problem also exists in style transfer in computer vision. To solve this problem, we propose a general evaluation metric for style transfer in natural language processing. There are two aspects of the evaluation metric; one is transfer strength and the other is content preservation.</p><p>In this paper, we explore two models for text style transfer, to approach the aforementioned problems of 1) lacking parallel training data and 2) hard to separate the style from the content. The models achieve the goals by multi-task learning <ref type="bibr" target="#b4">(Caruana 1998)</ref> and adversarial training (Goodfellow et al. <ref type="bibr">2014</ref>) of deep neural network. The first model implements a multi-decoder seq2seq proposed by Sutskever, Vinyals, and Le <ref type="bibr">(2014)</ref>, where the encoder is used to capture the content c of the input X, and the multi-decoder contains n(n ≥ 2) decoders to generate outputs in different styles. The second model uses the same encoding strategy, but introduces style embeddings that are jointly trained with the model. The style embeddings are used to augment the encoded representations, so that only one decoder needs to be learned to generate outputs in different styles.</p><p>The experiments on two tasks: paper-news title transfer and positive-negative review transfer showed that each of the proposed model has its own strength and can be used in different transfer requests, and the proposed content preservation metric has a high correlation with human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>The contributions of this paper are three fold: • We compose a dataset 1 of paper-news titles to facilitate the research in language style transfer. • We propose two general evaluation metrics for style transfer, which considers both transfer strength and content preservation. The evaluation metric is highly correlated to the human evaluation. • We proposed and evaluated two models for learning style transfer without parallel corpora. The proposed models addressed the key challenge of lacking parallel data for training in style transfer, and each model has its own advantages under different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Style Transfer in Computer Vision</head><p>In recent years, style transfer has made significant progress in computer vision. Gatys, Ecker, and Bethge <ref type="formula" target="#formula_2">(2016)</ref>   <ref type="bibr">(2017)</ref> proposes CycleGAN to do image-image translation. It firstly learns a mapping G : X → Y using an adversarial loss, and then a reverse mapping F : Y → X with a cycle loss F (G(X)) ≈ X which performs unpaired image to image translation. CycleGAN shows qualitative results, nevertheless, discrete text is hard to implement cycle training. Li et al. <ref type="bibr">(2017)</ref> proposes to treat style transfer as a domain adaptation problem. They theoretically show that Gram metrics is equivalent to minimize the Maximum Mean Discrepancy (MMD) for image. But there is no evidence showing similar metric works on text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style Transfer in Natural Language Processing</head><p>Jhamtani et al. <ref type="bibr">(2017)</ref> explores automatic methods to transform text from modern English to Shakespearean English using parallel data. The model was based on seq2seq and enriched it with pointer network (Vinyals, Fortunato, and Jaitly 2015). They used a modern-Shakespeare word dictionary to form candidate words for pointer network, however, pairedword dictionary is a scares resource that does not exist in most style transfer tasks, and it required parallel corpora.</p><p>There are previous work on style transfer without parallel data. Mueller, Gifford, and Jaakkola (2017) proposed a variational auto-encoder (VAE) based model to revise a new sequence to improve its associated outcome. However, there is no significative evaluation for style transfer. It uses nonparallel data. Shen et al. <ref type="bibr">(2017)</ref> explored style transfer for sentiment modification, decipherment of word substitution ciphers and recovery of word order. They used VAE as the base model and used an adversarial network to align different styles. However, their evaluation only considered the classification accuracy. We argue that content preservation is another indispensable evaluation metric for style transfer.</p><p>Other threads of work that are closely related to us including style analysis and style-controlled text generation. <ref type="bibr" target="#b3">Braud and Søgaard (2017)</ref> explores many types of features for style prediction, ranging from n-grams to discourse, and found that simple models performed well. Ficler and Goldberg (2017) controls linguistic style of generated text using conditioned recurrent neural networks (CRNN). The major difference between these work and ours is that they do not have source sentences where we need to transfer the style.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Networks for Domain Separation</head><p>Adversarial networks have been successfully applied to domain separation problems. <ref type="bibr" target="#b8">(Ganin and Lempitsky 2015)</ref> proposed deep domain adaptation approach to encourage domain-invariant features. This model can be trained on labeled source domain data and unlabeled target domain data. <ref type="bibr" target="#b2">(Bousmalis et al. 2016)</ref> used adversarial networks to learned shared representations between two domains which don't contain the individual features of each domain. <ref type="bibr" target="#b5">(Chen et al. 2017</ref>) proposed a multi-task framework to generate shared and private representations for sentences. The shared layer is also reinforced by adversarial networks. (Long, Wang, and Jordan 2017) proposed a joint adaptation network, which adopted the adversarial strategy to maximize joint maximum mean discrepancy. The major difference between these work and ours is that they do not need to generate new sentences. How adversarial networks work on controlled generation is largely untested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We propose two models for style transfer in this paper: multi-decoder and style-embedding. Both models are based on the neural sequence to sequence model. The common ground of the two models is to learn a representation for the input sentence that only contains the content information. Then the multi-decoder model uses different decoders, one for each style, to generate texts in the corresponding style. The style-embedding model, in contrast, learns style embeddings addition to the content representations. Then a single decoder is trained to generate texts in different styles based on both the content representation and the style embedding. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the two models. We give more details about each model in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background: Auto-encoder Seq2seq Model</head><p>Auto-encoder (Rumelhart, Hinton, and Williams 1985) is a type of neural networks that learns a hidden representation for the input. It was mainly used for dimension reduction in the past, but more recently, the concepts have been widely used for generative models. In the auto-encoder seq2seq model, an encoder is learned to generate intermediate representation of input sequence X = (x 1 , . . . , x Tx ) of length T x . Then a decoder is trained to recover the input X using the intermediate representation. For the style transfer problem, we use the auto-encoder seq2seq model as our base model, since we expect minimum changes from the input to the output. We give more details about this model as we also use the components of this model in our proposed models.</p><p>Encoder In auto-encoder seq2seq model, both the encoder and decoder are recurrent neural networks (RNNs). We employ the gated recurrent unit (GRU) variant which uses gates to control the information flow. A GRU unit is composed of the following components:</p><formula xml:id="formula_0">sj = zj hj + (1 − zj) sj−1, (1) hj = tanh (WE[xj−1] + rj (Usj−1)) , (2) rj = σ (WrE[xj−1] + Ursj−1) , (3) zj = σ (WzE[xj−1] + Uzsj−1) ,<label>(4)</label></formula><p>where s j is the activation of GRU at time j; h j is an intermediate state computes the candidate activation. r j is a reset gate that controls how much to reset from the previous activation for the candidate activation. Similarly, z j is an update gate that controls how much to update the current activation based on the previous activation and the candidate activation. E is a word embedding matrix that is used to convert the input words to vector representations. E, W, U, W r , U r , W z , U z are model parameters. We use Θ e to denote all the parameters of the encoder, then the encoder can be abstracted as:</p><formula xml:id="formula_1">S = Encoder(x; Θe)<label>(5)</label></formula><p>Decoder The decoder takes the last state of the encoder to start the generation process. It generates tokens by predicting the most probable next token based on previous tokens. The probability of an output sequence given an input P (y i |x i ) is defined by Equation 6, where i indexes the instances, j the output tokens. The probability p(.) of generating each token can be computed by the softmax function. </p><p>The loss function of the encoder-decoder seq2seq model (Equation 7) minimizes the negative log probability of the training data, where M denotes the size of the training data, Θ e and Θ d are the parameters of the encoder and the decoder, respectively. The model can be trained end-to-end.</p><formula xml:id="formula_3">Lseq2seq (Θe, Θ d ) = − M i=1 log P (yi|xi; Θe, Θ d )<label>(7)</label></formula><p>In auto-encoder, we let the output sequence y to be the same as the input sequence x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-decoder Model</head><p>The multi-decoder model for style transfer is similar to an auto-encoder with several decoders, with the exception that the encoder now tries to learn some content representations that do not reflect styles. The style specific decoders (one for each style) then take the content representations and generate texts in different styles. The challenge of this model is how to generate content representation c from input x. In the original auto-encoder model, the encoder generates representations that contain both content and style information. Chen et al. <ref type="bibr">(2017)</ref> used an adversarial network to separate the shared and the private features for multi-task learning to help chinese word segmentation. We use a similar adversarial network to separate the content representation c from the style. The adversarial network is composed of two parts. The first part aims at classifying the style of x given the representation learned by the encoder. The loss function minimizes the negative log probability of the style labels in the training data, as denoted in Equation 8:</p><formula xml:id="formula_4">L adv1 (Θc) = − M i=1 log p (li|Encoder(xi; Θe); Θc) ,<label>(8)</label></formula><p>where Θ c is the parameters of a multi-layer perceptron (MLP) for predicting the style labels. The second part of the adversarial network aims at making the classifier unable to identify the style of x by maximize the entropy (minimize the negative entropy) of the predicted style labels, as denoted in Equation 9.</p><formula xml:id="formula_5">L adv2 (Θe) = − M i=1 N j=1 H (p (j|Encoder(xi; Θe); Θc)) , (9)</formula><p>where Θ e is the parameters of the encoder and N is the number of styles, as introduced in previous sections. Note that the two parts of the adversarial network update different sets of parameters, and they work together to make sure that outputs of encoder Encoder(x i ; Θ e ) do not contain style information.</p><p>While the encoder is trained to produce content representations, the multiple decoders are trained to take the representations produced by the encoder and generate outputs in different styles. The loss function for each decoder is similar to Equation 7, and the total generation loss is the sum of the generation loss of each decoder, as defined in <ref type="figure" target="#fig_0">Equation 10</ref>.</p><formula xml:id="formula_6">Lgen1 (Θe, Θ d ) = L i=1 L i seq2seq Θe, Θ i d<label>(10)</label></formula><p>The final loss function of the multi-decoder model is composed of three parts: two for the adversarial network and one for the sequence to sequence generation. It simply takes an unweighted sum of the three parts as illustrated in Equation 11.</p><formula xml:id="formula_7">L total1 (Θe, Θ d , Θc) = Lgen1 (Θe, Θ d ) + L adv1 (Θc) + L adv2 (Θe)<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style-embedding Model</head><p>Our second model uses style embeddings to control the generated styles. This is inspired by <ref type="bibr" target="#b15">(Li et al. 2016)</ref>, which proposed a model to embed personal information into vector representations for persona-conversation, and (Ficler and Goldberg 2017) which generated text with different contents and styles using conditional RNNs that conditioned on both content and style parameters. In this model, the encoder and the adversarial network parts are the same as the multi-decoder model, to generate content representations c. In addition, style embeddings E ∈ R N ×ds are introduced to represent the styles, where N denotes the number of styles and d s is the dimension of style embedding. A single decoder is trained in this model, which takes the concatenation of the content representation c and the style embedding e of a sentence as the input to generate texts in different styles.</p><p>The loss function of the style-embedding model is defined in (12), where L gen2 is the loss function for the seq2seq generation very similar to Equation 7. The only difference is that it also contains the parameter E for style embeddings, that are jointly trained with the mode. The total loss is similar to the multi-decoder model in Equation 11, where L adv1 and L adv2 are the same as in Equations 8 and 9.</p><formula xml:id="formula_8">L total2 (Θe, Θ d , Θc, E) = Lgen2 (Θe, Θ d , E) + L adv1 (Θc) + L adv2 (Θe)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Estimation</head><p>We use Adadelta <ref type="bibr" target="#b30">(Zeiler 2012)</ref> with the initial learning rate 0.0001 and batch size 128 to learn the parameters for all models. The best parameters are decided based on the perplexity on the validation data with a maximum of 50 training epochs for paper-news task and 10 training epochs for positive-negative task.</p><p>For the multi-decoder model, we train the multiple decoders alternately, using the data in the corresponding style. For the style-embedding model, we randomly shuffled the data during training, and jointly learned the style embeddings with the encoder-decoder part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>Evaluation plays an important role in style transfer. Automatic evaluation metrics speed up development. And they provide criteria to compare different models.</p><p>BLEU <ref type="bibr" target="#b21">(Papineni et al. 2002</ref>) is a popular evaluation metric in neural machine translation and ROUGE <ref type="bibr" target="#b18">(Lin 2004)</ref> is popular in text summarization. NIST <ref type="bibr" target="#b6">(Doddington et al. 2000)</ref> and Meteor (Banerjee and Lavie 2005) are also used widely in Natural Language Processing. They evaluate the similarity between model output and ground truth by word overlapping. AM-FM (Banchs and Li 2011) proposes an automatic evaluation for NMT without ground truth. This model computes sentence embedding first and then computes cosine similarity between source language input and target language output. It gets sentence embedding by Singular Value Decomposition (SVD), which trains source and target language together. RUBER <ref type="bibr" target="#b26">(Tao et al. 2017</ref>) was proposed to evaluate dialog system, it divides evaluation into referenced and unreferenced part. In referenced part, it calculates the similarity between model output and ground truth by cosine distance of sentence embedding.</p><p>We propose two general evaluation metrics, one is transfer strength, the other one is content preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Strength</head><p>The main task of this model is to transfer source style to target style, so transfer strength evaluates whether the style is transferred. We define the metric as transfer strength and implement it using a classifier. There are more than 100,000 training data for this task. We use a LSTM-sigmoid classifier which performs well in big data. The style is defined in (13). This classifier is based on keras examples 2 . Transfer strength accuracy is defined as N right N total , N total is the number of test data, and N right is the number of correct case which is transferred to target style. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Preservation</head><p>Another important aspect of style transfer is content preservation. It is easy to train a model that has 100% transfer dataset title review style type paper news positive negative #sentences 107,538 108,503 400,000 400,000 vocabulary size 80,000 60,000 </p><formula xml:id="formula_9">score total = M test i=1 scorei (19)</formula><p>For word embedding, we use pre-trained Glove (Pennington, Socher, and Manning 2014) published at stanford nlp 3 . This project contains word embedding trained on 6 billion tokens, containing 400k vocabularies, with dimension 50, 100, 200 and 300. In our model, we use dimension 100. Although a single integrated metric that combines transfer strength and content preservation as F1 score seems plausible to measure the performance of the systems, it is not the best for style transfer, since sometimes the transfer strength is more important, while in other cases the content preservation is the focus. A weighted integration would be ideal for different scenarios. We leave the weighted integration for the future work and report both metrics in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup Datasets</head><p>We used two datasets to evaluate the performances of the proposed methods. One is the paper-news title dataset, the other is the positive-negative review dataset; both are nonparallel corpora. We composed the first dataset ourselves and used the data released by <ref type="bibr" target="#b13">He and McAuley (2016)</ref> as the second dataset. For both datasets, we divided them into three parts: training, validation, and test data. The size of the validation and test data is 2,000 sentences, and the rest are used as training data. And the partition is the same between model and evaluation.</p><p>We ignored the sentences that contain more than 20 words, and converted all characters to lower cases. We also replace all the numbers to a special string " NUM " as a preprocessing step. Some statistics about the datasets is summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Paper-News Title Dataset In this dataset, the paper titles are crawled from academic websites including ACM Digital Library 4 , Arxiv 5 , Springer 6 , ScienceDirect 7 , and Nature 8 . The news titles are from UC Irvine Machine Learning Repository <ref type="bibr" target="#b17">(Lichman 2013)</ref>, which contains 422,937 news titles. We filtered it down to 108,503 titles which belong to science and technology category.</p><p>Positive-Negative Review Dataset This dataset contains Amazon product reviews published by <ref type="bibr" target="#b13">He and McAuley (2016)</ref>. It contains 142,800,000 product reviews from 1996 to 2014 in Amazon, which span the domains of books, electronics, movies, etc. We randomly select 400,000 positive and 400,000 negative reviews to compose our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Settings</head><p>Since this is an exploratory paper, we compare several parameters settings instead of trying to find a single set of "best parameters". For paper-news title transfer, we explored word embedding size of 64, encoder hidden vector size among {32,64,128}, and style embedding size among {32,64,128}. For positive-negative review transfer, we explored word embedding size of 64 for multi-decoder and {64,128} for style-embedding model, encoder hidden vector size among {16,32,64}, and style embedding size among {16,32,64}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Settings</head><p>As is introduced in previous sections, an LSTM-sigmoid classifier is needed to measure the transfer strength. We train an LSTM with the input word embedding dimension and hidden state dimension both be 128. On the paper-news title transfer dataset, the training stops after two epochs, and the accuracy on the validation data is 98.8%. For the positivenegative review dataset, the training also stops after two epochs, with an accuracy of 84.8% on validation.</p><p>For the content preservation metric, we use pretrained 100-dimensional word embeddings to compute sentence similarities. For the positive and negative review transfer task, we filter out the sentiment words to make sure the content preservation metric indeed measures the content similarity. A positive and negative word dictionary is used to conduct the filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>As we discussed in previous sections, this paper is exploratory. We are exploring whether we can learn style transfer with non-parallel data, and whether we can define some evaluation metrics to measure how well the models do in text style transfer. Therefore, we first examine how does the proposed evaluation metrics compare to human judgments. <ref type="figure">Figure 2</ref>: Results for auto-encoder, multi-decoder and style embedding for two tasks, paper-news title style transfer (left) and positive-negative review style transfer (right). Different nodes for the same model denote different hyper-parameters. <ref type="figure">Figure 3</ref>: Score correlation of content preservation and human evaluation. Gaussian noise is added to human evaluation for better visualization. The partial enlarged graph is shown on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Human Judgments</head><p>To ensure our proposed content preservation metric is efficient in measuring the sentence similarities, we compare it against human judgments. The human judgments are obtained by randomly sampling 200 paper-news transferred pair from the test data, target transferred sentences are generated by style-embedding model, and ask three different people to rate the pairs with scores {0, 1, 2}. 2 means the two sentences are very similar; 1 means the two sentences are somewhat similar; and 0 indicates the two sentences are not similar. We conduct this experiment on Amazon Mechanical Turk 9 . The scores for each pair from different people are averaged to generate the final human judgment scores. We then calculate the Spearman's coefficient (a measurement for accessing monotonic relationships) between the human judgment scores and our content preservation metric. The correlation score is 0.5656 with p-value&lt;0.0001, indicates a high correlation between human judgment scores and the content preservation metric. <ref type="figure">Figure 3</ref> illustrates the correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Performances</head><p>We then explore the effect of different parameters on different models for style transfer. <ref type="figure">Figure 2</ref> gives an overview of the results. We can see that in both tasks and all the models, transfer strength and content preservation are negatively correlated. This indicates that within the same model, to get more style changes, one has to lose some contents. We also see the slopes of the trade-off curves appear to be less steep in our proposed models than in the auto-encoder, which indicates our models strike a better balance between the two aspects (transfer strength and content preservation) of style transfer. We now give detailed analysis of the performances of different models with different parameters on the two tasks, respectively. More details about the influences of the hyper-parameters can be found at https: //arxiv.org/abs/1711.06861.</p><p>Paper-News Title Transfer For the paper-news title transfer task, the auto-encoder is able to recover most of the content, but with few transfer strength, just as we expected. The multi-decoder performs better on transfer strength, while style-embedding performs better on content preservation. Both are also able to achieve considerably high scores in two metrics, so there is no clear winning model.</p><p>More specifically, for the style-embedding model, the transfer strength ranges from 0.2 to 0.6 when using different hyper-parameters, and the content preservation ranges from 0.89 to 0.95. Both cover a wide range and would be useful for certain downstream tasks. For the multi-decoder model, it generally tends to generate results with high transfer strength but low content preservation. Therefore, we suggest using multi-decoder and style-embedding in different request scenarios.</p><p>Positive-Negative Review Transfer For the positivenegative review transfer task, the transfer strength of autoencoder is no longer nearly zero like in the paper-news title transfer task, probably because the classifier used to measure the transfer strength is not perfect 10 . The transfer strength measure is not as reliable as it is in the paper-news title task.</p><p>For the style-embedding model, it covers a quite wide range in both transfer strength and content preservation. The multi-decoder model still shows high transfer strength as is in the paper-news title transfer task, and it achieved higher content preservation than that in paper-news title transfer. In source positive: all came well sharpened and ready to go . auto-encoder:</p><p>→negative: all came well sharpened and ready to go . multi-decoder: →negative: all came around , they did not work . style-embedding: →negative: my NUM and still never cut down it . source negative: my husband said it was obvious so i had to return it . auto-encoder:</p><p>→positive: my husband said it was obvious so i had to return it . multi-decoder: →positive: my husband was no problems with this because i had to use . style-embedding: →positive: my husband said it was not damaged from i would pass right . source paper: an efficient and integrated algorithm for video enhancement in challenging lighting conditions auto-encoder:</p><p>→news: an efficient and integrated algorithm for video enhancement in challenging lighting conditions multi-decoder: →news: an efficient and integrated and google smartphone for conflict roku together wrong style-embedding: →news: an efficient and integrated algorithm, for video enhancement in challenging power worldwide source news: luxury fashion takes on fitness technology auto-encoder:</p><p>→paper: luxury fashion takes on fitness technology multi-decoder: →paper: foreign banking carbon on fitness technology style-embedding: →paper: luxury fashion algorithms on fitness technology </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis in a Multi-task Learning View</head><p>Auto-encoder, style-embedding and multi-decoder can be seen as different strength implement of multi-task learning. In our model, generating different titles can be seen as different tasks. For some kind of multi-task learning, different tasks share parameters to share features in different tasks.</p><p>For auto-encoder, two tasks share all the parameters, so it does not have the ability to generate different style sequence. For style-embedding, two tasks share encoder and decoder with separate style embedding, so it has weak ability to generate different style sequence. For multi-decoder, two tasks share encoder with two separate decoders, so it shows high ability to generate different style sequence. For content preservation, more parameters are shared, less distinction between two tasks and more content is preserved. Since the style-embedding model shares more parameters among tasks, less training data is needed to train the model, but the style embeddings have heavier burden to encode the style information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lower Bound for Content Preservation</head><p>We also estimate the lower bound of the content preservation metric, to gauge how well our model performed in preserving the content. The lower bound is estimated by randomly sampling 2,000 sentence pairs from the two datasets, respectively. Results show that the estimated lower bound of content preservation on the paper-news title dataset is 0.609 and 0.863 on the positive-negative review dataset. For both datasets, our models achieved much higher content preservation scores than the lower bound. This indicates that the proposed model learned to preserve the content of the source sentence well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Study</head><p>To give people some intuitive sense about how our models perform, we sampled one instance from each style transfer case, and show the results of three models in <ref type="table" target="#tab_2">Table 2</ref>. We can see that the auto-encoder almost always produce the identical output text as the input. The other two models tend to generate results that replace a few significant words or phrases, but preserve most of the content. Both models perform quite well on the positive-negative style transfer, but less well on the paper-news transfer. <ref type="table" target="#tab_4">Table 3</ref> shows results of auto-encoder in paper-news task. With the increment of encoder (decoder) dimension, ability to recover source sequence increases. So content preservation is larger and transfer strength is smaller (more indeterminacy decreasing). <ref type="table" target="#tab_5">Table 4</ref> shows results of multi-decoder in paper-news task. Similar to auto-encoder, with the dimension increasing, it shows higher ability to recover sentence. <ref type="table">Table 5</ref> shows results of style-embedding in paper-news task. In most cases, encoder dimension has little influence on results, However, style embedding dimension has a large influence on results. Larger style embedding dimension (also decoder dimension), more content preserved. The performance of decoder to recover sentence increases with dimension.    <ref type="table">Table 5</ref>: Results for style-embedding in paper-news task. Word dimension is 64. Decoder dimension equals to summing encoder dimension and style embedding dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Dimension Influence</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two models in this paper, multi-decoder (left) and style-embedding (right). Content c represents output of the encoder. Multi-layer Perceptron (MLP) and Softmax constitute the classifier. This classifier aims at distinguishing the style of input X. An adversarial network is used to make sure content c does not have style representation. In style-embedding, content c and style embedding s are concatenated and [c, e] is fed into decoder GRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>P</head><label></label><figDesc>(yi|xi; Θ d ) = Ty j=1 p (yi,j|Encoder(xi; Θe), yi,1, . . . , yi,j−1; Θ d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>l style = paper(positive) output ≤ 0.5 news(negative) output &gt; 0.5 (13) For similar task, (Shen et al. 2017) uses classifier to evaluate style transfer. (Zhou et al. 2017) controls emotion of conversation, it also uses a classifier to evaluate chatbot generated emotional response.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Size of datasets strength by only generating the target style words. Therefore, we propose a metric for content preservation, which can evaluate the similarity between source text and target text. Content preservation rate is defined as cosine distance (18) between source sentence embedding v s and target sentence embedding v t . Sentence embedding consists of max,min,mean pooling of word embedding defined in (17).</figDesc><table><row><cell cols="2">vmin[i] = min{w1[i], . . . wn[i]}</cell><cell>(14)</cell></row><row><cell cols="2">vmean[i] = mean{w1[i], . . . wn[i]}</cell><cell>(15)</cell></row><row><cell cols="2">vmax[i] = max{w1[i], . . . wn[i]}</cell><cell>(16)</cell></row><row><cell cols="2">v = [vmin, vmean, vmax]</cell><cell>(17)</cell></row><row><cell>score =</cell><cell>v s vt vs · vt</cell><cell>(18)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Case study of style transfer this dataset, the multi-decoder model performs better than the style-embedding model on both metrics (the red line is on the upper right over the green line).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results for auto-encoder in paper-news task. Word embedding dimension is 64.</figDesc><table><row><cell>encoder (decoder) dimension</cell><cell>transfer strength</cell><cell>content preservation</cell></row><row><cell>32</cell><cell>0.9915</cell><cell>0.8335</cell></row><row><cell>64</cell><cell>0.9705</cell><cell>0.8440</cell></row><row><cell>128</cell><cell>0.842</cell><cell>0.8705</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results for multi-decoder in paper-news task. Word embedding dimension is 64.</figDesc><table><row><cell>encoder dimension</cell><cell>style embedding dimension</cell><cell>transfer strength</cell><cell>content preservation</cell></row><row><cell>32</cell><cell>32</cell><cell>0.593</cell><cell>0.8966</cell></row><row><cell>32</cell><cell>64</cell><cell>0.465</cell><cell>0.9157</cell></row><row><cell>32</cell><cell>128</cell><cell>0.282</cell><cell>0.9435</cell></row><row><cell>64</cell><cell>32</cell><cell>0.596</cell><cell>0.8957</cell></row><row><cell>64</cell><cell>64</cell><cell>0.485</cell><cell>0.9153</cell></row><row><cell>64</cell><cell>128</cell><cell>0.2745</cell><cell>0.9440</cell></row><row><cell>128</cell><cell>32</cell><cell>0.595</cell><cell>0.8960</cell></row><row><cell>128</cell><cell>64</cell><cell>0.4825</cell><cell>0.9146</cell></row><row><cell>128</cell><cell>128</cell><cell>0.267</cell><cell>0.9439</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at https://github.com/fuzhenxin/textstyletransferdata</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/fchollet/keras/blob/ master/examples/imdb lstm.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://nlp.stanford.edu/projects/glove/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://dl.acm.org 5 https://arxiv.org 6 https://link.springer.com 7 http://www.sciencedirect.com 8 https://www.nature.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://www.mturk.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">The accuracy of this classifier is only 84.8% on the validation data, probably because some sentences in the positive-negative review dataset do not have significant sentiment</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank Jin-ge Yao for discussions on this paper. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We studied the problem of style transfer with non-parallel corpora. We proposed two models and two evaluation metrics to advance the research in this area. We also composed two datasets: paper-news title dataset and positive-negative review dataset, to gauge the efficiency of the proposed models and evaluation metrics. Experiments showed that the proposed models can be used to learn style transfer from nonparallel data, and the proposed content preservation evaluation metric is highly correlated to human judgment.</p><p>In the future, we plan to propose more comprehensive evaluation metrics (including sentence fluency) and conduct through study with human evaluation, to better shape the research in style transfer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Am-fm: a semantic framework for translation quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Is writing style predictive of scientific fraud?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04095</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multitask learning. In Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial multi-criteria learning for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The nist speaker recognition evaluation-overview, methodology, systems, results, perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="254" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02633</idno>
		<title level="m">Controlling linguistic style aspects in neural language generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05897</idno>
		<title level="m">Preserving color in neural artistic style transfer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating topical poetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shakespearizing modern language using copyenriched sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01161</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06155</idno>
		<title level="m">A persona-based neural conversation model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to better sequence: continuous revision of combinatorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09655</idno>
		<title level="m">Style transfer from non-parallel text by cross-alignment</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ruber: An unsupervised method for automatic evaluation of opendomain dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03079</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">i, poet: Automatic chinese poetry composition through a generative summarization framework under constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2197" to="2203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">i, poet: Automatic poetry composition through recurrent neural networks with iterative polishing schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2238" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01074</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
