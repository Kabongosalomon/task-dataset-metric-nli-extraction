<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks with Adversarial Traces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Imani</surname></persName>
							<email>imani.moh@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Saidur</forename><surname>Rahman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Mathews</surname></persName>
							<email>nate.mathews@mail.rit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wright</surname></persName>
							<email>matthew.wright@rit.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Anomali Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Cybersecurity Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Center for Cybersecurity Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Center for Cybersecurity Rochester Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks with Adversarial Traces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anonymity System</term>
					<term>Defense</term>
					<term>Privacy</term>
					<term>Website Fingerprinting</term>
					<term>Ad- versarial Machine Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Website Fingerprinting (WF) is a type of traffic analysis attack that enables a local passive eavesdropper to infer the victim's activity even when the traffic is protected by encryption, a VPN, or an anonymity system like Tor. Leveraging a deep-learning classifier, a WF attacker can gain over 98% accuracy on Tor traffic. Existing WF defenses are either very expensive in terms of bandwidth and latency overheads (e.g. two-to-three times as large or slow) or ineffective against the latest attacks. In this paper, we explore a novel defense, Mockingbird, based on the idea of adversarial examples that have been shown to undermine machine-learning classifiers in other domains. Since the attacker gets to design his classifier based on the defense design, we first demonstrate that at least one technique for generating adversarial-example based traces fails to protect against an attacker using adversarial training for robust classification. We then propose Mockingbird, a technique for generating traces that resists adversarial training by moving randomly in the space of viable traces and not following more predictable gradients. The technique drops the accuracy of the state-of-the-art attack hardened with adversarial training from 98% to as low as 29% while incurring only 56% bandwidth overhead. The attack accuracy is generally lower than state-of-the-art defenses, and much lower when considering Top-2 accuracy, while incurring lower overheads in most settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The Tor anonymity system is vulnerable to traffic analysis attacks. One such attack is Website Fingerprinting (WF), which enables an eavesdropper between the client and the first Tor node on her path to identify which websites the client is visiting. <ref type="figure" target="#fig_0">Figure 1</ref> shows a WF attack model in the Tor network. This local passive adversary could be sniffing the client's wireless connection, have compromised her cable/DSL modem, or gotten access to the client's ISP or workplace network. * The authors contribute equally to this paper.</p><p>The WF attack can be modeled as a supervised classification problem, in which the website domain names are labels and each traffic trace is an instance to be classified or used for training. Recently proposed WF attacks have used deep-learning classifiers to great success. The state-of-the-art WF attacks, Deep Fingerprinting (DF) <ref type="bibr" target="#b34">[36]</ref> and Var-CNN <ref type="bibr" target="#b2">[4]</ref>, utilize convolutional neural networks (CNN) to identify obtuse patterns in traffic data. These attacks can achieve above 98% accuracy to identify sites using undefended traffic in a closed-world setting <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b34">36]</ref>, and both attacks achieve high precision and recall in the more realistic open-world setting.</p><p>In response to the threat of WF attacks <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b36">38]</ref>, there have been numerous defenses proposed <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40]</ref>. WF defenses perturb the traffic so as to hide patterns and confound the classifier. Recently, two relatively lightweight defenses have been proposed, WTF-PAD <ref type="bibr" target="#b15">[17]</ref> and Walkie-Talkie (W-T) <ref type="bibr" target="#b37">[39]</ref>. WTF-PAD offers lower overheads and is considered to be a leading candidate for deployment in Tor <ref type="bibr" target="#b29">[31]</ref>. Recent state-ofthe-art attacks, however, show that WTF-PAD is ineffective with these attacks achieving over 90% accuracy in the closed-world 1 <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b34">36]</ref>. Furthermore, DF attack is able to reliably narrow down site identities to two candidates when pitted against the W-T defense, achieving a Top-2 accuracy of 98% <ref type="bibr" target="#b34">[36]</ref>. These issues have motivated the need for new defenses that perform well against these state-ofthe-art deep-learning based attacks.</p><p>In this paper, we introduce Mockingbird, 2 a new defense strategy using adversarial examples generated by a deep neural network. There are various techniques to generate these examples in the context of image classification <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b35">37]</ref>, however most of them perform a search in the space of possible images. Starting with the original image, the search moves gradually towards a slightly modified image that is classified differently from the source. This search is typically guided by the gradient of the neural network's loss function or the logits, the inputs to the neural network's final layer for classification decision. Adversarial examples generated by these methods have had tremendous success in fooling deeplearning models to misclassify with as much as 100% effectiveness 1 WF attacks are evaluated in either the closed-world or open-world settings. The closedworld setting makes assumptions that benefit the attacker, whereas the open-world setting relaxes these assumptions and is more indicative of real-world performance. <ref type="bibr" target="#b0">2</ref> The Northern mockingbird imperfectly but effectively imitates the calls of a wide range of other birds, and one of its call types is known as a chatburst that it uses for territorial defense: https://en.wikipedia.org/wiki/Northern_mockingbird. 1 and low amounts of distortion in the images <ref type="bibr" target="#b7">[9]</ref>. This makes adversarial examples intriguing for defending against WF attacks using deep learning, as they could fool the classifier without adding large amounts of distortion in the form of padding or delay.</p><p>While neural networks can be made more robust against adversarial examples, most of these techniques are not effective upon careful analysis <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b5">7]</ref> or do not generalize to new approaches to generating the examples <ref type="bibr" target="#b30">[32]</ref>. In the WF context, however, the WF attacker has the advantage of being able to generate adversarial examples using the same technique as the defender. In particular, he can use the defense as an oracle to generate these traces and use them to train his classifier to be more robust. This adversarial training approach has been shown to be effective when the classifier knows how the adversarial traces are being generated <ref type="bibr" target="#b16">[18]</ref>.</p><p>To address this, we propose a novel technique to generate adversarial traces that seeks to limit the effectiveness of adversarial training by increasing the randomness of the search process and reducing the influence of the design and training of the neural network in finding a good adversarial example. In particular, as we search for the new trace, we select a random target trace and gradually reduce the distance to the target. We also change to other randomly selected targets multiple times during the search. The neural network is only used to give a confidence value on whether the current trace fools the classifier, and we do not access the loss function, the logits, or any other aspect of the neural network. The resulting adversarial traces can go in many different directions from the original source traces instead of consistently following the same paths that result from, e.g., following the gradient of the loss function. In this way, the technique selects paths that are hard to find through adversarial training. Further, each new trace generated from the same source typically ends near a different target each time, helping to reduce the attacker's top-2 accuracy.</p><p>With this technique, Mockingbird reliably causes misclassification in a deep-learning classifier hardened with adversarial training using only moderate amounts of bandwidth overhead.</p><p>The key contributions of this work are as follows:</p><p>• We are the first to leverage the concept of adversarial examples in defending anonymous network traffic from WF attacks with our Mockingbird defense. • We show how the state-of-the-art algorithms in generating adversarial examples in computer vision fail as a defense in the WF setting. • We evaluate Mockingbird and find that Mockingbirdsignificantly reduces accuracy of the state-of-the-art WF attack hardened with adversarial training from 98% to 29%-57%, depending on the scenario. The bandwidth overhead is 57% for full-duplex traffic with no added delays, which is better than both W-T and WTF-PAD. • We show that it is difficult for an attacker to scope down to just two sites against our defense. The state-of-the-art attack can reach at most 57% Top-2 accuracy against our defense, while the Top-2 accuracy on W-T and WTF-PAD is 97% and 90%, respectively. • Using the WeFDE framework <ref type="bibr" target="#b17">[19]</ref>, we measure the information leakage of our defense, and find that Mockingbird has less leakage for many types of features than either W-T or WTF-PAD. We note that Mockingbird has significant implementation challenges, perhaps most importantly that finding suitable traces requires significant computational time. At a minimum, Mockingbird has potential for use in server-side WF defenses <ref type="bibr" target="#b9">[11]</ref>, in which the computation need only be done once for each significant change to the site. Additionally, this work serves as a potential first step towards more practical general-purpose defenses that build on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THREAT &amp; DEFENSE MODEL 2.1 Threat Model</head><p>We assume that the client browses the Internet through the Tor network to hide her activities (see <ref type="figure" target="#fig_0">Figure 1</ref>). The adversary of interest is local, which means the attacker is positioned somewhere in the network between the client and Tor guard node. The attacker is assumed to already know the identity of the client. His goal is to detect the websites that the client is visiting. A local adversary can be an eavesdropper on the user's local network, local system administrators, Internet Service Providers, any networks between the user and the entry node, or the operator of the entry node. The attacker is passive, meaning that the he only observes and records the traffic traces that pass through the network. He does not have the ability to drop, delay, or modify real packets in the traffic stream.</p><p>In a website fingerprinting (WF) attack, the attacker feeds the collected network traffic into a trained machine-learning or deeplearning classifier. For the purpose of training, the WF attacker needs to collect traffic of various sites as a client of Tor network. The attacker must deal with the limitation that it is not feasible to collect the network traffic of all the sites on the web. To address this, the attacker identifies a set of monitored sites that he wants to track. The attacker limits the scope of his attack to the identification of any website visits that are within the monitored set. The set of all other sites is known as the unmonitored set.</p><p>WF attacks and defenses are evaluated in two different settings: closed-world and open-world. In the closed-world setting, we assume that the client is limited to visiting only the monitored sites. The training and testing set used by the attacker only include samples from the monitored set. The closed-world scenario models an ideal setting for the attacker and is not indicative of the attack's real-world performance. From the perspective of developing a WF defense, defeating a closed-world attack means potentially defeating the attacker from the most advantageous situation.</p><p>In contrast, the open-world scenario models a more realistic setting in which the client may visit websites from both the monitored and unmonitored sites. In this setting, the attacker trains on the monitored sites and a representative (but not comprehensive) sample of unmonitored sites. The open-world classifier is then evaluated against both monitored and unmonitored sites, where the set of unmonitored sites used for testing does not intersect with the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Defense Model</head><p>The purpose of a WF defense is to make a traffic trace of one site indistinguishable from traces of other sites. To achieve this, the real traffic stream must be manipulated in some way. Because traffic is bidirectional, the deployment of a successful WF defense requires participation from both the client and a cooperating node. It is difficult, if not impossible, for the client to defend their traffic alone as they have control over only outgoing traffic in the communication stream. To defend both directions in the network stream, the client must have the cooperation of one of the nodes in its Tor circuit. We call this cooperating node the bridge. <ref type="bibr" target="#b1">3</ref> The bridge must be located somewhere between the adversary and the client's destination server so that the adversary only has access to the obfuscated traffic stream. However, the guard node has knowledge about the identity of the client and is in a position to act as a WF adversary. Therefore, it would be ideal to set up the bridge at the middle node, which cannot directly identify the client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND &amp; RELATED WORK 3.1 WF Attacks</head><p>Website fingerprinting attacks have applied a variety of classifiers. The three best attacks based on manual feature engineering are k-NN <ref type="bibr" target="#b36">[38]</ref>, CUMUL <ref type="bibr" target="#b24">[26]</ref>, and k-FP <ref type="bibr" target="#b13">[15]</ref>, which all reached over 90% accuracy in closed-world tests on datasets with 100 samples per site. In the rest of this section, we examine the more recent deep-learning-based WF attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDAE.</head><p>Recently, deep-learning techniques have attracted the attention of privacy researchers due to their excellent performance in image recognition tasks. The first to investigate their usage for WF was Abe and Goto <ref type="bibr" target="#b0">[2]</ref> who developed an attack based on Stacked Denoising Autoencoders (SDAE). Unlike prior work, their attack did not utilize handcrafted features. Rather, their model was trained on the raw packet direction, represented by a sequence of "+1" and "-1" values for outgoing and incoming packets, respectively. Despite this innovation, their attack achieved a lower accuracy rate than the previous state-of-the-art attacks at only 88% in the closed-world setting.</p><p>Automated Website Fingerprinting (AWF). Rimmer et al. <ref type="bibr" target="#b32">[34]</ref> proposed using deep learning to bypass the feature engineering phase of traditional WF attacks. To more effectively utilize DL techniques, they collected a larger dataset of 900 sites with 2,500 trace instances per site. They applied several different DL architectures-SDAE, Convolutional Neural Network (CNN), and Long Short-Term Memory (LSTM)-on the traffic traces. They found that their CNN model outperforms the other DL models they developed, obtaining 96% accuracy in the closed-world setting.</p><p>Deep Fingerprinting (DF). Sirinam et al. <ref type="bibr" target="#b34">[36]</ref> further developed a deep CNN model that could outperform all the previous models, reaching up to 98% accuracy rate in the closed-world setting. They evaluated their attack against a dataset of 100 sites with 1,000 instances each. They also examined the effectiveness of their model against WF defenses, where they showed that their model can achieve concerningly high performance against even some defended traffic. Most notably, their attack achieved 90% accuracy against WTF-PAD <ref type="bibr" target="#b15">[17]</ref>, the most likely WF defense to be deployed on Tor, and 98% Top-2 accuracy against Walkie-Talkie <ref type="bibr" target="#b37">[39]</ref>.</p><p>Var-CNN. Recently Bhat et al. <ref type="bibr" target="#b2">[4]</ref> developed a more sophisticated CNN based WF attack with reference to a ResNet based architecture. In particular, Var-CNN shows attack effectiveness achieving 98.8% accuracy. For our defense evaluation, we evaluated against the DF model instead of Var-CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">WF Defenses</head><p>In an effort to defeat WF attackers, researchers have explored various defense designs that generate cover traffic to hide the features present in website traffic. WF defenses are able to manipulate the traffic stream with two operations: sending dummy packets and delaying real packets. These manipulations, however, come at a cost: sending dummy packets adds an additional bandwidth overhead to the network, while delaying packets adds latency overhead that directly impacts the time required to load the page. Several studies have thus tried to balance the trade-off between the WF defense's overhead and efficacy of the defense against WF attacks. In this section, we review these WF defenses.</p><p>Constant-rate padding defenses. This family of defenses transmits traffic at a constant rate in order to normalize trace characteristics. BuFLO <ref type="bibr" target="#b10">[12]</ref> is the first defense of this kind, and it sends the packets in the same constant rate in both directions. The defense ends transmission after the page has finished loading and a minimum amount of time has passed. The overhead of the traffic is governed by both the transmission rate and the minimum time threshold for the stopping condition. Moreover, although the defense covers finegrained features like burst information, the course-grained features like the volume and load time of the page still leak information about the website. Tamaraw <ref type="bibr" target="#b4">[6]</ref> and CS-BuFLO <ref type="bibr" target="#b3">[5]</ref> extend the BuFLO design with the goal of addressing these issues. To provide better cover traffic, after the page is loaded, Tamaraw keeps padding until the total number of transmitted bytes is a multiple of a fixed parameter. Similarly, CS-BuFLO pads the traffic to a power of two, or to a multiple of the power of the amount of transmitted bytes. BuFLO family defenses are expensive, requiring two to three times as much time as Tor to fetch a typical site and more than 100% bandwidth overhead.</p><p>Supersequence defenses. This family of defenses depends on finding a supersequence for traffic traces. To do this, these defenses first cluster websites into anonymity sets and then find a representative sequence for each cluster such that it contains all the traffic sequences. All the websites that belong to the same cluster are then molded to the representative supersequence. This family includes Supersequence <ref type="bibr" target="#b36">[38]</ref>, Glove <ref type="bibr" target="#b23">[25]</ref>, and Walkie-Talkie <ref type="bibr" target="#b37">[39]</ref>. Supersequence and Glove use approximation algorithms to estimate the supersequence of a set of sites. The traces are then padded in such a way so as to be equivalent to its supersequence. However, applying the molding directly to the cell sequences creates high bandwidth and latency costs. Walkie-Talkie (WT) differs from the other two defenses in that it uses anonymity sets of just two sites, and traces are represented as burst sequences rather than cell sequences. Even with anonymity sets of sizes of just two, this produces a theoretical maximum accuracy of 50% . Wang and Goldberg report just 31% bandwidth overhead for their defense, but also 34% latency overhead due to the use of half-duplex communication. Then Sirinam et al. <ref type="bibr" target="#b34">[36]</ref> achieved 49.7% attack accuracy against WT.</p><p>Adaptive Padding (AP). Shmatikov and Wang <ref type="bibr" target="#b33">[35]</ref> proposed Adaptive Padding (AP) as a countermeasure against end-to-end traffic analysis. Juarez et al. <ref type="bibr" target="#b15">[17]</ref> extended the idea of AP and proposed the WTF-PAD defense as an adaptation of AP to protect Tor traffic against WF attacks. WTF-PAD tries to fill in large delays between packets (inter-packet arrival times). Whenever there is a large interpacket arrival time (where "large" is determined probabilistically), WTF-PAD sends a fake burst of dummy packets. This approach does not add any artificial delays to the traffic. Juarez et al. show that WTF-PAD can drop the accuracy of the k-NN attack from 92% to 17% with a cost of 60% bandwidth overhead. Sirinam et al. <ref type="bibr" target="#b34">[36]</ref>, however, show that their DF attack can achieve up to 90% accuracy against WTF-PAD in the closed-world setting.</p><p>Application-level defenses. Cherubin et al. <ref type="bibr" target="#b9">[11]</ref> propose the first WF defenses designed to work at the application layer. They proposed two defenses in their work. The first of these defenses, ALPaCA, operates on the webserver of destination websites. ALPaCA works by altering the size distribution for each content type, e.g. PNG, HTML, CSS, to match the profile for an average onion site. In the best case, this defense has 41% latency overhead and 44% bandwidth overhead and reduces the accuracy of the CUMUL attack from 56% to 33%. Their second defense, LLaMA, operates on the client exclusively. This defense adds random delays to HTTP requests in an effort to affect the order of the packets by manipulating HTTP request and responses patterns. LLaMA drops the accuracy of the CUMUL attack on Onion Sites from 56% to 34% at cost of 9% latency overhead and 7% bandwidth overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adversarial Examples &amp; Adversarial Training</head><p>Adversarial Examples. Szegedy et al. <ref type="bibr" target="#b35">[37]</ref> were the first to discover that otherwise accurate ML and DL image classification models could be given image inputs with slight perturbations that would be largely imperceptible to humans however completely misclassified by the models. These perturbed inputs are called adversarial examples, and they call into question the robustness of many of the advances being made in machine learning. The state-of-the-art DL models can be fooled into misclassifying adversarial examples with surprisingly high confidence. For example, Papernot et al. <ref type="bibr" target="#b25">[27]</ref> show that adversarial images cause a targeted deep neural network to misclassify 84% of the time.</p><p>The idea of creating adversarial examples is to modify samples from one class to make them be misclassified to another class where the amount of modification is limited. More precisely, given an input sample x and target class t such that the original class of x and the target class t are not the same C * (x) t, the goal is to find x ′ which is close to x according to some distance metrics and C(x ′ ) = t.</p><p>In this case, x ′ is called a targeted adversarial example since it is misclassified to a particular target label t. However, if an adversarial example x ′ may be misclassified to any other classes except the true class (C * (x)), it is called an untargeted adversarial example.</p><p>In response to the adversarial examples, many defense techniques have been introduced to make classifiers more robust against being fooled. Recent research <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b7">9]</ref> shows that almost none of these recent defense techniques are effective. In particular, we can generate adversarial examples that counter these defense techniques by including the defense techniques directly into the optimization algorithm used to create the adversarial examples. We can also overcome many defense approaches by simply increasing the amount of perturbation used <ref type="bibr" target="#b5">[7]</ref>.</p><p>Adversarial Training. Recent research shows that adversarial training increases the robustness of a model by incorporating adversarial examples in the training data <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b20">22]</ref>. The idea is to train a network with adversarial examples so that they can be classified correctly. This approach is limited, as it does not adapt well to techniques for generating adversarial examples that haven't been trained on. In the WF setting, however, the classifier has the advantage of knowing how the adversarial examples are being generated, as they would be part of the open-source Tor code. Thus, adversarial training is a significant concern for our system. model may also be misclassified in other models that were trained on different subsets of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adversarial Examples as a WF Denfense</head><p>From the perspective of developing a WF defense, the goal is to insert dummy packets to the original network traffic to fool the trained WF classifier. The number of dummy packets in the traffic should be as small as possible to keep bandwidth consumption low. These two goals of WF defenses, high rates of misclassification and low overhead, correspond to the two compelling properties of adversarial examples, which are to fool the classifier with transferablity using relatively small perturbations. Therefore, we propose to use the method of generating adversarial examples as a method of WF defense.</p><p>To develop a WF defense based on the adversarial examples, first we need to create a representation for the traffic traces to apply the adversarial examples algorithms to them. We model the traffic trace as a sequence of incoming (server to client) and outgoing (client to server) bursts. We define a burst as a sequence of consecutive packets in the same direction (see <ref type="figure" target="#fig_1">Figure 2</ref>). Given this, we can increase the length of any burst by sending padding packets in the direction of the burst. We cannot, however, decrease the size of the bursts by dropping real packets due to the retransmissions this would cause, changing the traffic patterns and adding delays for the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Existing Methods as a WF Defense</head><p>Several different algorithms have been proposed for generating adversarial examples within the field of computer vision, including the Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b11">[13]</ref>, the Jacobian-Based Saliency Map Attack (JSMA) <ref type="bibr" target="#b26">[28]</ref>, and optimization-based methods <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b18">20]</ref>. For our initial exploration of adversarial examples in WF defense, we examine the technique proposed by Carlini and Wagner.</p><p>In 2017, Carlini and Wagner proposed a powerful optimizationbased algorithm to generate adversarial examples <ref type="bibr" target="#b6">[8]</ref>. This method was shown to defeat the defensive distillation approach of blocking adversarial examples <ref type="bibr" target="#b27">[29]</ref>. The algorithm is successful with 100% probability. We modified their technique to suit our needs to generate adversarial traces out of the burst sequences. This algorithm is designed to work on images, which are 2D, so we modified it to work on the 1D traffic traces.</p><p>Given a sample x and a model F , the algorithm finds a perturbation δ that makes x ′ = x + δ to be misclassified to any other class than C(x) (C(x) = arдmax i F (x) i ), or in the case of a targeted attack, it is classified to target class t. The algorithm tries to find x ′ that is similar to x based on distance metric D. The distance metrics can be an L p -norm such as L 0 , L 2 , or L ∞ . The algorithm is formulated as follows:</p><formula xml:id="formula_0">min ∥ δ ∥ p + c.f (x + δ ) such that x + δ ∈ [0, 1] n (1)</formula><p>The algorithm will find δ such that it minimizes the distance metric, which is l p norm, and the objective function f (x + δ ). c is a constant to scale both the distance metric and objective function in the same range. Carlini and Wagner <ref type="bibr" target="#b6">[8]</ref> used binary search to find the proper value for c. They explored several objective functions and found two taht work the best. For a targeted attack scenarios with target class t, the best objective function is:</p><formula xml:id="formula_1">f (x ′ ) = max i t (F (x ′ ) i ) − F (x ′ ) t<label>(2)</label></formula><p>For non-targeted attack scenarios where the true class for sample x is class y, the best objective function is:</p><formula xml:id="formula_2">f (x ′ ) = F (x ′ ) y − max i y (F (x ′ ) i )<label>(3)</label></formula><p>We evaluated the performance of this technique in two different WF attack scenarios: without-adversarial-training and withadversarial-training. The without-adversarial-training scenario represents the scenario most typically seen in the adversarial example literature, in which the classifier has not been trained on any adversarial instances. In this scenario, we generated the adversarial examples against a target model and tested them against the different WF attacks trained on the original traffic traces. We find that our adversarial traces are highly effective against the WF attacks. The accuracy of DF <ref type="bibr" target="#b34">[36]</ref>, the state-of-the-art deep-learning attack, is reduced from 98% to 3%, and the accuracy of CUMUL <ref type="bibr" target="#b24">[26]</ref> drops from 92% to 31%. Our adversarial traces generated using this method are highly transferable, as we generated them against a target CNN model and they are effective against both DF and CUMUL. The full details of these evaluations can be found in Appendix A.1.</p><p>This scenario is not realistic, unfortunately, as it is likely (and usually assumed) that the attacker can discern what type of defense is in effect and train on representative samples. This scenario was examined in the with-adversarial-training evaluations. When evaluated under this scenario, Carlini and Wagner's technique fails completely with 97% attack accuracy. The full details of these evaluations can be found in Appendix A.2. In addition, we also investigated a tuned C&amp;W method for generating adversarial traces which also show ineffectiveness as a WF defense. We discuss the tuned C&amp;W in Appendix B.</p><p>The results of this evaluation led to a very important insight: the scenario in which the effectiveness of adversarial examples are typically evaluated is notably different than that of a WF defense. Thus, the techniques that excel at producing adversarial examples for traditional attacks are poorly suited for our problem. In response to this discovery, we focused our efforts on the development of a new technique designed specifically for our needs. We discuss our method in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generating Adversarial Traces</head><p>We now introduce Mockingbird, a novel algorithm to generate adversarial traces that more reliably fool the classifier in the adversarial training setting. The ultimate goal is to generate untargeted adversarial traces that cause the classifier to label a traffic trace as coming from any other site except the original site, i.e. to generate an untargeted sample.</p><p>To defend a given trace, the source sample, Mockingbird first generates a set of potential target traces selected randomly from the traces of various sites other than the source site. It then randomly picks one of these traces as the target sample and gradually changes the source sample to get closer to the target sample. The process stops when a trained classifier called the detector determines that the class of the sample has changed (see <ref type="figure" target="#fig_2">Figure 3</ref>). Note that it does not need to have changed to the target sample's class, as the goal is to generate an untargeted adversarial trace. The amount of change applied to the source sample governs the bandwidth overhead of Mockingbird, and as such should be minimized.</p><p>Unlike most other algorithms used to generate adversarial examples, Mockingbird does not focus on the loss function (like FGSM and IGSM) or logits (like Carlini &amp; Wagner) of the detector network. Instead, it aims to move the source sample towards the target sample and only uses the detector network to estimate the confidence with which the trace is misclassified. This lessens the reliance on the shape of the detector network and helps to generate adversarial traces that are more robust against adversarial training.</p><p>Mockingbird Algorithm. We assume that we have a set of sensitive sites S that we want to protect. We train a detector f (x) on a set of data from S. We discuss the design and training of f (x) in Section 5. We consider traffic trace I s as an instance of source class s ∈ S. Our goal is to alter I s such that it is classified to any other class t, t = f (I s ) and t s. I s is a sequence of the bursts,</p><formula xml:id="formula_3">I s = b I 0 , b I 1 , ..., b I n ,</formula><p>where n is the length of the trace. The only allowed operation on a burst b I i is to add some positive values δ i &gt;= 0 to that burst, b I i = b I i + δ i . The reason for using δ i &gt;= 0 is that we want to only increase the volume of the bursts. If δ i &lt; 0, that would mean we should drop some packets to reduce the burst volume, but dropping real packets means losing data and requires re-transmission of the dropped packet. To protect source sample I s , we first select τ potential target samples from other classes t i s. We then select the target t as the one nearest to the source sample based on the l 2 norm distance. This helps to minimize overhead as we will move the source towards the target. More formally, we pick a target pool P s of p random samples from other classes, P s = I 0 0 , I 1 1 , .., I p m , where I j i is the j-th sample in the target pool and belongs to target class t i s. The target sample I t is selected as shown in <ref type="bibr">Equation 4</ref>.</p><formula xml:id="formula_4">I t = argmin I ∈P s D(I s , I ) (4) D(x, y) = l 2 (x − y)<label>(5)</label></formula><p>To make the source sample leave the source class, we change it with the minimum amount of perturbation in the direction that makes it closer to the target (I t ). We define ∆ as the perturbation vector that we will add to the source sample to generate its defended form I new</p><formula xml:id="formula_5">s . ∆ = [δ 0 , δ 1 , · · · , δ n ] (δ i &gt;= 0)<label>(6)</label></formula><formula xml:id="formula_6">I new s = I s + ∆<label>(7)</label></formula><p>We need to find a ∆ that adds the least amount of perturbation to the source sample while still making it closer to the target sample. Therefore, we find ∆ that minimizes distance D(I new s , I T ). To do so, we compute the gradient of the distance with respect to the input. Note that most work in adversarial example generation uses the gradient of the loss function of the discriminator network rather than distance, and this may make those techniques more sensitive to the design and training of the classifier. The gradient points in the direction of steepest ascent, which would maximize the distance. Therefore, we compute the gradient of the negative of the distance with respect to the input, and we modify the source sample in that direction towards the target sample. In particular:</p><formula xml:id="formula_7">∇(−D(I, I T )) = − ∂D(I, I T ) ∂I = − ∂D(I, I T ) ∂b i i ∈[0, ··· ,n]<label>(8)</label></formula><p>Where b i is the i-th burst in input I .</p><p>To modify the source sample, we change bursts such that their corresponding values in ∇(−D(I, I T )) are positive. Our perturbation vector ∆ is:</p><formula xml:id="formula_8">δ i = −α × ∂D(I, I T ) ∂b i − ∂D(I, I T ) ∂b i &gt; 0 0 − ∂D(I, I T ) ∂b i ⩽ 0<label>(9)</label></formula><p>where α is the parameter that amplifies the output of the gradient. The choice of α has an impact on the convergence and the bandwidth overhead. If we pick a large value for α, we will take bigger steps toward the target sample and add more overhead, while small values of α require more iterations to converge. We modify the source sample by summing it with ∆, (I new s = I s + ∆). We iterate this process, computing ∆ for I s and updating the source sample at each iteration until we leave the source class, f (I new s ) s or the number of iterations passes the maximum allowed iterations. Note that at the end of each iteration, we update the current source sample with the modified one, I s = I new s . Leaving the source class means that we have less confidence on the source class. So we fix a threshold value (τ c = 0.01) for measuring the confidence. If the confidence of the detector on the source class is less than the threshold (f s (I new s ) &lt; τ c ), Mockingbird will stop changing the source sample (I s ).</p><p>As we only increase the size of the bursts where − ∂D(I, I T ) ∂b i &gt; 0, we may run into cases that after some iterations ∇(−D(I, I T )) does not have any positive values or all the positive values are extremely small such that they do not make any significant changes to I s . In such cases, if I new s − I s is smaller than a threshold τ D = 0.0001 for λ consecutive iterations (we used λ = 10), and we are still in the source class, we select a new target. In particular, we effectively restart the algorithm by picking a new pool of potential target samples, selecting the nearest target from the pool, and continuing the process. It is to note that, the source sample at this point is already in the changed form I new s and the algorithm starts changing from I new s . In this process, the confusion added by Mockingbird in the final trace is proportional to the number of target changed to reach into the final adversarial trace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION 5.1 Datasets</head><p>We apply our algorithm for generating adversarial examples on the traffic traces on the burst level. We can get the burst sequence of the traffic traces from both full-duplex (FD) and half-duplex (HD)  communication modes. Walkie-Talkie (WT) <ref type="bibr" target="#b37">[39]</ref> works on the halfduplex communication and it finds the supersequence in the burst level. In our experiments, we use burst sequences for both FD and HD datasets.</p><p>Full-Duplex (FD) : We use the FD traffic traces provided by Sirinam et al. <ref type="bibr" target="#b34">[36]</ref>Their dataset has 95 classes with 1000 instances each. 95 sites are from the top 100 sites in Alexa [1]. We further process their data by removing any instances with less than 50 packets and the instances that start with incoming packet. After this processing, we end up with 518 instances for each site. We also use the FD openworld (OW) dataset from Sirinam et al. <ref type="bibr" target="#b34">[36]</ref> in our experiments which has 40,716 different sites with 1 instance each. We process the OW dataset as well.</p><p>Half-Duplex (HD) : We use the HD dataset provided by Sirinam et al. <ref type="bibr" target="#b34">[36]</ref>. This dataset contains 100 sites which are also from top 100 sites in Alexa.com [1], with 900 instances for each class. We preprocess this dataset in the same way we processed FD dataset. After preprocessing, we ended up with 83 classes with 720 instances per class. Moreover, they also have OW data of 40,000 sites with 1 instance each. We also process OW dataset. We use both of these HD closed-world (CW) and OW datasets for our HD evaluations.</p><p>In order to use these datasets for our defense, further preprocessing is required before we can use those in our models. The burst sequences varies in size between different visits and sites. However, the input to our models should be fixed in size. In consequence, we must determine an ideal size of input and adjust our dataset to this size. To do this, we consider the distribution of burst sequence lengths within our datasets. <ref type="figure" target="#fig_4">Figure 4</ref> shows the CDF of the burst sequence lengths for both HD and FD datasets. The figure shows that more than 80% of traces have less than 750 bursts for HD CW dataset, and more than 80% of the traces for the CW FD dataset has less than 500 bursts. We found that using input sizes of 750 bursts and 1500 bursts on both HD and HD datasets provides 1% improvement on accuracy on DF attack.The differences between the performances between the full size burst length and its short form (750 bursts) would thus appear to be negligible. To decrease the computational cost to generate adversarial examples, we use the input size of 750 bursts for both FD and HD datasets in our evaluations.</p><p>In our evaluation, we need to address the needs of both the Mockingbird system and the adversary to have training data. We  thus break each dataset (full-duplex (FD) and half-duplex (HD)) into two non-overlapping sets: Adv Set A and Detector Set D (see <ref type="table" target="#tab_0">Table 1</ref>). The Adv Set and Detector Set both contains half of the instances of each class. For FD data, each set contains 259 instances from each of 95 classes, while for HD data, each set contains 83 classes each with 360 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Method</head><p>Mockingbird needs to train a detector f (x) with instances of a variety of sites to generate adversarial traces. We use the Deep Fingerprinting (DF) CNN model designed by Sirinam et al. <ref type="bibr" target="#b34">[36]</ref> as our detector and train it on the traces of the Detector Set. Sirinam et al. suggest using an input size of 5,000 packets, but our padded traces have more traffic, so we use an input size of 10,000 packets, which is the 80th percentile of packet sequence lengths in our defended traces. The Detector Set is only used to train the detector. The source samples from Adv Set (I s ∈ A) are used to generate adversarial traces for the training, validation, and testing of the adversary's classifier.</p><p>We also use DF as the primary attack classifier for most of our experiments, as it is the state-of-the-art attack. Later, we show results using other attacks from the literature. In all of the evaluations in this section, the classifier is trained using adversarial training, where the attacker has full access to the defense and uses it to generate defended samples for each class in the monitored set. We also discuss the impact of using a less powerful detector (weaker surrogate network) in Appendix C.  detector is responsible to verify whether the perturbed source sample is still in the source class. We are interested to know how the algorithm performs if we fill the target pool with instances of sites that the detector has been trained on and has not been trained on. We examine the bandwidth overhead and reduction in the attack accuracy of traces protected by our method in these two different scenarios.</p><p>• Case I: We fill the target pool with instances from the Adv Set. Therefore, both source samples (I s ∈ A) and target samples (I j i ∈ A which T i s) are from the Adv Set. In this case, we assume that the detector has been trained on the target classes, which makes it more effective at identifying when the sample has left one class for another. However, it may be less effective if the adversary trains on the source class but none of the other target classes used in detection. • Case II: We fill the target pool with instances from unmonitored sites that are not in the Adv Set. We select the target samples (I j i ) from the open-world dataset. The source samples (I s ) are from Adv Set and we generate their defended forms. In this case, we assume the detector has not been trained on the target samples, so it may be less effective in identifying when the sample leaves the class. That may make it more robust when the attacker also trains on a different set of classes in his monitored set. We generate defended samples with various settings. We vary α to evaluate their effect on the strength of the defended traces and the overhead. We also vary the number of iterations required to generate the adversarial traces. Each iteration moves the sample closer to the target, improving the likelihood it is misclassified, but also adds bandwidth overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments with Full-Duplex Data</head><p>The full-duplex version of Mockingbird is the easier to deploy and leads to lower latency costs than the half-duplex version <ref type="bibr" target="#b37">[39]</ref>, so we lead with the full-duplex results.</p><p>Choice of α <ref type="figure" target="#fig_5">Figure 5</ref> shows the bandwidth overhead and attack accuracy of full-duplex data with respect to α values for both Case I (solid lines) and Case II (dashed lines) with 500 iterations. As expected, the bandwidth overhead increases and the attack accuracy decreases as we increase α, with longer steps towards the selected targets. For Case I, the adversary's accuracy against Mockingbird with α=5 and α=7 are both 35%, but the bandwidth overhead is lower for α = 5 at 56% compared to 59% for α = 7. For Case II, the adversary's accuracy and the bandwidth overhead are both slightly lower for α=5 than that of α=7.</p><p>As expected, we also observe that Case I leads to lower accuracy and comparable bandwidth overhead to Case II. When α=5 and α=7, the attack accuracies for Case I are at least 20% lower than that of Case II. Therefore, picking target samples from classes that the detector has been trained on drops the attacker's accuracy. <ref type="figure" target="#fig_9">Figure 8</ref> shows the trade-off between the accuracy and bandwidth overhead with respect to the number of iterations to generate the adversarial traces. As mentioned earlier, increasing the number of iterations also increases the number of packets (overhead) in the defended traces. We vary the number of iterations from 100 to 500 for both Case I <ref type="figure" target="#fig_9">(Figure 8a</ref>) and Case II <ref type="figure" target="#fig_9">(Figure 8b</ref>) to see their impact on the overhead and the accuracy rate of the DF attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Iterations.</head><p>For Case I, we can see that the DF attack accuracy for both 400 and 500 iterations is 35% when α=5, while the bandwidth overheads are 54% and 56%, respectively. For α = 7, the attacker's accuracy is higher and the bandwidth costs are higher. For Case II, using α=5 leads to 57% accuracy with 53% bandwidth overhead for 400 iterations and 55% accuracy and 56% bandwidth overhead for 500 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiments with Half-Duplex Data</head><p>Using half-duplex communication increases the complexity of deployment and adds latency overhead <ref type="bibr" target="#b37">[39]</ref>, but it offers more precise control over burst patterns such that we can achieve reduced attacker accuracy as shown in the following results.</p><p>Choice of α. As seen in <ref type="figure" target="#fig_7">Figure 6</ref> (all for 500 iterations), the lowest accuracy rates are 35.5% and 28.8% for Case I and Case II, respectively, when α=7. The bandwidth overheads are 62.7% and 73.5% for Case I and Case II, respectively. When α=5, the attack accuracy is 50% for both Case I and Case II with bandwidth overheads of 57% 8   and 69%, respectively. As expected, higher α values mean lower attacker accuracies at the cost of higher bandwidth. Additionally, we note that as in the full-duplex setting, Case I provides lower bandwidth overhead and lower detectability than Case II.</p><p>Number of Iterations. <ref type="figure" target="#fig_8">Figure 7</ref> shows the trade-off between attacker accuracy and bandwidth overhead with the number of iterations for α=7. We vary the number of iterations from 100 to 500 for both Case I and Case II. For Case I, the accuracy is 35.5% with 62.7% bandwidth overhead with 500 iterations. With 400 iterations, the accuracy is 37% with 59% bandwidth overhead. For Case II, with 500 iterations, we can get the lowest attack accuracy among all the settings in our defense which is 28.8%, with a cost of 73.5% bandwidth overhead. We can observe that the bandwidth overhead for Case I is always lower than Case II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results Analysis</head><p>We show a comparison of attack accuracies of Mockingbird against DF <ref type="bibr" target="#b34">[36]</ref>, AWF <ref type="bibr" target="#b32">[34]</ref>, CUMUL <ref type="bibr" target="#b24">[26]</ref>, k-FP <ref type="bibr" target="#b13">[15]</ref>, and k-NN <ref type="bibr" target="#b36">[38]</ref> attacks for both Case I and Case II in <ref type="table" target="#tab_1">Table 2</ref>. We also show the comparison of the performance of Mockingbird with two state-of-the-art defenses WTF-PAD <ref type="bibr" target="#b15">[17]</ref> and Walkie-Talkie (W-T) <ref type="bibr" target="#b37">[39]</ref>. In <ref type="table" target="#tab_2">Table 3</ref>, we show the comparison of Top-2 accuracy of Mockingbird , WTF-PAD defense, and W-T defense.</p><p>Bandwidth Overhead. As described in Section 4.4, we designed Mockingbird to minimize packet addition by keeping the amount of change in a trace by a threshold value of 0.0001. Keeping the amount of change at a minimum keeps the bandwidth overhead as low as possible. We show comparison against WTF-PAD and W-T because these are the two lightweight and state-of-the-art defenses, while the other proposed WF defenses have two-to-three times higher delays and bandwidth overheads. From <ref type="table" target="#tab_1">Table 2</ref>, we can see that, with full-duplex (FD) network traffic, the bandwidth overhead in Case I and Case II of Mockingbird are the same 56.5% and at least 7% and 15% lower than WTF-PAD and W-T, respectively. In contrast, the bandwidth overhead for half-duplex (HD) network traffic is lower only for Case I than WTF-PAD and W-T. 9</p><p>In HD network traffic, Case II requires at least 10% higher bandwidth overhead than Case I. In terms of bandwidth overhead, Case I is better for both FD and HD network traffic, although not to a significant degree. Overall, Case II for HD network traffic provides the strongest defense (28.8% accuracy) but requires the highest cost (73.5% bandwidth overhead) for any of the lightweight defenses.</p><p>In summary, our bandwidth overheads are better than WTF-PAD and W-T in most of the cases, except one. This amount of bandwidth overhead can be acceptable in comparison to other WF defenses for Tor.</p><p>Attack Accuracy. Interestingly, we observe that the accuracies of the DF attack in Case I for both FD and HD network traffic are 35.2% and 35.5%, respectively, indicating that DF attack fails against our defense. In addition, the DF attack accuracies of Mockingbird are significantly lower than that of WTF-PAD, and slightly lower than that of W-T. More precisely, we can see from <ref type="table" target="#tab_1">Table 2</ref> that accuracies are at least 49% and 0.5% lower than that of WTF-PAD and W-T.</p><p>In Case II, the accuracy of the DF attack against FD network traffic is 55.5% which is higher than W-T but at least 34% lower than WTF-PAD. Surprisingly, the HD adversarial traces provide the best performance against the DF attack and can drastically drop the accuracy down to 28.8%, showing that DF attack also fails against Case II of our defense. In addition, the DF attack accuracy of Case II HD is at least 56% and at least 7% lower than that of WTF-PAD and W-T.</p><p>The AWF attack accuracies of Mockingbird for all the cases are lower than that of WTF-PAD and W-T defenses. The highest accuracy of AWF of Mockingbird is 34% which is still 5% lower than that of WTF-PAD and W-T defenses.</p><p>The CUMUL attack can achieve at best 36.8% accuracy against FD network traffic of Case I. In all the other cases, the CUMUL attack accuracies against our defense are closer to 30% or lower. We also want to point out that, CUMUL attack accuracies against our defense are lower than that of WTF-PAD and closer to W-T (see <ref type="table" target="#tab_1">Table 2</ref>). It is important to note that the CUMUL attack is also unsuccessful against our defense. This results show that the effectiveness of Mockingbird is not limited to only against the deeplearning based DF attack, but also traditional machine-learning based CUMUL attack.</p><p>Though our defense has lower accuracy than WTF-PAD against k-FP attack, for two cases it is slightly higher than W-T, and and for one case equal to W-T. However, k-NN performs worst against our defense in all the cases than WTF-PAD and W-T.</p><p>It is also notable that both Case I and Case II provide consistencies between the attack accuracies for both FD and HD network traffic against all the attacks. It is interesting that all the attacks accuracies against Case I FD is lower than that of Case I HD. On the other hand, attacks accuracies against Case II FD is higher than that of Case II HD. Overall, the results demonstrate that Mockingbird is effective and has lower attack performance compared to the state-of-the-art WF defenses, WTF-PAD and W-T.</p><p>Top-k Accuracy. In general, prior works have focused their analysis on Top-1 accuracy, which is normally referred to simply as the accuracy of the attack. Top-1 accuracy, however, does not provide a full picture of the effectiveness of a defense, as an attacker may use additional insights about their target (language, locale, interests, etc) to further deduce what website their target is likely to visit. Furthermore, most WF defenses are evaluated in a simulated environment like our own. These simulators abstract away networking issues that may occur when the defense is implemented and limited by real-world network conditions. These real-world limitations can produce imperfections when applying the defense and may expose the user to increased fingerprintability.</p><p>As such, it is desirable to examine the accuracy of Top-k predictions, where k is the number of top-ranked classes in the prediction. In evaluations of WF, we are particularly interested in the Top-2 accuracy. A high Top-2 accuracy indicates that the classifier is able to reliably determine the identity of a trace to one of two candidate websites. This is a threat to a user even when the attacker is unable to use additional information to predict the true site. The knowledge that the target may be visiting a sensitive site is actionable and can encourage the attacker to expend additional resources to further analyze their target.</p><p>To better evaluate the efficacy of Mockingbird against this threat, we compute the Top-2 accuracy and compare it to that of WTF-PAD and W-T. The results show that Mockingbird is somewhat resistant to Top-2 identification, with an accuracy of 56.9% in the worst case. On the other hand, W-T struggles in this scenario with 97% Top-2 accuracy as its defense design does not protect beyond confusion between two classes. In addition, WTF-PAD also has a very high 89.7% Top-2 accuracy. This Top-2 accuracy of Mockingbird indicates and ensures notably lower risk of de-anonymization of Tor client than WTF-PAD and W-T.</p><p>We also analyze the Top-3 to Top-10 accuracy and discuss why Mockingbird is good at restricting the Top-k accuracy in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Information Leakage Analysis</head><p>Recent works have shown that classification accuracy is not a complete metric to evaluate the objective efficacy of a WF defense <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b17">19]</ref>. To address this issue, we have adopted the WeFDE information leakage estimation technique <ref type="bibr" target="#b17">[19]</ref> for our defense's evaluation. The WeFDE technique allows us to determine how many bits of information each feature leaks for a given defense. The authors of the WeFDE paper made their code available to us, and for speed and memory requirements, we re-implemented most of the code ourselves and validated our common results against theirs. We perform information leakage analysis on undefended full-duplex traffic and defended traffic for 3043 features spread across 14 categories. The defenses we examine are WTF-PAD, W-T, and the full-duplex Case I variant of Mockingbird. The leakage for each feature and defense is shown in <ref type="figure" target="#fig_10">Figure 9</ref>. It is to note that our Timing and Pkt. per Second categories do not include W-T or Mockingbird measurements as the simulations for these defenses are unable to produce accurate timestamp estimations.</p><p>In general, we find that the information leakage of Mockingbird to be comparable to the other defenses. We find that any given feature leaks at most 1.9 bits of information in Mockingbird. W-T has similar maximum leakage, with at most 2.2 bits per feature, while WTF-PAD leaks at most 2.6 bits. The maximum amount of leakage seen for the undefended traffic was 3.4 bits, nearly twice that of Mockingbird. Our defense seems to be most vulnerable to the N -gram class of features, which is not surprising, as these features seem to be effective for all traffic defenses we examined. On the other hand, our defense is quite effective against packet count features, which was previously the most significant class of features for the undefended traffic and is also useful against W-T. Additionally, Mockingbird shows notable improvements over W-T in the Transposition, First 30, Last 30, and Burst features, while W-T is better than Mockingbird in the Pkt. Distribution feature.</p><p>Overall, the results of our information leakage analysis are largely in line with what we see in our Top-1 accuracy measurements: W-T and Mockingbird achieve similar performance, with Mockingbird slightly edging out W-T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION 6.1 Deployment Challenges</head><p>Generating adversarial examples for WF defense is different than that of other domains such as computer vision or voice recognition. The main challenges we face in crafting adversarial examples for web traffic traces are as follows:</p><p>No Packet Dropping, only Insertion: Perturbation to traffic traces can be done only through the sending of the dummy packets. Dropping real packets to perturb the traffic trace is undesirable as dropped packets will need to be re-queued before subsequent packets, causing time delays. To overcome this challenge, we introduce constraints in our algorithm to insure that the packet burst sizes are only ever increased (see <ref type="bibr">Section 4.4)</ref>.</p><p>Two Parties are Involved: Traffic traces are a combination of download and upload streams. Neither the client nor the server has a full control of both streams. Therefore, the WF defense has two elements to control both sides of the traffic: the client side and a bridge in the network. This dynamic imposes unique restrictions on the implementation of our design which do no exist in other domains for adversarial examples.</p><p>Crafting Adversarial Examples on the Fly: To craft the adversarial examples from traffic traces, we have to deal with the transmission of packets. Ideally, the traffic trace should be captured packet by packet and the adversarial examples should be generated in real time so that we can send and receive packets simultaneously. In addition, as the entire trace is not accessible to the algorithm during the application of the defense, the adversarial examples should be generated on the fly. Furthermore, the client must use a pre-trained detector that models the attacker when generating the adversarial traces. These detectors are deep-learning classification models, and consequently require significant computing capacity to generate traces for each site visit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation &amp; Deployment Plan</head><p>In order to deploy our defense in the realistic world, it is necessary to address several outstanding issues.</p><p>Live Trace Generation: Our investigations have yet to find a solution which allows for live packet-by-packet generation of adversarial traces. Consequently, our defense requires that the full traffic burst sequence be known before generating an associated adversarial example. This requires that the defense implementation maintains a database of relatively recent reference burst sequences for sites of interest. The responsibility for gathering and maintaining this database would most appropriately be given to the Tor network. In particular, special directory servers could be deployed to maintain the database and distribute fresh reference sequences periodically to clients. This same mechanism may be used to distribute the pre-trained detector models to clients. The clients can then use this data to generate adversarial traces locally before each site visit.</p><p>Padding Mechanism: Padding mechanisms must be designed so they can manipulate a trace sequence to match an adversarial sequence. To address this problem, burst molding must be performed by both the client and bridge, similar to that of the W-T defense. Burst molding is difficult to achieve in a live setting as padding must be added to the end of bursts, however, the end of a burst is difficult to reliably identify. To address this, we propose that a burst molding mechanism hold packets in a queue for each burst. The queue is dumped onto the network only after a timeout threshold expires after no additional packets are received from downstream. The size of the next burst can be easily communicated from the client to the bridge by embedding this information in the dummy packets of the prior burst. In this way, bursts can be easily molded to their appropriate size at the cost of some additional delay.</p><p>Computation Requirements: The adversarial trace generation process currently requires several hundred iterations, where the reference trace is perturbed, and the resulting trace is checked against the detector each iteration. For our defense to be deployable, the full generation process must take at most a few seconds before each visit. As previously noted, significant computation resources (e.g. a dedicated GPU device) is currently necessary to generate within our time limitations. Fortunately, there are possible solutions to this problem. At the moment, our algorithm performs adversarial trace generation using the state-of-the-art classifiers. These classifiers were designed to be used by comparatively high resource attackers who have access to GPU arrays that a client would likely not. This highlights a need for an alternative model that is more efficient than its state-of-the-art brethren. Works in other domains have explored techniques that allow models to be run efficiently by low-resource entities such as mobile devices. Techniques such as model compression <ref type="bibr" target="#b12">[14]</ref> and data quantization <ref type="bibr" target="#b39">[41]</ref> have been shown to provide massive speedups in resource constrained environments with relatively small performance degradation. These techniques can be leveraged to create efficient detector models that consumer-grade devices can easily run to generate traces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Server-Side Defense</head><p>Cherubin et al. <ref type="bibr" target="#b9">[11]</ref> proposed to apply website fingerprinting defenses at the application layer, which is especially beneficial to onion services that are accessed only through Tor. A server-side application of Mockingbird could be an effective and practical way to defend a site from attacks. It would involve the website operator running a tool to examine their site trace from Tor, running Mockingbird to generate a set of effective traces, and then adding dummy web objects and padding existing web objects to specific sizes (following the techniques of Cherubin et al. <ref type="bibr" target="#b9">[11]</ref>) to modify the network trace as needed. A web server can generate traces as needed during down times. Also, depending on the threat model of the site, it may only need to create a new trace periodically, such as once per day or per hour. At that rate of change, the attacker would need to download a new set of traces very frequently, increasing the attack cost and potentially making the attack less stealthy for less popular onion sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We propose a novel WF defense, Mockingbird, that can fool the state-of-the-art WF attack with a significant amount of reduced attack accuracy. Our defense has lower bandwidth overhead than that of WTF-PAD and Walkie-Talkie, the state-of-the-art lightweight defenses. Mockingbird uses a novel mechanism that adapts techniques to create adversarial traces against machine-learning and deep-learning classifiers. It generates adversarial traces that can significantly limit the ability of a WF adversary to distinguish a site, even though the adversary adopts adversarial training to train his classifier. Our defense mechanism results in 56% bandwidth overhead and drops the accuracy of the state-of-the-art WF attack from 98% to 29%-57%, depending on the scenario. In addition, our defense performs better than the two state-of-the-art defenses: WTF-PAD and W-T. The Top-2 attack accuracy of our defense against the DF attack is at most 57%, whereas it is 90% for WTF-PAD and 97% for W-T. The results of information leakage analysis are in-line with our previous conclusions when evaluating raw accuracy metrics. We emphasize that our experiments are conducted in the closed-world setting, where the attacker knows that the Tor client is assumed to visit one of the monitored sites. In a more realistic open-world setting, where the client could visit any site on the Web, 57% accuracy is very likely to lead to many false positives for the attacker.</p><p>Although Mockingbird has implementation challenges that must be addressed before it could be practically deployed in Tor, it shows the significant potential of an approach inspired by adversarial examples. Furthermore, it may be possible to leverage Mockingbird for server-side defense in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PERFORMANCE OF C&amp;W METHOD</head><p>In order to evaluate the efficacy of the adversarial traces generated by the Carlini and Wagner <ref type="bibr" target="#b6">[8]</ref> method described in Section 4.3, we consider two different attack scenarios based on when the defenses are applied. The defense can be applied either after the attack (Without-Adversarial-Training) or before the attack (With-Adversarial-Training). We evaluate both scenarios in the following sections. We use half-duplex (HD) closed-world dataset for these experiments. We explain the pre-processing of the data in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Without-Adversarial-Training</head><p>In this scenario, we assume that the attacker has trained its classifier on the traces that have not been defended. We assume that the attacker is not aware of a defense in place. Such a scenario can be valid in the case that we have an attacker that does not target any specific client and his goal is to identify the behavior of large number of users, but some of the clients may use some defense to protect their traffic.</p><p>For this scenario, we first train a classifier on our undefended traces and then we generate the adversarial traces. We examine the efficacy of the generated samples by testing them against the trained attacker model.</p><p>In our evaluation we break the data into two sets, Adv Set, and Detector Set, each set has 83 classes with 360 instances each. The attacker trains the classifier on the Detector Set. The traces in Detector Set are not protected by any defenses. The WF attacks that we apply on the Adv Set are the DF and CUMUL attacks. We chose DF attack as a state-of-the-art WF attack representing deep-Learning (DL) based WF attack and CUMUL attack as the representative of the machine-learning (ML) based WF attack. CUMUL attack uses SVM classifier and has high performance compared to other tradition WF attacks <ref type="bibr" target="#b34">[36]</ref>. We apply the method described in 4.3 to generate adversarial traces from the traces in Adv Set, we call these traces in our evaluation as Adversarial Traces. To generate Adversarial Traces, we use a simple CNN as the target model (F) and the adversarial traces will be generated based on this simple CNN. The architecture of the simple CNN is shown in <ref type="table" target="#tab_4">Table 4</ref>.   <ref type="table" target="#tab_5">Table 5</ref> shows the results of our evaluations. Adversarial Traces add 62% bandwidth overhead. Adversarial Traces generated for Simple CNN can confound the target model 98% of the times. In addition, the accuracy of DF and CUMUL attacks are 3% and 31%. This means that Adversarial Traces generated based on a target model with simple CNN architecture can be highly transferable to other machine learning models. Almost all the adversarial traces generated by simple CNN can confound DF attack, which is also a deep learning model. The results show that the adversarial traces are more transferable to DL model than traditional ML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 With-Adversarial-Training</head><p>In this scenario, we assume that the attacker knows that the client is using some sort of defense mechanisms to protect her traffic. The attacker then collects the traces protected by the same method as the client and trains his classifier with those traces. In this scenario, the training set and testing set are both traces protected by the same WF defense method. This scenario is more realistic because it has been shown that the effectiveness of the WF attacks depends on the attacker's knowledge of the clients <ref type="bibr" target="#b14">[16]</ref>. Moreover, once a defense is deployed it is supposed to be accessible for all the users and used by all the users. Therefore, the attacker can also use the same defense as other clients.</p><p>For evaluation in this scenario, we protect the traces in Adv Set by Adversarial Traces (described in Section 4.3 using a target model with the architecture in <ref type="table" target="#tab_4">Table 4</ref>). Then we train the simple CNN, DF, and CUMUL attacks, on 90% defended traces and test them with the remaining 10% of defended traces.</p><p>To generate Adversarial Traces, we train a target model with the same architecture as simple CNN with the traces in Detector Set and used in generating Adversarial Traces on Adv Set. The results of the evaluation in this scenario are shown in <ref type="table" target="#tab_3">Table 6</ref>. As shown in the table, even when adversarial traces are generated based on a target model with similar architecture as simple CNN, they are highly detectable on simple CNN as we train simple CNN on the adversarial traces, and its accuracy is 91%. Moreover, DF and CUMUL attacks can also detect the adversarial traces with high accuracy, 97% and 91%, respectively. This means that generated adversarial traces are ineffective when the adversary adopts the technique of adversarial training. Generating the adversarial traces works like a data augmentation technique in this case. If the attacker is trained on them, the attacker will detect them correctly. This highlights the necessity of the creation of a new adversarial example generation technique designed specifically for WF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TUNED C&amp;W METHOD</head><p>We also mixed Mockingbird with Carlini and Wagner (C&amp;W) method <ref type="bibr" target="#b6">[8]</ref>. We only show the evaluation for half-duplex (HD) dataset. In this implementation instead of gradient of distance, we optimized the adversarial traces over the loss function. Let's assume we have trace I s (belonging to source class s) and the generated adversarial trace is I new s :</p><formula xml:id="formula_9">I new s = I s + ∆<label>(10)</label></formula><p>Where ∆ the perturbation added to the source trance. We define ∆ in a way that it does not add too much overhead:</p><formula xml:id="formula_10">∆ = δ 2 * scale<label>(11)</label></formula><p>Where δ is the perturbation optimized by the algorithm and the scale is a variable to adjust the overhead not to go above a maximum allowed overhead. The power 2 is to force only positive perturbations. We define constant variable MAX as the maximum allowed overhead to the traces. MAX should be between 0 and 1 which lead to 0 to 100% bandwidth overhead, respectively. </p><p>For each source trace I s , we pick a target class t. We keep minimizing the objective function till I new s is classified as class t for maximum k iterations. At the end of k iterations, if I new s is still not in class t, we change target t to another class and minimize the objective function over the new target. We change the target maximum T times.</p><p>The architecture of our surrogate network is shown in <ref type="table" target="#tab_4">Table 4</ref>. This surrogate network works as the detector (or model F ). We trained the detector on Detector Set D and generate adversarial traces on Adv Set A on our half-duplex dataset. <ref type="figure" target="#fig_0">Figure 10</ref> shows the accuracy of the DF attack on generated adversarial traces compared to W-T and Mockingbird HD. In this experiment we set MAX equal to 0.5 and 0.7 which lead to overhead 50% and 70%, respectively.</p><p>We kept the maximum target change, T , fixed as 8 and changed the maximum iteration of iterations k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C WEAKER SURROGATE NETWORK</head><p>One of the concerns of generating adversarial traces with respect to a powerful detector is that the powerful detector today can be obsolete tomorrow and a more powerful attack might make this defense ineffective. To address this concern, we generate adversarial traces with respect to a very week detector. However, we still use a powerful attack model DF for adversarial training. The architecture of this detector is shown in <ref type="table" target="#tab_4">Table 4</ref>. We only show the experimental results for half-duplex dataset. Our results show that our defense remain effective even though we use a weak surrogate network. The bandwidth overheads for Case I and Case II is 62% and 73%, respectively. The Top-1 attack accuracy is 35% and 29% for Case I and Case II, respectively. We can see the Top-1 to Top-10 accuracy in <ref type="figure" target="#fig_0">Figure 11</ref>. Top-10 accuracy for both Case I and Case II is lower than 90%. In particular, Top-10 accuracies for both Case I and Case II are 86% and 81%, respectively. D ANALYSIS OF TOP-K ACCURACY <ref type="figure" target="#fig_0">Figure 12</ref>: Top-k Accuracy: full-duplex and half-duplex. xaxis represents the Top-k where k=1....10 and y-axis represents the attack accuracy. Red and Green solid line represents the results for full-duplex dataset and dashed line represents the results for half-duplex dataset.</p><p>We can see from <ref type="figure" target="#fig_0">Figure 12</ref> that Mockingbird can limit the Top-10 accuracies of full-duplex (FD) data to 84% and 92% for Case I and Case II, respectively. For half-duplex (HD), Top-10 accuracies are 86% and 80% for Case I and Case II. These set of results show that Mockingbird is better to limit the Top-k accuracy to a significant degree than that of W-T and WTF-PAD defenses. Mockingbird is doing such a good job because of the way it picks a target trace. Robust randomness to pick a target trace and the role of detector enables Mockingbird to be resistant to Top-k accuracy. We mention in our defense design that Mockingbird picks a new target trace each time detector indicates the adversarial sample confidence is not lower than a threshold value. In this way, the final generated adversarial trace is a result of picking multiple target trace. We visualize the total number of changes of the target trace for both  full-duplex and half-duplex datasets in a box-plot in <ref type="figure" target="#fig_0">Figure 13</ref>. For FD Case I, the median, 1 st quartile, 3 r d quartile, lower whisker, and upper whisker are 400, 248, 704, 57, and 1193, respectively. For Case II, the median, 1 st quartile, 3 r d quartile, lower whisker, and upper whisker are 522, 285, 843, 65, and 1546, respectively. For HD Case I, the median, 1 st quartile, 3 r d quartile, lower whisker, and upper whisker are 609, 419, 744, 85, and 1218, respectively. For Case II, the median, 1 st quartile, 3 r d quartile, lower whisker, and upper whisker are 360, 174, 617, 23, and 1026, respectively.</p><p>For a better understanding of the target change, we analyse site-5. In our dataset, FD site 5 and HD site 5 is the same. We can see the box-plot of the number of target change in <ref type="figure" target="#fig_0">Figure 14</ref>. The median number of target changes for FD are 77 and 74 for Case I and Case II, respectively. For HD, median number of target changes are 2 and 1 for Case I and Case II, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Website Fingerprinting Attack Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A visualization of bursts, with six outgoing bursts interspersed with six incoming bursts.As we mentioned in above section, adversarial examples are generated from the distribution of correctly classified samples with slight perturbations. Moreover, adversarial examples have decent resistance against the existing defenses and they can remain undetectable. These two properties are compelling for developing a WF defense. In addition, adversarial examples can be general and transferable in that the adversarial examples crafted for a certain 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Mockingbird Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The CDF of the umber of bursts in the full duplex and half duplex traces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>α Value for full-duplex: the accuracy and bandwidth overhead of generated samples as α varies. Dashed lines show the results of Case I and solid lines show the results of Case II.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Evaluation Scenarios. Mockingbird changes the source sample toward a target sample drawn randomly from our target pool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>α Value for half-duplex: the accuracy and bandwidth overhead of generated samples as α varies. Dashed lines show Case I and solid lines show Case II.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Half-duplex: the attacker's accuracy and the bandwidth overhead of the generated samples as we vary the number of iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Full-duplex: the attacker's accuracy and the bandwidth overhead of the generated samples as we vary the number of iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Individual feature information leakage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>S)Figure 11 :Figure 10 :</head><label>1110</label><figDesc>= (1 + MAX ) * Size(I s ) − Size(I new s Weak Detector: Half-duplex Performance. x-axis represents the Top-k where k=1....10 and y-axis represents the attack accuracy. DF attack accuracy as k (the number of iterations) varies. Given a sample I s and a model F , the algorithm finds a perturbation δ that makes I new s = I s + ∆ to be misclassified to target class t. The algorithm tries to find I new sthat is similar to I s . The algorithm will find δ such that it minimizes the objective function f (I new s ). C&amp;W<ref type="bibr" target="#b6">[8]</ref> explored several objective functions and found that the best objective function is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Total Number Target Changes for both HD and FD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Site-5: Number of Target Changes in both HD and FD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset Split: Adv Set &amp; Detector Set. FD: Full-Duplex, HD: Half-Duplex, NC: Number of Classes, NI: Number of Instances, CW: Closed-World, OW: Open-World.</figDesc><table><row><cell></cell><cell cols="4">Adv Set A Detector Set D</cell><cell>CW</cell><cell>OW</cell></row><row><cell></cell><cell cols="2">(NC × N I )</cell><cell cols="2">(NC × N I )</cell><cell>Total</cell></row><row><cell cols="2">FD</cell><cell>95×259</cell><cell>95×259</cell><cell></cell><cell cols="2">95×518 40,716</cell></row><row><cell cols="2">HD</cell><cell>83×360</cell><cell>83×360</cell><cell></cell><cell cols="2">83×720 40,000</cell></row><row><cell>Attack Accuracy</cell><cell>0.45 0.55 0.65 0.75</cell><cell></cell><cell cols="2">Case I Case II</cell><cell></cell><cell>0.45 0.55 0.65 0.75</cell><cell>Bandwidth Overhead</cell></row><row><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell></row><row><cell></cell><cell>0.25</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>0.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell>α Value</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The Evaluation of the Mockingbird against DF, AWF, CUMUL, k-FP, and k-NN Attacks &amp; Comparison against WTF-PAD and W-T Defenses. S×I: Sites×Instances, BWO: Bandwidth Overhead, FD: Full-Duplex, HD: Half-Duplex.</figDesc><table><row><cell>Cases</cell><cell>Dataset</cell><cell>S×I</cell><cell cols="5">BWO DF [36] AWF [34] CUMUL [26] k-FP [15] k-NN [38]</cell></row><row><cell></cell><cell cols="2">Undefended (FD) 95×259</cell><cell>-</cell><cell>0.95</cell><cell>0.88</cell><cell>0.93</cell><cell>0.85</cell><cell>0.86</cell></row><row><cell></cell><cell cols="2">Undefended (HD) 83×360</cell><cell>-</cell><cell>0.98</cell><cell>0.93</cell><cell>0.92</cell><cell>0.92</cell><cell>0.90</cell></row><row><cell></cell><cell>WTF-PAD [17]</cell><cell cols="2">95×463 0.640</cell><cell>0.85</cell><cell>0.39</cell><cell>0.55</cell><cell>0.44</cell><cell>0.17</cell></row><row><cell></cell><cell>W-T [39]</cell><cell cols="2">82×360 0.720</cell><cell>0.362</cell><cell>0.39</cell><cell>0.36</cell><cell>0.30</cell><cell>0.35</cell></row><row><cell>Case I</cell><cell cols="3">Mockingbird (FD) 95×259 0.565 Mockingbird (HD) 83×360 0.627</cell><cell>0.352 0.355</cell><cell>0.20 0.27</cell><cell>0.232 0.300</cell><cell>0.21 0.33</cell><cell>0.04 0.06</cell></row><row><cell>Case II</cell><cell cols="3">Mockingbird (FD) 95×259 0.565 Mockingbird (HD) 83×360 0.735</cell><cell>0.555 0.288</cell><cell>0.34 0.22</cell><cell>0.368 0.316</cell><cell>0.34 0.30</cell><cell>0.06 0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Top-2 Accuracy of DF Attack against Mockingbird and W-T. FD: Full-duplex, HD: Half-Duplex.</figDesc><table><row><cell>Cases</cell><cell>Dataset</cell><cell>DF [36] Top-2 Accuracy</cell></row><row><cell></cell><cell>WTF-PAD [17]</cell><cell>0.897</cell></row><row><cell></cell><cell>W-T [39]</cell><cell>0.970</cell></row><row><cell>Case I</cell><cell>Mockingbird (FD) Mockingbird (HD)</cell><cell>0.514 0.528</cell></row><row><cell>Case II</cell><cell>Mockingbird (FD) Mockingbird (HD)</cell><cell>0.569 0.432</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>The evaluation of the defenses against the state-ofthe-art WF attacks as the attackers are trained on the defended traces. BWO: Bandwidth Overhead, CNN is the simple CNN ofTable 4.</figDesc><table><row><cell></cell><cell cols="4">BWO CNN DF [36] CUMUL [26]</cell></row><row><cell>Undefended</cell><cell>-</cell><cell>92%</cell><cell>98%</cell><cell>92%</cell></row><row><cell cols="2">Adversarial Traces 62%</cell><cell>91%</cell><cell>97%</cell><cell>91%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Model Architecture of Simple CNN.</figDesc><table><row><cell>Layer type</cell><cell>size</cell></row><row><cell>Convolution + ReLU</cell><cell>1 × 8 × 32</cell></row><row><cell>Convolution + ReLU</cell><cell>1 × 8 × 64</cell></row><row><cell>Convolution + ReLU</cell><cell>1 × 8 × 128</cell></row><row><cell>Fully Connected + ReLU</cell><cell>512</cell></row><row><cell cols="2">Fully Connected + Softmax number of classes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The evaluation of the defenses against the state-ofthe-art WF attacks as the attackers are trained on the undefended traces. BWO: Bandwidth Overhead, CNN is the simple CNN ofTable 4.</figDesc><table><row><cell></cell><cell cols="4">BWO CNN DF [36] CUMUL [26]</cell></row><row><cell>Undefended</cell><cell>-</cell><cell>92%</cell><cell>98%</cell><cell>92%</cell></row><row><cell cols="2">Adversarial Traces 62%</cell><cell>2%</cell><cell>3%</cell><cell>31%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Tor bridges are usually used for evading censorship, but they can be used for prototyping WF defenses such as used in WTF-PAD<ref type="bibr" target="#b15">[17]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">DEFENSE DESIGN We now motivate and describe the design of the Mockingbird defense, starting with a brief background on adversarial examples and adversarial training, then showing the limitations of applying existing techniques for generating adversarial examples, and finally showing the Mockingbird design in detail.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This material is based upon work supported by the National Science Foundation (NSF) under Grants Numbers 1423163, 1722743, and 1816851.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resources</head><p>The code and datasets will be released upon the publication of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fingerprinting attack on Tor anonymity using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia Pacific Advanced Network (APAN)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Var-CNN: A Data-Efficient Website Fingerprinting Attack Based on Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjit</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Privacy Enhancing Technologies Symposium (PETS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CS-BuFLO: A Congestion Sensitive Website Fingerprinting Defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Nithyanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Privacy in the Electronic Society (WPES)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Systematic Approach to Developing and Evaluating Website Fingerprinting Defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Nithyanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer and Communications Security (CCS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Magnet and&quot; efficient defenses against adversarial attacks&quot; are not robust to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08478</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 38th IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>eeding of the 38th IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISec@CCS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">2017. Bayes, not Naïve: Security bounds on website fingerprinting defenses. Privacy Enhancing Technologies Symposium (PETS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Website fingerprinting defenses at the application layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Cherubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Juarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Privacy Enhancing Technologies Symposium (PETS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Peek-a-Boo, I Still See You: Why Efficient Traffic Analysis Countermeasures Fail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Coull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Shrimpton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization, and Huffman Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">2016. k-fingerprinting: A Robust Scalable Website Fingerprinting Technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Danezis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium. USENIX Association</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Critical Evaluation of Website Fingerprinting Attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Juarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadia</forename><surname>Afroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunes</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Greenstadt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer and Communications Security (CCS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Toward an Efficient Website Fingerprinting Defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Juárez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Research in Computer Security (ESORICS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ensemble Adversarial Training: Attacks and Defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measuring information leakage in website fingerprinting attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Hopper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer and Communications Security (CCS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HTTPOS: Sealing Information Leaks with Browser-side Obfuscation of Encrypted Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiapu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network &amp; Distributed System Security Symposium (NDSS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">DeepFool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Seyed-Mohsen Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1511.04599</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: A Bespoke Website Fingerprinting Defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Nithyanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Privacy in the Electronic Society (WPES)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: A Bespoke Website Fingerprinting Defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Nithyanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Privacy in the Electronic Society (WPES)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Website Fingerprinting at Internet Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Lanze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Pennekamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zinnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Henze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Wehrle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network &amp; Distributed System Security Symposium (NDSS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Asia Conference on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1511.07528</idno>
		<title level="m">The Limitations of Deep Learning in Adversarial Settings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:arXiv:1511.04508</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Experimental Defense for Website Traffic Fingerprinting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Perry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Tor Project Blog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Tor Protocol Specification Proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Perry</surname></persName>
		</author>
		<ptr target="https://gitweb.torproject.org/torspec.git/tree/proposals/254-padding-negotiation.txt" />
		<imprint>
			<date type="published" when="2015-02-15" />
		</imprint>
	</monogr>
	<note>Padding Negotiation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Certified defenses against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payap</forename><surname>Mohammad Saidur Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Sirinam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kantha</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Girish Gangadhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06421</idno>
		<title level="m">Tik-Tok: The utility of packet timing in website fingerprinting attacks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated Website Fingerprinting through Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Rimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Preuveneers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Juarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Van Goethem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Joosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network &amp; Distributed System Security Symposium (NDSS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Timing Analysis in Low-latency Mix Networks: Attacks and Defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsiu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Research in Computer Security (ESORICS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Fingerprinting: Undermining Website Fingerprinting Defenses with Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payap</forename><surname>Sirinam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Imani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Juarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer and Communications Security (CCS)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Effective Attacks and Provable Defenses for Website Fingerprinting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Nithyanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium. USENIX Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Walkie-talkie: An efficient defense against passive website fingerprinting attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium. USENIX Association</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Traffic morphing: An efficient defense against statistical traffic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cv V Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Se E Coull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monrose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network &amp; Distributed System Security Symposium (NDSS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Trained Ternary Quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenzhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
