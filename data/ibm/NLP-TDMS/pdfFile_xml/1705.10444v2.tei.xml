<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Person Re-identification: Clustering and Fine-tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Person Re-identification: Clustering and Fine-tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Large-scale person re-identification</term>
					<term>unsuper- vised learning</term>
					<term>convolutional neural network</term>
					<term>clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this paper, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between 1) pedestrian clustering and 2) fine-tuning of the convolutional neural network (CNN) to improve the original model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning when the model is weak, CNN is fine-tuned on a small amount of reliable examples which locate near to cluster centroids in the feature space. As the model becomes stronger in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/ Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised Person Re-identification:</p><p>Clustering and Fine-tuning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hehe Fan, Liang Zheng and Yi Yang</head><p>Abstract-The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this paper, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between 1) pedestrian clustering and 2) fine-tuning of the convolutional neural network (CNN) to improve the original model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning when the model is weak, CNN is fine-tuned on a small amount of reliable examples which locate near to cluster centroids in the feature space. As the model becomes stronger in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/ Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.</p><p>Index Terms-Large-scale person re-identification, unsupervised learning, convolutional neural network, clustering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HIS paper discusses person re-identification (re-ID), a task which provides critical insights whether a person re-appears in another camera after being spotted in one <ref type="bibr" target="#b0">[1]</ref>. Recent works view it as a search problem aiming to retrieving the relevant images to the top ranks <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>Large-scale learning methods based on the convolutional neural network (CNN) constitute the recent advances in this field. Broadly speaking, current deep learning methods can be categorized into two classes, i.e., similarity learning and representation learning. For the former, the training input can be image pairs <ref type="bibr" target="#b3">[4]</ref>, triplets <ref type="bibr" target="#b4">[5]</ref>, or quadruplets <ref type="bibr" target="#b5">[6]</ref>. These methods directly output the similarity score of two input images by focusing on their distinct body patches, without an explicit feature extraction process. This line of methods are less efficient during testing. On the other hand, when learning feature representations, contrastive loss or softmax loss are most commonly employed. For these methods, there is an H. Fan, L. <ref type="bibr">Zheng</ref>  explicit feature extraction process for the query and gallery images, which is learned to discriminate among a pool of identities or among image pairs/triplets <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Then the re-ID procedure is rather like a retrieval problem under some commonly used similarity measurement such as Euclidean distance. Due to its efficiency in large-scale galleries and the potentials in further acceleration, the deep feature learning strategy has become more popular <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Therefore, we adopt the classification model proposed in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref> for feature learning.</p><p>Despite the impressive progress achieved by the deep learning methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b7">[8]</ref>, most of these methods focus on re-ID with sufficient training data in the same environment. In fact, the lack of labeled training data under a new environment has been addressed by many previous works by resorting to unsupervised or semi-supervised learning <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. These works typically deal with the relatively small datasets <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> and to our knowledge, none of these works are integrated into the deep learning framework that has been proven as the state of the art under the large-scale datasets <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In fact, the problem of high annotation cost is more severe for deep learning based methods due to the intensive demand of data. So under this circumstances, we make initial attempts by developing a easy-to-implement method for unsupervised deep representation learning in the re-ID community. Our method can be easily extended to the semi-supervised setting by annotating a small amount of bounding boxes. We also notice some works on humanin-the-loop learning <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, introducing labels by human feedback. We envision our work is complementary to this line of methods.</p><p>In this paper, we aim at finding "better" feature representations without labeled training data to adapt to the unlabeled dataset. Named "progressive unsupervised learning" (PUL), our method is an iterative process and each iteration consists of two components (shown in <ref type="figure">Fig. 1</ref>). 1) We deploy the model pre-trained on some external labeled data to extract features of images from the unlabeled training set. Standard k-means clustering is performed on the CNN features, followed by a selection operation to choose reliable samples.</p><p>2) The original model is fine-tuned using the selected training samples. We then use this newborn model to extract features and begin another iteration of training. At the beginning, when the model is weak, the proposed approach learns from a small amount of reliable examples, which are near to cluster centroids in the feature space, to avoid getting stuck at a bad optimum or oscillating. As the model becomes stronger in subsequent iterations, more data instances are adaptively selected and This process can also be called self-paced learning <ref type="bibr" target="#b21">[22]</ref>. The contributions of this paper are three-fold: 1) Among the early efforts, we propose an easy-toimplement unsupervised deep learning framework for largescale person re-ID.</p><p>2) We are the first to discover the underlying relation between clustering and self-paced learning and integrate the self-paced learning paradigm into the unsupervised learning regime. This learning strategy facilitates PUL to extract faithful knowledge from highly unreliable clustering results in learning.</p><p>3) Extensive experiments on three large-scale datasets indicate that our method noticeably improves the re-ID accuracy in the cross-dataset evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deeply Learned Person Representations</head><p>Deep learning models have been popular since Krizhevsky et al. <ref type="bibr" target="#b22">[23]</ref> won ILSVRC12 by a large margin. Deeply learned person representations are producing state-of-the-art re-ID performance recently. The first two works in re-ID to use deep learning were <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b24">[25]</ref>. Generally speaking, from the perspective of model, classification models as used in image classification <ref type="bibr" target="#b22">[23]</ref> and siamese models that use image pairs <ref type="bibr" target="#b25">[26]</ref> or triplets <ref type="bibr" target="#b26">[27]</ref> are two types of CNN models that are commonly employed in re-ID. At the beginning when the training datasets are not big enough, such as VIPeR <ref type="bibr" target="#b27">[28]</ref> that provides only two images for each identity, the siamese model dominates re-ID community. As the scale of re-ID dataset becomes large, such as Market-1501 <ref type="bibr" target="#b1">[2]</ref>, the classification model is widely employed. A survey of re-ID can be viewed in <ref type="bibr" target="#b0">[1]</ref>.</p><p>On the other hand, comparing with deep similarity learning methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, the representation learning methods are more extendable for large galleries. For example, Hermans et al. <ref type="bibr" target="#b7">[8]</ref> used an efficient variant of the triplet loss based on the distance between samples. Another popular choice consists in training an identification network and extracting the intermediate output as a discriminative embedding <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. For example, Xiao et al. <ref type="bibr" target="#b30">[31]</ref> propose an online instance matching loss to address the problem of having only few training samples each class. Lin et al. <ref type="bibr" target="#b9">[10]</ref> combine attribute classification losses and the re-ID loss for embedding learning. In order to exploit more training data, Zheng et al. <ref type="bibr" target="#b2">[3]</ref> employ the generative adversarial network to generate samples which are expected to generate uniform prediction probabilities in the softmax layer. In this paper, we adopt the baseline identification model, named "ID-discriminative Embedding" (IDE) in <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised Person Re-ID</head><p>Although deeply learned person representations have achieved significant progress by supervised learning methods, less attention is paid on unsupervised learning for re-ID. Kodirov et al. <ref type="bibr" target="#b10">[11]</ref> utilize a graph regularized dictionary learning to capture discriminative cues for cross-view identity matching. Yang et al. <ref type="bibr" target="#b13">[14]</ref> propose a weighted linear coding method to learn multi-level descriptors from raw pixel data in an unsupervised manner. Wang et al. <ref type="bibr" target="#b31">[32]</ref> use a kernel subspace learning model to learn cross-view identity-specific information from unlabeled data. Peng et al. <ref type="bibr" target="#b11">[12]</ref> propose a multi-task dictionary learning model to transfer a view-invariant representation from a number of existing labeled source datasets to an unlabeled target dataset. Ma et al. <ref type="bibr" target="#b12">[13]</ref> utilize image-sequence information to fuse different descriptions. These methods focus on small-scale datasets and do not adopt deep features.</p><p>Another choice in unsupervised re-ID consists in deploying hand-crafted features directly. Several effective descriptors have been developed in recent years. In the earlier works, Farenzena <ref type="bibr" target="#b32">[33]</ref> et al. use the weighted color histogram, the maximally stable color regions, and the recurrent highstructured patches to segment the pedestrian foreground from the background. Gray and Tao <ref type="bibr" target="#b27">[28]</ref> use 8 color channels and 21 texture filters on the luminance channel, and the pedestrian is partitioned into horizontal stripes. Recently, Zhao et al. <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> use the 32-dim LAB color histogram and the 128dim SIFT descriptor are extracted from each 10 × 10 patch densely sampled with a step size of 5 pixels. Liao et al. <ref type="bibr" target="#b36">[37]</ref> propose the local maximal occurrence (LOMO) descriptor, which includes the color and SILTP histograms. LOMO is later employed by <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> and a similar set of features is used by Chen et al. <ref type="bibr" target="#b39">[40]</ref>. Zheng et al. <ref type="bibr" target="#b1">[2]</ref> propose extracting the 11-dim color names descriptor for each local patch, and aggregating them into a global vector through a Bag-of-Words model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-paced Learning</head><p>In the human learning process, knowledge is imparted in the form of curriculum and organized from easy to difficult. Inspired by this process, the theory of Curriculum Learning (CL) <ref type="bibr" target="#b40">[41]</ref> is proposed to accelerate the speed of convergence of the training process to a minimum. The main challenge of CL is that it requires a known separation to indicate whether a sample in a given training dataset is easy or hard. To alleviate this deficiency, self-paced learning <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> embeds easiness identification into model learning. The learning principle behind SPL is to prevent latent variable models from getting stuck in a bad local optimum or oscillating. Then SPL is widely used in weakly-supervised and semi-supervised learning. For example, Zhang et al. <ref type="bibr" target="#b43">[44]</ref> integrate self-paced learning into multi-instance learning for co-Saliency detection.</p><p>In this paper, we utilize SPL to select reliable samples to fine tune the original model. If we consider the labels of the samples from the unlabeled dataset as latent variables, our method also belongs to latent variable model. Therefore, it is necessary to avoid getting stuck in bad optimums or oscillating using SPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROGRESSIVE UNSUPERVISED LEARNING</head><p>In this section, we present our progressive unsupervised learning framework for re-ID in detail. The proposed approach begins with a basic convolutional neural network, such as the ResNet-50 <ref type="bibr" target="#b44">[45]</ref> network trained on the ImageNet. Firstly, we fine tune the basic model by an irrelevant labeled dataset, which is stored as the original model. This step is also called original model initialization. Secondly, the original model is used to extract feature for the samples from the unlabeled dataset. The data instances are clustered and selected to generate a reliable training set. Thirdly, the original model is fine tuned by this reliable training set. Lastly, we use the new model to extract feature, and the second and third steps are repeated until the scale of the reliable training set becomes stable. The progressive unsupervised learning framework is demonstrated in <ref type="figure">Fig 1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Formulation</head><p>Suppose the unlabeled dataset contains N cropped person</p><formula xml:id="formula_0">images {x i } N i=1 with K identities. Since the data is not labeled, we consider the identities {y i } N i=1 of {x i } N i=1</formula><p>as latent variables. Therefore, our PIL belongs to latent variable model.</p><p>Let v i be the selection indicator of sample x i . If v i equals to 1, x i is selected as a reliable sample; otherwise, x i is discarded when fine tuning the original model. We further denote v = [v 1 , . . . , v N ] as the selection indication vector and y = [y 1 , . . . , y N ] ∈ {1, . . . , K} N as the label vector</p><formula xml:id="formula_1">for {x i } N i=1 .</formula><p>The CNN model is as denoted φ(·; θ), which is parameterized by θ. After φ(·; θ), a one-dimensional feature vector is generated, which is fed into a classifier parameterized by w. The θ and w are optimized simultaneously under a classification model.</p><p>We formulate our idea as optimizing the following three problems alternately:</p><formula xml:id="formula_2">min y,c1,...,c K K k=1 yi=k φ(x i ; θ) − c k 2 . (1) min v K k=1 yi=k v i φ(x i ; θ) − c k − λ v 1 , s.t. v i ∈ {0, 1}; yi=k v i ≥ 1, ∀k. (2) min θ,w N i=1 v i L(y i , φ(x i ; θ); w),<label>(3)</label></formula><p>where c k is the mean of points {φ(x i ; θ)} yi=k in the feature space, λ is a positive parameter and L denotes a loss function for classification. The first step (Eq. 1) is to infer the images' labels y by the results of the standard k-means clustering. Since the noisy clustering results will make this latent variable model prone to getting stuck in a bad local optimum or oscillating, it is inappropriate to directly use them to fine-tune the CNN model.</p><p>To alleviate the above problem, rather than considering all samples simultaneously, our PUL selects the training data in a meaningful order that facilitates learning. The order of the samples is determined by how reliable they are. The second step (Eq. 2) is to select reliable data instances that are close enough to the cluster centroids. We formulate this operation as self-paced learning <ref type="bibr" target="#b21">[22]</ref>. The "easiness" self-paced learning is equal to "reliability" in PUL, and in essence, it depends on the Euclidean distance in clustering. The sample closer to the centroid is considered as an easy and reliable training instance. The constraint yi=k v i ≥ 1 can guarantee that each cluster contains at least one reliable sample. The third step (Eq. 3) is to fine-tune the model by the clustering and selection results from Eq. 1 and Eq. 2. Generally speaking, there are three types of loss functions for CNN models are commonly employed in the community. The first one is the classification method, in which a classifier is added at the end of model φ(x; θ) as a fully-connected layer. When v i equals to 0, the loss will not be calculated, which means the corresponding sample will not be used to fine tune the original model. Eq. 3 can be replaced with the contrastive loss <ref type="bibr" target="#b25">[26]</ref> or triplet loss <ref type="bibr" target="#b26">[27]</ref>. In this paper, we mainly focus on the classification model.</p><p>Eq. 1, Eq. 2 and Eq. 3 constitute a training iteration. As training goes on, we obtain more discriminative CNN models which facilitate the clustering process with smaller clustering errors. That is, with the optimization of Eq. 1 and Eq. 3, the clustering error φ(x i ; θ) − c k 2 will become smaller and smaller. From the perspective of feature representations, the optimization of Eq. 3 will change the distribution of data instances in the feature space, which makes the images corresponding to the same person get closer to cluster centroid. This leads to an "self-paced" scheme in which more selection indicators v i are expected to become 1 in order to minimize 2. Therefore, more and more examples are selected into the training set, until no more reliable data instances can be added. <ref type="figure" target="#fig_0">Fig 2 illustrates</ref> how the training set changes during optimization. We also list two visual examples of this "selfpaced" process from the following experiments in <ref type="figure" target="#fig_2">Fig 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization Procedure</head><p>The optimization Eq. 1, Eq. 2 and Eq. 3 can be solved in an alternate manner.</p><p>1) Optimize y and c 1 , ..., c K when θ and v are fixed. In this step, the optimization problem reduces to the classic k-means clustering and can be easily solved.</p><p>2) Optimize v when θ, y and c 1 , ..., c K are fixed. The goal of this step is to select reliable data instances to fine tune the CNN model with the clustering results y in Eq. 1. To minimize the first component of Eq. 2, all selection indicators {v i } can be trivially set to be 0, which means all shots are not selected. However, the second component l 1 -norm − v 1 monotonically increases as the number of selected shots decreases. So it encourages all selection indicators to Algorithm 1: Progressive Unsupervised Learning fine-tune &lt; φ(·, θ o ), w &gt; with selected samples → θ t ; 18 end be 1. By making a balance between these two components, the objective function aims to training the CNN model with reliable shots. A sample will be selected if the distance of its feature to its corresponding cluster centroid is smaller than λ.</p><formula xml:id="formula_3">Input : Unlabeled data {x i } N i=1 ; Reliability threshold λ; Number of clusters K; Original model φ(·, θ o ). Output: Model φ(·, θ t ). Initialization: θ o → θ t . 1 while not convergence do 2 randomly initialize w; 3 extract feature: f i = φ(x i , θ t ) for all i; 4 k-means: update {y i } N i=1 and {c k } K k=1 ; 5 select center features: {c k } K k=1 → {f k } K k=1 ; 6 l 2 -normalize {f i } N i=1</formula><p>Here, λ is a threshold to control the reliable sample selection.</p><p>3) Optimize θ and w when v, y and c 1 , ..., c K are fixed. At the beginning of each iteration, the w is initialized randomly.</p><p>In practice, we use cosine to measure the similarity between two feature vectors. The optimization algorithm of PUL is presented in Alg. 1. To guarantee each cluster contains at least one reliable sample, we choose the nearest feature to the corresponding centroid as the center of the cluster. In the algorithm, samples with f i · f k &gt; λ will be selected for fine-tuning (v i = 1); otherwise, sample will not be selected (v i = 0). When the number of selected reliable samples is saturated, the algorithm will converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semi-supervised PUL</head><p>The PUL framework can be easily extended to semisupervised learning. To integrate the labeled data into the training process, we just need to directly add the labeled data into the selected reliable training set in every iteration.</p><p>The semi-supervised PUL is a general case of our model. When all training data is labeled, since there is no need to infer labels or select reliable samples, the semi-supervised PUL degenerates supervised learning; when no labeled data is available, the method has to rely on clustering and self-paced learning to obtain reliable training data.  IV. EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Settings</head><p>We mainly evaluate the proposed method on DukeMTMC-reID <ref type="bibr" target="#b2">[3]</ref> and Market-1501 <ref type="bibr" target="#b1">[2]</ref>, because they are two relatively large datasets with multiple cameras. We also report results on CUHK03 <ref type="bibr" target="#b24">[25]</ref>, which is a relatively small dataset. We do not use MARS <ref type="bibr" target="#b28">[29]</ref>, another large-scale dataset, because it is an extension of Market-1501 and hence has a similar data distribution with Market-1501.</p><p>DukeMTMC-reID is a subset of the pedestrian tracking dataset DukeMTMC <ref type="bibr" target="#b45">[46]</ref>. This dataset contains 36,411 images with 1,812 identities captured from 8 different view points. Following Market-1501, the dataset is also split into three parts: 16,522 images with 702 identities for training, 17,661 images with 1,110 identities in gallery, and another 2,228 images with 702 identities in the gallery for query. Images in this dataset vary in size. For simplicity, we use "Duke" to represent this dataset.</p><p>Market-1501 contains 32,668 images of 1,501 identities in total, which are captured from 6 cameras placed in front of a campus supermarket. The images are detected and cropped using the Deformable Part Model (DPM) <ref type="bibr" target="#b47">[48]</ref>. The dataset is split into three parts: 12,936 images with 751 identities for training, 19,732 images with 750 identities for gallery, and another 3,368 hand-drawn images with the same 750 gallery identities for query. All the images are of the size 128 × 64. For simplicity, we use "Market" to represent this dataset.</p><p>CUHK03 contains 14,096 images of 1,467 identities. Each identity is captured from 2 cameras in the CUHK campus, and has an average of 4.8 images in each camera. The dataset provides both manually labeled images and DPMdetected images. We experiment on the DPM-detected images. Note that the original evaluation protocol of CUHK03 has 20 train/test splits. Nevertheless, to be in consistency with Market-1501 and Duke-reID, we use the train/test protocol proposed in <ref type="bibr" target="#b46">[47]</ref>: 7,365 images with 767 identities for training, 5,332 images with the remaining 700 identities for gallery, and 1,400 images with the same 700 gallery identities for query. Images in this dataset also vary in size. We list the statistics and some some samples of the these three datasets in <ref type="table" target="#tab_2">Table I</ref> and <ref type="figure" target="#fig_3">Fig 4.</ref> We report the rank-1,5,10,20 accuracy and mean average precision (mAP) for all the three datasets. All experiments use single query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We use ResNet-50 <ref type="bibr" target="#b44">[45]</ref> pre-trained on ImageNet as our basic CNN model. To fine tune the model using the classification framework <ref type="bibr" target="#b0">[1]</ref>, we modify the fully-connected layer to adapt to different datasets. We also insert a dropout layer before the fully-connected layer and set the dropout rate to 0.5 for all datasets. All images are resized to 224 × 224. For data augmentation, we randomly rotate images within 20 degrees and shift them horizontally and vertically within 45 pixels. Batch size is set to 16. We use stochastic gradient descent with a momentum of 0.9 and the learning rate is set to 0.001 without any decay during the whole training process. Unless otherwise specified, ResNet-50 is fine-tuned for 20 epochs within each PUL iteration.</p><p>During feature extraction, for a given image, we use the output of the average-pooling layer as the visual representation. The l 2 normalized representations are used in sample selection and retrieval, while clustering uses features without normalization (l 2 normalization in clustering yields inferior results in our preliminary experiments).</p><p>For the clustering step, we use k-means++ <ref type="bibr" target="#b48">[49]</ref> to select initial cluster centers for k-mean clustering to speed up convergence. The maximum number of iterations of the k-means algorithm within each PUL iteration is set to 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of Baseline System</head><p>The basic ResNet-50 model is fine-tuned for 40 epochs on each training set. Note that our focus is not on how to improve the performance by modifying neural networks <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref> or post-processing <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Therefore, results in <ref type="table" target="#tab_2">Table II</ref> do not represent the state of the art.</p><p>We compare PUL with deploying the fine-tuned CNN model on an unseen testing dataset (baseline). Results are shown in <ref type="table" target="#tab_2">Table II</ref>.The reliability threshold λ is set to be 0.85 for all the three datasets. Since Duke, Market and CUHK03 have 702, 751 and 767 training IDs respectively, the number of clusters K is set to be 700 for Duke, and 750 for Market and CUHK03. When the fine-tuned model is directly deployed on other datasets, re-ID accuracy is far inferior to training/testing on the same dataset. For example, the CNN model fine-tuned and tested on Market achieves 76.2% in rank-1 accuracy, but drops to 21.9% when trained on Duke and tested on Market. This is because of the changes in data distribution between different datasets. In stark contrast, PUL effectively improves D. Evaluation of PUL 1) Ablation study -PUL without sample selection: The selection of reliable samples for CNN fine-tuning is a key component in the PUL system. Here we investigate how the system works without reliable sample selection. In our experiment, for each PUL iteration, we use all the samples as training data for CNN fine-tuning. That is, after k-means clustering, all the images in each cluster are viewed as a class and fed into CNN.</p><p>Results are presented in <ref type="table" target="#tab_2">Table III</ref>, <ref type="figure">Fig. 5</ref>(c) and <ref type="figure">Fig. 5(d)</ref>. Comparing with the baseline results in <ref type="table" target="#tab_2">Table II</ref>, we hardly observe any noticeable improvement of using PUL without sample selection. When K = 1, 250, the largest gain over the baseline we can obtain is +1.4% in rank-1 accuracy, and +1.2% in mAP. This indicates that the samples within each cluster are very noisy, and that if we use all the noisy samples for fine-tuning without any selection, the learned CNN model gets stuck in a bad optimum. These results suggest that reliable sample selection is a necessary component in PUL.</p><p>2) Further understanding of PUL with parameter changes: Two important parameters are involved in this paper, i.e., the reliability threshold λ and the number of clusters K. To this end, we mainly deploy the fine-tuned ResNet-50 model on Duke described in Section IV-C. We apply PUL using this CNN model on Market-1501 without any labels. PUL is always trained for 25 iterations.</p><p>First, we evaluate the impact of K (we fix λ to 0.85), which is sampled from {250, 500, 750, 1, 000, 1, 250}, because Market has 751 training identities (not known in practice). Results are shown in <ref type="figure">Fig. 6</ref>. Since the Market dataset has 751 identities, K = 750 achieves the best performance as expected.</p><p>Second, we evaluate the impact of λ, keeping K = 750. Since we use the cosine distance to measure the similarity of To further narrow the test range for λ, we present the proportions of selected samples under different λ in the first training iteration in <ref type="figure">Fig. 5(a)</ref>. When λ &lt; 0.7, nearly all of data instances are selected as being reliable, which is undesirable. Therefore, we choose λ from {0.70, 0.75, 0.80, 0.85, 0.90}. <ref type="figure">Fig. 5(b)</ref> demonstrates the change of the portion of selected samples during training. As training goes on, more and more samples are selected. Let us take λ = 0.85 for instance. Before the 9th training iteration, the proportion of selected samples increases fast from 31.8% to 45.3%. After that, the number oscillates between 45.3% and 47.4%. The curve becomes saturated after 20 epochs, which indicates that PUL training can be stopped when the number of selected samples converges.</p><p>An interesting phenomenon in <ref type="figure">Fig 5(b)</ref> is that, when λ ≤ 0.85, the proportion of reliable samples in the 1st training iteration witnesses a sudden drop after the 2nd training iteration. This is because, in the 1st training iteration, the original model fine-tuned on Duke is too weak to discriminate the data from Market. As a result, too many samples are incorrectly selected as reliable samples. In the 2nd training iteration, the model has been fine-tuned on Market and can better separate reliable and unreliable samples on Market.</p><p>We report the re-ID performance with different λ in <ref type="figure">Fig 5(c)</ref> and <ref type="figure">Fig 5(d)</ref>. In general, performance improves with more iterations, indicating that the model gradually gets stronger. Specifically, λ = 0.85 yields superior accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Semi-supervised Re-ID</head><p>The semi-supervised learning scheme is briefly introduced in Section III-C. In this case, we initialize the CNN model with labeled images of L IDs. These labeled images are also used in the subsequent CNN fine-tuning iterations. In our experiment, we set L = 25 and L = 50. We report the results in <ref type="table" target="#tab_2">Table IV</ref>. Two major observations can be made.</p><p>First, comparing with the unsupervised learning method (PUL), adding 25 or 50 IDs as supervised information notably improves the re-ID accuracy. For example, when testing on Market-1501 with 25 labeled IDs, Duke and CUHK03, we observe improvement of +2.2%, +2.7% and +0.8% in rank-1 accuracy, respectively. The improvement on CUHK03 is smaller compared with Market and Duke. It is probably because of the severe illumination change and less diversified training samples in CUHK03: learning an invariant feature is extremely difficult.</p><p>Second, we observe that using more IDs as training samples is beneficial to the system. For example, when testing on Market-1501, using 50 IDs brings about +3.2% more improvement than using 25 IDs. On the other two testing sets, the additional improvement brought by the 50 IDs is +3.8% and +0.2% in rank-1 accuracy, respectively. These results suggest that using more labeled data will increase the model performance at the cost of higher labeling cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with Non-deep Methods</head><p>We then compare the PUL method with competing unsupervised methods. As we mentioned in Section II-B, limited attention has been paid to unsupervised learning for re-ID and fewer works have adopted deep features. Since these unsupervised methods mainly focus on re-ID on relatively small datasets, benchmarking results are lacking on larger datasets such as Market. In this section, we evaluate the performances of two hand-crafted features, i.e., Bag-of-Words (BoW) <ref type="bibr" target="#b1">[2]</ref>, local maximal occurrence (LOMO) <ref type="bibr" target="#b36">[37]</ref> and one transfer  <ref type="table" target="#tab_2">(FINE-TUNED RESNET-50  ON TWO OF THE THREE DATASETS AND TESTED ON THE LAST ONE), AND PUL (BOTH THE UNSUPERVISED AND THE SEMI-SUPERVISED VERSIONS ARE   SHOWN</ref> learning method, i.e., the Unsupervised Multi-task Dictionary Learning (UMDL) model <ref type="bibr" target="#b11">[12]</ref> on the large-scale datasets. To our knowledge, except hand-crafted features, UMDL is the only work on unsupervised re-ID whose code is released. Note that, for BoW and LOMO, there is no training process. We directly apply these two hand-crafted features on the test datasets. After feature extraction, we normalize the feature vectors and use Euclidean distance to measure the similarity.</p><p>UMDL needs multiple datasets to learning a taskindependent dictionary. To meet this requirement, we use two of the three datasets as labeled datasets and treat the remaining one as an unlabeled dataset. Since the released code of UMDL does not include feature extraction, we use the BoW feature proposed in <ref type="bibr" target="#b1">[2]</ref> for this method. Since the computational complexity of UMDL is prohibitively high, we choose 2,500 images from each dataset in training. For PUL, we still fix the reliability threshold λ to 0.85 for all the three datasets. The number of clusters K is set to 750 for Market and CUHK03, and 700 for Duke. Results are presented in <ref type="table" target="#tab_2">Table IV</ref>, from which two conclusions can be drawn.</p><p>First, when comparing <ref type="table" target="#tab_2">Table II and Table IV</ref>, we find that initialization using more labeled datasets does not noticeably improve re-ID accuracy. Sometimes, multi-dataset initialization yields even worse results than single dataset initialization. For example, when tested on duke, PUL achieves 30.0% in rank-1 accuracy when initialized by both Market+CUHK03 but 30.4% in rank-1 accuracy when initialized only by Market. The baseline approach also exhibits similar phenomenon when tested on CUHK03. This indicates that using more labeled datasets may be not a best choice for unsupervised re-ID. In comparison, PUL with unlabeled data can always improve the re-ID accuracy, regardless of the datasets used for CNN initialization.</p><p>Second, both the CNN baseline and PUL outperform UMDL by a large margin. For example, when tested on three datasets, the CNN baseline outperforms UMDL by +5.5%, +4.4%, and +2.9% in rank-1 accuracy, respectively. When PUL is performed, we observe larger performance gaps, arriving at +8.7%, +11.5% and +6.7% in rank-1 accuracy on the three datasets, respectively. Under the semi-supervised setting (Section III-C), the superiority of PUL is even more significant. For example, when using 50 labeled IDs, PUL outperforms UMDL by +15.3%, +17.0%, and +7.5% in rank-1 accuracy on the three datasets, respectively. In fact, as more data is labeled, UMDL does not exhibit noticeable improvement, while PUL shows clear performance gain. Moreover, considering the training inefficiency of UMDL, we can thus validate the effectiveness of PUL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, a simple progressive unsupervised learning (PUL) method is proposed for person re-ID, based on the iterations between k-means clustering and CNN fine-tuning. We show that reliable sample selection is a key component to its success. Progressively, the proposed method produces CNN models with high discriminative ability.</p><p>The proposed method is easy to implement and a number of further extensions can be made. For example, as mentioned in <ref type="bibr" target="#b0">[1]</ref>, in video re-ID, frames within a tracklet can be viewed as belonging to the same ID, which can be used to initialize clustering and fine-tuning. Another promising idea is to integrate diversity into PUL, so that training samples from multiple cameras can be selected for fine-tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the learning process. Reliable samples covered in the large blue circle are used for CNN fine-tuning. As the CNN model gets finetuned, more challenging training data can be mined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>; 7 for k = 1 to K do 8 for i = 1 to N do 9 calculate inner product δ i = f i · f k ; 10 if δ i &gt; λ then 11 v i ← 1 ;</head><label>78910111</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Two visual examples of the working mechanism of progressive unsupervised learning (PUL). Each color denotes a distinct ID and the red circles represent reliable areas. From training iteration 2 to training iteration 3, more samples are selected reliable samples. In PUL, when the number of selected samples is saturated, the algorithm reaches convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Sample images from Duke, Market and CUHK03 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>The impact of λ on re-ID accuracy (K = 750). Since we use cosine distance, a larger λ means a stricter sample selection process, and vice versa.(a): proportion of selected samples in the 1st training iteration; (b): proportion of selected samples during the whole training process; (c) and (d): performance changes during whole training process. The baseline is plotted in the dashed horizontal line. Impact of the number of clusters K on re-ID accuracy. (λ = 0.85). (a): rank-1 accuracy. (b): mAP. two feature vectors and ResNet uses ReLU (rectified linear unit) as activation function, the range of λ should be [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Y. Yang are with the Center for Artificial Intelligence, University of Technology Sydney, Australia. Email: hehe.fan@student.uts.edu.au, liang.zheng@uts.edu.au, yi.yang@uts.edu.au.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF DUKE [3], MARKET [2] AND CUHK03 [25] DATASETS. NOTE THAT WE USE THE TRAIN/TEST PROTOCOL PROPOSED IN [47] FOR CUHK03. FOR MARKET AND CUHK03, THE QUERY AND GALLERY SHARE THE SAME IDS. HOWEVER, FOR DUKE, EXCEPT FOR THE SHARED 702 IDS, THE GALLERY CONTAINS ANOTHER 408 IDS.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">training</cell><cell></cell><cell></cell><cell>test</cell><cell></cell></row><row><cell>datasets</cell><cell># cams</cell><cell cols="2"># IDs # images</cell><cell cols="4">query # IDs # images # IDs # images gallery</cell></row><row><cell>Duke</cell><cell>8</cell><cell>702</cell><cell>16,522</cell><cell>702</cell><cell>2,228</cell><cell cols="2">1,110 17,661</cell></row><row><cell>Market</cell><cell>6</cell><cell>751</cell><cell>12,936</cell><cell>750</cell><cell>3,368</cell><cell>750</cell><cell>19,732</cell></row><row><cell>CUHK03</cell><cell>2</cell><cell>767</cell><cell>7,365</cell><cell>700</cell><cell>1,400</cell><cell>700</cell><cell>5,332</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II PERSON</head><label>II</label><figDesc>RE-ID ACCURACY (RANK-1,5,10,20 PRECISION AND MAP) OF TWO METHODS: THE BASELINE (FINE-TUNED RESNET-50 ON ONE OF DATASETS AND TESTED ON ANOTHER ONE) AND PUL. NOTE THAT PUL WORK ON UNSUPERVISED SETTINGS WHICH NEED THE BASELINE MODEL FOR INITIALIZATION. rank-5 rank-10 rank-20 mAP rank-1 rank-5 rank-10 rank-20 mAP rank-1 rank-5 rank-10 rank-20 mAP</figDesc><table><row><cell cols="2">methods rank-1 supervised learning 63.4</cell><cell>78.9</cell><cell>Duke 83.3</cell><cell>87.4</cell><cell>42.6</cell><cell>76.2</cell><cell>89.5</cell><cell>Market 93.3</cell><cell>95.8</cell><cell>53.1</cell><cell>24.8</cell><cell>41.1</cell><cell>CUHK03 52.4</cell><cell>64.2</cell><cell>23.2</cell></row><row><cell>baseline (Duke)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.1</cell><cell>52.8</cell><cell>60.9</cell><cell>69.2</cell><cell>14.2</cell><cell>4.4</cell><cell>9.9</cell><cell>13.7</cell><cell>19.4</cell><cell>4.0</cell></row><row><cell>PUL (Duke)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>44.7</cell><cell>59.1</cell><cell>65.6</cell><cell>71.7</cell><cell>20.1</cell><cell>5.6</cell><cell>11.2</cell><cell>15.8</cell><cell>22.7</cell><cell>5.2</cell></row><row><cell>baseline (Market)</cell><cell>21.9</cell><cell>38.3</cell><cell>44.4</cell><cell>50.5</cell><cell>10.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.5</cell><cell>10.7</cell><cell>14.4</cell><cell>19.2</cell><cell>4.4</cell></row><row><cell>PUL (Market)</cell><cell>30.4</cell><cell>44.5</cell><cell>50.7</cell><cell>56.0</cell><cell>16.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7.6</cell><cell>13.8</cell><cell>18.4</cell><cell>25.1</cell><cell>7.3</cell></row><row><cell>baseline (CUHK03)</cell><cell>14.9</cell><cell>25.5</cell><cell>31.4</cell><cell>37.5</cell><cell>7.0</cell><cell>30.0</cell><cell>46.4</cell><cell>53.9</cell><cell>61.3</cell><cell>11.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PUL (CUHK03)</cell><cell>23.0</cell><cell>34.0</cell><cell>39.5</cell><cell>44.2</cell><cell>12.0</cell><cell>41.9</cell><cell>57.3</cell><cell>64.3</cell><cell>70.5</cell><cell>18.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III RE</head><label>III</label><figDesc>-ID ACCURACY OF PUL WITHOUT SAMPLE SELECTION.</figDesc><table><row><cell># clusters (K)</cell><cell>250</cell><cell>500</cell><cell>750</cell><cell>1,000</cell><cell>1,250</cell></row><row><cell>rank-1</cell><cell>32.2</cell><cell>34.6</cell><cell>37.3</cell><cell>36.7</cell><cell>37.5</cell></row><row><cell>mAP</cell><cell>12.8</cell><cell>14.5</cell><cell>15.7</cell><cell>14.6</cell><cell>15.4</cell></row><row><cell cols="6">the re-ID accuracy over the baseline. For example, when using</cell></row><row><cell cols="6">Duke to train the original CNN model, PUL leads to an</cell></row><row><cell cols="6">improvement of +8.6% in rank-1 accuracy and +5.9% in mAP</cell></row><row><cell>on Market-1501.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV PERSON</head><label>IV</label><figDesc>RE-ID ACCURACY (RANK-1,5,10,20 PRECISION AND MAP) OF FIVE MODELS: BOW, LOMO, UMDL, THE BASELINE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>).methods   Duke, CUHK03 → Market Market, CUHK03 → Duke Duke, Market → CUHK03 rank-1 rank-5 rank-10 rank-20 mAP rank-1 rank-5 rank-10 rank-20 mAP rank-1 rank-5 rank-10 rank-20 mAP</figDesc><table><row><cell>unsupervised</cell><cell cols="2">hand UMDL[12] BoW[2] LOMO[37] Deep feature (baseline) Deep feature (PUL)</cell><cell>35.8 27.2 34.5 40.0 45.5</cell><cell>52.4 41.6 52.6 56.5 60.7</cell><cell>60.3 49.1 59.6 63.7 66.7</cell><cell>67.6 57.2 68.0 70.2 72.6</cell><cell>14.8 17.1 8.0 12.3 12.4 18.5 17.0 22.9 20.5 30.0</cell><cell>28.8 21.3 31.4 37.3 43.4</cell><cell>34.9 26.6 37.6 44.2 48.5</cell><cell>41.2 33.3 44.7 50.8 54.0</cell><cell>8.3 4.8 7.3 11.3 16.4</cell><cell>2.1 0.6 1.4 4.3 8.1</cell><cell>4.6 1.9 5.0 10.1 15.8</cell><cell>7.0 3.6 7.1 15.1 22.0</cell><cell>11.2 5.4 9.7 19.4 28.5</cell><cell>1.9 0.7 1.3 3.9 7.9</cell></row><row><cell>semi-sup</cell><cell>25ID 50ID</cell><cell>UMDL[12] Deep feature (PUL) UMDL[12] Deep feature (PUL)</cell><cell>35.2 47.7 35.6 50.9</cell><cell>53.2 63.6 53.5 66.5</cell><cell>60.2 69.3 60.4 72.6</cell><cell>68.4 75.5 68.6 78.3</cell><cell>13.3 18.9 22.2 32.7 13.4 19.5 24.8 36.5</cell><cell>31.7 47.2 32.4 52.6</cell><cell>37.8 53.0 38.6 57.9</cell><cell>45.2 59.1 45.7 64.5</cell><cell>7.6 18.8 8.3 21.5</cell><cell>1.4 8.9 1.6 9.1</cell><cell>5.2 18.4 5.4 18.6</cell><cell>7.2 23.8 7.5 25.1</cell><cell>10.0 28.5 10.2 32.5</cell><cell>1.3 8.8 1.4 9.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by GAN improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person reidentification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised l 1 graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised video matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multilevel descriptors for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhancing person re-identification in a self-trained subspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</title>
		<meeting>IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05693</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-in-the-loop person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pop: Person reidentification post-rank optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CNN image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MARS: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07732</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01850</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards unsupervised openset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sample-specific SVM learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-paced cotraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Co-saliency detection via a self-paced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">k-means++: the advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Person reidentification via ranking aggregation of similarity pulling and dissimilarity pushing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2553" to="2566" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Query-adaptive late fusion for image search and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
