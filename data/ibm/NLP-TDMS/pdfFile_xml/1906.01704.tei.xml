<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Yang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Lei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Yuan</forename><surname>Zong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhen</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Song</surname></persName>
						</author>
						<title level="a" type="main">A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-EEG emotion recognition</term>
					<term>bi-hemispheric dis- crepancy</term>
					<term>spatial-temporal network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The neuroscience study <ref type="bibr" target="#b0">[1]</ref> has revealed the discrepancy of emotion expression between left and right hemispheres of human brain. Inspired by this study, in this paper, we propose a novel bi-hemispheric discrepancy model (BiHDM) to learn the asymmetric differences between two hemispheres for electroencephalograph (EEG) emotion recognition. Concretely, we first employ four directed recurrent neural networks (RNNs) based on two spatial orientations to traverse electrode signals on two separate brain regions, which enables the model to obtain the deep representations of all the EEG electrodes' signals while keeping the intrinsic spatial dependence. Then we design a pairwise subnetwork to capture the discrepancy information between two hemispheres and extract higher-level features for final classification. Besides, in order to reduce the domain shift between training and testing data, we use a domain discriminator that adversarially induces the overall feature learning module to generate emotion-related but domain-invariant feature, which can further promote EEG emotion recognition. We conduct experiments on three public EEG emotional datasets, and the experiments show that the new state-of-the-art results can be achieved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-The neuroscience study <ref type="bibr" target="#b0">[1]</ref> has revealed the discrepancy of emotion expression between left and right hemispheres of human brain. Inspired by this study, in this paper, we propose a novel bi-hemispheric discrepancy model (BiHDM) to learn the asymmetric differences between two hemispheres for electroencephalograph (EEG) emotion recognition. Concretely, we first employ four directed recurrent neural networks (RNNs) based on two spatial orientations to traverse electrode signals on two separate brain regions, which enables the model to obtain the deep representations of all the EEG electrodes' signals while keeping the intrinsic spatial dependence. Then we design a pairwise subnetwork to capture the discrepancy information between two hemispheres and extract higher-level features for final classification. Besides, in order to reduce the domain shift between training and testing data, we use a domain discriminator that adversarially induces the overall feature learning module to generate emotion-related but domain-invariant feature, which can further promote EEG emotion recognition. We conduct experiments on three public EEG emotional datasets, and the experiments show that the new state-of-the-art results can be achieved.</p><p>Index Terms-EEG emotion recognition, bi-hemispheric discrepancy, spatial-temporal network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Emotion, as a common mental phenomenon, is closely related to our daily life. Although it is easy to sense other people's emotion in human-human interaction, it is still difficult for machines to understand the complicated emotions of human beings <ref type="bibr" target="#b1">[2]</ref>. As the first step to make machines capture human emotions, emotion recognition has received substantial attention from human-machine-interaction (HMI) and pattern recognition research communities in recent years <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>Human emotional expressions are mostly based on verbal behavior methods (e.g., speech), and nonverbal behavior methods (e.g., facial expression). Thus, a large body of literature concentrates on learning the emotional components <ref type="bibr">Yang</ref>  from speech and facial expression data. However, from the viewpoint of neuroscience, humans emotion originates from a variety of brain cortex regions, such as the orbital frontal cortex, ventral medial prefrontal cortex, and amygdala <ref type="bibr" target="#b5">[6]</ref>, which provides us a potential approach to decode emotion by recording the continuous human brain activity signals over these brain regions. For example, by placing the EEG electrodes on the scalp, we can record the neural activities in the brain, which can be used to recognize human emotions.</p><p>Most existing EEG emotion recognition methods focus on two fundamental challenges. One is how to extract discriminative features related to emotions. Typically, EEG features can be extracted from the time domain, frequency domain, and time-frequency domain. In <ref type="bibr" target="#b6">[7]</ref>, Jenke et al. evaluated all the existing features by using machine learning techniques on a self-recorded dataset. The other challenge is how to classify the features correctly. Many EEG emotion recognition models and methods have been proposed over the past years <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. For example, Zheng et al. <ref type="bibr" target="#b9">[10]</ref> proposed a group sparse canonical correlation analysis method for simultaneous EEG channel selection and emotion recognition. Li et al. <ref type="bibr" target="#b10">[11]</ref> fused the information propagation patterns and activation difference in the brain to improve the performance of emotional recognition. These techniques have shown excellent performance on some EEG emotional datasets.</p><p>Recently, many researchers have attempted to consider the neuroscience findings of emotion as the prior knowledge to extract features or develop models, effectively enhancing the performance of EEG emotion recognition. For example, Hinrikus et al. <ref type="bibr" target="#b11">[12]</ref> used EEG spectral asymmetry index for the depression detection. It is well realized through neuroscience study that although the anatomy of human brain looks like symmetric, the left and right hemispheres have different responses to emotions. For example, from the view of neuroscience, Dimond et al. <ref type="bibr" target="#b0">[1]</ref>, Davidson et al. <ref type="bibr" target="#b12">[13]</ref>, and Herrington et al. <ref type="bibr" target="#b13">[14]</ref> have studied the asymmetry of emotion expression, and Schwartz et al. <ref type="bibr" target="#b14">[15]</ref>, Wager et al. <ref type="bibr" target="#b15">[16]</ref>, and Costanzo et al. <ref type="bibr" target="#b16">[17]</ref> have discussed the emotion lateralization. Furthermore, the literature of EEG emotion recognition has seen the use of asymmetry to classify EEG emotional signal. Lin et al. <ref type="bibr" target="#b3">[4]</ref> investigated the relationships between emotional states and brain activities, and extracted power spectrum density, differential asymmetry power, and rational asymmetry power as the features. Motivated by their previous findings of critical brain areas for emotion recognition, Zheng et al. <ref type="bibr" target="#b4">[5]</ref> selected six symmetrical temporal lobe electrodes as the critical channels for EEG emotion recognition. Li et al. <ref type="bibr" target="#b17">[18]</ref> separately extracted two brain hemispheric features and achieved the state-of-the-art classification performance. The above researches demonstrate that it is a promising and fruitful way to integrate the unique characteristics of EEG signal into the machine learning algorithms. It will be an interesting and meaningful topic of how to utilize this discrepancy property of two brain hemispheres to improve EEG emotion recognition.</p><p>Thus, in this paper, we propose a novel neural network model BiHDM to learn the bi-hemispheric discrepancy for EEG emotion recognition. BiHDM aims to obtain the deep discrepant features between the left and right hemispheres, which is expected to contain more discriminative information to recognize the EEG emotion signals. To achieve this goal, we need to solve two major problems, i.e., how to extract the features for each hemispheric EEG data and meanwhile measure the difference between them. Unlike other data structures such as skeletal action data, in which the position of each node varies with time, the EEG data consists of several electrodes that are set under the predefined coordinates on the scalp. Hence, to avoid losing this intrinsic graph structural information of EEG data, we can simplify the graph structure learning process by using the horizontal and vertical traversing RNNs, which will construct a complete relationship graph and generate discriminative deep features for all the EEG electrodes. After obtaining these deep features of each electrodes, we can extract the asymmetric discrepancy information between two hemispheres by performing specific pairwise operations for any paired symmetric electrodes. The concrete process is as follows:</p><p>(1) Firstly, we employ individual two RNN modules to separately scan all spatial electrodes' data on the left and right hemispheres and generate deep feature representations for all the EEG electrodes. Herein, when the RNN module traverses the spatial regions, it will walk under two predefined stack strategies determined with respect to the horizontal and vertical direction streams; (2) In each stream, we will obtain the deep features of all the electrodes, and perform specific pairwise operations for the paired electrodes. Herein, the rule of identifying the paired electrodes refers to the symmetric locations on the brain scalp, and the pairwise operations include subtraction, division, and inner product. These operations will model the discrepancy information from different aspects. Subsequently, another RNN summarizes all the asymmetric discrepancy information and produces a global deep representation in each directional stream; (3) Finally, we integrate the global features from horizon and vertical streams with learnable linear transformation matrices and use a classifier to map this representation into the label space. Considering the tremendous data distribution shift of EEG emotional signal, especially in the case of subject-independent task where the source (training) and target (testing) data come from different subjects, we leverage a domain discriminator that works cooperatively with the classifier to encourage the emotion-related but domain-invariant data representation appeared. To the best of our knowledge, this is the first work to integrate the electrodes' discrepancy relation on two hemispheres into deep learning models to improve EEG emotion recognition. The experimental results verify the discrimination and effectiveness of this differential information between the left and right hemispheres for EEG emotion recognition.</p><p>The remainder of this paper is organized as follows: In section II, we specify the method of BiHDM as well as its application to EEG emotion recognition. In section III, we conduct extensive experiments to evaluate the proposed method for EEG emotion recognition. In sections IV and V, we discuss the paper and conclude it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE PROPOSED MODEL FOR EEG EMOTION RECOGNITION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The BiHDM model</head><p>To specify the proposed method clearly, we illustrate the framework of the BiHDM model in <ref type="figure" target="#fig_0">Fig. 1</ref>. Its goal is to capture the asymmetric differential information between two hemispheres. We adopt three steps to achieve this goal. First, we obtain the deep representations of all the electrodes' data. Subsequently, we characterize the relationship between the identified paired electrodes on two hemispheres, and generate a more discriminative and higher-level discrepancy feature for final classification. Third, we leverage a classifier and a discriminator to corporately induce the above process to generate the emotion-related but domain-invariant features. The overall process is described as follows.</p><p>1) Obtaining the deep representation for each electrode: In BiHDM, we attempt to separately extract the EEG electrodes' deep features on left and right brain hemispheres by using two independent RNN modules. To avoid losing the intrinsic graph structural information of EEG data, for each hemispheric EEG data, we build the RNN module traversing the spatial regions under two predefined stacks, which are determined with respect to horizontal and vertical directions. These two directional RNNs are actually complementary for simplifying the technology to construct a complete relationship graph of electrodes' locations. Concretely, for an EEG sample X t , it can be split as</p><formula xml:id="formula_0">X t = [X l t , X r t ] = [x l 1 , · · · , x l N 2 , x r 1 , · · · , x r N 2 ] ∈</formula><p>R d×N , where X l t and X r t denote the EEG electrodes' data on the left and right hemispheres, d is the dimension of each EEG electrode's data and N is the number of electrodes. When modeling spatial dependencies, two graphs, i.e., G l ={N l , E l } and G r = {N r , E r }, are used to separately represent the electrodes' spatial relations on the left and right hemispheres, where N l = {x l i } and N r = {x r i }, (i = 1, 2, · · · , N 2 ) denote the electrode sets, while E l = {e l ij } and E r = {e r ij } represent the edges between spatially neighboring electrodes. Then we traverse through G l and G r separately with a predefined forward evolution sequence so that the input state and the previous states can be defined for an RNN unit. This process can be formulated as </p><formula xml:id="formula_1">s l i = σ(U l x l i + N/2 j=1 e l ij V l s l j + b l ) ∈ R d l ×1 ,<label>(1)</label></formula><formula xml:id="formula_2">s r i = σ(U r x r i + N/2 j=1 e r ij V r s r j + b r ) ∈ R dr×1 , (2)</formula><formula xml:id="formula_3">e · ij = 1, if x · j ∈ N (x · i ) , 0, otherwise,<label>(3)</label></formula><p>where s l i , s r i and d l , d r are the hidden units and the dimensions of RNN modules on the left and right hemispheres, respectively; σ(·) denotes the nonlinear operation such as Sigmoid function;</p><formula xml:id="formula_4">{U l ∈R d l ×d , V l ∈R d l ×d l , b l ∈R d l ×1 } and {U r ∈ R dr×d , V r ∈ R dr×dr , b r ∈ R dr×1</formula><p>} are the learnable transformation matrices of the two hemispheric RNN modules; and N (x · i ) denotes the set of predecessors of the node x · i . Here d l = d r . As the RNN modules traverse all the nodes in N l and N r , the obtained hidden states s l i and s r i can be used as the deep features to represent the i-th electrode's data on two hemispheres.</p><p>Particularly, for the left and right hemispheric RNN modules, they traverse the spatial regions under two predefined horizontal and vertical stacks. Therefore, we will obtain two paired deep feature sets, i.e., (</p><formula xml:id="formula_5">S lh t , S rh t ) and (S lv t , S rv t ), where S lh t = {s lh i } ∈ R d l ×(N/2) and S rh t = {s rh i } ∈ R dr×(N/2)</formula><p>represent the left and right hemispheric electrodes' deep features under horizontal direction, while S lv t = {s lv i } ∈ R d l ×(N/2) and S rv t = {s rv i } ∈ R dr×(N/2) represent the deep features under vertical direction. So far, we obtain the deep representation of each electrode, which has the emotional discriminative information and keeps the location structural relation.</p><p>2) Interaction between the paired electrodes on two hemispheres: After obtaining the deep features of every electrode above, i.e., (S lh t , S rh t ) and (S lv t , S rv t ), we perform a specific pairwise operation on the paired electrodes referring to the symmetric locations on the brain scalp to identify the asym-metric differential information between two hemispheres. This operation can be expressed aŝ</p><formula xml:id="formula_6">S h t = F(S lh t , S rh t ) = F({s lh i }, {s rh i }) ∈ R dp×(N/2) , (4) S v t = F(S lv t , S rv t ) = F({s lv i }, {s rv i }) ∈ R dp×(N/2) ,<label>(5)</label></formula><formula xml:id="formula_7">whereŜ h t = {ŝ h i } andŜ v t = {ŝ v i }</formula><p>are the deep asymmetric differential features, F(·) denotes the pairwise operation between any two paired electrodes' data representations. Concretely, for any paired data (s l· i , s r· i ), we perform subtraction, division and inner product on it, which can be formulated as</p><formula xml:id="formula_8">F({s l· i }, {s r· i }) =        {s l· i − s r· i } ∈ R dp 1 × N 2 , {s l· i /s r· i } ∈ R dp 2 × N 2 , {s l· i,1 · s r· i,1 + · · · + s l· i, N 2 · s r· i, N 2 } ∈ R dp 3 × N 2 ,<label>(6)</label></formula><p>where d p1 = d p2 = d l and d p3 = 1. 1 To further capture the higher-level discrepancy discriminative features, we utilize another RNN module that performs on the obtained differential asymmetric features {ŝ h i } and {ŝ v i } from the horizontal and vertical streams. Formally, the operations on them can be written as</p><formula xml:id="formula_9">s h i = σ(U hŝh i + V hsh i−1 + b h ) ∈ R dg×1 ,<label>(7)</label></formula><formula xml:id="formula_10">s v i = σ(U vŝv i + V vsv i−1 + b v ) ∈ R dg×1 ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_11">{U h ∈ R dg×dp , V h ∈ R dg×dg , b h ∈ R dg×1 } and {U v ∈ R dg×dp , V v ∈ R dg×dg , b v ∈ R dg×1</formula><p>} are the learnable parameter matrices, and d g is the hidden unit's dimension of the high-level RNN module. Moreover, to automatically detect the salient information related to emotion among these paired differential features, projection matrices are applied to the higher-level discrepancy discriminative features {s h i } and {s v i } obtained by Eq. <ref type="formula" target="#formula_9">(7)</ref> and <ref type="bibr" target="#b7">(8)</ref>. Denoting the projection matrices for the horizontal and vertical traversing directions by</p><formula xml:id="formula_12">W h = [w h ik ] (N/2)×K and W v = [w v ik ] (N/2)×K</formula><p>, the projection can be written as</p><formula xml:id="formula_13">s h k = σ( N/2 i=1 w h iks h i +b h ) ∈ R dg×1 , k = 1, 2, · · · , K, (9) s v k = σ( N/2 i=1 w v iks v i +b v ) ∈ R dg×1 , k = 1, 2, · · · , K. (10)</formula><p>Finally, we use two learnable mapping matrices</p><formula xml:id="formula_14">G h ∈ R do×dg and G v ∈ R do×dg to summarize the stimulusS h t = {s h k } ∈ R dg×K andS v t = {s v k } ∈ R dg×K from two directional streams, namely, S hv t = G hSh t + G vSv t ∈ R do×K .<label>(11)</label></formula><p>Until now, for an input EEG sample X t , the output feature S hv t is obtained.</p><p>3) Discriminative prediction and domain adversarial strategy: Like most supervised models, we add a supervision term into the network by applying the softmax function to the output feature S hv t = {s hv k }, (k = 1, · · · , K) to predict the class label.</p><formula xml:id="formula_15">Let o = [(s hv 1 ) T , (s hv 2 ) T , · · · , (s hv K ) T ] ∈ R 1×Kdo denotes the output feature vector, then y = oP + b c = {y 1 , y 2 , · · · , y C } ∈ R 1×C ,<label>(12)</label></formula><p>where P ∈ R Kdo×C and b c ∈ R 1×C are the transform matrices, and C is the number of emotion types. Finally, the output vector of BiHDM is fed into the softmax layer for emotion classification, which can be written as</p><formula xml:id="formula_16">P (c|X t ) = exp(y c )/ C i=1 exp(y i ),<label>(13)</label></formula><p>where P (c|X t ) denotes the predicted probability that the input sample X t belongs to the c-th class. As a result, the labell t of sample X t is predicted as</p><formula xml:id="formula_17">l t = arg max c P (c|X t ).<label>(14)</label></formula><p>Consequently, the loss function of the classifier can be expressed as</p><formula xml:id="formula_18">L c (X t ; θ f , θ c ) = M1 k=1 C c=1 −τ (l t , c) × logP (c|X t ),<label>(15)</label></formula><formula xml:id="formula_19">τ (l t , c) = 1, if l t = c, 0, otherwise.<label>(16)</label></formula><p>Here θ f and θ c denote the learnable parameters of the feature extraction module and the classifier, while l t and M 1 are the ground-truth label of sample X t and the number of training samples. By minimizing the above loss function, discriminative features could be extracted for emotion recognition.</p><p>To align the feature distributions between source and target domains, we adopt the domain adversarial strategy by adding a discriminator into the network. It works cooperatively with the classifier to induce the feature extraction process to generate emotion-distinguishable but domain-invariant features.</p><p>Concretely, we predefine the source domain label set D S = {0, 0, · · · , 0} ∈ Z M1×1 and target domain label set D T = {1, 1, · · · , 1} ∈ Z M2×1 , where M 2 is the number of testing samples. Then through maximizing the loss function of the discriminator, which can be denoted as</p><formula xml:id="formula_20">L d (X S t , X T t ; θ f , θ d ) = − M1 k=1 logP (0|X S t ) − M2 k =1 logP (1|X T t ),<label>(17)</label></formula><p>the feature extraction process expects to have the ability to generate the data representation to confuse the discriminator to distinguish which domain the input comes from (i.e., the domain-invariant features). Here X S t and X T t denote the t-th and t -th sample in the source and target data set respectively, and θ d represents the learnable parameter of discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The optimization of BiHDM</head><p>The overall optimization of BiHDM can be expressed as</p><formula xml:id="formula_21">min L(X; θ f , θ c , θ d ) = min L c (X S ; θ f , θ c ) + max L d (X S , X T ; θ f , θ d ),<label>(18)</label></formula><p>where L(·) is the loss function of the overall model, and X denotes the entire data set that consists of the source data set X S and target data set</p><formula xml:id="formula_22">X T , i.e., X = [X S , X T ] ∈ R d×N ×(M1+M2) .</formula><p>This max-minimizing loss function will force the parameters of feature extraction module to generate emotion-related but domain-invariant data representation, which benefits for EEG emotion recognition because of the tremendous data distribution shift for EEG emotional signal, especially in the case of subject-independent task where the source and target data come from different subjects.</p><p>Specifically, the maximizing problem can be transferred to a minimizing problem by using a gradient reversal layer (GRL) <ref type="bibr" target="#b18">[19]</ref> before the discriminator, which can be optimized by using stochastic gradient descent (SGD) algorithm <ref type="bibr" target="#b19">[20]</ref> easily. GRL acts as an identity transform in the forwardpropagation but reverses the gradient sign while performing the back-propagation operation. The overall optimization process follows the rules below</p><formula xml:id="formula_23">θ c ← θ c − α ∂L c ∂θ c , θ d ← θ d − α ∂L d ∂θ d ,<label>(19)</label></formula><formula xml:id="formula_24">θ f ← θ f − α( ∂L c ∂θ f − ∂L d ∂θ f ),<label>(20)</label></formula><p>where α is the learning rate. In this way, we can iteratively train the classifier and the discriminator to update the parameters with the similar approach of standard deep learning methods by chain rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS A. Setting up</head><p>To evaluate the proposed BiHDM model, in this section, we will conduct experiments on three public EEG emotional datasets. All the three datasets were collected when the participants sat in front of a monitor comfortably and watched emotional video clips. The EEG signals are recorded from 62 electrode channels using ESI NeuroScan with a sampling rate of 1000 Hz. The locations of electrodes are on the basis of the international 10-20 system. Thus in the experiment, we perform the pairwise operation on the 31 paired electrodes based on the symmetric locations on the left and right brain hemispheric scalps. The detailed information of these datasets are described as follows:</p><p>(1) SEED <ref type="bibr" target="#b20">[21]</ref>. SEED dataset contains 15 subjects, and each subject has three sessions. During the experiment, the participants watched three kinds of emotional film clips, i,e, happy, neutral and sad, where each emotion has 5 film clips. Consequently, there are totally 15 trails, and each trail has 185-238 samples for one session of each subject. Then there are totally about 3400 samples in one session; (2) SEED-IV 2 <ref type="bibr" target="#b4">[5]</ref>. SEED-IV dataset also contains 15 subjects, and each subject has three sessions. But it includes four emotion types with the extra emotion fear compared with SEED, and each emotion has 6 film clips. Thus there are totally 24 trails, and each trail has 12-64 samples for one session of each subject. Then there are totally about 830 samples in one session; (3) MPED 2 <ref type="bibr" target="#b21">[22]</ref>. MPED dataset contains 30 subjects and each subject has one session. It includes seven refined emotion types, i.e., joy, funny, neutral, sad, fear, disgust and anger, and each emotion has 4 film clips. There are totally 28 trails, and each trail has 120 samples. There are totally 3360 samples in one subject. To evaluate the proposed BiHDM model adequately, we design two kinds of experiments including the subject-dependent and subject-independent ones. We use the released handcrafted features, i.e., the differential entropy (DE) in SEED and SEED-IV, and the Short-Time Fourier Transform (STFT) in MPED, as the input to feed our model. Thus the sizes d × N of the input sample X t are 5 × 62, 5 × 62 and 1 × 62 for these three datasets, respectively. Moreover, in the experiment, we respectively set the dimension d l of each electrode's deep representation to 32; the parameters d g and K of the global high-level feature to 32 and 6; and the dimension d o of the output feature to 16 without elaborate traversal. Specifically, we implemented BiHDM using TensorFlow on one Nvidia 1080Ti GPU. The learning rate, momentum and weight decay rate are set as 0.003, 0.9 and 0.95 respectively. The network is trained using SGD with batch size of 200. In addition, we adopt the subtraction as the pairwise operation of the BiHDM model in the experiment section, and discuss the other two types of operations in section III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The EEG emotion recognition experiments</head><p>1) The subject-dependent experiment: In this experiment, we adopt the same protocols as <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b21">[22]</ref>. Namely, for SEED, we use the former nine trails of EEG data per session of each subject as source (training) domain data while using the remaining six trials per session as target (testing) domain data; for SEED-IV, we use the first sixteen trials per session of each subject as the training data, and the last eight trials containing all emotions (each emotion with two trials) as the testing data; for MPED, we use twenty-one trials of EEG data as training data and the rest seven trails consist of seven emotions as testing data for each subject. The mean accuracy (ACC) and standard deviation (STD) are used as the final evaluation metrics for all the subjects in the dataset.</p><p>To validate the superiority of BiHDM, we also conduct the same experiments using twelve methods, including linear support vector machine (SVM) <ref type="bibr" target="#b22">[23]</ref>, random forest (RF) <ref type="bibr" target="#b23">[24]</ref>, canonical correlation analysis (CCA) <ref type="bibr" target="#b24">[25]</ref>, group sparse canonical correlation analysis (GSCCA) <ref type="bibr" target="#b9">[10]</ref>, deep believe network (DBN) <ref type="bibr" target="#b20">[21]</ref>, graph regularization sparse linear regression (GRSLR) <ref type="bibr" target="#b25">[26]</ref>, graph convolutional neural network (GCNN) <ref type="bibr" target="#b26">[27]</ref>, dynamical graph convolutional neural network (DGCNN) <ref type="bibr" target="#b27">[28]</ref>, domain adversarial neural networks (DANN) <ref type="bibr" target="#b18">[19]</ref>, bi-hemisphere domain adversarial neural network (BiDANN) <ref type="bibr" target="#b28">[29]</ref>, EmotionMeter <ref type="bibr" target="#b4">[5]</ref>, and attention-long short-term memory (A-LSTM) <ref type="bibr" target="#b21">[22]</ref>. All the methods compared in our paper are the representative ones in the previous studies. We directly take (or reproduce) their results from the literature to ensure a convincing comparison with the proposed method. The results are summarized in <ref type="table" target="#tab_1">Table I</ref>. From <ref type="table" target="#tab_1">Table I</ref>, we can see that the proposed BiHDM model outperforms all the compared methods on all the three public EEG emotional datasets, which verifies the effectiveness of BiHDM. Especially for the result on SEED-IV, the proposed method improves over the state-of-the-art method Emotion-Meter by 4%. Besides, we can see that the compared method BiDANN, which also considers the bi-hemispheric asymmetry, achieves a comparable performance. The main difference between BiDANN and BiHDM is that the former adopts two hemispheric local discriminators to separately narrow the left and right hemispheric data distribution gaps in either source or target domain but not directly captures the discrepancy information. In contrast, the latter (i.e., the proposed BiHDM) focuses on constructing model to learn the discrepancy relation between two hemispheres and these differential components are beneficial for emotion recognition. Meanwhile, both the results of BiHDM and BiDANN indicate the importance of considering the difference between the left and right cerebral hemispheric data for EEG emotion recognition.</p><p>To test if the proposed BiHDM is statistically significantly better than the baseline method, paired t-test statistical analysis is conducted at the significant level of 0.05. When the improvement of BiHDM over the method is statistically significant, the results will be underlined in the table. <ref type="table" target="#tab_1">Table II</ref> shows the t-test statistical analysis results, from which we can see BiHDM is significantly better than the baseline method. Besides, although the representative methods DANN and BiDANN in <ref type="table" target="#tab_1">Table I</ref> have used the unlabelled testing data to enhance their performance, some compared baseline methods only use the labelled training data to learn the model. To have a fair comparison with them, we follow their setting by taking off the discriminator and only using the labelled training data to conduct the same experiments. The accuracy becomes 91.07%, 72.22% and 38.55% on SEED, SEED-IV and MPED datasets, which still achieves comparable performance. This indicates our differential features are indeed more discriminative.</p><p>2) The subject-independent experiment: In this experiment, we adopt the leave-one-subject-out (LOSO) cross-validation strategy <ref type="bibr" target="#b29">[30]</ref> to evaluate the proposed BiHDM model. LOSO strategy uses the EEG signals of one subject as testing data and the rest subjects' EEG signals as training data. This procedure is repeated such that the EEG signals of each subject will be used as testing data once. Again, the mean accuracy (ACC) and standard deviation (STD) are used as the evaluation metrics.</p><p>In addition, for comparison purpose, we use twelve methods including Kullback-Leibler importance estimation procedure (KLIEP) <ref type="bibr" target="#b30">[31]</ref>, unconstrained least-squares importance fitting (ULSIF) <ref type="bibr" target="#b31">[32]</ref>, selective transfer machine (STM) <ref type="bibr" target="#b32">[33]</ref>, linear SVM, transfer component analysis (TCA) <ref type="bibr" target="#b33">[34]</ref>, transfer component analysis (TCA) <ref type="bibr" target="#b34">[35]</ref>, geodesic flow kernel (GFK) <ref type="bibr" target="#b35">[36]</ref>, DANN, DGCNN, deep adaptation network (DAN) <ref type="bibr" target="#b36">[37]</ref>, BiDANN, and A-LSTM, to conduct the same experiments. Note that the distribution gap in the subject-independent task is much larger than the subject-dependent one, so that transfer learning methods always achieve promising performance.</p><p>Therefore, in the subject-independent task, we include lots of domain adaptation methods in the comparison. By doing so, we can effectively validate the state-of-the-art performance of our method. The results are shown in <ref type="table" target="#tab_1">Table III. 3   TABLE III:</ref> The classification performance for subjectindependent EEG emotion recognition on SEED, SEED-IV and MPED datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ACC / STD (%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEED</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEED-IV MPED</head><p>From <ref type="table" target="#tab_1">Table III</ref>, it can be clearly seen that the proposed BiHDM method achieves the best performance in the three public datasets, which verifies the effectiveness of BiHDM in dealing with subject-independent EEG emotion recognition. For the three datasets, the improvements on accuracy are 2.2%, 3.5% and 2.4%, respectively, when compared with the existing state-of-the-art methods. On the other hand, we also perform the paired t-test between BiHDM and the baseline method at the significant level of 0.05 to see whether BiHDM has an improvement of recognition rate. <ref type="table" target="#tab_1">Table II</ref> shows the t-test statistical analysis results, from which we can see BiHDM is significantly better than the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Confusion matrix</head><p>To see the confusions of BiHDM in recognizing different emotions, we depict the confusion matrices of the above two experiments in <ref type="figure" target="#fig_1">Fig. 2</ref> from which, we have two observations: (1) From <ref type="figure" target="#fig_1">Fig. 2(a)</ref> and <ref type="figure" target="#fig_1">Fig. 2(d)</ref> corresponding to the SEED dataset, we can see that the emotions happy and neutral are much easier to be recognized than sad. But comparing the results between these two kinds of experiments, it is easy to see that in the subject-independent experiment, when the training and testing data come from different people, the recognition rates of emotions neutral and sad will decrease about 10% and 9% while the emotion happy only decreases 3%. We can also observe the same case from the two confusion matrices of the SEED-IV dataset from <ref type="figure" target="#fig_1">Fig. 2(b)</ref> and <ref type="figure" target="#fig_1">Fig. 2(e)</ref>. This shows that the emotion happy causes more similar brain reflection over different people than neutral and sad; (2) For MPED, which consists of seven emotion types, it is much more complicated than the other two datasets. For the subject-dependent experimental result in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>, we can find that the emotions funny, neutral and sad are much easier to be recognized than the other four emotions. However, comparing it with the subjectindependent confusion matrix in <ref type="figure" target="#fig_1">Fig. 2(f)</ref>, we can see that the recognition rate of sad decreases significantly, which is same as the case observed in the above (i.e., point <ref type="formula" target="#formula_1">(1)</ref>). It is possibly because the pattern of emotion sad varies considerably from one subject to another. Moreover, it is interesting to see that the recognition rate of emotion funny decreases significantly but the emotion anger increases, which may be because the participants share common response to the anger emotional videos but have different interpretation about funny.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Different pairwise operations</head><p>In this section, we investigate the performance of using different pairwise operations in BiHDM, as shown in Eq. <ref type="bibr" target="#b5">(6)</ref>. Here, we denote the subtraction, division and inner product variants as BiHDM-S, BiHDM-D, and BiHDM-I, respectively. The results are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. As seen, the subtraction operation achieves the best performance among the three pairwise operations. This may be because the subtraction operation directly measure the discrepancy between two hemispheres, whereas the other two operations describe the difference from various aspects. However, both BiHDM-I and BiHDM-D achieve comparable performance compared with the other methods shown in <ref type="table" target="#tab_1">Table I</ref> and III, which can show the effectiveness of considering the differential information between two cerebral hemispheres. We will explore more pairwise operations such as nonlinear kernel functions in the future work.</p><p>To further verify the performance of pairwise operations, we conduct additional experiments for subject-dependent EEG emotion recognition on SEED, SEED-IV and MPED datasets by replacing subtraction with concatenation, and obtain 90.52%, 72.68% and 37.89% in accuracy. It is clearly inferior to the proposed subtraction operation (93.12%, 74.35% and 40.34%). This shows that using pairwise operation to explicitly extract the discrepancy indeed helps EEG emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The activity maps of the paired EEG electrodes</head><p>To explore the contribution of the differential information from various brain areas for emotion expression, we depict the electrode activity maps in <ref type="figure" target="#fig_3">Fig. 4</ref>. The contribution is evaluated by computing each column's 2-norm of the asymmetric differential featuresŜ h t andŜ v t in Eq. (4) and <ref type="bibr" target="#b4">(5)</ref> for all the testing data and mapping these values into the corresponding electrodes. The two electrodes in a pair share the same value. From <ref type="figure" target="#fig_3">Fig. 4</ref>, we can see that the frontal EEG asymmetry appears to serve as a more important role in emotion recognition for all the three datasets, which is consistent with the cognition observation in biological psychology <ref type="bibr" target="#b37">[38]</ref>. Moreover, for the MPED dataset, which consists of more emotion types, the temporal lobe asymmetry also makes important contribution as the frontal asymmetry. Specifically, to explore where the differential information coming from in terms of the emotion expressed, we separately depict the electrode activity maps corresponding to each emotion in <ref type="figure" target="#fig_4">Fig. 5</ref>. Although it looks quite similar with <ref type="figure" target="#fig_3">Fig. 4</ref>, i.e., the asymmetry on frontal and temporal lobes make more contribution to discriminate different emotions, we can observe some delicate distinctions from these maps of different emotions:</p><p>(1) For the positive emotions (happy in SEED and SEED-IV, joy and funny in MPED), we can see that the asymmetry on temporal lobe actives as same as (or even more than)  the frontal lobe; (2) On the contrary, for the neutral emotion ( <ref type="figure" target="#fig_4">Fig. 5(b)</ref>, <ref type="figure" target="#fig_4">Fig. 5</ref>(e) and <ref type="figure" target="#fig_4">Fig. 5(j)</ref>), the asymmetry on frontal lobe contributes more than temporal lobe; (3) For the sad emotion ( <ref type="figure" target="#fig_4">Fig. 5(c)</ref>, <ref type="figure" target="#fig_4">Fig. 5</ref>(f) and <ref type="figure" target="#fig_4">Fig. 5(k)</ref>), the asymmetry on frontal lobe basically dominates this emotion expression. But we can find that there is a broad activation area closing to the frontal-central lobe, especially in SEED-IV and MPED which contains more types of emotions than SEED; (4) For the fear emotion ( <ref type="figure" target="#fig_4">Fig. 5(g)</ref>), the asymmetry on frontal lobe dominates the emotion expression. But on MPED dataset, which includes more negative emotions (anger and disgust), the temporal lobe will have the same contribution with the frontal lobe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Electrodes reduction</head><p>For the emotion recognition system in real-world applications, fewer electrodes will be preferred considering the feasibility and comfort. Thus in this section, we investigate how the performance varies with relatively small numbers of electrodes. Motivated by the results from <ref type="figure" target="#fig_3">Fig. 4 and 5</ref>, we select the paired electrodes on four brain areas referring to the locations of frontal and temporal lobes, denoted by Frontal (6), Frontal (10), Temporal <ref type="bibr" target="#b5">(6)</ref> and Temporal (9) 4 . The experimental results are shown in <ref type="table" target="#tab_1">Table IV. We can have  two observation from this table:</ref> (1) The obtained recognition results based on fewer electrodes are comparable with that based on all the 31 paired electrodes especially on SEED dataset, which agrees with the observation of <ref type="figure" target="#fig_3">Fig. 4 and 5</ref>. It also verifies that the frontal and temporal lobes' asymmetry indeed contributes more to the EEG emotion recognition than the other brain areas. It is possible to consider utilizing fewer electrodes in EEG emotion recognition systems; (2) Comparing between these two important brain regions, we can see the results based on temporal lobe electrodes outperform that based on frontal lobe. It seems temporal lobe associated more with emotion expression than frontal lobe for EEG emotion recognition; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The performance based on single hemispheric EEG data</head><p>From the above discussion, we can see the discrepancy information between the left and right hemispheres indeed contributes to the EEG emotion recognition task. On the other hand, it will be interesting to investigate which hemisphere is more tightly associated to emotion recognition. Therefore, <ref type="bibr" target="#b3">4</ref> Concretely, Frontal (6) includes the paired electrodes of FP1-Fp2, AF3-AF4, F7-F8, F5-F6, F3-F4 and F1-F2; Frontal (10) includes the paired electrodes of Frontal (6) and FT7-FT8, FC5-FC6, FC3-FC4 and FC1-FC2; Temporal (6) includes the paired electrodes of FT7-FT8, FC5-FC6, T7-T8, C5-C6, TP7-TP8 and CP5-CP6; Temporal (9) includes the paired electrodes of Temporal (6) and FC3-FC4, C3-C4 and CP3-CP4. in this section, we focus on this problem and conduct the same experiments by separately feeding our BiHDM with the left and right hemispheric data. The obtained experimental results are shown in <ref type="table" target="#tab_4">Table V</ref>, from which we can see the left hemisphere is superior to the right for EEG emotion recognition, especially in the experiments on SEED-IV and MPED datasets. Besides, comparing it with the results in <ref type="table" target="#tab_1">Table IV</ref>(a), which are based on feeding the model with less symmetric electrodes' data, we can observe that results are comparable or even better than this experiment based on single hemisphere data. This verifies the effectiveness of discrepancy information for EEG emotion recognition from another aspect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The effect of two directional RNNs to extract spatial information</head><p>In BiHDM, the horizontal and vertical RNNs are adopted to model the structural relation between the electrodes. To evaluate the effect of this spatial information extraction for emotion recognition, we modified the framework of BiHDM with a single directional RNN, denoted by BiHDM-h and BiHDM-v respectively, to conduct the same experiments. The results are summarized in <ref type="table" target="#tab_1">Table VI</ref>, from which we can see that the predefined strategy of traversing the spatial region with horizontal and vertical RNNs achieves much better performance than the single directional RNN. This shows that the proposed spatial feature learning method is helpful to extract the discriminative information for EEG emotion recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a novel bi-hemispheric discrepancy model (BiHDM) for EEG emotion recognition. The proposed framework is easy to implement and generally achieves the state-of-the-art performance. This shows the effectiveness of incorporating the asymmetric differential information into EEG emotion recognition. In the future work, we will further investigate more left and right hemispheric differential operations to explore the potential efficacy of cerebral hemisphere asymmetry in EEG emotion recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The framework of BiHDM. BiHDM consists of four RNN modules to capture each hemispheric EEG electrodes' information from horizontal and vertical streams. Then all the electrodes' data representations interact and construct the final vector for the classifier and discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The confusion matrices of the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The experimental results by using BiHDM-S, BiHDM-D and BiHDM-I models on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>The EEG electrode activity maps of the subjectdependent experiments. Darker red denotes more significant contribution. Note that these maps are symmetric because it shows the value computed by pairwise operation shared by paired electrodes. (Best Viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>The EEG electrode activity maps in terms of different emotions based on the results of subject-dependent experiments. Darker red denotes more significant contribution. (a)-(c), (d)-(g), and (h)-(n) are the results on SEED, SEED-IV, and MPED datasets respectively. Note that these maps are symmetric because it shows the value computed by pairwise operation shared by paired electrodes. (Best Viewed in Color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Li and Tengfei Song are with the Key Laboratory of Child Development and Learning Science (Ministry of Education), and the Department of Information Science and Engineering, Southeast University, Nanjing, Jiangsu, 210096, China. Wenming Zheng and Yuan Zong are with the Key Laboratory of Child Development and Learning Science (Ministry of Education), School of Biological Sciences and Medical Engineering, Southeast University, Nanjing, Jiangsu, 210096, China.( * Corresponding author: Wenming Zheng (E-mail: wenming zheng@seu.edu.cn).) Lei Wang is with the School of Computing and Information Technology, University of Wollongong, NSW, 2500, Australia. Lei Qi is with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, 210096, China.</figDesc><table /><note>Tong Zhang and Zhen Cui are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, 210096, China.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>The classification performance for subjectdependent EEG emotion recognition on SEED, SEED-IV and MPED datasets. * indicates the experiment results obtained are based on our own implementation. − indicates the experiment results are not reported on that dataset.</figDesc><table><row><cell>Method</cell><cell>SEED</cell><cell>ACC / STD (%) SEED-IV</cell><cell>MPED</cell></row><row><cell>SVM [23]</cell><cell>83.99/09.72</cell><cell>56.61/20.05  *</cell><cell>32.39/09.53  *</cell></row><row><cell>RF [24]</cell><cell>78.46/11.77</cell><cell>50.97/16.22  *</cell><cell>23.83/06.82  *</cell></row><row><cell>CCA [25]</cell><cell>77.63/13.21</cell><cell>54.47/18.48  *</cell><cell>29.08/07.96  *</cell></row><row><cell>GSCCA [10]</cell><cell>82.96/09.95</cell><cell>69.08/16.66  *</cell><cell>36.78/07.76  *</cell></row><row><cell>DBN [21]</cell><cell>86.08/08.34</cell><cell>66.77/07.38  *</cell><cell>35.07/11.25  *</cell></row><row><cell>GRSLR [26]</cell><cell>87.39/08.64</cell><cell>69.32/19.57  *</cell><cell>34.58/08.41  *</cell></row><row><cell>GCNN [27]</cell><cell>87.40/09.20</cell><cell>68.34/15.42  *</cell><cell>33.26/06.44  *</cell></row><row><cell>DGCNN [28]</cell><cell>90.40/08.49</cell><cell>69.88/16.29  *</cell><cell>32.37/06.08  *</cell></row><row><cell>DANN [19]</cell><cell>91.36/08.30</cell><cell>63.07/12.66  *</cell><cell>35.04/06.52  *</cell></row><row><cell>BiDANN [29]</cell><cell>92.38/07.04</cell><cell>70.29/12.63  *</cell><cell>37.71/06.04  *</cell></row><row><cell>EmotionMeter [5]</cell><cell>−</cell><cell>70.59/17.01</cell><cell>−</cell></row><row><cell>A-LSTM [22]</cell><cell>88.61/10.16  *</cell><cell>69.50/15.65  *</cell><cell>38.99/07.53  *</cell></row><row><cell>BiHDM</cell><cell>93.12/06.06</cell><cell>74.35/14.09</cell><cell>40.34/07.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>The t-test statistics analysis between BiHDM and the baseline method at the significance level of 0.05. When the improvement of BiHDM over the method is statistically significant, the result will be underlined.</figDesc><table><row><cell>Method</cell><cell>SEED</cell><cell>p-value SEED-IV</cell><cell>MPED</cell></row><row><cell>BiHDM vs. BiDANN</cell><cell>0.0580 a 0.0451 b</cell><cell>0.0344 a 0.0188 b</cell><cell>0.0488 a 0.0091</cell></row></table><note>b a and b indicate the subject-dependent and independent experiment results respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>The classification performance based on the frontal and temporal lobe EEG data for subject-dependent and subject-independent EEG emotion recognition on SEED, SEED-IV and MPED datasets.</figDesc><table><row><cell cols="4">(a) Subject-dependent experiment results</cell></row><row><cell>Electrode area</cell><cell>SEED</cell><cell>ACC / STD (%) SEED-IV</cell><cell>MPED</cell></row><row><cell>Frontal (6)</cell><cell>80.15/09.86</cell><cell>57.93/13.88</cell><cell>29.02/05.68</cell></row><row><cell>Frontal (10)</cell><cell>84.49/08.83</cell><cell>63.02/16.95</cell><cell>32.37/06.79</cell></row><row><cell>Temporal (6)</cell><cell>88.16/08.03</cell><cell>64.88/15.76</cell><cell>33.61/07.19</cell></row><row><cell>Temporal (9)</cell><cell>90.16/07.44</cell><cell>65.19/16.03</cell><cell>33.13/07.06</cell></row><row><cell>All (31)</cell><cell>93.12/06.06</cell><cell>74.35/14.09</cell><cell>40.34/07.59</cell></row><row><cell cols="4">(b) Subject-independent experiment results</cell></row><row><cell>Electrode area</cell><cell>SEED</cell><cell>ACC / STD (%) SEED-IV</cell><cell>MPED</cell></row><row><cell>Frontal (6)</cell><cell>74.33/08.70</cell><cell>67.28/08.19</cell><cell>23.54/02.73</cell></row><row><cell>Frontal (10)</cell><cell>80.28/09.94</cell><cell>68.16/07.85</cell><cell>25.44/04.95</cell></row><row><cell>Temporal (6)</cell><cell>85.04/07.13</cell><cell>65.07/08.74</cell><cell>26.07/04.32</cell></row><row><cell>Temporal (9)</cell><cell>84.09/07.78</cell><cell>66.92/08.74</cell><cell>26.43/04.55</cell></row><row><cell>All (31)</cell><cell>85.40/07.53</cell><cell>69.03/08.66</cell><cell>28.27/04.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>The classification performance based on single hemispheric EEG data for subject-dependent EEG emotion recognition on SEED, SEED-IV and MPED datasets.</figDesc><table><row><cell>Hemisphere</cell><cell>SEED</cell><cell>ACC / STD (%) SEED-IV</cell><cell>MPED</cell></row><row><cell>BiHDM-left</cell><cell>86.63/08.88</cell><cell>64.48/15.34</cell><cell>35.92/07.26</cell></row><row><cell>BiHDM-right</cell><cell>86.39/07.54</cell><cell>60.11/13.53</cell><cell>33.08/08.30</cell></row><row><cell>BiHDM-overall</cell><cell>93.12/06.06</cell><cell>74.35/14.09</cell><cell>40.34/07.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>The classification performance of different spatial feature extraction methods for EEG emotion recognition on SEED, SEED-IV and MPED datasets.</figDesc><table><row><cell cols="4">(a) Subject-dependent experiment results</cell></row><row><cell>Electrode area</cell><cell>SEED</cell><cell>ACC / STD (%) SEED-IV</cell><cell>MPED</cell></row><row><cell>BiHDM-h</cell><cell>87.47/09.17</cell><cell>62.06/15.01</cell><cell>36.24/08.41</cell></row><row><cell>BiHDM-v</cell><cell>86.75/07.09</cell><cell>65.57/15.43</cell><cell>36.69/08.20</cell></row><row><cell>BiHDM</cell><cell>93.12/06.06</cell><cell>74.35/14.09</cell><cell>40.34/07.59</cell></row><row><cell cols="4">(b) Subject-independent experiment results</cell></row><row><cell>Electrode area</cell><cell>SEED</cell><cell>ACC / STD (%) SEED-IV</cell><cell>MPED</cell></row><row><cell>BiHDM-h</cell><cell>82.38/09.33</cell><cell>66.80/08.22</cell><cell>28.05/04.98</cell></row><row><cell>BiHDM-v</cell><cell>81.03/10.28</cell><cell>66.96/08.28</cell><cell>27.86/05.06</cell></row><row><cell>BiHDM</cell><cell>85.40/07.53</cell><cell>69.03/08.66</cell><cell>28.27/04.99</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Precisely, division operation models the differential information in terms of the relative ratio of magnitude; the (minus) inner product operation models such information in terms of (dis) similarity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that both SEED-IV and MPED are multi-modal datasets. MPED consists of 30 subjects' EEG data, among which 23 subjects contain multimodal data. In this experiment, we only use the EEG modal data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that the subspace based methods, such as TCA, SA and GFK, are problematic to handle a large amount of EEG data due to the computer memory limitation and computational issue. Therefore, to compare with them we have to randomly select 3000 EEG feature samples from the training data set to train these methods.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Differing emotional response from right and left hemispheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dimond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Farrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="issue">5562</biblScope>
			<biblScope unit="page">690</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Affective computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emotion recognition in human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsapatsoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Votsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fellenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="80" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eeg-based emotion recognition in music listening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Duann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1798" to="1806" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emotionmeter: A multimodal framework for recognizing human emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1110" to="1122" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural correlates of social and nonsocial emotions: An fmri study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Britton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Berridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Liberzon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="397" to="409" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature extraction and selection for emotion recognition from eeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotions recognition using eeg signals: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Alarcao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A review on nonlinear methods using electroencephalographic recordings for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez-Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alcaraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fernández-Caballero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eeg based emotion recognition by combining functional connectivity network and local activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Electroencephalographic spectral asymmetry index for detection of depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hinrikus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhhova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aadamsoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ü</forename><surname>Võhma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tuulik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical &amp; Biological Engineering &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1291</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approach-withdrawal and cerebral asymmetry: emotional expression and brain physiology: I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Saron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Senulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">330</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Localization of asymmetric brain function in emotion and depression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Herrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Banich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="442" to="454" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Right hemisphere lateralization for emotion in the human brain: Interactions with cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="issue">4211</biblScope>
			<biblScope unit="page" from="286" to="288" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Valence, gender, and lateralization of functional brain anatomy in emotion: a meta-analysis of findings from neuroimaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Liberzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="513" to="531" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hemispheric specialization in affective responses, cerebral dominance for language, and handedness: lateralization of emotion, language, and dexterity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Costanzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Villarreal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Drucaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ortiz-Villafañe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Wainsztein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ladrón-De Guevara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Brusco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioural brain research</title>
		<imprint>
			<biblScope unit="volume">288</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A bihemisphere domain adversarial neural network model for eeg emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Autonomous Mental Development</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="162" to="175" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mped: A multi-modal physiological emotion database for discrete emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="12" to="177" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Statistics in Behavioral Science</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Eeg emotion recognition based on graph regularized sparse linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Processing Letters</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Eeg emotion recognition using dynamical graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1561" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Personalizing eeg-based affective models with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2732" to="2738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1433" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A least-squares approach to direct importance estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1391" to="1445" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="545" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cross-subject emotion recognition using deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="403" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Frontal eeg asymmetry as a moderator and mediator of emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Coan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological psychology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="50" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
