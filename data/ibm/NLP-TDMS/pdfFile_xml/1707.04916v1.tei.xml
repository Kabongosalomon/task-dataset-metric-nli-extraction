<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-LABEL MUSIC GENRE CLASSIFICATION FROM AUDIO, TEXT, AND IMAGES USING DEEP FEATURES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Music Technology Group</orgName>
								<address>
									<country>Universitat Pompeu Fabra</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Nieto</surname></persName>
							<email>onieto@pandora.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Pandora Media Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">TALN Group</orgName>
								<orgName type="institution" key="instit2">Universitat Pompeu Fabra</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
							<email>xavier.serra@upf.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Music Technology Group</orgName>
								<address>
									<country>Universitat Pompeu Fabra</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MULTI-LABEL MUSIC GENRE CLASSIFICATION FROM AUDIO, TEXT, AND IMAGES USING DEEP FEATURES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Music genres allow to categorize musical items that share common characteristics. Although these categories are not mutually exclusive, most related research is traditionally focused on classifying tracks into a single class. Furthermore, these categories (e.g., Pop, Rock) tend to be too broad for certain applications. In this work we aim to expand this task by categorizing musical items into multiple and fine-grained labels, using three different data modalities: audio, text, and images. To this end we present MuMu, a new dataset of more than 31k albums classified into 250 genre classes. For every album we have collected the cover image, text reviews, and audio tracks. Additionally, we propose an approach for multi-label genre classification based on the combination of feature embeddings learned with state-of-the-art deep learning methodologies. Experiments show major differences between modalities, which not only introduce new baselines for multi-label genre classification, but also suggest that combining them yields improved results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Music genres are useful labels to classify musical items into broader categories that share similar musical, regional, or temporal characteristics. Dealing with large collections of music poses numerous challenges when retrieving and classifying information <ref type="bibr" target="#b2">[3]</ref>. Music streaming services tend to offer catalogs of tens of millions of tracks, for which tasks such as music classification are of utmost importance. Music genre classification is a widely studied problem in the Music Information Research (MIR) community <ref type="bibr" target="#b40">[40]</ref>. However, almost all related work is concentrated in multiclass classification of music items into broad genres (e.g., Pop, Rock), assigning a single label per item. This is problematic since there may be hundreds of more specific music genres <ref type="bibr" target="#b33">[33]</ref>, and these may not be necessarily mutually exclusive (i.e., a song could be Pop, and at the same time have elements from Deep House and a Reggae grove). In this work we aim to advance the field of music classification by framing it as multi-label genre classification of fine-grained genres.</p><p>To this end, we present MuMu, a new large-scale multimodal dataset for multi-label music genre classification. MuMu contains information of roughly 31k albums classified into one or more 250 genre classes. For every album we analyze the cover image, text reviews, and audio tracks, with a total number of approximately 147k audio tracks and 447k album reviews. Furthermore, we exploit this dataset with a novel deep learning approach to learn multiple genre labels for every album using different data modalities (i.e., audio, text, and image). In addition, we combine these modalities to study how the different combinations behave.</p><p>Results show how feature learning using deep neural networks substantially surpasses traditional approaches based on handcrafted features, reducing the gap between text-based and audio-based classification <ref type="bibr" target="#b29">[29]</ref>. Moreover, an extensive comparative of different deep learning architectures for audio classification is provided, including the usage of a dimensionality reduction approach that yields improved results. Finally, we show how the late fusion of feature vectors learned from different modalities achieves better scores than each of them individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Most published music genre classification approaches rely on audio sources <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">40]</ref>. Traditional techniques typically use handcrafted audio features, such as Mel Frequency Cepstral Coecients (MFCCs) <ref type="bibr" target="#b19">[20]</ref>, as input of a machine learning classifier (e.g., SVM) <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b44">44]</ref>. More recent deep learning approaches take advantage of visual representations of the audio signal in form of spectrograms. These visual representations are used as input to Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">34]</ref>, following approaches similar to those used for image classification.</p><p>Text-based approaches have also been explored for this task. For instance, in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">29]</ref> album customer reviews are used as input for the classification, whereas in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref> song lyrics are employed. By contrast, there are a limited number of papers dealing with image-based genre classi-fication <ref type="bibr" target="#b17">[18]</ref>. Most multimodal approaches for this task found in the literature combine audio and song lyrics as text <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">27]</ref>. Moreover, the combination of audio and video has also been explored <ref type="bibr" target="#b37">[37]</ref>. However, the authors are not aware of published multimodal approaches for music genre classification that involve deep learning.</p><p>Multi-label classification is a widely studied problem <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">43]</ref>. Despite the scarcity in terms of approaches for multi-label classification of music genres <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b46">46]</ref>, there is a long tradition in MIR for tag classification, which is a multi-label problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTIMODAL DATASET</head><p>To the best of our knowledge, there are no publicly available large-scale datasets that encompass audio, images, text, and multi-label annotations. Therefore, we present MuMu, a new Multimodal Music dataset with multilabel genre annotations that combines information from the Amazon Reviews dataset <ref type="bibr" target="#b22">[23]</ref> and the Million Song Dataset (MSD) <ref type="bibr" target="#b0">[1]</ref>. The former contains millions of album customer reviews and album metadata gathered from Amazon.com. The latter is a collection of metadata and precomputed audio features for a million songs.</p><p>To map the information from both datasets we use Mu-sicBrainz <ref type="bibr" target="#b0">1</ref> . For every album in the Amazon dataset, we query MusicBrainz with the album title and artist name to find the best possible match. Matching is performed using the same methodology described in <ref type="bibr" target="#b30">[30]</ref>, following a pairwise entity resolution approach based on string similarity. Following this approach, we were able to map 60% of the Amazon dataset. For all the matched albums, we obtain the MusicBrainz recording ids of their songs. With these, we use an available mapping from MSD to MusicBrainz 2 to obtain the subset of recordings present in the MSD. From the mapped recordings, we only keep those associated with a unique album. This process yields the final set of 147,295 songs, which belong to 31,471 albums.</p><p>The song features provided by the MSD are not generally suitable for deep learning <ref type="bibr" target="#b45">[45]</ref>, so we instead use in our experiments audio previews between 15 and 30 seconds retrieved from 7digital.com. For the mapped set of albums, there are 447,583 customer reviews in the Amazon Dataset. In addition, the Amazon Dataset provides further information about each album, such as genre annotations, average rating, selling rank, similar products, cover image url, etc. We employ the provided image url to gather the cover art of all selected albums. The mapping between the three datasets (Amazon, MusicBrainz, and MSD), genre annotations, data splits, text reviews, and links to images are released as the MuMu dataset 3 . Images and audio files can not be released due to copyright issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Genre Labels</head><p>Amazon has its own hierarchical taxonomy of music genres, which is up to four levels in depth. In the first level there are 27 genres, and almost 500 genres overall. In our dataset, we keep the 250 genres that satisfy the condition of having been annotated in at least 12 albums. Every album in Amazon is annotated with one or more genres from different levels of the taxonomy. The Amazon Dataset contains complete information about the specific branch from the taxonomy used to classify each album. For instance, an album annotated as Traditional Pop comes with the complete branch information Pop / Oldies / Traditional Pop. To exploit either the taxonomic and the co-occurrence information, we provide every item with the labels of all their branches. For example, an album classified as Jazz / Vocal Jazz and Pop / Vocal Pop is annotated in MuMu with the four labels: Jazz, Vocal Jazz, Pop, and Vocal Pop. There are in average 5.97 labels for each song (3.13 standard deviation). The labels in the dataset are highly unbalanced, following a distribution which might align well with those found in real world scenarios. In <ref type="table" target="#tab_0">Table 1</ref> we see the top 10 most and least represented genres and the percentage of albums annotated with each label. The unbalanced character of the genre annotations poses an interesting challenge for music classification that we also aim to exploit. Among the multiple possibilities that this dataset may offer to the MIR community, we focus our work on the multi-label classification problem, described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MULTI-LABEL CLASSIFICATION</head><p>In multi-label classification, multiple target labels may be assigned to each classifiable instance. More formally: given a set of n labels L = {l 1 , l 2 , . . . , l n }, and a set of m items I = {i 1 , i 2 , . . . , i m }, we aim to model a function f able to associate a set of c labels to every item in I, where c ∈ [1, n] varies for every item.</p><p>Deep learning approaches are well-suited for this problem, as these architectures allow to have multiple outputs in their final layer. The usual architecture for large multilabel classification using deep learning ends with a logistic regression layer with sigmoid activations evaluated with the cross-entropy loss, where target labels are encoded as high-dimensional sparse binary vectors <ref type="bibr" target="#b42">[42]</ref>. This method, which we refer as LOGISTIC, implies the assumption that the classes are statistically independent (which is not the case in music genres).</p><p>A more recent approach <ref type="bibr" target="#b6">[7]</ref>, relies on matrix factorization to reduce the dimensionality of the target labels. This method makes use of the interrelation between labels, embedding the high-dimensional sparse labels onto lowerdimensional vectors. In this case, the target of the network is a dense lower-dimensional vector which can be learned using the cosine proximity loss, as these vectors tend to be l2-normalized. We denote this technique as COSINE, and we provide a more formal definition next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Labels Factorization</head><p>Let M be the binary matrix of items I and labels L where m ij = 1 if i i is annotated with label l j and m ij = 0 otherwise. Using M , we calculate the matrix X of Positive Pointwise Mutual Information (PPMI) for the set of labels L. Given L i as the set of items annotated with label l i , the PPMI between two labels is defined as:</p><formula xml:id="formula_0">X(l i , l j ) = max 0, log P (L i , L j ) P (L i )P (L j ) (1) where P (L i , L j ) = |L i ∩L j |/|I| and P (L i ) = |L i |/|I|.</formula><p>The PPMI matrix X is then factorized using Singular Value Decomposition (SVD) such that X ≈ U ΣV , where U and V are unitary matrices, and Σ is a diagonal matrix of singular values. Let Σ d be the diagonal matrix formed from the top d singular values, and let U d be the matrix produced by selecting the corresponding columns from U , the matrix C d = U d · √ Σ d contains the label factors of d dimensions. Finally, we obtain the matrix of item factors F d as F d = C d ·M T . Further information on this technique may be found in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Factors present in matrices C d and F d are embedded in the same space. Thus, a distance metric such as cosine distance can be used to obtain distance measures between items and labels. Similar labels are grouped in the space, and at the same time, items with similar sets of labels are near each other. These properties can be exploited in the label prediction problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>The evaluation of multi-label classification is not necessarily straightforward. Evaluation measures vary according to the output of the system. In this work we are interested in measures that deal with probabilistic outputs, instead of binary. The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. Thus, the area under the ROC curve (AUC) is often taken as an evaluation measure to compare such systems. We selected this metric to compare the performance of the different approaches as it has been widely used for genre and tag classification problems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>The output of a multi-label classifier is a label-item matrix. Thus, it can be evaluated either from the labels or the items perspective. We can measure how accurate the classification is for every label, or how well the labels are ranked for every item. In this work, the former point of view is evaluated with the AUC measure, which is computed for every label and then averaged. We are interested in classification models that strengthen the diversity of label assignments. As the taxonomy is composed of broad genres which are over-represented in the dataset (see <ref type="table" target="#tab_0">Table 1)</ref>, and more specific subgenres (e.g., Vocal Jazz, Britpop), we want to measure whether the classifier is focusing only on over-represented genres, or on more fine-grained ones. To this end, catalog coverage (also known as aggregated diversity) is an evaluation measure used in the extreme multi-label classification <ref type="bibr" target="#b13">[14]</ref> and the recommender systems <ref type="bibr" target="#b32">[32]</ref> communities. Coverage@k measures the percentage of normalized unique labels present in the top k predictions made by an algorithm across all test items. Values of k = 1, 3, 5 are typically employed in multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ALBUM GENRE CLASSIFICATION</head><p>In this section we exploit the multimodal nature of the MuMu dataset to address the multi-label classification task. More specifically, and since each modality on this set (i.e., cover image, text reviews, and audio tracks) is associated with a music album, our task focuses on album classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Audio-based Approach</head><p>A music album is composed by a series of audio tracks, each of which may be associated with different genres. In order to learn the album genre from a set of audio tracks we split the problem into three steps: (1) track feature vectors are learned while trying to predict the genre labels of the album from every track in a deep neural network. (2) Track vectors of each album are averaged to obtain album feature vectors. (3) Album genres are predicted from the album feature vectors in a shallow network where the input layer is directly connected to the output layer.</p><p>It is common in MIR to make use of CNNs to learn higher-level features from spectrograms. These representations are typically contained in R F ×N matrices with F frequency bins and N time frames. In this work we compute 96 frequency bin, log-compressed constant-Q transforms (CQT) <ref type="bibr" target="#b38">[38]</ref> for all the tracks in our dataset using librosa <ref type="bibr" target="#b23">[24]</ref> with the following parameters: audio sampling rate at 22050 Hz, hop length of 1024 samples, Hann analysis window, and 12 bins per octave. In addition, logamplitude scaling is applied to the CQT spectrograms. Following a similar approach to <ref type="bibr" target="#b45">[45]</ref>, we address the variability of the length N across songs by sampling one 15seconds long patch from each track, resulting in the fixedsize input to the CNN.</p><p>To learn the genre labels we design a CNN with four convolutional layers and experiment with different number of filters, filter sizes, and output configurations (see Section 6.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Text-based Approach</head><p>In the presented dataset, each album has a variable number of customer reviews. We use an approach similar to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">29]</ref> for genre classification from text, where all reviews from the same album are aggregated into a single text. The aggregated result is truncated at 1000 characters, thus balancing the amount of text per album, as more popular artists tend to have a higher number of reviews. Then we apply a Vector Space Model approach (VSM) with tfidf weighting <ref type="bibr" target="#b47">[47]</ref> to create a feature vector for each album. Although word embeddings <ref type="bibr" target="#b25">[25]</ref> with CNNs are state-ofthe-art in many text classification tasks <ref type="bibr" target="#b14">[15]</ref>, a traditional VSM approach is used instead, as it seems to perform better when dealing with large texts <ref type="bibr" target="#b31">[31]</ref>. The vocabulary size is limited to 10k as it was a good balance of network complexity and accuracy.</p><p>Furthermore, a second approach is proposed based on the addition of semantic information, similarly to the method described in <ref type="bibr" target="#b29">[29]</ref>. To semantically enrich the album texts, we adopted Babelfy, a state-of-the-art tool for entity linking <ref type="bibr" target="#b26">[26]</ref>, a task to associate, for a given textual fragment candidate, the most suitable entry in a reference KB. Babelfy maps words from a given text to Wikipedia 4 . In Wikipedia, categories are used to organize resources. We take all the Wikipedia categories of entities identified by Babelfy in each document and add them at the end of the text as new words. Then a VSM with tf-idf weighting is applied to the semantically enriched texts, where the vocabulary is also limited to 10k terms. Note that either words or categories may be part of this vocabulary.</p><p>From this representation, a feed forward network with two dense layers of 2048 neurons and a Rectified Linear Unit (ReLU) after each layer is trained to predict the genre labels in both LOGISTIC and COSINE configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Image-based Approach</head><p>Every album in the dataset has an associated cover art image. To perform music genre classification from these images, we use Deep Residual Networks (ResNets) <ref type="bibr" target="#b10">[11]</ref>. They are the state-of-the-art in various image classification tasks like Imagnet <ref type="bibr" target="#b35">[35]</ref> and Microsoft COCO <ref type="bibr" target="#b18">[19]</ref>. ResNet is a common feed-forward CNN with residual learning, which consists on bypassing two or more convolution layers. We employ a slightly modified version of the original ResNet 5 : the scaling and aspect ratio augmentation are obtained from <ref type="bibr" target="#b41">[41]</ref>, the photometric distortions from <ref type="bibr" target="#b11">[12]</ref>, and weight decay is applied to all weights and biases. The network we use is composed of 101 layers (ResNet-101), initialized with pretrained parameters learned on Im-ageNet. This is our starting point to finetune the network on the genre classification task. Our ResNet implementation has a logistic regression final layer with sigmoid activations and uses the binary cross entropy loss. <ref type="bibr" target="#b3">4</ref> http://wikipedia.org 5 https://github.com/facebook/fb.resnet.torch/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multimodal approach</head><p>We aim to combine all of these different types of data into a single model. There are several works claiming that learning data representations from different modalities simultaneously outperforms systems that learn them separately <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">28]</ref>. However, recent work in multimodal learning with audio and text in the context of music recommendation <ref type="bibr" target="#b31">[31]</ref> reflects the contrary. We have observed that deep networks are able to find an optimal minimum very fast from text data. However, the complexity of the audio signal can significantly slow down the training process. Simultaneous learning may under-explore one of the modalities, as the stronger modality may dominate quickly. Thus, learning each modality separately warrants that the variability of the input data is fully represented in each of the feature vectors.</p><p>Therefore, from each modality network described above, we separately obtain an internal feature representation for every album after training them on the genre classification task. Concretely, the input to the last fully connected layer of each network becomes feature vector for its respective modality. Given a set of feature vectors, l2-regularization is applied on each of them. They are then concatenated into a single feature vector, which becomes the input to a simple Multi Layer Perceptron (MLP), where the input layer is directly connected to the output layer. The output layer may have either a LOGISTIC or a COSINE configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head><p>We apply the architectures defined in the previous section to the MuMu dataset. The dataset is divided as follows: 80% for training, 10% for validation, and 10% for test. We first evaluate every modality in isolation in the multilabel genre classification task. Then, from each modality, a deep feature vector is obtained for the best performing approach in terms of AUC. Finally, the three modality vectors are combined in a multimodal network. All results are reported in <ref type="table" target="#tab_1">Table 2</ref>. Performance of the classification is reported in terms of AUC score and Coverage@k with k = 1, 3, 5. The training speed per epoch and number of network hyperparameters are also reported. All source code and data splits used in our experiments are available on-line <ref type="bibr" target="#b5">6</ref> .</p><p>The matrix of album genre annotations of the training and validation sets is factorized using the approach described in Section 4.1, with a value of d = 50 dimensions. From the set of album factors, those annotated with a single label from the top level of the taxonomy are plotted in <ref type="figure" target="#fig_0">Figure 1</ref> using t-SNE dimensionality reduction <ref type="bibr" target="#b20">[21]</ref>. It can be seen how the different albums are properly clustered in the factor space according to their genre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Audio Classification</head><p>We explore three network design parameters: convolution filter size, number of filters per convolutional layer,  and target layer. For the filter size we compare three approaches: square 3x3 filters as in <ref type="bibr" target="#b4">[5]</ref>, a filter of 4x96 that convolves only in time <ref type="bibr" target="#b45">[45]</ref>, and a musically motivated filter of 4x70, which is able to slightly convolve in the frequency domain <ref type="bibr" target="#b34">[34]</ref>. To study the width of the convolutional layers we try with two different settings: HIGH with 256, 512, 1024, and 1024 in each layer respectively, and LOW with 64, 128, 128, 64 filters. Max-pooling is applied after each convolutional layer. Finally, we use the two different network targets defined in Section 4, LOGISTIC and COSINE. We empirically observed that dropout regularization only helps in the HIGH plus COSINE configurations. Therefore we applied dropout with a factor of 0.5 to these configurations, and no dropout to the others. Apart from these configurations, a baseline approach is added. This approach consists in a traditional audio-based approach for genre classification based on the audio descriptors present in the MSD <ref type="bibr" target="#b0">[1]</ref>. More specifically, for each song we aggregate four different statistics of the 12 timbre coefficient matrices: mean, max, variance, and l2norm. The obtained 48 dimensional feature vectors are fed into a feed forward network as the one described in Section 5.4 with LOGISTIC output. This approach is denoted as TIMBRE-MLP.</p><p>The results show that CNNs applied over audio spectrograms clearly outperform traditional approaches based on handcrafted features. We observe that the TIMBRE-MLP approach achieves 0.792 of AUC, contrasting with the 0.888 from the best CNN approach. We note that the LO-GISTIC configuration obtains better results when using a lower number of filters per convolution (LOW). Configurations with fewer filters have less parameters to optimize, and their training processes are faster. On the other hand, in COSINE configurations we observe that the use of a higher number of filters tends to achieve better performance. It seems that the fine-grained regression of the factors benefits from wider convolutions. Moreover, we observe that 3x3 square filter settings have lower performance, need more time to train, and have a higher number of parameters to optimize. By contrast, networks using time convolutions only (4X96) have a lower number of parameters, are faster to train, and achieve comparable performance. Furthermore, networks that slightly convolve across the frequency bins (4X70) achieve better results with only a slightly higher number of parameters and training time. Finally, we observe that the COSINE regression approach achieves better AUC scores in most configurations, and also their results are more diverse in terms of catalog coverage. <ref type="figure">Figure 2</ref>. Particular of the t-SNE of randomly selected image vectors from five of the most frequent genres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Text Classification</head><p>For text classification, we obtain two feature vectors as described in Section 5.2: one built from the texts VSM, and another built from the semantically enriched texts VSM+SEM. Both feature vectors are trained in the multilabel genre classification task using the two output configurations LOGISTIC and <ref type="bibr">COSINE.</ref> Results show that the semantic enrichment of texts clearly yields better results in terms of AUC and diversity. Furthermore, we observe that the COSINE configuration slightly outperforms LOGISTIC in terms of AUC, and greatly in terms of catalog coverage. The text-based results are overall slightly superior to the audio-based ones.</p><p>We also studied the information gain of words in the different genres. We observed that genre labels present in the texts have important information gain values. However, it is remarkable that band is a very informative word for Rock, song for Pop, and dope, rhymes, and beats are discriminative features for Rap albums. Place names have also important weights, as Jamaica for Reggae, Nashvile for Country, or Chicago for Blues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Image Classification</head><p>Results show that genre classification from images has lower performance in terms of AUC and catalog coverage compared to the other modalities. Due to the use of an already pre-trained network with a logistic output (ImageNet <ref type="bibr" target="#b35">[35]</ref>) as initialization of the network, it is not straightforward to apply the COSINE configuration. Therefore, we only report results for the LOGISTIC configuration.</p><p>In <ref type="figure">Figure 2</ref> a set of cover images of five of the most frequent genres in the dataset is shown using t-SNE over the obtained image feature vectors. In the left top corner the ResNet recognizes women faces on the foreground, which seems to be common in Country albums (red). The jazz albums (green) on the right are all clustered together probably thanks to the uniform type of clothing worn by the people of their covers. Therefore, the visual style of the cover seems to be informative when recognizing the album genre. For instance, many classical music albums include an instrument in the cover, and Dance &amp; Electronics covers are often abstract images with bright colors, rarely including human faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Multimodal Classification</head><p>From the best performing approaches in terms of AUC of each modality (i.e., AUDIO / COSINE / HIGH-4X70, TEXT / COSINE / VSM+SEM and IMAGE / LOGISTIC / RESNET), a feature vector is obtained as described in Section 5.4. Then, these three feature vectors are aggregated in all possible combinations, and genre labels are predicted using the MLP network described in Section 5.4. Both output configurations LOGISTIC and COSINE are used in the learning phase, and dropout of 0.7 is applied in the CO-SINE configuration.</p><p>Results suggest that the combination of modalities outperforms single modality approaches. As image features are learned using a LOGISTIC configuration, they seem to improve multimodal approaches with LOGISTIC configuration only. Multimodal approaches that include text features tend to improve the results. Nevertheless, the best approaches are those that exploit the three modalities of MuMu. COSINE approaches have similar AUC than LO-GISTIC approaches but a much better catalog coverage, thanks to the spatial properties of the factor space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>An approach for multi-label music genre classification using deep learning architectures has been proposed. The approach was applied to audio, text, image data, and their combination. For its assessment, MuMu, a new multimodal music dataset with over 31k albums and 135k songs has been gathered. We showed how representation learning approaches for audio classification outperform traditional handcrafted feature based approaches. Moreover, we compared the effect of different design parameters of CNNs in audio classification. Text-based approaches seem to outperform other modalities, and benefit from the semantic enrichment of texts via entity linking. While the image-based classification yielded the lowest performance, it helped to improve the results when combined with other modalities. Multimodal approaches appear to outperform single modality approaches, and the aggregation of the three modalities achieved the best results. Furtheremore, the dimensionality reduction of target labels led to better results, not only in terms of accuracy, but also in terms of catalog coverage. This paper is an initial attempt to study the multi-label classification problem of music genres from different perspectives and using different data modalities. In addition, the release of the MuMu dataset opens up a number of unexplored research possibilities. In the near future we aim to modify the ResNet to be able to learn latent factors from images as we did in other modalities and apply the same multimodal approach to other MIR tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>t-SNE of album factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Top-10 most and least represented genres</figDesc><table><row><cell>Genre</cell><cell>% of albums</cell><cell>Genre</cell><cell>% of albums</cell></row><row><cell>Pop</cell><cell>84.38</cell><cell>Tributes</cell><cell>0.10</cell></row><row><cell>Rock</cell><cell>55.29</cell><cell>Harmonica Blues</cell><cell>0.10</cell></row><row><cell>Alternative Rock</cell><cell>27.69</cell><cell>Concertos</cell><cell>0.10</cell></row><row><cell>World Music</cell><cell>19.31</cell><cell>Bass</cell><cell>0.06</cell></row><row><cell>Jazz</cell><cell>14.73</cell><cell>European Jazz</cell><cell>0.06</cell></row><row><cell>Dance &amp; Electronic</cell><cell>12.23</cell><cell>Piano Blues</cell><cell>0.06</cell></row><row><cell>Metal</cell><cell>11.50</cell><cell>Norway</cell><cell>0.06</cell></row><row><cell>Indie &amp; Lo-Fi</cell><cell>10.45</cell><cell>Slide Guitar</cell><cell>0.06</cell></row><row><cell>R&amp;B</cell><cell>10.10</cell><cell>East Coast Blues</cell><cell>0.06</cell></row><row><cell>Folk</cell><cell>9.69</cell><cell>Girl Groups</cell><cell>0.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results for Multi-label Music Genre Classification of Albums</figDesc><table><row><cell>Modality</cell><cell>Target</cell><cell>Settings</cell><cell>Params</cell><cell>Time</cell><cell>AUC</cell><cell>C@1</cell><cell>C@3</cell><cell>C@5</cell></row><row><cell>AUDIO</cell><cell>LOGISTIC</cell><cell>TIMBRE-MLP</cell><cell>0.01M</cell><cell>1s</cell><cell>0.792</cell><cell>0.04</cell><cell>0.14</cell><cell>0.22</cell></row><row><cell>AUDIO</cell><cell>LOGISTIC</cell><cell>LOW-3X3</cell><cell>0.5M</cell><cell>390s</cell><cell>0.859</cell><cell>0.14</cell><cell>0.34</cell><cell>0.54</cell></row><row><cell>AUDIO</cell><cell>LOGISTIC</cell><cell>HIGH-3X3</cell><cell>16.5M</cell><cell>2280s</cell><cell>0.840</cell><cell>0.20</cell><cell>0.43</cell><cell>0.69</cell></row><row><cell>AUDIO</cell><cell>LOGISTIC</cell><cell>LOW-4X96</cell><cell>0.2M</cell><cell>140s</cell><cell>0.851</cell><cell>0.14</cell><cell>0.32</cell><cell>0.48</cell></row><row><cell>AUDIO</cell><cell>LOGISTIC</cell><cell>HIGH-4X96</cell><cell>5M</cell><cell>260s</cell><cell>0.862</cell><cell>0.12</cell><cell>0.33</cell><cell>0.48</cell></row><row><cell>AUDIO</cell><cell>LOGISTIC</cell><cell>LOW-4X70</cell><cell>0.35M</cell><cell>200s</cell><cell>0.871</cell><cell>0.05</cell><cell>0.16</cell><cell>0.34</cell></row><row><cell>AUDIO</cell><cell>LOGISTIC</cell><cell>HIGH-4X70</cell><cell>7.5M</cell><cell>600s</cell><cell>0.849</cell><cell>0.08</cell><cell>0.23</cell><cell>0.38</cell></row><row><cell>AUDIO</cell><cell>COSINE</cell><cell>LOW-3X3</cell><cell>0.33M</cell><cell>400s</cell><cell>0.864</cell><cell>0.26</cell><cell>0.47</cell><cell>0.65</cell></row><row><cell>AUDIO</cell><cell>COSINE</cell><cell>HIGH-3X3</cell><cell>15.5M</cell><cell>2200s</cell><cell>0.881</cell><cell>0.30</cell><cell>0.54</cell><cell>0.69</cell></row><row><cell>AUDIO</cell><cell>COSINE</cell><cell>LOW-4X96</cell><cell>0.15M</cell><cell>135s</cell><cell>0.860</cell><cell>0.19</cell><cell>0.40</cell><cell>0.52</cell></row><row><cell>AUDIO</cell><cell>COSINE</cell><cell>HIGH-4X96</cell><cell>4M</cell><cell>250s</cell><cell>0.884</cell><cell>0.35</cell><cell>0.59</cell><cell>0.75</cell></row><row><cell>AUDIO</cell><cell>COSINE</cell><cell>LOW-4X70</cell><cell>0.3M</cell><cell>190s</cell><cell>0.868</cell><cell>0.26</cell><cell>0.51</cell><cell>0.68</cell></row><row><cell>AUDIO (A)</cell><cell>COSINE</cell><cell>HIGH-4X70</cell><cell>6.5M</cell><cell>590s</cell><cell>0.888</cell><cell>0.35</cell><cell>0.60</cell><cell>0.74</cell></row><row><cell>TEXT</cell><cell>LOGISTIC</cell><cell>VSM</cell><cell>25M</cell><cell>11s</cell><cell>0.905</cell><cell>0.08</cell><cell>0.20</cell><cell>0.37</cell></row><row><cell>TEXT</cell><cell>LOGISTIC</cell><cell>VSM+SEM</cell><cell>25M</cell><cell>11s</cell><cell>0.916</cell><cell>0.10</cell><cell>0.25</cell><cell>0.44</cell></row><row><cell>TEXT</cell><cell>COSINE</cell><cell>VSM</cell><cell>25M</cell><cell>11s</cell><cell>0.901</cell><cell>0.53</cell><cell>0.44</cell><cell>0.90</cell></row><row><cell>TEXT (T)</cell><cell>COSINE</cell><cell>VSM+SEM</cell><cell>25M</cell><cell>11s</cell><cell>0.917</cell><cell>0.42</cell><cell>0.70</cell><cell>0.85</cell></row><row><cell>IMAGE (I)</cell><cell>LOGISTIC</cell><cell>RESNET</cell><cell>1.7M</cell><cell>4009s</cell><cell>0.743</cell><cell>0.06</cell><cell>0.15</cell><cell>0.27</cell></row><row><cell>A + T</cell><cell>LOGISTIC</cell><cell>MLP</cell><cell>1.5M</cell><cell>2s</cell><cell>0.923</cell><cell>0.10</cell><cell>0.40</cell><cell>0.64</cell></row><row><cell>A + I</cell><cell>LOGISTIC</cell><cell>MLP</cell><cell>1.5M</cell><cell>2s</cell><cell>0.900</cell><cell>0.10</cell><cell>0.38</cell><cell>0.66</cell></row><row><cell>T + I</cell><cell>LOGISTIC</cell><cell>MLP</cell><cell>1.5M</cell><cell>2s</cell><cell>0.921</cell><cell>0.10</cell><cell>0.37</cell><cell>0.63</cell></row><row><cell>A + T + I</cell><cell>LOGISTIC</cell><cell>MLP</cell><cell>2M</cell><cell>2s</cell><cell>0.936</cell><cell>0.11</cell><cell>0.39</cell><cell>0.66</cell></row><row><cell>A + T</cell><cell>COSINE</cell><cell>MLP</cell><cell>0.3M</cell><cell>2s</cell><cell>0.930</cell><cell>0.43</cell><cell>0.74</cell><cell>0.86</cell></row><row><cell>A + I</cell><cell>COSINE</cell><cell>MLP</cell><cell>0.3M</cell><cell>2s</cell><cell>0.896</cell><cell>0.32</cell><cell>0.57</cell><cell>0.76</cell></row><row><cell>T + I</cell><cell>COSINE</cell><cell>MLP</cell><cell>0.3M</cell><cell>2s</cell><cell>0.919</cell><cell>0.43</cell><cell>0.74</cell><cell>0.85</cell></row><row><cell>A + T + I</cell><cell>COSINE</cell><cell>MLP</cell><cell>0.4M</cell><cell>2s</cell><cell>0.931</cell><cell>0.42</cell><cell>0.72</cell><cell>0.86</cell></row><row><cell cols="9">Number of network hyperparameters, epoch training time, AUC-ROC, and catalog</cell></row><row><cell cols="6">coverage at k = 1, 3, 5 for different settings and modalities.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://musicbrainz.org/ 2 http://labs.acousticbrainz.org/million-song-dataset-echonest-archive 3 https://www.upf.edu/web/mtg/mumu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/sergiooramas/tartarus</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>This work was partially funded by the Spanish Ministry of Economy and Competitiveness under the Maria de Maeztu Units of Excellence Programme (MDM-2015-0502). The Tesla K40 used for this research was donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The million song dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thierry Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-collection evaluation for music classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Bogdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alastair</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Content-based music information retrieval: Current directions and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remco</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masataka</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="668" to="696" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What is this song about anyway?: Automatic classification of subject using user interpretations and lyrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kahyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Ha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen Downie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM/IEEE-CS Joint Conference on Digital Libraries</title>
		<meeting>the 14th ACM/IEEE-CS Joint Conference on Digital Libraries</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="453" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automatic tagging using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ISMIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04243</idno>
		<title level="m">Convolutional recurrent neural networks for music classification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Information-theoretical label embeddings for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Audio-based music classification with a pretrained convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philémon</forename><surname>Sander Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-toend learning for music audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6964" to="6968" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards score following in sheet music images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Arzt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Some improvements on deep convolutional neural network based image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5402</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining music reviews: Promising preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Stephen Downie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">F</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extreme multi-label loss functions for recommendation, tagging, ranking &amp; other missing label applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal music mood classification using audio and lyrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Laurier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Grivolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Applications, 2008. ICMLA&apos;08. Seventh International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="688" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You can judge an artist by an album cover: Using images for music annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janis</forename><surname>Libeks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Turnbull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mel frequency cepstral coefficients for music modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rhyme and style features for musical genre classification by song lyrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Neumayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Eric Battenberg, and Oriol Nieto. librosa: Audio and Music Signal Analysis in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcvicar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">Proc. of the 14th Python in Science Conf., (Scipy</title>
		<meeting>of the 14th Python in Science Conf., (Scipy</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<title level="m">Entity Linking meets Word Sense Disambiguation: A Unified Approach. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integration of text and audio features for genre classification in music information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Neumayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="724" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring customer reviews for music genre classification and evolutionary studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aonghus</forename><surname>Lawlor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flabase: Towards the creation of a flamenco music knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilia</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquín</forename><surname>Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A Deep Multimodal Approach for Coldstart Music Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Sordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sound and music recommendation with knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vito</forename><forename type="middle">Claudio</forename><surname>Ostuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><forename type="middle">Di</forename><surname>Noia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><forename type="middle">Di</forename><surname>Sciascio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A taxonomy of musical genres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cazaly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Content-Based Multimedia Information Access</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1238" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Experimenting with musically motivated convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Content-Based Multimedia Indexing (CBMI), 2016 14th International Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhancing multilabel music genre classification through ensemble techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sanden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An audiovisual approach to music genre classification through affective color features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="61" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Constant-Q transform toolbox for music processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schörkhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anssi</forename><surname>Klapuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Sound and Music Computing Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Using block-level features for genre classification, tag classification and music similarity estimation. Submission to Audio Music Similarity and Retrieval Task of MIREX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Seyerlehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Pohle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Knees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of evaluation in music genre recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Adaptive Multimedia Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perry</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;13 Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tag integrated multi-label music style classification with hypergraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsunori</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring the similarity space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="34" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
