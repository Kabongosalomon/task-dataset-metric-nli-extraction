<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RETHINKING EMBEDDING COUPLING IN PRE-TRAINED LANGUAGE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung</forename><surname>Won</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename></persName>
							<email>hwchung@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Févry</surname></persName>
							<email>thibaultfevry@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Tsai</surname></persName>
							<email>henrytsai@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
							<email>melvinp@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">Ruder</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">RETHINKING EMBEDDING COUPLING IN PRE-TRAINED LANGUAGE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Preprint. Under review.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that allocating additional capacity to the output embedding provides benefits to the model that persist through the fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the number of parameters at the fine-tuning stage. * equal contribution † Work done as a member of the Google AI Residency Program. 1 Output embedding is sometimes referred to as "output weights", i.e., the weight matrix in the output projection in a language model. 2  We focus on encoder-only models, and do not consider encoder-decoder models like T5 <ref type="bibr" target="#b45">(Raffel et al., 2020)</ref> where none of the embedding matrices are discarded after pre-training. Output embeddings may also be</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The performance of models in natural language processing (NLP) has dramatically improved in recent years, mainly driven by advances in transfer learning from large amounts of unlabeled data <ref type="bibr" target="#b23">(Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b15">Devlin et al., 2019)</ref>. The most successful paradigm consists of pre-training a large Transformer <ref type="bibr" target="#b56">(Vaswani et al., 2017)</ref> model with a self-supervised loss and fine-tuning it on data of a downstream task <ref type="bibr" target="#b47">(Ruder et al., 2019</ref>). Despite its empirical success, inefficiencies have been observed related to the training duration <ref type="bibr" target="#b34">(Liu et al., 2019b)</ref>, pre-training objective <ref type="bibr" target="#b11">(Clark et al., 2020b)</ref>, and training data <ref type="bibr" target="#b13">(Conneau et al., 2020a)</ref>, among others. In this paper, we reconsider a modeling assumption that may have a similarly pervasive practical impact: the coupling of input and output embeddings 1 in state-of-the-art pre-trained language models.</p><p>State-of-the-art pre-trained language models <ref type="bibr" target="#b15">(Devlin et al., 2019;</ref><ref type="bibr" target="#b34">Liu et al., 2019b)</ref> and their multilingual counterparts <ref type="bibr" target="#b15">(Devlin et al., 2019;</ref><ref type="bibr" target="#b13">Conneau et al., 2020a)</ref> have inherited the practice of embedding coupling from their language model predecessors <ref type="bibr" target="#b44">(Press &amp; Wolf, 2017;</ref><ref type="bibr" target="#b25">Inan et al., 2017)</ref>. However, in contrast to their language model counterparts, embedding coupling in encoder-only pre-trained models such as <ref type="bibr" target="#b15">Devlin et al. (2019)</ref> is only useful during pre-training since output embeddings are generally discarded after fine-tuning. <ref type="bibr">2</ref> In addition, given the willingness of researchers to exchange additional compute during pre-training for improved downstream performance (Raffel <ref type="table">Table 1</ref>: Overview of the number of parameters in (coupled) embedding matrices of state-of-the-art multilingual (top) and monolingual (bottom) models with regard to overall parameter budget. |V |: vocabulary size. N , N emb : number of parameters in total and in the embedding matrix respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Languages |V | N N emb %Emb.</p><p>mBERT <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref> 104 120k 178M 92M 52% XLM-R Base <ref type="bibr" target="#b13">(Conneau et al., 2020a)</ref> 100 250k 270M 192M 71% XLM-R Large <ref type="bibr" target="#b13">(Conneau et al., 2020a)</ref> 100 250k 550M 256M 47%</p><p>BERT Base <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref> 1 30k 110M 23M 21% BERT Large <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref> 1 30k 335M 31M 9% et al., 2020;  and the fact that pre-trained models are often used for inference millions of times , pre-training-specific parameter savings are less important overall.</p><p>On the other hand, tying input and output embeddings constrains the model to use the same dimensionality for both embeddings. This restriction limits the researcher's flexibility in parameterizing the model and can lead to allocating too much capacity to the input embeddings, which may be wasteful. This is a problem particularly for multilingual models, which require large vocabularies with high-dimensional embeddings that make up between 47-71% of the entire parameter budget <ref type="table">(Table 1</ref>), suggesting an inefficient parameter allocation.</p><p>In this paper, we systematically study the impact of embedding coupling on state-of-the-art pretrained language models, focusing on multilingual models. First, we observe that while naïvely decoupling the input and output embedding parameters does not consistently improve downstream evaluation metrics, decoupling their shapes comes with a host of benefits. In particular, it allows us to independently modify the input and output embedding dimensions. We show that the input embedding dimension can be safely reduced without affecting downstream performance. Since the output embedding is discarded after pre-training, we can increase its dimension, which improves fine-tuning accuracy and outperforms other capacity expansion strategies. By reinvesting saved parameters to the width and depth of the Transformer layers, we furthermore achieve significantly improved performance over a strong mBERT <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref> baseline on multilingual tasks from the XTREME benchmark <ref type="bibr">(Hu et al., 2020)</ref>. Finally, we combine our techniques in a Rebalanced mBERT (RemBERT) model that outperforms XLM-R <ref type="bibr" target="#b13">(Conneau et al., 2020a)</ref>, the state-of-the-art cross-lingual model while having been pre-trained on 3.5× fewer tokens and 10 more languages.</p><p>We thoroughly investigate reasons for the benefits of embedding decoupling. We observe that an increased output embedding size enables a model to improve on the pre-training task, which correlates with downstream performance. We also find that it leads to Transformers that are more transferable across tasks and languages-particularly for the upper-most layers. Overall, larger output embeddings prevent the model's last layers from over-specializing to the pre-training task <ref type="bibr" target="#b63">(Zhang et al., 2020;</ref><ref type="bibr" target="#b53">Tamkin et al., 2020)</ref>, which enables training of more general Transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Embedding coupling Sharing input and output embeddings in neural language models was proposed to improve perplexity and motivated based on embedding similarity <ref type="bibr" target="#b44">(Press &amp; Wolf, 2017)</ref> as well as by theoretically showing that the output probability space can be constrained to a subspace governed by the embedding matrix for a restricted case <ref type="bibr" target="#b25">(Inan et al., 2017)</ref>. Embedding coupling is also common in neural machine translation models where it reduces model complexity <ref type="bibr" target="#b18">(Firat et al., 2016)</ref> and saves memory <ref type="bibr" target="#b26">(Johnson et al., 2017)</ref>, in recent state-of-the-art language models <ref type="bibr" target="#b36">(Melis et al., 2020)</ref>, as well as all pre-trained models we are aware of <ref type="bibr" target="#b15">(Devlin et al., 2019;</ref><ref type="bibr" target="#b34">Liu et al., 2019b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transferability of representations</head><p>Representations of large pre-trained models in computer vision and NLP have been observed to transition from general to task-specific from the first to the useful for domain-adaptive pre-training <ref type="bibr" target="#b23">(Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b19">Gururangan et al., 2020)</ref>, probing <ref type="bibr" target="#b16">(Elazar &amp; Goldberg, 2019)</ref>, and tasks that can be cast in the pre-training objective <ref type="bibr" target="#b1">(Amrami &amp; Goldberg, 2019)</ref>. last layer <ref type="bibr" target="#b62">(Yosinski et al., 2014;</ref><ref type="bibr" target="#b23">Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b33">Liu et al., 2019a)</ref>. In Transformer models, the last few layers have been shown to become specialized to the MLM task and-as a result-less transferable <ref type="bibr" target="#b63">(Zhang et al., 2020;</ref><ref type="bibr" target="#b53">Tamkin et al., 2020)</ref>.</p><p>Multilingual models Recent multilingual models are pre-trained on data covering around 100 languages using a subword vocabulary shared across all languages <ref type="bibr" target="#b15">(Devlin et al., 2019;</ref><ref type="bibr" target="#b43">Pires et al., 2019;</ref><ref type="bibr" target="#b13">Conneau et al., 2020a)</ref>. In order to achieve reasonable performance for most languages, these models need to allocate sufficient capacity for each language, known as the curse of multilinguality <ref type="bibr" target="#b13">(Conneau et al., 2020a;</ref><ref type="bibr" target="#b41">Pfeiffer et al., 2020)</ref>. As a result, such multilingual models have large vocabularies with large embedding sizes to ensure that tokens in all languages are adequately represented.</p><p>Efficient models Most work on more efficient pre-trained models focuses on pruning or distillation <ref type="bibr" target="#b22">(Hinton et al., 2015)</ref>. Pruning approaches remove parts of the model, typically attention heads <ref type="bibr" target="#b37">(Michel et al., 2019;</ref><ref type="bibr" target="#b57">Voita et al., 2019)</ref> while distillation approaches distill a large pre-trained model into a smaller one . Distillation can be seen as an alternative form of allocating pre-training capacity via a large teacher model. However, distilling a pre-trained model is expensive  and requires overcoming architecture differences and balancing training data and loss terms <ref type="bibr" target="#b38">(Mukherjee &amp; Awadallah, 2020)</ref>. Our proposed methods are simpler and complementary to distillation as they can improve the pre-training of compact student models <ref type="bibr" target="#b55">(Turc et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL METHODOLOGY</head><p>Efficiency of models has been measured along different dimensions, from the number of floating point operations <ref type="bibr" target="#b49">(Schwartz et al., 2019)</ref> to their runtime . We follow previous work  and compare models in terms of their number of parameters during finetuning (see Appendix A.1 for further justification of this setting). For completeness, we generally report the number of pre-training (PT) and fine-tuning (FT) parameters.</p><p>Baseline Our baseline has the same architecture as multilingual BERT (mBERT; <ref type="bibr" target="#b15">Devlin et al., 2019)</ref>. It consists of 12 Transformer layers with a hidden size H of 768 and 12. Input and output embeddings are coupled and have the same dimensionality E as the hidden size, i.e. E out = E in = H. The total number of parameters during pre-training and fine-tuning is 177M (see Appendix A.2 for further details). We train variants of this model that differ in certain hyper-parameters but otherwise are trained under the same conditions to ensure a fair comparison.</p><p>Tasks For our experiments, we employ tasks from the XTREME benchmark <ref type="bibr">(Hu et al., 2020</ref>) that require fine-tuning, including the XNLI <ref type="bibr" target="#b12">(Conneau et al., 2018)</ref>, NER <ref type="bibr" target="#b40">(Pan et al., 2017)</ref>, PAWS-X <ref type="bibr" target="#b61">(Yang et al., 2019)</ref>, XQuAD <ref type="bibr" target="#b4">(Artetxe et al., 2020)</ref>, MLQA <ref type="bibr" target="#b32">(Lewis et al., 2020)</ref>, and TyDiQA-GoldP <ref type="bibr" target="#b10">(Clark et al., 2020a)</ref> datasets. We provide details for them in Appendix A.4. We average results across three fine-tuning runs and evaluate on the dev sets unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMBEDDING DECOUPLING REVISITED</head><p>Naïve decoupling Embeddings make up a large fraction of the parameter budget in state-of-theart multilingual models (see <ref type="table">Table 1</ref>). We now study the effect of embedding decoupling on such models. In <ref type="table" target="#tab_0">Table 2</ref>, we show the impact of decoupling the input and output embeddings in our baseline model with coupled embeddings. Naïvely decoupling the output embedding matrix slightly improves the performance as evidenced by a 0.4 increase on average. However, the gain is not uniformly observed in all tasks. Overall, these results suggest that decoupling the embedding matrices naïvely while keeping the dimensionality fixed does not greatly affect the performance of the model. What is more important, however, is that decoupling the input and output embeddings decouples the shapes, endowing significant modeling flexibility, which we investigate in the following.</p><p>Input vs output embeddings Decoupling input and output embeddings allows us to flexibly change the dimensionality of both matrices and to determine which one is more important for good transfer performance of the model. To this end, we compare the performance of a model with  </p><formula xml:id="formula_0">E in = 768, E out = 128</formula><p>to that of a model with E in = 128, E out = 768 3 . During fine-tuning, the latter model has 43% fewer parameters. We show the results in <ref type="table" target="#tab_1">Table 3</ref>. Surprisingly, the model pretrained with a larger output embedding size slightly outperforms the comparison method on average despite having 77M fewer parameters during fine-tuning. 4</p><p>Reducing the input embedding dimension saves a significant number of parameters at a noticeably smaller cost to accuracy than reducing the output embedding size. In light of this, the parameter allocation of multilingual models (see <ref type="table">Table 1</ref>) seems particularly inefficient. For a multilingual model with coupled embeddings, reducing the input embedding dimension to save parameters as proposed by <ref type="bibr" target="#b30">Lan et al. (2020)</ref> is very detrimental to performance (see Appendix A.5 for details).</p><p>The results in this section indicate that the output embedding plays an important role in the transferability of pre-trained representations. For multilingual models in particular, a small input embedding dimension frees up a significant number of parameters at a small cost to performance. In the next section, we study how to improve the performance of a model by resizing embeddings and layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EMBEDDING AND LAYER RESIZING FOR MORE EFFICIENT FINE-TUNING</head><p>Increasing the output embedding size In §4, we observed that reducing E out hurts performance on the fine-tuning tasks, suggesting E out is important for transferability. Motivated by this result, we study the opposite scenario, i.e., whether increasing E out beyond H improves the performance. We experiment with an output embedding size E out in the range {128, 768, 3072} while keeping the input embedding size E in = 128 and all other parts of the model the same.</p><p>We show the results in <ref type="table" target="#tab_2">Table 4</ref>. In all of the tasks we consider, increasing E out monotonically improves the performance. The improvement is particularly impressive for the more complex question answering datasets. It is important to note that during fine-tuning, all three models have the exact same sizes for E in and H. The only difference among them is the output embedding, which is discarded after pre-training. These results show that the effect of additional capacity during pre-training persists through the fine-tuning stage even if the added capacity is discarded after pre-training. We perform an extensive analysis on this behavior in §6. We show results with an English BERT Base model in Appendix A.6, which show the same trend.</p><p>Adding capacity via layers We investigate alternative ways of adding capacity during pre-training such as increasing the number of layers and discarding them after pre-training. For a fair comparison with the E out = 768 model, we add 11 additional layers (total of 23) and drop the 11 upper layers after pre-training. This setting ensures that both models have the same pre-training and fine-tuning parameters. We show the results in <ref type="table" target="#tab_3">Table 5</ref>. The model with additional layers performs poorly on the  question answering tasks, likely because the top layers contain useful semantic information <ref type="bibr" target="#b54">(Tenney et al., 2019)</ref>. In addition to higher performance, increasing E out relies only a more expensive dense matrix multiplication, which is highly optimized on typical accelerators and can be scaled up more easily with model parallelism <ref type="bibr" target="#b50">(Shazeer et al., 2018)</ref> because of small additional communication cost. We thus focus on increasing E out to expand pre-training capacity and leave an exploration of alternative strategies to future work.</p><p>Reinvesting input embedding parameters Reducing E in from 768 to 128 reduces the number of parameters from 177M to 100M. We redistribute these 77M parameters for the model with E out = 768 to add capacity where it might be more useful by increasing the width or depth of the model. Specifically, we 1) increase the hidden dimension H of the Transformer layers from 768 to 1024 5 and 2) increase the number of Transformer layers (L) from 12 to 23 at the same H to obtain models with similar number of parameters during fine-tuning. <ref type="table" target="#tab_4">Table 6</ref> shows the results for these two strategies. Reinvesting the input embedding parameters in both H and L improves performance on all tasks while increasing the number of Transformer layers L results in the best performance, with an average improvement of 3.9 over the baseline model with coupled embeddings and the same number of fine-tuning parameters overall.</p><p>A rebalanced mBERT We finally combine and scale up our techniques to design a rebalanced mBERT model that outperforms the current state-of-the-art unsupervised model, XLM-R <ref type="bibr" target="#b13">(Conneau et al., 2020a)</ref>. As the performance of Transformer-based models strongly depends on their number of parameters <ref type="bibr" target="#b45">(Raffel et al., 2020)</ref>, we propose a Rebalanced mBERT (RemBERT) model that matches XLM-R's number of fine-tuning parameters (559M) while using a reduced embedding size, resized layers, and more effective capacity during pre-training. The model has a vocabulary size of 250k, E in = 256, E out = 1536, and 32 layers with 1152 dimensions and 18 attention heads per layer and was trained on data covering 110 languages. We provide further details in Appendix A.7.</p><p>We compare RemBERT to XLM-R and the best-performing models on the XTREME leaderboard in <ref type="table" target="#tab_4">Table 16</ref> (see Appendix A.8 for the per-task results). <ref type="bibr">6</ref> The models in the first three rows use additional task or translation data for fine-tuning, which significantly boosts performance <ref type="bibr">(Hu et al., 2020)</ref>. XLM-R and RemBERT are the only two models that are fine-tuned using only the English training data of the corresponding task. XLM-R was trained with a batch size of 2 13 sequences each with 2 9 tokens and 1.5M steps (total of 6.3T tokens). In comparison, RemBERT is trained with 2 11 sequences of 2 9 tokens for 1.76M steps (1.8T tokens). Even though it was trained with 3.5× fewer tokens and has 10 more languages competiting for the model capacity, RemBERT outperforms XLM-R on all tasks we considered. This strong result suggests that our proposed methods are also effective at scale. We will release the pre-trained model checkpoint and the source code for RemBERT in order to promote reproducibility and share the pre-training cost with other researchers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ON THE IMPORTANCE OF THE OUTPUT EMBEDDING SIZE</head><p>We carefully design a set of experiments to analyze the impact of an increased output embedding size on various parts of the model. We study the nature of the decoupled input and output representations ( §6.1) and the transferability of the Transformer layers with regard to task-specific ( §6.2) and language-specific knowledge ( §6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">NATURE OF INPUT AND OUTPUT EMBEDDING REPRESENTATIONS</head><p>We first investigate to what extent the representations of decoupled input and output embeddings differ based on word embedding association tests <ref type="bibr" target="#b9">(Caliskan et al., 2017)</ref>. Similar to <ref type="bibr" target="#b44">(Press &amp; Wolf, 2017)</ref>, for a given pair of words, we evaluate the correlation between human similarity judgements of the strength of the relationship and the dot product of the word embeddings. We evaluate on MEN <ref type="bibr" target="#b8">(Bruni et al., 2014)</ref>, MTurk771 <ref type="bibr" target="#b20">(Halawi et al., 2012)</ref>, Rare-Word <ref type="bibr" target="#b35">(Luong et al., 2013)</ref>, SimLex999 <ref type="bibr" target="#b21">(Hill et al., 2015)</ref>, and Verb-143 <ref type="bibr" target="#b5">(Baker et al., 2014)</ref>. As our model uses subwords, we average the token representations for words with multiple subwords.</p><p>We show the results in <ref type="table" target="#tab_6">Table 8</ref>. In the first two rows, we can observe that the input embedding of the decoupled model performs similarly to the embeddings of the coupled model while the output embeddings have lower scores. <ref type="bibr">7</ref> We note that higher scores are not necessarily desirable as they only measure how well the embedding captures semantic similarity at the lexical level. Focusing on the difference in scores, we can observe that the input embedding learns representations that capture semantic similarity in contrast to the decoupled output embedding. At the same time, the decoupled model achieves higher performance in masked language modeling.</p><p>The last three rows of <ref type="table" target="#tab_6">Table 8</ref> show that as E out increases, the difference in the input and output embedding increases as well. With additional capacity, the output embedding progressively learns representations that differ more significantly from the input embedding. We also observe that the MLM accuracy increases with E out . Collectively, the results in <ref type="table" target="#tab_6">Table 8</ref> suggest that with increased capacity, the output embeddings learn representations that are worse at capturing traditional semantic similarity (which is purely restricted to the lexical level) while being more specialized to the MLM task (which requires more contextual representations). Decoupling embeddings thus give the model the flexibility to avoid encoding relationships in its output embeddings that may not be useful for its pre-training task. As pre-training performance correlates well with downstream performance <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>, forcing output embeddings to encode lexical information can hurt the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">CROSS-TASK TRANSFERABILITY OF TRANSFORMER LAYER REPRESENTATIONS</head><p>We investigate to what extent more capacity in the output embeddings during pre-training reduces the MLM-specific burden on the Transformer layers and hence prevents them from over-specializing to the MLM task.</p><p>Dropping the last few layers We first study the impact of an increased output embedding size on the transferability of the last few layers. Previous work <ref type="bibr" target="#b63">(Zhang et al., 2020;</ref><ref type="bibr" target="#b53">Tamkin et al., 2020)</ref> randomly reinitialized the last few layers to investigate their transferability. However, those parameters are still present during fine-tuning. We propose a more aggressive pruning scheme where we completely remove the last few layers. This setting demonstrates more drastically whether a model's upper layers are over-specialized to the pre-training task by assessing whether performance can be improved with millions fewer parameters. <ref type="bibr">8</ref> We show the performance of models with 8-12 remaining layers (removing up to 4 of the last layers) for different output embedding sizes E out on XNLI in <ref type="figure">Figure 1</ref>. For both E out = 128 and E out = 768, removing the last layer improves performance. In other words, the model performs better even with 7.1M fewer parameters. With E out = 128, the performance remains similar when removing the last few layers, which suggests that the last few layers are not critical for transferability.</p><p>As we increase E out , the last layers become more transferable. With E out = 768, removing more than one layer results in a sharp reduction in performance. Finally when E out = 3072, every layer is useful and removing any layer worsens the performance. This analysis demonstrates that increasing E out improves the transferability of the representations learned by the last few Transformer layers.</p><p>Probing analysis We further study whether an increased output embedding size improves the general natural language processing ability of the Transformer. We employ the probing analysis of <ref type="bibr" target="#b54">Tenney et al. (2019)</ref> and the mix probing strategy where a 2-layer dense network is trained on top of a weighted combination of the 12 Transformer layers. We evaluate performance with regard to core NLP concepts including part-of-speech tagging (POS), constituents (Consts.), dependencies (Deps.), entities, semantic role labeling (SRL), coreference (Coref.), semantic proto-roles (SPR), and relations (Rel.). For a thorough description of the task setup, see <ref type="bibr" target="#b54">Tenney et al. (2019)</ref>. 9  We show the results of the probing analysis in <ref type="table" target="#tab_7">Table 9</ref>. As we increase E out , the model improves across all tasks, even though the number of parameters is the same. This demonstrates that increasing E out enables the Transformer layers to learn more general representations. 10 6.3 CROSS-LINGUAL TRANSFERABILITY OF TRANSFORMER LAYER REPRESENTATIONS So far, our analyses were not specialized to multilingual models. Unlike monolingual models, multilingual models have another dimension of transferability: cross-lingual transfer, the ability to transfer knowledge from one language to another.</p><p>Previous work <ref type="bibr" target="#b43">(Pires et al., 2019;</ref><ref type="bibr" target="#b4">Artetxe et al., 2020)</ref> has found that MLM on multilingual data encourages cross-lingual alignment of representations without explicit cross-lingual supervision. While it has been shown that multilingual models learn useful cross-lingual representations, overspecialization to the pre-training task may result in higher layers being less cross-lingual and focusing on language-specific phenomena necessary for predicting the next word in a given language.</p><p>To investigate to what extent this is the case and whether increasing E out improves cross-lingual alignment, we evaluate the model's nearest neighbour translation accuracy <ref type="bibr" target="#b43">(Pires et al., 2019)</ref> on English-to-German translation (see Appendix A.9 for a description of the method).</p><p>We show the nearest neighbor translation accuracy for each layer in <ref type="figure">Figure 2</ref>. As E out increases, we observe that a) the Transformer layers become more language-agnostic as evidenced by higher accuracy and b) the language-agnostic representation is maintained to a higher layer as indicated by a flatter slope from layer 7 to 11. In all cases, the last layer is less language-agnostic than the previous one. The sharp drop in performance after layer 8 at E out = 128 is in line with previous results on cross-lingual retrieval <ref type="bibr" target="#b43">(Pires et al., 2019;</ref><ref type="bibr">Hu et al., 2020)</ref> and is partially mitigated by an increased E out . In sum, not only does a larger output embedding size improve cross-task transferability but it also helps with cross-lingual alignment and thereby cross-lingual transfer on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have assessed the impact of embedding coupling in pre-trained language models. We have identified the main benefit of decoupled embeddings to be the flexibility endowed by decoupling their shapes. We showed that input embeddings can be safely reduced and that larger output embeddings and reinvesting saved parameters lead to performance improvements. Our rebalanced multilingual BERT (RemBERT) outperforms XLM-R with the same number of fine-tuning parameters while having been trained on 3.5× fewer tokens. Overall, we found that larger output embeddings lead to more transferable and more general representations, particularly in a Transformer's upper layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 EFFICIENCY COMPARISON BASED ON PARAMETER COUNT DURING FINE-TUNING</head><p>We compare the efficiency of models based on their number of parameters. We believe this to be a reasonable proxy for a model's efficiency as the performance of Transformer-based language models has been shown to improve monotonically with the number of parameters <ref type="bibr" target="#b45">Raffel et al., 2020;</ref><ref type="bibr" target="#b31">Lepikhin et al., 2020;</ref><ref type="bibr" target="#b51">Shoeybi et al., 2019;</ref><ref type="bibr" target="#b0">Aharoni et al., 2019)</ref>. As the number of parameters during pre-training and fine-tuning may differ 11 , we compare models based on their number of parameters during the fine-tuning stage (without the task-specific head). We argue that this is the most practically relevant number as a model is generally pre-trained only once but may be fine-tuned or used for inference millions of times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 BASELINE MODEL DETAILS</head><p>Our baseline model has the same architecture as multilingual BERT (mBERT; <ref type="bibr" target="#b15">Devlin et al., 2019)</ref>. It consists of 12 Transformer layers with a hidden size H of 768 and 12 attention heads with 64 dimensions each. Input and output embeddings are coupled and have the same dimensionality E as the hidden size, i.e. E out = E in = H. The total number of parameters during pre-training and fine-tuning is 177M. We do not use dropout following the recommendation from <ref type="bibr" target="#b30">Lan et al. (2020)</ref>. We use the SentencePiece tokenizer <ref type="bibr" target="#b29">(Kudo &amp; Richardson, 2018)</ref> and a shared vocabulary of 120k subwords. The model is trained on Wikipedia dumps in 104 languages following <ref type="bibr" target="#b15">Devlin et al. (2019)</ref> using masked language modeling (MLM). We choose this baseline as its behavior has been thoroughly studied <ref type="bibr" target="#b27">(K et al., 2020;</ref><ref type="bibr" target="#b14">Conneau et al., 2020b;</ref><ref type="bibr" target="#b43">Pires et al., 2019;</ref><ref type="bibr" target="#b60">Wu &amp; Dredze, 2019)</ref>. [1 × 10 −5 , 2 × 10 −5 , 3 × 10 −5 ] 32 3 SQuAD</p><p>[2 × 10 −5 , 3 × 10 −5 , 4 × 10 −5 ] 32 3 NER</p><p>[1 × 10 −5 , 2 × 10 −5 , 3 × 10 −5 , 4 × 10 −5 , 5 × 10 −5 ] 32 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 TRAINING DETAILS</head><p>For all pre-training except for the large scale RemBERT, we trained using 64 Google Cloud TPUs. We trained over 26B tokens of Wikipedia data. All fine-tuning experiments were run on 8 Cloud TPUs. For all fine-tuning experiments other than RemBERT, we use batch size of 32. We sweep over the learning rate values specified in <ref type="table" target="#tab_8">Table 10</ref>.</p><p>We used the SentencePiece tokenizer trained with unigram language modeling A.4 XTREME TASKS</p><p>For our experiments, we employ tasks from the XTREME benchmark <ref type="bibr">(Hu et al., 2020)</ref>. We show statistics for them in <ref type="table" target="#tab_9">Table 11</ref>. XTREME includes the following datasets: The Cross-lingual Natural Language Inference (XNLI; <ref type="bibr" target="#b12">Conneau et al., 2018)</ref> corpus, the Cross-lingual Paraphrase Adversaries from Word Scrambling (PAWS-X; <ref type="bibr" target="#b61">Yang et al., 2019)</ref> dataset, part-of-speech (POS) tagging data from the Universal Dependencies v2.5 <ref type="bibr" target="#b39">(Nivre et al., 2018)</ref>   <ref type="bibr" target="#b65">Zweigenbaum et al., 2018)</ref>, and the Tatoeba dataset <ref type="bibr" target="#b3">(Artetxe &amp; Schwenk, 2019)</ref>. We refer the reader to <ref type="bibr">(Hu et al., 2020)</ref> for more details. We average results across three fine-tuning runs and evaluate on the dev sets unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 COMPARISON TO LAN ET AL. (2020)</head><p>Crucially, our finding differs from the dimensionality reduction in ALBERT <ref type="bibr" target="#b30">(Lan et al., 2020)</ref>. While they show that smaller embeddings can be used, their input and output embeddings are coupled and use a much smaller vocabulary (30k vs 120k). In contrast, we find that simultaneously decreasing both the input and output embedding size drastically reduces the performance of multilingual models.</p><p>In <ref type="table" target="#tab_0">Table 12</ref>, we show the impact of their factorized embedding parameterization on a monolingual and a multilingual model. While the English model suffers a smaller (0.8%) drop in accuracy, the  So far, we have focused on multilingual models as the number of saved parameters when reducing the input embedding size is largest for them. We now apply the same techniques to the English 12-layer BERT Base with a 30k vocabulary <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>. Specifically, we decouple the embeddings, reduce E in to 128, and increase the output embedding size or the number of layers during pre-training. We show the performance on MNLI  and SQuAD <ref type="bibr" target="#b46">(Rajpurkar et al., 2016)</ref> in <ref type="table" target="#tab_1">Table 13</ref>. By adding more capacity during pre-training, performance monotonically increases similar to the multilingual models. Interestingly, pruning a 24-layer model to 12 layers reduces performance, presumably because some upper layers still contain useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 REMBERT DETAILS</head><p>We design a Rebalanced mBERT (RemBERT) to leverage capacity more effectively during pretraining. The model has 995M parameters during pre-training and 575M parameters during finetuning. We pre-train on large unlabeled text using both Wikipedia and Common Crawl data, covering 110 languages. The details of hyperparameters and architecture are shown in <ref type="table" target="#tab_2">Table 14</ref>.</p><p>For each language l, we define the empirical distribution as p l = n l l ∈L n l</p><p>where n l is the number of sentences in l's pre-training corpus. Following <ref type="bibr" target="#b15">Devlin et al. (2019)</ref>, we use an exponentially smoothed distribution, i.e., we exponentiaate p l by α = 0.5 and renormalize to obtain the sampling distribution.</p><p>Hyperparameters and pre-training details are summarized in <ref type="table" target="#tab_2">Table 14</ref>. Hyperparameters used for the leaderboard submission are shown in <ref type="table" target="#tab_3">Table 15</ref>.  We show the detailed results for RemBERT and the comparison per task on the XTREME leaderboard in <ref type="table" target="#tab_4">Table 16</ref>. Compared to <ref type="table" target="#tab_5">Table 7</ref>, which shows the average across task categories, this table shows the average across tasks. For an English-to-German translation, we sample M = 5000 pairs of sentences from WMT16 <ref type="bibr" target="#b6">(Bojar et al., 2016)</ref>. For each sentence in each language, we obtain a representation v (l) LANG at each layer l by averaging the activations of all tokens (except the [CLS] and [SEP] tokens) at that layer. We then compute a translation vector from English to German by averaging the difference between the vectors of each sentence pair across all pairs:v (l)</p><formula xml:id="formula_2">EN→DE = 1 M M i=1 v (l) DEi − v (l) ENi .</formula><p>For each English sentence v (l) ENi , we can now translate it with this vector: v (l) ENi +v (l) EN→DE . We locate the closest German sentence vector based on 2 distance and measure how often the nearest neighbour is the correct pair.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>XNLI accuracy with the last layers removed. Larger E out improves transferability. Nearest-neighbor English-to-German translation accuracy of each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Effect of decoupling the input and output embedding matrices on performance on multiple tasks in XTREME. PT: Pre-training. FT: Fine-tuning.</figDesc><table><row><cell></cell><cell cols="2"># PT params params # FT</cell><cell cols="4">XNLI NER PAWS-X XQuAD Acc F1 Acc EM/F1</cell><cell>MLQA EM/F1</cell><cell cols="2">TyDi-GoldP Avg EM/F1</cell></row><row><cell>Coupled</cell><cell>177M</cell><cell>177M</cell><cell>70.7</cell><cell>69.2</cell><cell>85.3</cell><cell cols="2">46.2/63.2 37.3/53.1</cell><cell>40.7/56.7</cell><cell>62.3</cell></row><row><cell cols="2">Decoupled 269M</cell><cell>177M</cell><cell>71.3</cell><cell>68.9</cell><cell>85.0</cell><cell cols="2">46.9/63.8 37.3/53.1</cell><cell>42.8/58.1</cell><cell>62.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Performance of models with a large input and small output embedding size and vice versa.</figDesc><table><row><cell cols="2"># PT params params # FT</cell><cell cols="4">XNLI NER PAWS-X XQuAD Acc F1 Acc EM/F1</cell><cell>MLQA EM/F1</cell><cell cols="2">TyDi-GoldP Avg EM/F1</cell></row><row><cell>E in = 768, E out = 128 192M</cell><cell>177M</cell><cell>70.0</cell><cell>68.3</cell><cell>84.3</cell><cell cols="2">42.0/60.8 34.7/50.9</cell><cell>35.2/52.2</cell><cell>60.1</cell></row><row><cell>E in = 128, E out = 768 192M</cell><cell>100M</cell><cell>70.4</cell><cell>67.6</cell><cell>84.9</cell><cell cols="2">43.9/60.0 34.6/49.5</cell><cell>37.8/51.0</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Effect of an increased output embedding size E out on tasks in XTREME (E in = 128).</figDesc><table><row><cell></cell><cell cols="2"># PT params params # FT</cell><cell cols="4">XNLI NER PAWS-X XQuAD Acc F1 Acc EM/F1</cell><cell>MLQA EM/F1</cell><cell cols="2">TyDi-GoldP Avg EM/F1</cell></row><row><cell>E out = 128</cell><cell>115M</cell><cell>100M</cell><cell>68.1</cell><cell>65.2</cell><cell>83.3</cell><cell cols="2">38.6/54.8 30.9/45.2</cell><cell>32.2/44.2</cell><cell>56.6</cell></row><row><cell>E out = 768</cell><cell>192M</cell><cell>100M</cell><cell>70.4</cell><cell>67.6</cell><cell>84.9</cell><cell cols="2">43.9/60.0 34.6/49.5</cell><cell>37.8/51.0</cell><cell>60.2</cell></row><row><cell cols="2">E out = 3072 469M</cell><cell>100M</cell><cell>71.1</cell><cell>68.1</cell><cell>85.1</cell><cell cols="2">45.3/63.3 37.2/53.1</cell><cell>39.4/54.7</cell><cell>61.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Effect of additional capacity via more Transformer layers during pre-training (E in = 128).</figDesc><table><row><cell></cell><cell cols="2"># PT params params # FT</cell><cell cols="4">XNLI NER PAWS-X XQuAD Acc F1 Acc EM/F1</cell><cell>MLQA EM/F1</cell><cell cols="2">TyDi-GoldP Avg EM/F1</cell></row><row><cell>E out = 768</cell><cell>192M</cell><cell>100M</cell><cell>70.4</cell><cell>67.6</cell><cell>84.9</cell><cell cols="2">43.9/60.0 34.6/49.5</cell><cell>37.8/51.0</cell><cell>60.2</cell></row><row><cell cols="2">11 add. layers 193M</cell><cell>100M</cell><cell>71.2</cell><cell>67.3</cell><cell>85.0</cell><cell cols="2">38.8/55.5 31.4/46.6</cell><cell>31.3/45.5</cell><cell>58.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Effect of reinvesting the input embedding parameters to increase the hidden dimension H and number L of Transformer layers on XTREME tasks. E in = 128, E out = 768 model is included for an ablation study.</figDesc><table><row><cell></cell><cell cols="2"># PT params params # FT</cell><cell cols="4">XNLI NER PAWS-X XQuAD Acc F1 Acc EM/F1</cell><cell>MLQA EM/F1</cell><cell cols="2">TyDi-GoldP Avg EM/F1</cell></row><row><cell>Baseline</cell><cell>177M</cell><cell>177M</cell><cell>70.7</cell><cell>69.2</cell><cell>85.3</cell><cell cols="2">46.2/63.2 37.3/53.1</cell><cell>40.7/56.7</cell><cell>62.3</cell></row><row><cell cols="2">E in = 128, E out = 768 192M</cell><cell>100M</cell><cell>70.4</cell><cell>67.6</cell><cell>84.9</cell><cell cols="2">43.9/60.0 34.6/49.5</cell><cell>37.8/51.0</cell><cell>60.2</cell></row><row><cell>Reinvested in H</cell><cell>260M</cell><cell>168M</cell><cell>72.8</cell><cell>69.2</cell><cell>85.6</cell><cell cols="2">50.2/67.2 40.7/56.4</cell><cell>44.8/60.0</cell><cell>64.5</cell></row><row><cell>Reinvested in L</cell><cell>270M</cell><cell>178M</cell><cell>73.6</cell><cell>71.0</cell><cell>86.7</cell><cell cols="2">51.7/68.8 42.4/58.2</cell><cell>48.2/62.9</cell><cell>66.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Comparison of our model to other models on the XTREME leaderboard. Details about VECO are due to communication with the authors.</figDesc><table><row><cell></cell><cell># PT params</cell><cell># FT params</cell><cell>Langs</cell><cell>Add. task data</cell><cell>Trans-lation data</cell><cell cols="3">Sentence-pair Structured Classification Prediction Answering Avg Question Acc F1 EM/F1</cell></row><row><cell cols="4">Models fine-tuned on translations or additional task data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STiLTs (Phang et al., 2020)</cell><cell>559M</cell><cell>559M</cell><cell>100</cell><cell></cell><cell></cell><cell>83.9</cell><cell>69.4</cell><cell>67.2</cell><cell>73.5</cell></row><row><cell>FILTER (Fang et al., 2020)</cell><cell>559M</cell><cell>559M</cell><cell>100</cell><cell></cell><cell></cell><cell>87.5</cell><cell>71.9</cell><cell>68.5</cell><cell>76.0</cell></row><row><cell>VECO (Anonymous, 2021)</cell><cell>662M</cell><cell>662M</cell><cell>50</cell><cell></cell><cell></cell><cell>87.0</cell><cell>70.4</cell><cell>68.0</cell><cell>75.1</cell></row><row><cell cols="2">Models fine-tuned only on English task data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">XLM-R (Conneau et al., 2020a) 559M</cell><cell>559M</cell><cell>100</cell><cell></cell><cell></cell><cell>82.8</cell><cell>69.0</cell><cell>62.3</cell><cell>71.4</cell></row><row><cell>RemBERT (ours)</cell><cell>995M</cell><cell>575M</cell><cell>110</cell><cell></cell><cell></cell><cell>84.2</cell><cell>73.3</cell><cell>68.6</cell><cell>75.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Results on word embedding association tests for the input (I) and output (O) embeddings of models (left) and the models' masked language modeling performance (right). The first two rows show the performance of coupled and decoupled embeddings with the same embedding size E in = E out = 768. The last three rows show the performance as we increase the output embedding size with E in = 128.</figDesc><table><row><cell></cell><cell>I</cell><cell>MEN</cell><cell>O</cell><cell>MTurk771 I O</cell><cell>Rare-Word I O</cell><cell>Simlex999 I O</cell><cell>Verb-143 I O</cell><cell>MLM acc.</cell></row><row><cell>Coupled</cell><cell></cell><cell>40.8</cell><cell></cell><cell>37.5</cell><cell>25.0</cell><cell>20.1</cell><cell>56.0</cell><cell>Coupled</cell><cell>61.1</cell></row><row><cell>Decoupled</cell><cell cols="7">39.2 27.7 37.5 24.3 24.0 12.2 17.6 16.1 59.4 43.9</cell><cell>Decoupled</cell><cell>61.6</cell></row><row><cell>E out = 128</cell><cell cols="7">40.7 36.6 37.7 32.8 23.6 16.4 17.5 17.3 48.9 46.4</cell><cell>E out = 128</cell><cell>59.0</cell></row><row><cell>E out = 768</cell><cell cols="7">38.6 27.8 35.2 23.9 22.6 11.5 19.7 15.6 50.6 45.5</cell><cell>E out = 768</cell><cell>60.7</cell></row><row><cell cols="8">E out = 3072 40.1 10.8 36.2 8.8 22.6 -1.2 18.9 13.0 43.3 19.5</cell><cell>E out = 3072</cell><cell>62.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Probing analysis of Tenney et al. (2019) with mix strategy.</figDesc><table><row><cell>Eout = 128</cell><cell>115M</cell><cell>100M</cell><cell>96.7</cell><cell>87.9</cell><cell>94.3</cell><cell>93.7</cell><cell>91.7</cell><cell>95.0</cell><cell>67.2</cell><cell>83.0</cell><cell>82.7 77.0 86.9</cell></row><row><cell>Eout = 768</cell><cell>192M</cell><cell>100M</cell><cell>96.7</cell><cell>87.9</cell><cell>94.4</cell><cell>94.0</cell><cell>91.8</cell><cell>95.0</cell><cell>67.0</cell><cell>83.1</cell><cell>82.8 78.6 87.1</cell></row><row><cell>Eout = 3072</cell><cell>469M</cell><cell>100M</cell><cell>96.8</cell><cell>88.0</cell><cell>94.5</cell><cell>94.2</cell><cell>92.0</cell><cell>95.3</cell><cell>67.6</cell><cell>84.1</cell><cell>82.6 78.9 87.4</cell></row></table><note># PT params # FT params POS Const. Deps. Entities SRL Coref. O Coref. W SPR1 SPR2 Rel. Avg</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Fine-tuning hyperparameters for all models except RemBERT.</figDesc><table><row><cell></cell><cell>Learning rate</cell><cell cols="2">Batch size Train epochs</cell></row><row><cell>PAWS-X</cell><cell>[3 × 10 −5 , 4 × 10 −5 , 5 × 10 −5 ]</cell><cell>32</cell><cell>3</cell></row><row><cell>XNLI</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Statistics for the datasets in XTREME, including the number of training, development, and test examples as well as the number of languages for each task.</figDesc><table><row><cell>Task</cell><cell>Corpus</cell><cell>|Train|</cell><cell>|Dev|</cell><cell cols="2">|Test| |Lang.| Task</cell><cell>Metric</cell><cell>Domain</cell></row><row><cell>Classification</cell><cell>XNLI PAWS-X</cell><cell>392,702 49,401</cell><cell>2,490 2,000</cell><cell>5,010 2,000</cell><cell>15 NLI 7 Paraphrase</cell><cell>Acc. Acc.</cell><cell>Misc. Wiki / Quora</cell></row><row><cell>Structured</cell><cell>POS</cell><cell>21,253</cell><cell>3,974</cell><cell>47-20,436</cell><cell>33 POS</cell><cell>F1</cell><cell>Misc.</cell></row><row><cell>prediction</cell><cell>NER</cell><cell cols="3">20,000 10,000 1,000-10,000</cell><cell>40 NER</cell><cell>F1</cell><cell>Wikipedia</cell></row><row><cell>QA</cell><cell>XQuAD MLQA</cell><cell cols="2">87,599 34,726</cell><cell>1,190 4,517-11,590</cell><cell cols="3">11 Span extraction F1 / EM Wikipedia 7 Span extraction F1 / EM Wikipedia</cell></row><row><cell></cell><cell>TyDiQA-GoldP</cell><cell>3,696</cell><cell>634</cell><cell>323-2,719</cell><cell cols="3">9 Span extraction F1 / EM Wikipedia</cell></row><row><cell>Retrieval</cell><cell>BUCC Tatoeba</cell><cell>--</cell><cell cols="2">-1,896-14,330 -1,000</cell><cell>5 Retrieval 33 Retrieval</cell><cell>F1 Acc.</cell><cell>Wiki / news misc.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>treebanks, the Wikiann<ref type="bibr" target="#b40">(Pan et al., 2017)</ref> dataset for named entity recognition (NER), the Cross-lingual Question Answering Dataset (XQuAD;<ref type="bibr" target="#b4">Artetxe et al., 2020)</ref>, the Multilingual Question Answering (MLQA;<ref type="bibr" target="#b32">Lewis et al., 2020)</ref> dataset, the gold passage version of the Typologically Diverse Question Answering (TyDiQA;<ref type="bibr" target="#b10">Clark et al., 2020a)</ref> dataset, data from the third shared task of the workshop on Building and Using Parallel Corpora (BUCC;</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Effect of reducing the embedding size E for monolingual vs. multilingual models on MNLI and XNLI performance respectively. Monolingual numbers are from<ref type="bibr" target="#b30">Lan et al. (2020)</ref> and have vocabulary size of 30k.</figDesc><table><row><cell>English</cell><cell cols="2"># PT params # FT params</cell><cell>MNLI</cell><cell>Multilingual</cell><cell cols="2"># PT params # FT params</cell><cell>XNLI</cell></row><row><cell>E = H = 768</cell><cell>110M</cell><cell>110M</cell><cell>84.5</cell><cell>E = H = 768</cell><cell>177M</cell><cell>177M</cell><cell>70.7</cell></row><row><cell>E = H = 128</cell><cell>89M</cell><cell>89M</cell><cell>83.7</cell><cell>E = H = 128</cell><cell>100M</cell><cell>100M</cell><cell>68.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Effect of an increased output embedding size E out and additional layers during pre-training L = 15 on English BERT Base (E in = 128).</figDesc><table><row><cell></cell><cell># PT</cell><cell># FT</cell><cell cols="2">MNLI SQuAD</cell></row><row><cell></cell><cell cols="2">params params</cell><cell>Acc</cell><cell>EM/F1</cell></row><row><cell cols="2">BERT Base (ours) 110M</cell><cell>110M</cell><cell>79.8</cell><cell>78.4/86.2</cell></row><row><cell>E out = 128</cell><cell>93M</cell><cell>89M</cell><cell>75.9</cell><cell>75.5/84.2</cell></row><row><cell>E out = 768</cell><cell>112M</cell><cell>89M</cell><cell>77.5</cell><cell>77.5/85.5</cell></row><row><cell>E out = 3072</cell><cell>181M</cell><cell>89M</cell><cell>79.5</cell><cell>78.4/86.2</cell></row><row><cell>L = 15</cell><cell>114M</cell><cell>89M</cell><cell>80.1</cell><cell>78.7/86.3</cell></row><row><cell>L = 24</cell><cell>178M</cell><cell>89M</cell><cell>79.0</cell><cell>77.8/85.5</cell></row><row><cell cols="5">multilingual model's performance drops by 2.6%. Direct application of a factorized embedding</cell></row><row><cell cols="5">parameterization (Lan et al., 2020) is thus not viable for multilingual models.</cell></row><row><cell cols="2">A.6 ENGLISH MONOLINGUAL RESULTS</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Hyperparameters for RemBERT architecture and pre-training.</figDesc><table><row><cell>Hyperparameter</cell><cell>RemBERT</cell></row><row><cell>Number of layers</cell><cell>32</cell></row><row><cell>Hidden size</cell><cell>1152</cell></row><row><cell>Vocabulary size</cell><cell>250,000</cell></row><row><cell>Input embedding dimension</cell><cell>256</cell></row><row><cell>Output embedding dimension</cell><cell>1536</cell></row><row><cell>Number of attention heads</cell><cell>18</cell></row><row><cell>Attention head dimension</cell><cell>64</cell></row><row><cell>Dropout</cell><cell>0</cell></row><row><cell>Learning rate</cell><cell>0.0002</cell></row><row><cell>Batch size</cell><cell>2048</cell></row><row><cell>Train steps</cell><cell>1.76M</cell></row><row><cell>Adam β 1</cell><cell>0.9</cell></row><row><cell>Adam β 2</cell><cell>0.999</cell></row><row><cell>Adam</cell><cell>10 −6</cell></row><row><cell>Weight decay</cell><cell>0.01</cell></row><row><cell>Gradient clipping norm</cell><cell>1</cell></row><row><cell>Warmup steps</cell><cell>15000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>Hyperparameters for RemBERT fine-tuning.</figDesc><table><row><cell></cell><cell cols="3">Learning rate Batch size Train epochs</cell></row><row><cell>PAWS-X</cell><cell>8 × 10 −6</cell><cell>128</cell><cell>3</cell></row><row><cell>XNLI</cell><cell>1 × 10 −5</cell><cell>128</cell><cell>3</cell></row><row><cell>SQuAD</cell><cell>9 × 10 −6</cell><cell>128</cell><cell>3</cell></row><row><cell>POS</cell><cell>3 × 10 −5</cell><cell>128</cell><cell>3</cell></row><row><cell>NER</cell><cell>8 × 10 −6</cell><cell>64</cell><cell>3</cell></row><row><cell>A.8 XTREME TASK RESULTS</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Comparison of our model to other models on the XTREME leaderboard. Details about VECO are due to communication with the authors. Avg task is averaged over tasks whereas Avg is averaged over task categories just likeTable 7.</figDesc><table><row><cell></cell><cell cols="2"># PT params params # FT</cell><cell cols="4">XNLI POS NER PAWS-X XQuAD Acc F1 F1 Acc EM/F1</cell><cell>MLQA EM/F1</cell><cell cols="3">TyDi-GoldP Avg task Avg EM/F1</cell></row><row><cell cols="3">Models fine-tuned on translations or additional task data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STiLTs (Phang et al., 2020)</cell><cell>559M</cell><cell>559M</cell><cell>80.0</cell><cell>74.9 64.0</cell><cell>87.9</cell><cell cols="2">63.3/78.7 53.7/72.4</cell><cell>59.5/76.0</cell><cell>72.7</cell><cell>73.5</cell></row><row><cell>FILTER (Fang et al., 2020)</cell><cell>559M</cell><cell>559M</cell><cell>83.9</cell><cell>76.2 67.7</cell><cell>91.4</cell><cell cols="2">68.0/82.4 57.7/76.2</cell><cell>50.9/68.3</cell><cell>74.9</cell><cell>76.0</cell></row><row><cell>VECO (Anonymous, 2021)</cell><cell>662M</cell><cell>662M</cell><cell>83.0</cell><cell>75.1 65.7</cell><cell>91.1</cell><cell cols="2">66.3/79.9 54.9/73.1</cell><cell>58.9/75.0</cell><cell>74.1</cell><cell>75.1</cell></row><row><cell cols="2">Models fine-tuned only on English task data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">XLM-R (Conneau et al., 2020a) 559M</cell><cell>559M</cell><cell>79.2</cell><cell>73.8 65.4</cell><cell>86.4</cell><cell cols="2">60.8/76.6 53.2/71.6</cell><cell>45.0/65.1</cell><cell>70.1</cell><cell>71.4</cell></row><row><cell>RemBERT (ours)</cell><cell>995M</cell><cell>575M</cell><cell>80.8</cell><cell>76.5 70.1</cell><cell>87.5</cell><cell cols="2">64.0/79.6 55.0/73.1</cell><cell>63.0/77.0</cell><cell>74.4</cell><cell>75.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We linearly project the embeddings from Ein to H and from H to Eout.4  We observe the same trend if we control for the number of trainable parameters during fine-tuning by freezing the input embedding parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We choose 1024 dimensions to optimize efficient use of our accelerators.6  We do not consider retrieval tasks as they require intermediate task data<ref type="bibr" target="#b42">(Phang et al., 2020)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">This is opposite from what<ref type="bibr" target="#b44">Press &amp; Wolf (2017)</ref> observed in 2-layer LSTMs. They find that performance of the output embedding is similar to the embedding of a coupled model. This difference is plausible as the information encoded in large Transformers changes significantly throughout the model<ref type="bibr" target="#b54">(Tenney et al., 2019)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Each Transformer layer with H = 768 has about 7.1M parameters.9  The probing tasks are in English while our encoder is multilingual.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">In<ref type="bibr" target="#b54">(Tenney et al., 2019)</ref>, going from a BERT-base to a BERT-large model (with 3× more parameters) improves performance on average by 1.1 points, compared to our improvement of 0.5 points without increasing the number of fine-tuning parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">For encoder-only models such as BERT, parameters after the last Transformer layer (e.g. the output embeddings and the pooling layer) are discarded after pre-training.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massively Multilingual Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2019</title>
		<meeting>NAACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards better substitution-based word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Amrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12598</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">{VECO}: Variable encoder-decoder pre-training for cross-lingual understanding and generation</title>
		<ptr target="https://openreview.net/forum?id=YjNv-hzM8BE.underreview" />
	</analytic>
	<monogr>
		<title level="m">Submitted to International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Anonymous</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the ACL 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the Cross-lingual Transferability of Monolingual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An unsupervised model for instance level subcategorization acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="278" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Results of the wmt16 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kamran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="199" to="231" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCand lish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2020</title>
		<meeting>ICLR 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2018</title>
		<meeting>EMNLP 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.747" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emerging crosslingual structure in pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.536</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.536" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="6022" to="6034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13283</idno>
		<title level="m">oLMpics -On what Language Model Pre-training Captures</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05166</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero-Resource Translation with Multi-Lingual Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatos</forename><forename type="middle">T Yarman</forename><surname>Vural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="268" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal Language Model Fine-tuning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<meeting>ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
		<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017</title>
		<meeting>ICLR 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Google&apos;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-lingual ability of multilingual bert: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJeT3yrtDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling Laws for Neural Language Models. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Piyush Sharma, and Radu Soricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1eA7AEtvS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<title level="m">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating Cross-lingual Extractive Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Linguistic Knowledge and Transferability of Contextual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2019</title>
		<meeting>NAACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lstm</forename><surname>Mogrifier</surname></persName>
		</author>
		<title level="m">Proceedings of ICLR 2020</title>
		<meeting>ICLR 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Are Sixteen Heads Really Better than One?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS 2019</title>
		<meeting>NeurIPS 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">XtremeDistil : Multi-stage Distillation for Massive Multilingual Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2221" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Željko</forename><surname>Agić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Jesus</forename><surname>Aranzabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gashaw</forename><surname>Arutie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luma</forename><surname>Ateyah</surname></persName>
		</author>
		<idno>de- pendencies 2.2</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Mohammed Attia, et al. Universal</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2020</title>
		<meeting>EMNLP 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">English intermediate-task training improves zero-shot crosslingual transfer too</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13013</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1493" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/E17-2025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transfer learning in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10597</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8242-mesh-tensorflow-deep-learning-for-supercomputers.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="10414" to="10423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MobileBERT : a Compact Task-Agnostic BERT for Resource-Limited Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trisha</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Giovanardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14975</idno>
		<title level="m">Investigating Transferability in Pretrained Language Models. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">BERT rediscovers the classical NLP pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno>doi: 10.18653/ v1/P19-1452</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1452" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<title level="m">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2019</title>
		<meeting>ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2018</title>
		<meeting>NAACL-HLT 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">Rémi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1077</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1077" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">PAWS-X: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2019</title>
		<meeting>EMNLP 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3685" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05987</idno>
		<title level="m">Revisiting Fewsample BERT Fine-tuning. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05829</idno>
		<title level="m">HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Sharoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th Workshop on Building and Using Comparable Corpora</title>
		<meeting>11th Workshop on Building and Using Comparable Corpora</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="39" to="42" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
