<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">END-TO-END SPEECH RECOGNITION WITH ADAPTIVE COMPUTATION STEPS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Toshiba</settlement>
									<country>China) R&amp;D Center</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Toshiba</settlement>
									<country>China) R&amp;D Center</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hattori</forename><surname>Masanori</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Toshiba</settlement>
									<country>China) R&amp;D Center</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">END-TO-END SPEECH RECOGNITION WITH ADAPTIVE COMPUTATION STEPS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Adaptive Computation Steps</term>
					<term>Encoder-Decoder Framework</term>
					<term>End-to-End Training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present Adaptive Computation Steps (ACS) algorithm, which enables end-to-end speech recognition models to dynamically decide how many frames should be processed to predict a linguistic output. The model that applies ACS algorithm follows the encoder-decoder framework, while unlike the attention-based models, it produces alignments independently at the encoder side using the correlation between adjacent frames. Thus, predictions can be made as soon as sufficient acoustic information is received, which makes the model applicable in online cases. Besides, a small change is made to the decoding stage of the encoder-decoder framework, which allows the prediction to exploit bidirectional contexts. We verify the ACS algorithm on a Mandarin speech corpus AIShell-1, and it achieves a 31.2% CER in the online occasion, compared to the 32.4% CER of the attention-based model. To fully demonstrate the advantage of ACS algorithm, offline experiments are conducted, in which our ACS model achieves an 18.7% CER, outperforming the attention-based counterpart with the CER of 22.0%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The recent popularity of sequence-to-sequence (seq2seq) models in the field of automatic speech recognition (ASR) has significantly challenged the dominating role of traditional HMM-based ASR systems. Especially, the explorations in end-to-end recurrent neural networks (RNNs) enabled researchers to merge separate components of a traditional system (acoustic, pronunciation and language models) into one integral structure. Such end-to-end models not only reform the way of training by jointly optimizing all of their sub-components without any prior knowledge, but also make it possible for the ASR system to generate outputs at any linguistic level, ranging from phonemes to syllables or even words.</p><p>Unlike the HMM-based models predicting a target (HMM-state) for each frame, some seq2seq models recognize utterances by aligning to some parts of the inputs at each decoding step. These models are epitomized by the attention-based encoder-decoder networks, upon which groundbreaking progress was made on neural machine translation <ref type="bibr" target="#b0">[1]</ref>, image caption generation <ref type="bibr" target="#b1">[2]</ref> and handwriting synthesis <ref type="bibr" target="#b2">[3]</ref> tasks. The architecture was soon introduced to training ASR models, and achieved promising results in phoneme <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and character <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> recognitions. When exposed to sufficient training data, RNNs equipped with multi-head attention mechanism <ref type="bibr" target="#b7">[8]</ref> could even acquire competitive word level performance compared with HMM-DNN hybrids.</p><p>In this paper, we propose a novel end-to-end ASR model which dynamically adapts the number of computation steps that are needed to predict a linguistic output. The idea is inspired by the fact that the time we use to recognize a word or a syllable is always less than the time we use to listen to it. Besides, the predictions are only made when we are confident enough on the received acoustic information. Similarly, ASR apparatus takes in speech signals as a sequence of frames, and produces discrete outputs only at certain times. For most of the computation steps, the model may be seen as to ponder and consolidate the information in the input sequence and wait for a proper timing to emit an output with confidence. Therefore, we aim to make the ASR model spontaneously decide the time for making a prediction, and limit the time of decision-making to a small vicinity of input frames to avoid incurring big delays.</p><p>As a comparison to the attention-based models, we improve the performance of the end-to-end ASR system in the following aspects:</p><p>1. Instead of referring to the entire input sequence, for each output step, our model only focuses on a block of continuous inputs that are acoustically related to the target. 2. The speech is processed in a left-to-right frame-wise manner so that the model is applicable in online occasions. 3. The outputs predicted by the decoder will not only depend on the decoding histories, but on bidirectional contexts. 4. We combine an external RNN language model with the ASR model to make them work in parallel, which keeps the decoding process still in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATION TO PRIOR WORK</head><p>Though attention-based models have gained popularity due to the simplicity in training and efficiency in decoding, there are some problems caused by their intrinsic working principles. First, at each decoding step, the attention mechanism calculates a score for every hidden state (referred to as memory in terminology) of the RNN encoder, which brings huge computation burdens to the system, especially for long utterances. From the acoustic perspective, the information in a feature frame is only correlated to its a few neighbors. Therefore, even if attention is applied to all frames, only a small group of them are making contribution in predicting the target. As proposed in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, one of the solutions is to slide a window along the memory to restrict the frames to be attended to. However, as the size and stride of the window are empirically estimated from the training data, it is hard to ensure that all target-pronounced inputs are included in the window. This is very likely to happen when the speaking speed varies greatly in a corpus.</p><p>Moreover, we know that speech recognition is a strictly left-toright decoding process, so the inputs that have already been attended are not expected to be referred back at subsequent decoding steps. Several attempts have been made to encourage such monotonicity, including penalizing attentions that align to previously observed memory entries <ref type="bibr" target="#b3">[4]</ref>, predicting the next alignment center given the current attention distribution <ref type="bibr" target="#b8">[9]</ref>, and replacing the softmax-based attention mechanism with a hard monotonic aligning algorithm <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. Strategies above effectively force the attention-based decoder to move forward along the utterance, whereas the occurrences of extra complexities make the models even harder to optimize.</p><p>The model proposed in this paper will circumvent these problems by processing frames incrementally and predicting targets as soon as sufficient information is gathered from the computation steps. The detailed model architecture will be given in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ADAPTIVE COMPUTATION STEPS FOR SPEECH RECOGNITION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder-Decoder Architecture</head><p>The general framework of the model we proposed consists of two recurrent neural networks, referred to as encoder and decoder. Let = ( 1 , 2 , … , ) denotes the sequence of input feature vectors, and = ( 1 , 2 , … , ) be the output sequence generated from input x through the architecture. To begin with, the RNN encoder transforms input x into a sequence of higher dimensional representations = ( 1 , 2 , … , ). For standard multi-layer RNNs, h is in the same length with inputs x, as the network generates a hidden state for each inputting time-step. However, as we know, it usually dozens tens of frames to pronounce a syllable or word, thus keeping the representation for all time-steps might result in redundant information for the rest of model components. To solve this, we use the pyramidal RNN architecture <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> as the encoder, which down-samples the hidden states of the previous layer before feeding them to the next layer. The input to the ℎ layer at time-step j is calculated as:</p><formula xml:id="formula_0">= ( −1 , [ 2 −1 −1 , 2 −1 ])<label>(1)</label></formula><p>Compared to the frame-skipping strategy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> deployed at the input layer, pyramidal RNN reduces the layer-wise time resolution without losing the continuity of information, which helps to stabilize the following stage of pondering and encourage the model to make quicker decisions. See <ref type="figure">Figure 1</ref> for an example of such encoders. The RNN decoder is designed to predict the output sequence y conditioned on some entries of h, as well as the decoding histories.</p><p>To be concrete, the probability distribution over is a function of the decoder state −1 , the embedding of the previously decoded −1 , and the context summarized by a halting layer:</p><formula xml:id="formula_1">p(y ) = ( ( −1 , −1 , )) (2)</formula><p>The decoder, in a manner, plays the role of the language model as in the traditional ASR system. We achieve this by providing the ground-truth label as the previous prediction −1 in the training stage. The grammar dependencies are propagated in the decoder hidden states and help to produce rational sentences as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Computation Steps</head><p>Let be the number of transformations that the encoder performs before predicting a target at decoding step i. To determine , we first apply a 1-D convolutional neural network (CNN) on the representations h along encoding time j. Thus for each encoder time-step, the CNN extracts an energy vector from a small segment of h around :</p><formula xml:id="formula_2">= Convolution1d( � ) (3)</formula><p>where � is a window centered at . The dimension of depends on the number of output channels of the CNN kernel. Then is projected to a scalar by a sigmoidal halting unit <ref type="bibr" target="#b15">[16]</ref>, whose activation will be used to determine the halting probability of the current computation step:</p><formula xml:id="formula_3">= ( )<label>(4)</label></formula><p>where (•) is the logistic sigmoid function. We keep inspecting the accumulation of the along time j, and halt the encoder's computation as soon as the sum exceeds 1. Suppose the above described "pondering" process starts from time j = 1, and we can calculate N as:</p><formula xml:id="formula_4">= min � : � =1 ≥ 1 − � (5)</formula><p>Note that we expect the halting probabilities within the pondering interval to be summed up to 1, thus for the last computation step, named as remainder, should be modified to:</p><formula xml:id="formula_5">= 1 − � −1 =1 (6)</formula><p>Therefore, we summarize the definition of halting probability as:</p><formula xml:id="formula_6">= � = ℎ<label>(7)</label></formula><p>The ϵ in equation <ref type="formula">(5)</ref> is a small constant of offset (e.g. 0.01), whose purpose is to allow the encoder to halt computing after one single update, otherwise a minimum of two steps must be taken, as the activation of the sigmoid function can converge to but never reach 1.</p><p>After finishing the adapted steps of computation for one decoding time, the resulted halting probabilities turn out to be a distribution over a bunch of continuous representations that make up the <ref type="figure">Fig. 1</ref>. A 3-layer pyramidal RNN encoder. The top two layers totally reduce the frame rate by a factor of 4 (2 2 ). target being predicted. All of the entries observed within the computation steps ought to take part in emitting the current output. Thus, a context for blending the information in 1~ is given as:</p><formula xml:id="formula_7">= � =1<label>(8)</label></formula><p>We then feed to the RNN decoder to generate and repeat the procedures above starting from +1 until the end of speech is reached. <ref type="figure" target="#fig_0">Figure 2</ref> demonstrates the ACS algorithm for a complete pondering interval.</p><p>An alternative approach to determine whether to halt or continue the computation is to draw binary samples from the halting distribution <ref type="bibr" target="#b16">[17]</ref>, where the decision-making is treated as a Bernoulli process <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. However, the stochastic gradient estimation precludes the use of backpropagation during model training, and the escalating sampling noise will result in huge derivation for all following decisions <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding with Bidirectional Contexts</head><p>As shown in equation <ref type="formula">(2)</ref>, the prediction of a target depends on the decoding histories and a context vector computed for the current output step. However, we also expect to involve the future context information into the decoding stage to enhance the coherence within the output sequence. Thus, instead of feeding the context vector immediately to the decoder, we keep it until a few following ones are computed by the ACS algorithm. Then we concatenate the context vectors in both directions, and input them to the decoder:</p><formula xml:id="formula_8">P(y ) = ( ( −1 , −1 , [ − ; … ; ; … ; + ]))<label>(9)</label></formula><p>where w is the number of neighboring context vectors at each side. We restrict w to a small value (less than 3) to minimize the damage it does to the real-time performance. Note the online bidirectional contexts can only be applied when the encoder and decoder work independently (not the case for attention-based models), so that any number of context vectors can be produced within one decoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Joint Decoding with RNN Language Model</head><p>From the perspective of language modelling, the end-to end models suffer from the fact that they are only exposed to the speech transcriptions encountered in the training stage. One of the solutions is to incorporate a language model (LM) that is separately trained on large external corpora. Then the LM can be either embedded in a finite state transducer (FST) to construct a searching network, or used to rescore the beam search results obtained by the end-to-end decoding process <ref type="bibr" target="#b6">[7]</ref>. Our proposal is to introduce a RNN-LM, which works in parallel with the model's decoder and produces a LM probability ( ) for the output at each decoding step <ref type="bibr" target="#b18">[19]</ref>. We interpolate ( ) with p(y|x) that is calculated by the decoder to give the final score of the candidate:</p><formula xml:id="formula_9">s(y|x) = log p(y|x) + γ log ( ) (10)</formula><p>where γ is a scaling factor to balance the role of the LM, whose value can be determined on a development set. Here the RNN-LM ingests the grammar of the previously decoded sequence in its cell states, and acts on the subsequent outputs jointly with the decoder, which keeps the decoding process still in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>All experiments were conducted on an open-source Mandarin speech corpus AIShell-1 <ref type="bibr" target="#b19">[20]</ref>. We trained our models on the 150hour/120098-sentence set with all utterances containing more than 1000 frames removed to save the computation power, and used the 10-hour/14236-sentence development set for early-stopping. The models were finally evaluated on another 7176 sentences presenting speech of approximately 5 hours. The features as the model inputs consist of 1) 24-dimensional energy augmented mel frequency cepstral coefficients (MFCC) with delta and acceleration, 2) 1-dimensional sub-band time average cepstrum coefficient (STAC) with delta, as well as 3) 1-dimensional pitch information with delta and acceleration (total 74 dimensional features). The decoding targets include 7065 Chinese characters and four special tokens as &lt;UNK&gt; (unknown), &lt;PAD&gt; (padding), &lt;SOS&gt; (start of speech) and &lt;EOS&gt; (end of speech).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>We started from training an ACS model for online decoding with the unidirectional RNN encoder. Specifically, the encoder was a 3-layer 512-unit pyramidal GRU network with the top two layers totally reducing the frame rate by a factor of 4. The decoder was a 1-layer standard GRU with 256 units. For the halting layer, we used 64 CNN kernels with the width of 3 activated by a rectified linear unit (ReLU). Then, to fully explore the potential of the ACS algorithm, an ACS model with bidirectional RNN encoder was also trained for offline occasions. The encoder used a 3-layer GRU with 256 units for both directions, while the decoder and halting layer shared the same structure with their counterparts in the online ACS model. All weights were uniformly initialized within the range [-0.1, 0.1]. We trained each model for 30 epochs with Adam optimization algorithm <ref type="bibr" target="#b20">[21]</ref> and exponential weight decay strategy. The gradient norm <ref type="bibr" target="#b21">[22]</ref> was clipped to 2 for the first 20 epochs and 1 for the rest. Cross-entropy between the predicted Chinese character and its ground-truth label was used to define the loss function.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18.7%</head><p>The baseline in terms of character error rate (CER) came from the s5 recipe of the Kaldi toolkit <ref type="bibr" target="#b22">[23]</ref>, together with attention-based models for end-to-end level comparison. Note the encoder-decoder architectures in these attention-based models were exactly the same as in our ACS models, for both online and offline cases respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Detailed experiment results are given in <ref type="table" target="#tab_0">Table 1</ref>. For the online category, the vanilla ACS model achieves a 35.2 % CER parity with the 34.9% CER of the attention-based model. Additional improvements are obtained by applying bidirectional contexts in the decoding stage, and the resulted CER of 33.5% brings an error reduction rate (ERR) of 3.4% for the ACS model. When introducing an external RNN-LM, the CER further drops to 31.2%, lower than the 32.4% CER achieved by the attention-based model under the same condition.</p><p>In offline cases, the advantage of bidirectional RNN encoder makes both ACS and attention-based models outperform their online counterparts. Under such condition, the vanilla ACS model achieving 21.6% CER defeats the attention-based one (23.2%) by an absolute CER reduction of 1.6%, and the disparity rises to 3.3% (18.7% vs. 22.0%) when the bidirectional contexts and RNN-LM are involved. The best result attained by the end-to-end models is still far behind the 8.5% CER of the state-of-the-art HMM-DNN model, but we believe the inferiority will be mitigated by larger training corpora and advanced model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSIONS</head><p>Most of the prevailing attention mechanisms used in speech recognition are referred to as soft attention <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. It is assumed that the elements required in predicting an output are distributed in multiple inputs. Entries of significant relevance are assigned with higher attention scores to emphasize their roles in generating a transcription. To the contrary, the hard attention mechanism is also adopted by ASR tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, based on the fact that the inputs and outputs hold strictly monotonic alignment properties. Models equipped with hard attention are allowed to focus on single timesteps of the input sequence.</p><p>The alignment strategy implied in ACS algorithm combines the advantages of both soft and hard attention mechanisms. On one hand, the aligner calculates a probability of halting for each encoder timestep within the pondering interval, and accordingly summarizes a context vector as does the soft attention-based model. On the other hand, our model constantly inspects the accumulation of the halting probabilities, and makes hard decisions to emit outputs as soon as the sum reaches the threshold. Such eclecticism not only remains the reliabilities by taking advantages of multi-entry alignment used in soft attention mechanism, but also overcomes the problem that models are usually non-differentiable in hard attention architectures.</p><p>Moreover, as mentioned in Section 2, the soft attention-based model is computationally expensive as it passes over the entire input sequence at each decoding step, which results in a time complexity of (TU) <ref type="bibr" target="#b4">[5]</ref>, where T and U are the memory and output sequence lengths. In contrast, our model allows for decoding in linear-time complexity (T+U) by processing utterances in a monotonically frame-wise manner, where only T terms of halting probabilities are computed throughout the decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this paper, we propose a novel end-to-end ASR model that adaptively decides the number of computation steps towards the input frames before outputting the corresponding transcription. The model is reported to be more computationally efficient than the attentionbased architecture and naturally applicable in online occasions. The results evaluated on an open-source Mandarin test set show that the proposed model achieves better performances than the attentionbased models in both online and offline cases.</p><p>The core algorithm used to establish the model, known as adaptive computation steps, is realized by a halting layer, which consists of a CNN and a sigmoidal unit. The structure extracts the internal correlations between the adjacent frames, upon which the halting probability is calculated for the model to decide when to make a prediction. We may understand the halting probability as the contribution that an entry makes in predicting a target, which shares the same idea with the attention weights. However, the restriction of its value to (0, 1) imposed by the sigmoid function circumvents the problem caused by normalization, which can only be done after the whole pass of attention is performed over the entire inputs chain. This makes the models with ACS algorithm show great advantages over the attention-based ones in online applications.</p><p>Our ACS model performs well in tasks where the output units have relatively clear borders, like Mandarin character (syllable) in this paper. However, for smaller linguistic units that badly overlap in time, such as phoneme, it is difficult for the ACS algorithm to give explicit alignments of the targets all the time. This is also a common shortcoming for the ASR models that decode based on hard alignments. We will leave this problem for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The workflow of ACS algorithm. The halting probability is calculated out of the representations by the halting layer, which consists of a 1-D CNN layer and a sigmoidal unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Character Error Rate (CER) on HMM-DNN and end-toend models. The results of attention-based and ACS models were decoded using beam search algorithm with the width of 8.</figDesc><table><row><cell>Model</cell><cell>CER</cell></row><row><cell>HMM-Hybrid Models</cell><cell></cell></row><row><cell>HMM-DNN [19]</cell><cell>8.5%</cell></row><row><cell>Online Character Models</cell><cell></cell></row><row><cell>Attention</cell><cell>34.9%</cell></row><row><cell>Attention + RNN-LM</cell><cell>32.4%</cell></row><row><cell>ACS</cell><cell>35.2%</cell></row><row><cell>ACS + Bidirectional Contexts (w=1)</cell><cell>33.5%</cell></row><row><cell>ACS + Bidirectional Contexts (w=1) + RNN-LM</cell><cell>31.2%</cell></row><row><cell>Offline Character Models</cell><cell></cell></row><row><cell>Attention</cell><cell>23.2%</cell></row><row><cell>Attention + RNN-LM</cell><cell>22.0%</cell></row><row><cell>ACS</cell><cell>21.6%</cell></row><row><cell>ACS + Bidirectional Contexts (w=1)</cell><cell>19.8%</cell></row><row><cell>ACS + Bidirectional Contexts (w=1) + RNN-LM</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>All experiments were conducted using Tensorflow libraries <ref type="bibr" target="#b23">[24]</ref>.</p><p>The authors would like to thank Jiaxin Wen for her expertise in linguistics that helped to consolidate the idea proposed in this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>in ICML-15</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems: Workshop Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-Based Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="Http://arxiv.org/abs/1508.04395" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4960" to="4964" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">State-ofthe-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gonina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01769</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local monotonic attention mechanism for end-to-end speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="431" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online and linear-time attention by enforcing monotonic alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning online alignments with continuous rewards policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICASSP. IEEE</publisher>
			<biblScope unit="page" from="2801" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On online attention-based speech recognition and joint mandarin character-pinyin training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3404" to="3408" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Joint CTC-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06773</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simplifying long short-term memory acoustic models for fast training and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICASSP. IEEE</publisher>
			<biblScope unit="page" from="2284" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06947</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08575</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning algorithms for recurrent neural networks and their computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Back-propagation: Theory, architectures and applications</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="433" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Advances in joint ctc-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AIShell-1: An open-source Mandarin speech corpus and a speech recognition baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bengu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Oriental COCOSDA</title>
		<meeting>Oriental COCOSDA</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
