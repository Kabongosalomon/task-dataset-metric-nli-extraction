<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Random Search and Reproducibility for Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University AMEET TALWALKAR</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University and Determined AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Random Search and Reproducibility for Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Additional Key Words and Phrases: Neural Architecture Search</term>
					<term>Hyperparameter Optimization</term>
					<term>Automated Machine Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks-PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS [41], a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on multiple runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep learning offers the promise of bypassing the process of manual feature engineering by learning representations in conjunction with statistical models in an end-to-end fashion. However, neural network architectures themselves are typically designed by experts in a painstaking, ad-hoc fashion. Neural architecture search (NAS) presents a promising path for alleviating this pain by automatically identifying architectures that are superior to hand-designed ones. Since the work by Zoph and Le <ref type="bibr" target="#b52">[51]</ref>, there has been explosion of research activity on this problem <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b51">50]</ref>. Notably, there has been great industry interest in NAS, as evidenced by the vast computational <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b53">52]</ref> and marketing resources <ref type="bibr" target="#b18">[17]</ref> committed to industry-driven NAS research. However, despite a steady stream of promising empirical results <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b53">52]</ref>, we see three fundamental issues with the current state of NAS research:</p><p>Inadequate Baselines. Leading NAS methods exploit many of the strategies that were initially explored in the context of traditional hyperparameter optimization tasks, e.g., evolutionary search <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b41">40]</ref>, Bayesian optimization <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b45">44]</ref>, and gradient-based approaches <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b37">36]</ref>. Moreover, the NAS problem is in fact a specialized instance of the broader hyperparameter optimization problem. However, in spite of the close relationship between these two problems, existing comparisons between novel NAS methods and standard hyperparameter optimization methods are inadequate. In particular, to the best of our knowledge, no state-of-the-art hyperparameter optimization methods have been evaluated on standard NAS benchmarks. Without benchmarking against leading hyperparameter optimization baselines, it difficult to quantify the performance gains provided by specialized NAS methods.</p><p>Complex Methods. We have witnessed a proliferation of novel NAS methods, with research progressing in many different directions. New approaches introduce a significant amount of algorithmic complexity in the search process, including complicated training routines <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b48">47]</ref>, architecture transformations <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b47">46]</ref>, and modeling assumptions <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b51">50]</ref> (see <ref type="figure">Figure 1</ref> and Section 1.1 for more details). While many technically diverse NAS methods demonstrate good empirical performance, they often lack corresponding ablation studies Authors' addresses: Liam Li, me@liamcli.com, Carnegie Mellon University; Ameet Talwalkar, talwalkar@cmu.edu, Carnegie Mellon University and Determined AI. <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b51">50]</ref>, and as a result, it is unclear what NAS component(s) are necessary to achieve a competitive empirical result.</p><p>Lack of Reproducibility. Experimental reproducibility is of paramount importance in the context of NAS research, given the empirical nature of the field, the complexity of new NAS methods, and the steep computational costs associated with empirical evaluation. In particular, there are (at least) two important notions of reproducibility to consider: (1) "exact" reproducibility i.e., whether it is possible to reproduce explicitly reported experimental results; and "broad" reproducibility, i.e., the degree to which the reported experimental results are themselves robust and generalizable. Broad reproducibility is difficult to measure due to the computational burden of NAS methods and the high variance associated with extremal statistics. However, most of the published results in this field do not even satisfy exact reproducibility. For example, of the 12 papers published since 2018 at NeurIPS, ICML, and ICLR that introduce novel NAS methods (see <ref type="table" target="#tab_1">Table 1</ref>), none are exactly reproducible. Indeed, each fails on account of some combination of missing model evaluation code, architecture search code, random seeds used for search and evaluation, and/or undocumented hyperparameter tuning. <ref type="bibr" target="#b2">1</ref> While addressing these challenges will require community-wide efforts, in this work we present results that aim to make some initial progress on each of these issues. In particular, our contributions are as follows:</p><p>(1) We help ground existing NAS results by providing a new perspective on the gap between traditional hyperparameter optimization and leading NAS methods. Specifically, we evaluate a general hyperparameter optimization method combining random search with early-stopping <ref type="bibr" target="#b31">[30]</ref> on two standard NAS benchmarks (CIFAR-10 and PTB). With approximately the same amount of compute as DARTS <ref type="bibr" target="#b35">[34]</ref>, a state-of-the-art (SOTA) NAS method, this simple method provides a much more competitive baseline for both benchmarks: (1) on PTB, random search with early-stopping reaches test perplexity of 56.4 compared to the published result for ENAS <ref type="bibr" target="#b42">[41]</ref>, a leading NAS method, of 56.3, 2 and (2) for CIFAR-10, random search with early-stopping achieves a test error of 2.85%, whereas the published result for ENAS is 2.89%. While SOTA NAS methods like DARTS still outperform this baseline, our results demonstrate that the gap is not nearly as large as that suggested by published random search baselines on these tasks <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b42">41]</ref>. <ref type="bibr" target="#b3">(2)</ref> We identify a small subset of NAS components that are sufficient for achieving good empirical results. We construct a simple algorithm from the ground up starting from vanilla random search, and demonstrate that properly tuned random search with weight-sharing is competitive with much more complicated methods when using similar computational budgets. In particular, we identify the following meta-hyperparameters that impact the behavior of our algorithm: batch size, number of epochs, network size, and number of evaluated architectures. We evaluate our proposed method using the same search space and evaluation scheme as DARTS <ref type="bibr" target="#b35">[34]</ref>, a leading NAS method. We explore a few modifications of the meta-hyperparameters to improve search quality and make full use of available GPU memory and computational resources, and observe SOTA performance on the PTB benchmark and comparable performance to DARTS on the CIFAR-10 benchmark. We emphasize that we do not perform additional hyperparameter tuning of the final architectures discovered at the end of the search process.</p><p>We open-source all of the necessary code, random seeds, and documentation necessary to reproduce our experiments. Our single machine results shown in <ref type="table">Table 2</ref> and <ref type="table">Table 5</ref> follow a deterministic experimental <ref type="bibr" target="#b2">1</ref> It is important to note that these works vary drastically in terms of what materials they provide, and some authors such as Liu et al. <ref type="bibr" target="#b35">[34]</ref>, provide a relatively complete codebase for their methods. However, even in the case of DARTS, the code for the CIFAR-10 benchmark is not deterministic and Liu et al. <ref type="bibr" target="#b35">[34]</ref> do not provide random seeds or documentation regarding the post-processing steps in which they perform hyperparameter optimization on final architectures returned by DARTS. We were thus not able to reproduce the results in Liu et al. <ref type="bibr" target="#b35">[34]</ref>, but we were able to use the DARTS code repository (https://github.com/quark0/darts) as the launching point for our experimental setup. <ref type="bibr" target="#b3">2</ref> We could not reproduce this result using the initial final architecture and code provided by the authors (https://github.com/melodyguan/ enas setup, given a fixed random seed, and satisfy exact reproducibility. For these experiments on the two standard benchmarks, we study the broad reproduciblity of our random search with weight-sharing results by repeating our experiments with different random seeds. We observe non-trivial differences across independent runs and identify potential sources for these differences. Our results highlight the need for more careful reporting of experimental results, increased transparency of intermediate results, and more robust statistics to quantify the performance of NAS methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Background</head><p>We first provide an overview of the components of hyperparameter optimization and, by association, NAS. As shown in <ref type="figure">Figure 1</ref>, a general hyperparameter optimization problem has three components, each of which can have NAS-specific approaches. We provide a brief overview of the components below, drawing attention to NAS-specific methods (see the survey by Elsken et al. <ref type="bibr" target="#b13">[12]</ref> for a more thorough coverage of NAS). Search Space. Hyperparameter optimization involves identifying a good hyperparameter configuration from a set of possible configurations. The search space defines this set of configurations, and can include continuous or discrete hyperparameters in a structured or unstructured fashion <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b45">44]</ref>. NAS-specific search spaces usually involve discrete hyperparameters with additional structure that can be captured with a directed acyclic graph (DAG) <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b42">41]</ref>. Additionally, since a search space for designing an entire architecture would have too many nodes and edges, search spaces are usually defined over some smaller building block, i.e., cell blocks, that are repeated in some way via a preset or learned meta-architecture to form a larger architecture <ref type="bibr" target="#b13">[12]</ref>. We design our random search NAS algorithm for such a cell block search space, using the same search spaces for the CIFAR-10 and PTB benchmarks as DARTS for our experiments in Section 4. See Section 3 for a concrete example of one such search space.</p><p>Search Method. Given a search space, there are various search methods to select putative configurations to evaluate. Random search is the most basic approach, yet it is quite effective in practice <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b30">29]</ref>. Various general and NAS-specific adaptive methods have also been introduced, all of which attempt to bias the search in some way towards configurations that are more likely to perform well. In traditional hyperparameter optimization, the choice of search method can depends on the search space. Bayesian approaches based on Gaussian processes <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b46">45]</ref> and gradient-based approaches <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b37">36]</ref> are generally only applicable to continuous search spaces. In contrast, tree-based Bayesian <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b21">20]</ref>, evolutionary strategies <ref type="bibr" target="#b41">[40]</ref>, and random search are more flexible and can be applied to any search space. NAS-specific search methods can also be categorized into the same broad categories but are tailored for structured NAS search spaces (see Section 2.2 for a more involved discussion).</p><p>Evaluation Method. For each hyperparameter configuration considered by a search method, we must evaluate its quality. The default approach to perform such an evaluation involves fully training a model with the given hyperparameters, and subsequently measuring its quality, e.g., its predictive accuracy on a validation set. The first generation of NAS methods relied on full training evaluation, and thus required thousands of GPU days to achieve a desired result <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b53">52]</ref>. In contrast, partial training methods exploit early-stopping to speed up the evaluation process at the cost of noisy estimates of configuration quality. These methods use Bayesian optimization <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b28">27]</ref>, performance prediction <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b16">15]</ref>, or multi-armed bandits <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b31">30</ref>] to adaptively allocate resources to different configurations. NAS-specific evaluation methods exploit the structure of neural networks to provide even cheaper, heuristic estimates of quality. Many of these methods center around sharing and reuse: network morphisms build upon previously trained architectures <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b24">23]</ref>; hypernetworks and performance prediction encode information from previously seen architectures <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b32">31,</ref><ref type="bibr" target="#b51">50]</ref>; and weight-sharing methods <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b48">47]</ref> use a single set of weights for all possible architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We now provide additional context for the three issues we identified with the current state of NAS research in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inadequate Baselines</head><p>Existing works in NAS do not provide adequate comparison to random search and other hyperparameter optimization methods. Some works either compare to random search given a budget of just of few evaluations <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b42">41]</ref> or Bayesian optimization methods without efficient architecture evaluation schemes <ref type="bibr" target="#b24">[23]</ref>. While Real et al. <ref type="bibr" target="#b44">[43]</ref> and Cai et al. <ref type="bibr" target="#b7">[6]</ref> provide a thorough comparison to random search, they use random search with full training even though partial training methods have been shown to be orders-of-magnitude faster than standard random search <ref type="bibr" target="#b30">[29,</ref><ref type="bibr" target="#b31">30]</ref>.</p><p>While certain hyperparameter optimization methods <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b45">44]</ref> require non-trivial modification in order to work with NAS search spaces, others are easily applicable to NAS problems <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b31">30]</ref>. Of these applicable methods, we choose to use a simple method combining random search with early-stopping called ASHA <ref type="bibr" target="#b31">[30]</ref> to provide a competitive baseline for standard hyperparameter optimization. Li et al. <ref type="bibr" target="#b31">[30]</ref> showed ASHA to be a state-of-the-art, theoretically principled, bandit-based partial training method that outperforms leading adaptive search strategies for hyperparameter optimization. We compare the empirical performance of ASHA with that of NAS methods in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Complex Methods</head><p>Much of the complexity of NAS methods is introduced in the process of adapting search methods for NAS-specific search spaces: evolutionary approaches need to define a set of possible mutations to apply to different architectures <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b44">43]</ref>; Bayesian optimization approaches <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b27">26]</ref> rely on specially designed kernels; gradient-based methods transform the discrete architecture search problem into a continuous optimization problem so that gradients can be applied <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b48">47]</ref>; and Zoph and Le <ref type="bibr" target="#b52">[51]</ref>, Zoph et al. <ref type="bibr" target="#b53">[52]</ref>, and Pham et al. <ref type="bibr" target="#b42">[41]</ref> use reinforcement learning to train a recurrent neural network controller to generate good architectures. All of these search approaches add a significant amount of complexity with no clear winner, especially since methods some times use different search spaces and evaluation methods. To simplify the search process and help isolate important components of NAS, we use random search to sample architectures from the search space.</p><p>Additional complexity is also introduced by the NAS-specific evaluation methods mentioned previously. Network morphisms require architecture transformations that satisfy certain criteria; hypernetworks and performance prediction methods encode information from previously seen architectures in an auxiliary network; and weight-sharing methods <ref type="bibr" target="#b2">[1,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b48">47]</ref> use a single set of weights for all possible architectures and hence, can require careful training routines. Despite their complexity, these more efficient NAS evaluation methods are 1-3 orders-of-magnitude cheaper than full training (see <ref type="table">Table 5</ref> and <ref type="table">Table 2</ref>), at the expense of decreased fidelity to the true performance. Of these evaluation methods, network morphism still requires on the order of 100 GPU days <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b32">31]</ref> and, while hypernetworks and prediction performance based methods can be cheaper, weight-sharing is less complex since it does not require training an auxiliary network. In addition to the computational efficiency of weight-sharing methods <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b48">47]</ref>, which only require computation on the order of fully training a single architecture, this approach has also achieved the best result on the two standard benchmarks <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b35">34]</ref>. Hence, we use random search with weight-sharing as our starting point for a simple and efficient NAS method.</p><p>Our work is inspired by the result of Bender et al. <ref type="bibr" target="#b2">[1]</ref>, which showed that random search, combined with a well-trained set of shared weights can successfully differentiate good architectures from poor performing ones. However, their work required several modifications to stabilize training (e.g., a tunable path dropout schedule over edges of the search DAG and a specialized ghost batch normalization scheme <ref type="bibr" target="#b20">[19]</ref>). Furthermore, they only report experimental results on the CIFAR-10 benchmark, on which they fell slightly short of the results for leading NAS methods. In contrast, our combination of random search with weight-sharing greatly simplifies the training routine and we identify key variables needed to achieve competitive results on both CIFAR-10 and PTB benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lack of Reproducibility</head><p>The earliest NAS results lacked exact and broad reproducibility due to the tremendous amount of computation required to achieve the results <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b53">52]</ref>. Additionally, some of these methods used specialized hardware (i.e., TPUs) that were not easily accessible to researchers at the time <ref type="bibr" target="#b44">[43]</ref>. Although the final architectures were eventually provided <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b19">18]</ref>, the code for the search methods used to produce these results has not been released, precluding researchers from reproducing these results even if they had sufficient computational resources.</p><p>Recently, it has become feasible to evaluate the exact and broad reproducibility of many SOTA methods due to their reduced computational cost. However, while many authors have released code for their work [e.g., 5, 6, 34, 41], others have not made their code publicly available [e.g., <ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b51">50]</ref>, including the work most closely related to ours by Bender et al. <ref type="bibr" target="#b2">[1]</ref>. We summarize the reproducibility of recent NAS publications at some of the major machine learning conferences in <ref type="table" target="#tab_1">Table 1</ref> according to the availability of the following:</p><p>(1) Architecture search code. The output of this code is the final architecture that should be trained on the evaluation task.   <ref type="bibr" target="#b9">[8]</ref> No No All 4 criteria are necessary for exact reproducibility. Due to the absence of random seeds for all methods with released code, none of the methods in <ref type="table" target="#tab_1">Table 1</ref> are exactly reproducible from the search phase to the final architecture evaluation phase.</p><p>While only criteria 1-3 are necessary to estimate broad reproducibility, there is minimal discussion of the broad reproducibility of existing methods in published work. With the exception of NASBOT <ref type="bibr" target="#b27">[26]</ref> and DARTS <ref type="bibr" target="#b35">[34]</ref>, the methods in <ref type="table" target="#tab_1">Table 1</ref> only report the performance of the best found architecture, presumably resulting from a single run of the search process. Although this is understandable in light of the computational costs for some of these methods <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b36">35]</ref>, the high variance of extremal statistics makes it difficult to isolate the impact of the novel contributions introduced in each work. DARTS is particularly commendable in acknowledging its dependence on random initialization, prompting the use multiple runs to select the best architecture. In our experiments in Section 4, we follow DARTS and report the result of our random weight-sharing method across multiple trials; in fact, we go one step further and evaluate the broad reproducibility of our results with multiple sets of random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We now introduce our NAS algorithm that combines random search with weight-sharing. Our algorithm is designed for an arbitrary search space with a DAG representation, and in our in our experiments in Section 4, we use the same search spaces as that considered by DARTS <ref type="bibr" target="#b35">[34]</ref> for the standard CIFAR-10 and PTB NAS benchmarks.</p><p>For concreteness, consider the search space used by DARTS for designing a recurrent cell for the PTB benchmark: the DAG considered for the recurrent cell has N = 8 nodes and the operations considered include tanh, relu, sigmoid, and identity. To sample an architecture from this search space, we apply random search in the following manner: (1) For each node in the DAG, determine what decisions must be made. In the case of the PTB search space, we need to choose a node as input and a corresponding operation to apply to generate the output of the node. (2) For each decision, identify the possible choices for the given node. In the case of the PTB search space, if we number the nodes from 1 to N , node i can take the outputs of nodes 0 to node i − 1 as input (the initial input to the cell is index 0 and is also a possible input). Additionally, we can choose an operation from {tanh, relu, sigmoid, and identity} to apply to the output of node i. (3) Finally, moving from node to node, we sample uniformly from the set of possible choices for each decision that needs to be made. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example of an architecture from this search space. In order to combine random search with weight-sharing, we simply use randomly sampled architectures to train the shared weights. Shared weights are updated by selecting a single architecture for a given minibatch and updating the shared weights by back-propagating through the network with only the edges and operations as indicated by the architecture activated. Hence, the number of architectures used to update the shared weights is equivalent to the total number of minibatch training iterations.</p><p>After training the shared weights for a certain number of epochs, we use these trained shared weights to evaluate the performance of a number of randomly sampled architectures on a separate held out dataset. We select the best performing one as the final architecture, i.e., as the output of our search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relevant Meta-Hyperparameters</head><p>There are a few key meta-hyperparameters that impact the behavior of our search algorithm. We describe each of them below, along with a description of how we expect them to impact the search algorithm, both in terms of search quality and computational costs.</p><p>( Increasing the number of architectures that we evaluate using the shared weights allows for more exploration in the architecture search space. Intuitively, this should help assuming that there is a high correlation between the performance of an architecture evaluated using shared weights and the ground truth performance of that architecture when trained from scratch <ref type="bibr" target="#b2">[1]</ref>. Unsurprisingly, evaluating more architectures increases the computational time required for architecture search.</p><p>Other learning meta-hyperparameters will likely need to be adjusted accordingly for different settings of the key relevant meta-hyperparameters listed above. In our experiments in Section 4, we tune gradient clipping as a fifth meta-hyperparameter, though there are other possible meta-hyperparameters that may benefit from additional tuning (e.g., learning rate, momentum). In Section 4, following these intuitions, we incrementally explore the design space of our search method in order to improve search quality and make full use of the available GPU memory and computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory Footprint</head><p>Since we train the shared weights using a single architecture at a time, we have the option of only loading the weights associated with the operations and edges that are activated into GPU memory. Hence, the memory footprint of our random search with weight-sharing can be reduced to that of a single model. In this sense, our approach is similar to ProxylessNAS <ref type="bibr" target="#b8">[7]</ref> and allows us to perform architecture search with weight-sharing on the larger "proxyless" models that are usually used in the final architecture evaluation step instead of the smaller proxy models that are usually used in the search step. We take advantage of this in a subset of our experiments for the PTB benchmark in Section 4.1; performing random search with weight-sharing on a proxyless network for the CIFAR-10 benchmark is a direction for future work.</p><p>In contrast, Bender et al. <ref type="bibr" target="#b2">[1]</ref> trains the shared weights with a path dropout schedule that incrementally prunes edges within the DAG so that the sub-DAGs used to train the shared weights become sparser as training progresses. Under this training routine, since most of the edges in the search DAG are activated in the beginning, the memory footprint cannot be reduced to that of a single model to allow a proxyless network for the shared weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In line with prior work <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b52">51]</ref>, we consider the two standard benchmarks for neural architecture search: (1) language modeling on the Penn Treebank (PTB) dataset <ref type="bibr" target="#b38">[37]</ref> and (2) image classification on CIFAR-10 <ref type="bibr" target="#b29">[28]</ref>. For each of these benchmarks, we consider the same search space and use much of the same experimental setups as DARTS <ref type="bibr" target="#b35">[34]</ref>, and by association SNAS <ref type="bibr" target="#b48">[47]</ref>, to facilitate a fair comparison of our results to existing work.</p><p>To evaluate the performance of random search with weight-sharing on these two benchmarks, we proceed in the same three stages as Liu et al. <ref type="bibr" target="#b35">[34]</ref>:</p><p>• Stage 1: Perform architecture search for a cell block on a cheaper search task. • Stage 2: Evaluate the best architecture from the first stage by retraining a larger, network formed from multiple cell blocks of the best found architecture from scratch. This stage is used to select the best architecture from multiple trials.</p><p>• Stage 3: Perform the full evaluation of the best found architecture from the second stage by either training for more epochs (PTB) or training with more seeds (CIFAR-10). We start with the same meta-hyperparameter settings used by DARTS to train the shared weights. Then, we incrementally modify the meta-hyperparameters identified in Section 3.1 to improve performance until we either reach state-of-the-art performance (for PTB) or match the performance of DARTS and SNAS (for CIFAR-10).</p><p>For our evaluation of random search with early-stopping (i.e., ASHA) on these two benchmarks, we perform architecture search using partial training of the stage (2) evaluation network and then select the best architecture for stage (3) evaluation. For both benchmarks, we run ASHA with a starting resource per architecture of r = 1 epoch, a maximum resource of 300 epochs, and a promotion rate of η = 4, indicating the top 1/4 of architectures will be promoted in each round and trained for 4× more resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PTB Benchmark</head><p>We now present results for the PTB benchmark. We use the DARTS search space for the recurrent cell, which is described in Section 3. For this benchmark, due to higher memory requirements for their mixture operation, DARTS used a small recurrent network with embedding and hidden dimension of 300 to perform the architecture search followed by a larger network with embedding and hidden dimension of 850 to perform the evaluation. For the PTB benchmark, we refer to the network used in the first stage as the proxy network and the network in the later stages as the proxyless network. We next present the final search results. We subsequently explore the impact of various meta-hyperparameters on random search with weight-sharing, and finally evaluate the reproducibility of various methods on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Final Search Results</head><p>. We now present our final evaluation results in <ref type="table">Table 2</ref>. Specifically, we report the output of stage <ref type="formula" target="#formula_0">(3)</ref>, in which we train the proxyless network configured according to the best architectures found by different methods for 3600 epochs. This setup matches the evaluation scheme used for the reported results in <ref type="table">Table 2</ref> of Liu et al. <ref type="bibr" target="#b35">[34]</ref> (see Appendix A.2 for more details). We discuss various aspects of these results in the context of the three issues-baselines, complex methods, reproducibility-introduced in Section 1.</p><p>First, we evaluate the ASHA baseline using 2 GPU days, which is equivalent to the total cost of DARTS (second order). In contrast to the one random architecture evaluated by Pham et al. <ref type="bibr" target="#b42">[41]</ref> and the 8 evaluated by Liu et al. <ref type="bibr" target="#b35">[34]</ref> for their random search baselines, ASHA evaluated over 300 architectures with the allotted computation time. The best architecture found by ASHA achieves a test perplexity of 56.4, which is comparable to the published result for ENAS and significantly better than the random search baseline provided by Liu et al. <ref type="bibr" target="#b35">[34]</ref>, DARTS (first order), and the reproduced result for ENAS <ref type="bibr" target="#b35">[34]</ref>. Our result demonstrates that the gap between SOTA NAS methods and standard hyperparameter optimization approaches on the PTB benchmark is significantly smaller than that suggested by the existing comparisons to random search <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b42">41]</ref>.</p><p>Next, we evaluate random search with weight-sharing with tuned meta-hyperparameters (see Section 4.1.2 for details). With slightly lower search cost than DARTS, this method finds an architecture that reaches test perplexity 55.5, achieving SOTA perplexity compared to previous NAS approaches. We note that manually designed architectures are competitive with RNN cells designed by NAS methods on this benchmark. In fact, the work by Yang et al. <ref type="bibr" target="#b50">[49]</ref> using LSTM with mixture of experts in the softmax layer (MoS) outperforms automatically designed cells. Our architecture would likely also improve significantly with MoS, but we train without MoS to provide a fair comparison to ENAS and DARTS.</p><p>Finally, we examine the reproducibility of the NAS methods with available code for both architecture search and evaluation. For DARTS, exact reproducibility was not feasible since Liu et al. <ref type="bibr" target="#b35">[34]</ref> do not provide random seeds for the search process; however, we were able to reproduce the performance of their reported best architecture. We also evaluated the broad reproducibility of DARTS through an independent run, which reached a test perplexity of 55.9, compared to the published value of 55.7. For ENAS, end-to-end exact reproducibility was infeasible due to <ref type="table">Table 2</ref>. PTB Benchmark: Comparison with state-of-the-art NAS methods and manually designed networks. Lower test perplexity is better on this benchmark. The results are grouped by those for manually designed networks, published NAS methods, and the methods that we evaluated. Table entries denoted by "-" indicate that the field does not apply, while entries denoted by "N/A" indicate unknown entries. The search cost, unless otherwise noted, is measured in GPU days. Note that the search cost is hardware dependent and the search cost shown for our results are calculated for Tesla P100 GPUs; all other numbers are those reported by Liu et al. <ref type="bibr" target="#b35">[34]</ref>. # Search cost is in CPU-days. * We could not reproduce this result using the code released by the authors at https://github.com/melodyguan/enas. † The stage (1) cost shown is that for 1 trial as opposed to the cost for 4 trials shown for DARTS and Random search WS. It is unclear whether ENAS requires multiple trials followed by stage (2) evaluation in order to find a good architecture. non-deterministic code and missing random seeds for both the search and evaluation steps. Additionally, when we tried to reproduce their result using the provided final architecture, we could not match the reported test perplexity of 56.3 in our rerun. Consequently, in <ref type="table">Table 2</ref> we show the test perplexity for the final architecture found by ENAS trained using the DARTS code base, which Liu et al. <ref type="bibr" target="#b35">[34]</ref> observed to give a better test perplexity than using the architecture evaluation code provided by ENAS. We next considered the reproducibility of random search with weight-sharing. We verified the exact reproducibility of our reported results, and then investigated their broad reproducibility by running another experiment with different random seeds. In this second experiment, we observed a final text perplexity of 56.5, compared with a final test perplexity of 55.5 in the first experiment. Our detailed investigation in Section 4.1.3 shows that the discrepancies across both DARTS and random search with weight-sharing are unsurprising in light of the differing convergence rates among architectures on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Impact of Meta-Hyperparameters.</head><p>We now detail the meta-hyperparameter settings that we tried for random search with weight-sharing in order to achieve SOTA performance on the PTB benchmark. Similar to DARTS, in these preliminary experiments we performed 4 separate trials of each version of random search with weight-sharing, where each trial consists of executing stage (1) followed by stage <ref type="formula">(2)</ref>. In stage (1), we train the shared weights and then use them to evaluate 2000 randomly sampled architectures. In stage (2), we select the best architecture out of 2000, according to the shared weights, to train from scratch using the proxyless network for 300 epochs. We incrementally tune random search with weight-sharing by adjusting the following meta-hyperparameters associated with training the shared weights in stage (1): (1) gradient clipping, (2) batch size, and (3) network size. The settings we consider proceed as follows:</p><p>• Random (1): We train the shared weights of the proxy network using the same setup as DARTS with the same values for number of epochs, batch size, and gradient clipping; all other meta-hyperparameters are the same. • Random <ref type="formula">(2)</ref>: We decrease the maximum gradient norm to account for discrete architectures, as opposed to the weighted combination used by DARTS, so that gradient updates are not as large in each direction. • Random <ref type="formula" target="#formula_0">(3)</ref>: We decrease batch size from 256 to 64 in order to increase the number of architectures used to train the shared weights. • Random (4): We train the larger proxyless network architecture with shared weights instead of the proxy network, thereby significantly increasing the number of parameters in the model. The stage (2) performance of the final architecture after retraining from scratch for each of these settings is shown in <ref type="table" target="#tab_4">Table 3</ref>. With the extra capacity in the larger network used in Random (4), random search with weight-sharing achieves average validation perplexity of 64.7 across 4 trials, with the best architecture (shown in <ref type="figure" target="#fig_1">Figure 2</ref> in Section 3) reaching 63.8. In light of these stage (2) results, we focused in stage (3) on the best architecture found by Random (4) Run 1, and achieved test perplexity of 55.5 after training for 3600 epochs as reported in <ref type="table">Table 2</ref>.  <ref type="formula">(1)</ref>, random search is run with different settings to train the shared weights. The resulting networks are used to evaluate 2000 randomly sampled architectures. In stage (2), the best of these architectures for each trial is then trained from scratch for 300 epochs. We report the performance of the best architecture after stage <ref type="formula">(2)</ref>   <ref type="table" target="#tab_4">Table 3</ref> in the context of reproducibility. The first two rows of <ref type="table" target="#tab_4">Table 3</ref> show a comparison of the published stage (2) results for DARTS and our independent runs of DARTS. Both the best and average across 4 trials are worse in our reproduction of their results. Additionally, as previously mentioned, we perform an additional run of Random (4) with 4 different random seeds to test the broad reproducibility our result. The minimum stage (2) validation perplexity over these 4 trials is 63.9, compared to a minimum validation perplexity of 63.8 for the first set of seeds. Next, in <ref type="table" target="#tab_6">Table 4</ref> we compare the validation perplexities of the best architectures from ASHA, Random (4) Run 1, Random (4) Run 2, and our independent run of DARTS after training each from scratch for up to 3600 epochs. The swap in relative ranking across epochs demonstrates the risk of using noisy signals for the reward. In this case, we see that even partial training for 300 epochs does not recover the correct ranking; training using shared weights further obscures the signal. The differing convergence rates explain the difference in final test perplexity of the best architecture from Random (4) Run 2 and those from DARTS and Random (4) Run 1, despite Random (4) Run 2 reaching a comparable perplexity after 300 epochs.</p><p>Overall, the results of <ref type="table" target="#tab_4">Tables 3 and 4</ref> demonstrate a high variance in the stage (2) intermediate results across trials, along with issues related to differing convergence rates for different architectures. These two issues help explain the differences between the independent runs of DARTS and random search with weight-sharing. A third potential source of variation, which could in particular adversely impact our random search with weight-sharing results, stems from the fact that we did not perform any additional hyperparameter tuning in stage <ref type="formula" target="#formula_0">(3)</ref>; instead we used the same training hyperparameters that were tuned by Liu et al. <ref type="bibr" target="#b35">[34]</ref> for the final architecture found by DARTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CIFAR-10 Benchmark</head><p>We next present results for the CIFAR-10 benchmark. The DAG considered for the convolutional cell has N = 4 search nodes and the operations considered include 3 × 3 and 5 × 5 separable convolutions, 3 × 3 and 5 × 5 dilated separable convolutions, 3 × 3 max pooling, and 3 × 3 average pooling, and zero <ref type="bibr" target="#b35">[34]</ref>. To sample an architecture from this search space, we have to choose, for each node, 2 input nodes from previous nodes and associated operations to perform on each input (there are two initial inputs to the cell that are also possible input); we sample in this fashion twice, once for the normal convolution cell and one for the reduction cell (e.g., see <ref type="figure" target="#fig_2">Figure 3</ref>). Note that in contrast to DARTS, we include the zero operation when choosing the final architecture for each trial for further evaluation in stages <ref type="formula">(2)</ref> and <ref type="bibr" target="#b4">(3)</ref>. We hypothesize that our results may improve if we impose a higher complexity on the final architectures by excluding the zero op.</p><p>Due to higher memory requirements for weight-sharing, Liu et al. <ref type="bibr" target="#b35">[34]</ref> uses a smaller network with 8 stacked cells and 16 initial channels to perform the convolutional cell search, followed by a larger network with 20 stacked cells and 36 initial channels to perform the evaluation. Again, we will refer to the network used in the first stage as the proxy network and the network in the second stage the proxyless network.</p><p>Similar to the PTB results in Section 4.1, we will next present the final search results for the CIFAR-10 benchmark, and then dive deeper into these results to explore the impact of meta-hyperparameters on stage <ref type="bibr" target="#b3">(2)</ref> intermediate results, and finally evaluate associated reproducibility ramifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Final Search Results</head><p>. We now present our results after performing the final evaluation in stage <ref type="bibr" target="#b4">(3)</ref>. We use the same evaluation scheme used to produce the results in <ref type="table" target="#tab_1">Table 1</ref> of Liu et al. <ref type="bibr" target="#b35">[34]</ref>. In particular, we train the proxyless network configured according to the best architectures found by different methods with 10 different seeds and report the average and standard deviation. Again, we discuss our results in the context of the three issues introduced in Section 1.</p><p>First, we evaluate the ASHA baseline using 9 GPU days, which is comparable to the 10 GPU days we allotted to our independent run of DARTS. In contrast to the one random architecture evaluated by Pham et al. <ref type="bibr" target="#b42">[41]</ref> and the 24 evaluated by Liu et al. <ref type="bibr" target="#b35">[34]</ref> for their random search baselines, ASHA evaluated over 700 architectures in the allotted computation time. The best architecture found by ASHA achieves an average error of 3.03 ± 0.13, which is significantly better than the random search baseline provided by Liu et al. <ref type="bibr" target="#b35">[34]</ref> and comparable to DARTS (first order). Additionally, the best performing seed reached a test error of 2.85, which is lower than the published result for ENAS. Similar to the PTB benchmark, these results suggest that the gap between SOTA NAS methods and standard hyperparameter optimization is much smaller than previously reported <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b42">41]</ref>. <ref type="table">Table 5</ref>. CIFAR-10 Benchmark: Comparison with state-of-the-art NAS methods and manually designed networks. The results are grouped by those for manually designed networks, published NAS methods, and the methods that we evaluated. Models for all methods are trained with auxiliary towers and cutout. Test error for our contributions are averaged over 10 random seeds. Table entries denoted by "-" indicate that the field does not apply, while entries denoted by "N/A" indicate unknown entries. The search cost is measured in GPU days. Note that the search cost is hardware dependent and the search cost shown for our results are calculated for Tesla P100 GPUs; all other numbers follow those reported by Liu et al. <ref type="bibr" target="#b35">[34]</ref>. * We show results for the variants of these networks with comparable number of parameters. Larger versions of these networks achieve lower errors. # Reported test error averaged over 5 seeds. † The stage (1) cost shown is that for 1 trial as opposed to the cost for 4 trials shown for DARTS and Random search WS. It is unclear whether the method requires multiple trials followed by stage (2) evaluation in order to find a good architecture. ‡ Due to the longer evaluation we employ in stage (2) to account for unstable rankings, the cost for stage <ref type="formula">(2)</ref>  Next, we evaluate random search with weight-sharing with tuned meta-hyperparameters (see Section 4.2.2 for details). This method finds an architecture that achieves an average test error of 2.85 ± 0.08, which is comparable to the reported results for SNAS and DARTS, the top 2 weight-sharing algorithms that use a comparable search space, as well as GHN <ref type="bibr" target="#b51">[50]</ref>. Note that while the two manually tuned architectures we show in <ref type="table">Table 5</ref> outperform the best architecture discovered by random search with weight-sharing, they have over 7× more parameters. Additionally, the best-performing efficient NAS method, ProxylessNAS, uses a larger proxyless network and a significantly different search space than the one we consider. As mentioned in Section 3, random search with weight-sharing can also directly search over larger proxyless networks since it trains using discrete architectures. We hypothesize that using a proxyless network and applying random search with weight-sharing to the same search space as ProxylessNAS would further improve our results; we leave this as a direction for future work.</p><p>Finally, we examine the reproducibility of the NAS methods using a comparable search space with available code for both architecture search and evaluation (i.e., DARTS and ENAS; to our knowledge, code is not currently available for SNAS). For DARTS, exact reproducibility was not feasible since the code is non-deterministic and Liu et al. <ref type="bibr" target="#b35">[34]</ref> do not provide random seeds for the search process; hence, we focus on broad reproducibility of the results. In our independent run, DARTS reached an average test error of 2.78 ± 0.12 compared to the published result of 2.76 ± 0.09. Notably, we observed that the process of selecting the best architecture in stage (2) is unstable when training stage (2) models for only 100 epochs; see all of our CIFAR experiments, including our independent DARTS run, which explains the discrepancy in stage (2) costs between original DARTS and our independent run. For ENAS, the published results do not satisfy exact reproducibility due to the same issues as those for DARTS. We show in <ref type="table">Table 5</ref> the broad reproducibility experiment conducted by Liu et al. <ref type="bibr" target="#b35">[34]</ref> for ENAS; here, ENAS found an architecture that achieved a comparable test error of 2.91 in 8× the reported stage (1) search cost. As with the PTB benchmark, we then investigated the reproducibility of random search with weight-sharing. We verified exact reproducibility and then examined broad reproducibility by evaluating 5 additional independent runs of our method. We observe performance below 2.90 test error in 2 of the 5 runs and an average of 2.92 across all 6 runs. We investigate various sources for these discrepancies in Section 4.2.3. <ref type="table">Table 6</ref>. CIFAR-10 Benchmark: Comparison of Stage (2) Intermediate Search Results for Weight-Sharing Methods. In stage (1), random search is run with different settings to train the shared weights. The shared weights are then used to evaluate the indicated number of randomly sampled architectures. In stage (2), the best of these architectures for each trial is then trained from scratch for 600 epochs. We report the performance of the best architecture after stage (2) for each trial for each search method. † This run was performed using the DARTS code before we corrected for non-determinism (see Appendix A.2). 4.2.2 Impact of Meta-Hyperparameters. We next detail the meta-hyperparameter settings that we tried in order to reach competitive performance on the CIFAR-10 benchmark via random search with weight-sharing. Similar to DARTS, and as with the PTB benchmark, in these preliminary experiments we performed 4 separate trials of each version of random search with weight-sharing, where each trial consists of executing stage (1) followed by stage <ref type="bibr" target="#b3">(2)</ref>. In stage (1), we train the shared weights and use them to evaluate a given number of randomly sampled architectures on the test set. In stage (2), we select the best architecture, according to the shared weights, to train from scratch using the proxyless network for 600 epochs.</p><p>We incrementally tune random search with weight-sharing by adjusting the following meta-hyperparameters that impact both the training of shared weights and the evaluation of architectures using these trained weights: number of training epochs, gradient clipping, number of architectures evaluated using shared weights, and network size. The settings we consider for random search proceed as follows:</p><p>• Random (1): We start by training the shared weights with the proxy network used by DARTS and default values for number of epochs, gradient clipping, and number of initial filters; all other meta-hyperparameters are the same. • Random <ref type="formula">(2)</ref>: We increase the number of training epochs from 50 to 150, which concurrently increases the number of architectures used to update the shared weights. • Random <ref type="formula" target="#formula_0">(3)</ref>: We reduce the maximum gradient norm from 5 to 1 to adjust for discrete architectures instead of the weighted combination used by DARTS. • Random (4): We further increase the number of epochs for training the proxy network with shared weights to 300 and increase the number of architectures evaluated using the shared weights to 11k. • Random <ref type="formula">(5)</ref>: We separately increase the proxy network size to be as large as possible given the available memory on a Nvidia Tesla P100 GPU (i.e. by ≈ 50% due to increasing the number of initial channels from 16 to 24). The performance of the final architecture after retraining from scratch for each of these settings is shown in <ref type="table">Table 6</ref>. Similar to the PTB benchmark, the best setting for random search was Random <ref type="bibr" target="#b6">(5)</ref>, which has a larger network size. The best trial for this setting reached a test error of 2.83 when retraining from scratch; we show the normal and reduction cells found by this trial in <ref type="figure" target="#fig_2">Figure 3</ref>. In light of these stage (2) results, we focus in stage (3) on the best architecture found by Random (5) Run 1, and achieve an average test error of 2.85 ± 0.08 over 10 random seeds as shown in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Investigating Reproducibility.</head><p>Our results in this section show that although DARTS appears broadly reproducible, this result is surprising given the unstable ranking in architectures observed between 100 and 600 epochs for stage (2) evaluation. To begin, the first row of <ref type="table">Table 6</ref> shows our reproduced results for DARTS after training the best architecture for each trial from scratch for 600 epochs. In our reproduced run, DARTS reaches an average test error of 2.94 and a minimum of 2.77 across 4 trials (see <ref type="table">Table 6</ref>). Note that this is not a direct comparison to the published result for DARTS since there, the stage (2) evaluation was performed after training for only 100 epochs. <ref type="table">Table 7</ref>. CIFAR-10 Benchmark: Ranking of Intermediate Test Error for DARTS. Architectures are retrained from scratch using the proxyless network and the error on the test set is reported after training for the indicated number of epochs. Rank is calculated across the 4 trials. We also show the average over 10 seeds for the best architecture from the top trial for reference. † These results were run before we fixed the non-determinism in DARTS code (see Appendix A.2). Delving into the intermediate results, we compare the performance of the best architectures across trials from our independent run of DARTS after training each from scratch for 100 epochs and 600 epochs (see <ref type="table">Table 7</ref>). We see that the ranking is unstable between 100 epochs and 600 epochs, which motivated our strategy of training the final architectures across trials to 600 epochs in order to select the best architecture for final evaluation across 10 seeds. This suggests we should be cautious when using noisy signals for the performance of different architectures, especially since architecture search is conducted for DARTS and Random (5) for only 50 and 150 epochs respectively. Finally, we investigate the variance of random search with weight-sharing with 5 additional runs as shown in <ref type="table" target="#tab_10">Table 8</ref>. The stage (3) evaluation of the best architecture for these 5 additional runs reveal that 2 out of 5 achieve similar performance as Run 1, while the 3 remainder underperform but still reach a better test error than ASHA. These broad reproducibility results show that random search with weight-sharing has high variance between runs, which is not surprising given the change in intermediate rankings that we observed for DARTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computational Cost</head><p>As mentioned in Section 2, there is a trade off between computational cost and the quality of the signal that we get per architecture that we evaluate. To get a better sense of the tradeoff, we estimate the per architecture computational cost of different methods considered in our experiments, noting that these methods differ in per architecture cost by at least an order-of-magnitude as we move from expensive to cheaper evaluation methods:</p><p>(1) Full training: The random search baselines considered by Liu et al. <ref type="bibr" target="#b35">[34]</ref> cost 0.5 GPU days per architecture for the PTB benchmark and 4 GPU hours per architecture for the CIFAR-10 benchmark. (2) Partial training: The amortized cost of ASHA is 9 minutes per architecture for the PTB benchmark and 19 minutes per architecture for the CIFAR-10 benchmark. (3) Weight-sharing: It is difficult to quantify the equivalent number of architectures evaluated by DARTS and random search with weight-sharing. For DARTS, the final architecture is taken to be the highest weighted operation for each architectural decision, but it is unclear how much information is provided by the gradient updates to the architecture mixture weights during architecture search. For random search with weight sharing, although we evaluate a given number of architectures using the shared weights, as stated in Section 3, this is a tunable meta-hyperparameter and the quality of the performance estimates we receive can be noisy. <ref type="bibr" target="#b4">3</ref> Nonetheless, to provide a rough estimate of the cost per architecture for random search with weight-sharing, we calculate the amortized cost by dividing the total search cost by the number of architectures evaluated using the shared weights. Hence, the amortized cost for random search with weight-sharing is 0.2 minutes per architecture for the PTB benchmark and 0.8 minutes per architecture for the CIFAR-10 benchmark. Despite the apparent computational savings from weight-sharing methods, without more robustness and transparency, it is difficult to ascertain whether the total cost of applying existing weight-sharing methods to NAS problems warrants their broad application. In particular, the total costs of ProxylessNAS, ENAS, and SNAS are likely much higher than that reported in <ref type="table">Table 2</ref> and <ref type="table">Table 5</ref> since, as we saw with DARTS and random search with weight-sharing, multiple trials are needed due to sensitivity to initialization. Additionally, while we lightly tuned the meta-hyperparameter settings, we used DARTS' settings as our starting point and it is unclear whether the settings they use required considerable tuning. In contrast, we were able to achieve nearly competitive performance with the default settings of ASHA using roughly the same total computation as that needed by DARTS and random search with weight-sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Available Code</head><p>Unless otherwise noted, our results are exactly reproducible from architecture search to final evaluation using the code available at https://github.com/liamcli/randomNAS_release. The code we use for random search with weight-sharing on both benchmarks is deterministic conditioned on a fixed random seed. We provide the final architectures used for each of the trials shown in the tables above, as well as the random seeds used to find those architectures. In addition, we perform no additional hyperparameter tuning for final architectures and only tune the meta-hyperparameters according to the discussion in the text itself. We also provide code, final architectures, and random seeds used for our experiments using ASHA. However, we note that there is one uncontrolled source of randomness in our ASHA experiments-in the distributed setting, the asynchronous nature of the algorithm means that the results depend on the order in which different architectures finish (partially) training. Lastly, our experiments were conducted using Tesla P100 and V100 GPUs on Google Cloud. We convert GPU time on V100 to equivalent time on P100 by applying a multiple of 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We conclude by summarizing our results and proposing suggestions to push the field forward and foster broader adoption of NAS methods.</p><p>(1) Better baselines that accurately quantify the performance gains of NAS methods. The performance of random search with early-stopping evaluated in Section 4 reveals a surprisingly small performance gap between leading general-purpose hyperparameter optimization methods and specialized methods tailored for NAS. In traditional hyperparameter optimization benchmarks, random search has also been shown to be a difficult baseline to beat. For these benchmarks, an informative measure of the performance of a novel algorithm is its 'multiple of random search, ' i.e., how much more compute would random search need to achieve similar performance <ref type="bibr" target="#b30">[29]</ref>. An analogous baseline could be useful for NAS, where the impact of a novel NAS method can be quantified in terms of a multiplicative speedup relative to a standard hyperparameter optimization method such as random search with early-stopping. (2) Ablation studies that isolate the impact of individual NAS components. Our head-to-head experimental evaluation of two variants of random search (with early stopping and with weight-sharing) allows us to pinpoint the performance gains associated with the cheaper weight-sharing evaluation scheme. In contrast, the fact that random search with weight-sharing is comparable in performance to leading NAS methods calls into question the necessity of the auxiliary network used by GHN and the complicated algorithmic components employed by ENAS, SNAS, and DARTS. Relatedly, while ProxylessNAS achieves better average test error on CIFAR-10 than random search with weight-sharing, it is unclear to what degree these performance gains are attributable to the search space, search method, and/or proxyless shared-weights evaluation method. To promote scientific progress, we believe that ablation studies should be conducted to answer these questions in isolation. (3) Reproducible results that engender confidence and foster scientific progress. Reproducibility is a core tenet of scientific progress and crucial to promoting wider adoption of NAS methods. In traditional hyperparameter optimization, it is standard for empirical results to be reported over 10 independent experimental runs <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b30">29]</ref>. In contrast, as we discuss Section 2, results for NAS methods are often reported over a single experimental run <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b51">50]</ref>, without exact reproducibility. This is a consequence of the steep time and computational cost required to perform NAS experiments, e.g., generating the experiments reported in this paper alone required several months of wall-clock time and tens of thousands of dollars. However, in order to adequately differentiate between various methods, results need to be reported over several independent experimental runs, especially given the nature of the extremal statistics that are being reported. Consequently, we conclude that either significantly more computational resources need to be devoted to evaluating NAS methods and/or more computationally tractable benchmarks need to be developed to lower the barrier for performing adequate empirical evaluations. Relatedly, we feel it is imperative to evaluate the merits of NAS methods not only on their accuracy, but also on their robustness across these independent runs.</p><p>Reproducibility. The code released by Liu et al. <ref type="bibr" target="#b35">[34]</ref> did not produce deterministic results for the CNN benchmark due to non-determinism in CuDNN and in data loading. We removed the non-deterministic behavior in CuDNN by setting cudnn.benchmark = False cudnn.deterministic = True cudnn.enabled=True</p><p>Note that this only disables the non-deterministic functions in CuDNN and does not adversely affect training time as much as turning off CuDNN completely. We fix additional non-determinism from data loading by setting the seed for the random package in addition to numpy.random and pytorch seed and turning off multiple threads for data loading.</p><p>We ran ASHA and one set of trials for Random Search (5) with weight-sharing using the non-deterministic code before fixing the seeding to get deterministic results; all other settings for random search with weight-sharing are deterministic. Hence, the result for ASHA does not satisfy exact reproduciblity due to non-deterministic training and asynchronous updates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 2 ) 3 )</head><label>23</label><figDesc>Model evaluation code. The output of this code is the final performance on the evaluation task. (Hyperparameter tuning documentation. This includes code used to perform hyperparameter tuning of the final architectures, if any. (4) Random Seeds. This includes random seeds used for both the search and post-processing (i.e., retraining of final architecture as well as any additional hyperparameter tuning) phases. Most works provide the final architectures but random seeds are required to verify that the search process actually results in those final architectures and the performance of the final architectures matches the published result. Note the random seeds are only useful if the code for search and post-processing phases are deterministic up to a random seed; this was not the case for the DARTS code used for the CIFAR-10 benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Recurrent Cell on PTB Benchmark. The best architecture found by random search with weight-sharing in Section 4.1 is depicted. Each numbered square is a node of the DAG and each edge represents the flow of data from one node to another after applying the indicated operation along the edge. Nodes with multiple incoming edges (i.e., node 0 and output node h_{t} concatenate the inputs to form the output of the node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Section 4.2.3 for details. Hence, we use 600 epochs in Convolutional Cells on CIFAR-10 Benchmark: Best architecture found by random search with weight-sharing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). They have since released another repository (https://github.com/google-research/google-research/tree/master/enas_lm) that reports reproduced results but we have not verified these figures.</figDesc><table><row><cell>Search</cell><cell>Search</cell><cell>Evaluation</cell></row><row><cell>Space</cell><cell>Method</cell><cell>Method</cell></row><row><cell>Continuous &amp; Discrete</cell><cell>Random Search</cell><cell>Full Training</cell></row><row><cell>Unstructured &amp;</cell><cell>Evolutionary Search</cell><cell>Partial Training</cell></row><row><cell>Structured</cell><cell>Optimization Bayesian</cell><cell>Weight-Sharing</cell></row><row><cell>Cell Block</cell><cell></cell><cell>Network Morphism</cell></row><row><cell>Meta-Architecture</cell><cell>Gradient-Based Optimization</cell><cell>Hypernetworks</cell></row><row><cell></cell><cell>Reinforcement</cell><cell></cell></row><row><cell></cell><cell>Learning</cell><cell></cell></row></table><note>Fig. 1. Components of hyperparameter optimization. Primarily NAS-specific methods are outlined in purple.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Reproducibility of NAS Publications. Summary of the reproducibility status of recent NAS publications appearing in top machine learning conferences. For the hyperparameter tuning column, N/A indicates we are not aware that the authors performed additional hyperparameter optimization.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Architecture Model Evaluation Random Hyperparameter</cell></row><row><cell>Conference</cell><cell>Publication</cell><cell>Search Code</cell><cell>Code</cell><cell>Seeds</cell><cell>Tuning</cell></row><row><cell>ICLR 2018</cell><cell>Brock et al. [5]</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>N/A</cell></row><row><cell></cell><cell>Liu et al. [32]</cell><cell>No</cell><cell>No</cell><cell></cell><cell></cell></row><row><cell>ICML 2018</cell><cell>Pham et al. [41]  †</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>Undocumented</cell></row><row><cell></cell><cell>Cai et al. [6]</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>N/A</cell></row><row><cell></cell><cell>Bender et al. [1]</cell><cell>No</cell><cell>No</cell><cell></cell><cell></cell></row><row><cell>NIPS 2018</cell><cell>Kandasamy et al. [26]</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>N/A</cell></row><row><cell></cell><cell>Luo et al. [35]</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>Grid Search</cell></row><row><cell>ICLR 2019</cell><cell>Liu et al. [34]</cell><cell>Yes</cell><cell>Yes</cell><cell>No</cell><cell>Undocumented</cell></row><row><cell></cell><cell>Cai et al. [7]</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>N/A</cell></row><row><cell></cell><cell>Zhang et al. [50]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>† Published result is not reproducible for the PTB benchmark when training the reported final architecture with provided code.* Code to reproduce experiments was requested on OpenReview.* No No Xie et al. [47]* No No Cao et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1 )</head><label>1</label><figDesc>Training epochs. Increasing the number of training epochs while keeping all other parameters the same increases the total number of minibatch updates and hence, the number of architectures used to update the shared weights. Intuitively, training with more architectures should help the shared weights generalize better to what are likely unseen architectures in the evaluation step. Unsurprisingly, more epochs increase the computational time required for architecture search.(2) Batch size. Decreasing the batch size while keeping all other parameters the same also increases the number of minibatch updates but at the cost of noisier gradient update. Hence, we expect reducing the batch size to have a similar effect as increasing the number of training epochs but may necessitate adjusting other meta-hyperparameters to account for the noisier gradient update. Intuitively, more minibatch updates increase the computational time required for architecture search.(3) Network size. Increasing the search network size increases the dimension of the shared weights. Intuitively, this should boost performance since a larger search network can store more information about different architectures. Unsurprisingly, larger networks require more GPU memory. (4) Number of evaluated architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>PTB</figDesc><table /><note>Benchmark: Comparison of Stage (2) Intermediate Search Results for Weight-Sharing Methods. In stage</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>across 4 trials for each search method.</figDesc><table><row><cell></cell><cell cols="2">Setting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Network</cell><cell></cell><cell cols="2">Batch Gradient</cell><cell></cell><cell></cell><cell></cell><cell>Trial</cell></row><row><cell>Method</cell><cell>Config</cell><cell cols="3">Epochs Size Clipping</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>Best Average</cell></row><row><cell>DARTS [34]</cell><cell>proxy</cell><cell>50</cell><cell>256</cell><cell>0.25</cell><cell cols="4">67.3 66.3 63.4 63.4 63.4</cell><cell>65.1</cell></row><row><cell>Reproduced DARTS</cell><cell>proxy</cell><cell>50</cell><cell>256</cell><cell>0.25</cell><cell cols="4">64.5 67.7 64.0 67.7 64.0</cell><cell>66.0</cell></row><row><cell>Random (1)</cell><cell>proxy</cell><cell>50</cell><cell>256</cell><cell>0.25</cell><cell cols="4">65.6 66.3 66.0 65.6 65.6</cell><cell>65.9</cell></row><row><cell>Random (2)</cell><cell>proxy</cell><cell>50</cell><cell>256</cell><cell>0.1</cell><cell cols="4">65.8 67.7 65.3 64.9 64.9</cell><cell>65.9</cell></row><row><cell>Random (3)</cell><cell>proxy</cell><cell>50</cell><cell>64</cell><cell>0.1</cell><cell cols="4">66.1 65.0 64.9 64.5 64.5</cell><cell>65.1</cell></row><row><cell>Random (4) Run 1</cell><cell>proxyless</cell><cell>50</cell><cell>64</cell><cell>0.1</cell><cell cols="4">66.3 64.6 64.1 63.8 63.8</cell><cell>64.7</cell></row><row><cell>Random (4) Run 2</cell><cell>proxyless</cell><cell>50</cell><cell>64</cell><cell>0.1</cell><cell cols="4">63.9 64.8 66.3 66.7 63.9</cell><cell>65.4</cell></row></table><note>4.1.3 Investigating Reproducibility. We next examine the stage (2) intermediate results in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>PTB Benchmark: Ranking of Intermediate Validation Perplexity. Architectures are retrained from scratch using the proxyless network and the validation perplexity is reported after training for the indicated number of epochs. The final test perplexity after training for 3600 epochs is also shown for reference.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">Validation Perplexity by Epoch</cell><cell></cell><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell>300</cell><cell></cell><cell>500</cell><cell></cell><cell>1600</cell><cell></cell><cell>2600</cell><cell></cell><cell>3600</cell><cell></cell><cell cols="2">Perplexity</cell></row><row><cell cols="13">Search Method Value Rank Value Rank Value Rank Value Rank Value Rank Value Rank</cell></row><row><cell>DARTS</cell><cell>64.0</cell><cell>4</cell><cell>61.9</cell><cell>2</cell><cell>59.5</cell><cell>2</cell><cell>58.5</cell><cell>2</cell><cell>58.2</cell><cell>2</cell><cell>55.9</cell><cell>2</cell></row><row><cell>ASHA</cell><cell>63.9</cell><cell>2</cell><cell>62.0</cell><cell>3</cell><cell>59.8</cell><cell>4</cell><cell>59.0</cell><cell>3</cell><cell>58.6</cell><cell>3</cell><cell>56.4</cell><cell>3</cell></row><row><cell cols="2">Random (4) Run 1 63.8</cell><cell>1</cell><cell>61.7</cell><cell>1</cell><cell>59.3</cell><cell>1</cell><cell>58.4</cell><cell>1</cell><cell>57.8</cell><cell>1</cell><cell>55.5</cell><cell>1</cell></row><row><cell cols="2">Random (4) Run 2 63.9</cell><cell>2</cell><cell>62.1</cell><cell>4</cell><cell>59.6</cell><cell>3</cell><cell>59.0</cell><cell>3</cell><cell>58.8</cell><cell>4</cell><cell>56.5</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>is 1 GPU day for results reported by Liu et al.<ref type="bibr" target="#b35">[34]</ref> and 6 GPU days for our results.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Test Error</cell><cell>Params</cell><cell></cell><cell>Search Cost</cell><cell></cell><cell>Comparable</cell><cell>Search</cell></row><row><cell>Architecture</cell><cell cols="2">Source Best</cell><cell>Average</cell><cell>(M)</cell><cell cols="4">Stage 1 Stage 2 Total Search Space?</cell><cell>Method</cell></row><row><cell>Shake-Shake #</cell><cell>[9]</cell><cell>N/A</cell><cell>2.56</cell><cell>26.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>manual</cell></row><row><cell>PyramidNet</cell><cell>[48]</cell><cell>2.31</cell><cell>N/A</cell><cell>26</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>manual</cell></row><row><cell>NASNet-A # *</cell><cell>[52]</cell><cell>N/A</cell><cell>2.65</cell><cell>3.3</cell><cell>-</cell><cell>-</cell><cell>2000</cell><cell>N</cell><cell>RL</cell></row><row><cell>AmoebaNet-B  *</cell><cell>[43]</cell><cell cols="2">N/A 2.55 ± 0.05</cell><cell>2.8</cell><cell>-</cell><cell>-</cell><cell>3150</cell><cell>N</cell><cell>evolution</cell></row><row><cell>ProxylessNAS  †</cell><cell>[7]</cell><cell>2.08</cell><cell>N/A</cell><cell>5.7</cell><cell>4</cell><cell>N/A</cell><cell>N/A</cell><cell>N</cell><cell>gradient-based</cell></row><row><cell>GHN # †</cell><cell>[50]</cell><cell cols="2">N/A 2.84 ± 0.07</cell><cell>5.7</cell><cell>0.84</cell><cell>N/A</cell><cell>N/A</cell><cell>N</cell><cell>hypernetwork</cell></row><row><cell>SNAS  †</cell><cell>[47]</cell><cell cols="2">N/A 2.85 ± 0.02</cell><cell>2.8</cell><cell>1.5</cell><cell>N/A</cell><cell>N/A</cell><cell>Y</cell><cell>gradient-based</cell></row><row><cell>ENAS  †</cell><cell>[41]</cell><cell>2.89</cell><cell>N/A</cell><cell>4.6</cell><cell>0.5</cell><cell>N/A</cell><cell>N/A</cell><cell>Y</cell><cell>RL</cell></row><row><cell>ENAS</cell><cell>[34]</cell><cell>2.91</cell><cell>N/A</cell><cell>4.2</cell><cell>4</cell><cell>2</cell><cell>6</cell><cell>Y</cell><cell>RL</cell></row><row><cell>Random search baseline</cell><cell>[34]</cell><cell cols="2">N/A 3.29 ± 0.15</cell><cell>3.2</cell><cell>-</cell><cell>-</cell><cell>4</cell><cell>Y</cell><cell>random</cell></row><row><cell>DARTS (first order)</cell><cell>[34]</cell><cell cols="2">N/A 3.00 ± 0.14</cell><cell>3.3</cell><cell>1.5</cell><cell>1</cell><cell>2.5</cell><cell>Y</cell><cell>gradient-based</cell></row><row><cell>DARTS (second order)</cell><cell>[34]</cell><cell cols="2">N/A 2.76 ± 0.09</cell><cell>3.3</cell><cell>4</cell><cell>1</cell><cell>5</cell><cell>Y</cell><cell>gradient-based</cell></row><row><cell>DARTS (second order)  ‡</cell><cell>Ours</cell><cell cols="2">2.62 2.78 ± 0.12</cell><cell>3.3</cell><cell>4</cell><cell>6</cell><cell>10</cell><cell>Y</cell><cell>gradient-based</cell></row><row><cell>ASHA baseline</cell><cell>Ours</cell><cell cols="2">2.85 3.03 ± 0.13</cell><cell>2.2</cell><cell>-</cell><cell>-</cell><cell>9</cell><cell>Y</cell><cell>random</cell></row><row><cell>Random search WS  ‡</cell><cell>Ours</cell><cell cols="2">2.71 2.85 ± 0.08</cell><cell>4.3</cell><cell>2.7</cell><cell>6</cell><cell>9.7</cell><cell>Y</cell><cell>random</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>CIFAR-10 Benchmark: Broad Reproducibility of Random Search WS We report the stage 3 performance of the final architecture from 6 independent runs of random search with weight-sharing. This run was performed using the DARTS code before we corrected for non-determinism (see Appendix A.2). Average 2.85 ± 0.08 2.86 ± 0.09 2.88 ± 0.10 2.95 ± 0.09 2.98 ± 0.12 3.00 ± 0.19 2.92</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Test Error Across 10 Seeds</cell><cell></cell><cell></cell></row><row><cell>Run 1</cell><cell>Run 2  †</cell><cell>Run 3</cell><cell>Run 4</cell><cell>Run 5</cell><cell>Run 6</cell></row></table><note>†</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In principle, the equivalent number of architectures evaluated can be calculated by applying an oracle CDF of ground truth performance over randomly sampled architectures to the performance of the architectures found by the shared weights.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Maruan Al-Shedivat, Sebastian Caldas, Greg Ganger, Kevin Jamieson, Angela Jiang, Mikhail Khodak, Gregory Plumb, Afshin Rostamizadeh, Virginia Smith, and Daniel Wong for helpful comments and valuable discussion. Thanks also to Julien Siems and Frank Hutter's group for their efforts to reproduce our work, which led to insights on reproducibility and motivated additional experiments. This work was supported in part by DARPA FA875017C0141, the National Science Foundation grants IIS1705121 and IIS1838017, an Okawa Grant, a Google Faculty Award, an Amazon Web Services Award, and a Carnegie Bosch Institute Research Award. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA, the National Science Foundation, or any other funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DARTS trained the shared weights network with the zero operation included in the list of considered operations but removed the zero operation when selecting the final architecture to evaluate in stages (2) and (3). For our random search with weight-sharing, we decided to exclude the zero operation for both search and evaluation. Stage 3 Procedure. For stage (3) evaluation, we follow the ArXiv version of DARTS [33], which reported two sets of results, one after training for 1600 epochs and another fine tuned result after training for an additional 1000 epochs</title>
	</analytic>
	<monogr>
		<title level="j">Architecture Operations. In stage</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
	<note>In the ICLR version, Liu et al. [34] simply say they trained the final network to convergence. We trained for another 1000 epochs for a total of 3600 epochs to approximate training to convergence</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DARTS trained the shared weights network with the zero operation included in the list of considered operations but removed the zero operation when selecting the final architecture to evaluate in stages (2) and (3). For our random search with weight-sharing, we decided to include the zero operation for both search and evaluation. Stage 1 Procedure. For random search with weight-sharing, after the shared weights are fully trained, we evaluate randomly sampled architectures using the shared weights and select the best one for stage (2) evaluation. Due to the higher cost of evaluating on the full validation set, we evaluate each architecture using 10 minibatches instead. We split the total number of architectures to be evaluated into sets of 1000. For each 1000, we select the best 10 according the cheap evaluation on part of the validation set and evaluate on the full validation set</title>
	</analytic>
	<monogr>
		<title level="m">A.2 CIFAR-10 Benchmark In this section, we provide additional detail for the experiments in Section 4.2. Architecture Operations. In stage (1)</title>
		<imprint/>
	</monogr>
	<note>Then we select the top architecture across all sets of 1000 for stage (2) evaluation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-based optimization of hyperparameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SMASH: One-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Path-level network transformation for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learnable embedding space for efficient neural architecture compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-objective Architecture Search for CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<title level="m">Neural Architecture Search: A Survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bohb: Robust and efficient hyperparameter optimization at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient and robust automated machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Google vizier: A service for black-box optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sonik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google repo for amoebanet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/tpu/tree/master/models/official/amoeba_net" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename></persName>
		</author>
		<ptr target="https://cloud.google.com/automl/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Google repo for nasnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LION-5</title>
		<meeting>of LION-5</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Population based training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-stochastic best arm identification and hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Auto-Keras: Efficient Neural Architecture Search with Network Morphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gaussian process bandit optimisation with multi-fidelity evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-fidelity bayesian optimisation with continuous approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural Architecture Search with Bayesian Optimization and Optimal Transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast bayesian optimization of machine learning hyperparameters on large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical report</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Univsersity of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hyperband: Bandit-based configuration evaluation for hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Massively parallel hyperparameter tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Progressive Neural Architecture Search. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">DARTS: differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural Architecture Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">DeepArchitect: Automatically Designing and Training Deep Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tpot: A tree-based pipeline optimization tool for automating machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Automatic Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Regularized Evolution for Image Classifier Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-task bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
		<title level="m">Shakedrop regularization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph hypernetworks for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Neural Architecture Search with Reinforcement Learning. International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
