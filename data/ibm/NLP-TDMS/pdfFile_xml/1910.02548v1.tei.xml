<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Kernel Methods for Node Representation Learning on Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
							<email>xipeng@udel.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Delaware</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Kernel Methods for Node Representation Learning on Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph kernels are kernel methods measuring graph similarity and serve as a standard tool for graph classification. However, the use of kernel methods for node classification, which is a related problem to graph representation learning, is still ill-posed and the state-of-the-art methods are heavily based on heuristics. Here, we present a novel theoretical kernel-based framework for node classification that can bridge the gap between these two representation learning problems on graphs. Our approach is motivated by graph kernel methodology but extended to learn the node representations capturing the structural information in a graph. We theoretically show that our formulation is as powerful as any positive semidefinite kernels. To efficiently learn the kernel, we propose a novel mechanism for node feature aggregation and a data-driven similarity metric employed during the training phase. More importantly, our framework is flexible and complementary to other graph-based deep learning models, e.g., Graph Convolutional Networks (GCNs). We empirically evaluate our approach on a number of standard node classification benchmarks, and demonstrate that our model sets the new state of the art. The source code is publicly available at https://github.com/bluer555/KernelGCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph structured data, such as citation networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>, biological models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">47]</ref>, grid-like data <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b52">53]</ref> and skeleton-based motion systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, are abundant in the real world. Therefore, learning to understand graphs is a crucial problem in machine learning. Previous studies in the literature generally fall into two main categories: (1) graph classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>, where the whole structure of graphs is captured for similarity comparison; (2) node classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>, where the structural identity of nodes is determined for representation learning.</p><p>For graph classification, kernel methods, i.e., graph kernels, have become a standard tool <ref type="bibr" target="#b21">[22]</ref>. Given a large collection of graphs, possibly with node and edge attributes, such algorithms aim to learn a kernel function that best captures the similarity between any two graphs. The graph kernel function can be utilized to classify graphs via standard kernel methods such as support vector machines or k-nearest neighbors. Moreover, recent studies <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b48">49]</ref> also demonstrate that there has been a close connection between Graph Neural Networks (GNNs) and the Weisfeiler-Lehman graph kernel <ref type="bibr" target="#b33">[34]</ref>, and relate GNNs to the classic graph kernel methods for graph classification. Node classification, on the other hand, is still an ill-posed problem in representation learning on graphs. Although identification of node classes often leverages their features, a more challenging and important scenario is to incorporate the graph structure for classification. Recent efforts in Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b20">[21]</ref> have made great progress on node classification. In particular, these efforts broadly follow a recursive neighborhood aggregation scheme to capture structural information, where each node aggregates feature vectors of its neighbors to compute its new features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>. Empirically, these GCNs have achieved the state-of-the-art performance on node classification. However, the design of new GCNs is mostly based on empirical intuition, heuristics, and experimental trial-and-error.</p><p>In this paper, we propose a novel theoretical framework leveraging kernel methods for node classification. Motivated by graph kernels, our key idea is to decouple the kernel function so that it can be learned driven by the node class labels on the graph. Meanwhile, its validity and expressive power are guaranteed. To be specific, this paper makes the following contributions:</p><p>• We propose a learnable kernel-based framework for node classification. The kernel function is decoupled into a feature mapping function and a base kernel to ensure that it is valid as well as learnable. Then we present a data-driven similarity metric and its corresponding learning criteria for efficient kernel training. The implementation of each component is extensively discussed. An overview of our framework is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>• We demonstrate the validity of our learnable kernel function. More importantly, we theoretically show that our formulation is powerful enough to express any valid positive semidefinite kernels.</p><p>• A novel feature aggregation mechanism for learning node representations is derived from the perspective of kernel smoothing. Compared with GCNs, our model captures the structural information of a node by aggregation in a single step, other than a recursive manner, thus is more efficient.</p><p>• We discuss the close connection between the proposed approach and GCNs. We also show that our method is flexible and complementary to GCNs and their variants but more powerful, and can be leveraged as a general framework for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph Kernels. Graph kernels are kernels defined on graphs to capture the graph similarity, which can be used in kernel methods for graph classification. Many graph kernels are instances of the family of convolutional kernels <ref type="bibr" target="#b14">[15]</ref>. Some of them measure the similarity between walks or paths on graphs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>. Other popular kernels are designed based on limited-sized substructures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Most graph kernels are employed in models which have learnable components, but the kernels themselves are hand-crafted and motivated by graph theory. Some learnable graph kernels have been proposed recently, such as Deep Graph Kernels <ref type="bibr" target="#b44">[45]</ref> and Graph Matching Networks <ref type="bibr" target="#b22">[23]</ref>. Compared to these approaches, our method targets at learning kernels for node representation learning.</p><p>Node Representation Learning. Conventional methods for learning node representations largely focus on matrix factorization. They directly adopt classic techniques for dimension reduction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Other methods are derived from the random walk algorithm <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref> or sub-graph structures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b29">30]</ref>. Recently, Graph Convolutional Networks (GCNs) have emerged as an effective class of models for learning representations of graph structured data. They were introduced in <ref type="bibr" target="#b20">[21]</ref>, which consist of an iterative process aggregating and transforming representation vectors of its neighboring nodes to capture structural information. Recently, several variants have been proposed, which employ self-attention mechanism <ref type="bibr" target="#b39">[40]</ref> or improve network architectures <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b47">48]</ref> to boost the performance. However, most of them are based on empirical intuition and heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We begin by summarizing some of the most important concepts about kernel methods as well as representation learning on graphs and, along the way, introduce our notations.</p><p>Kernel Concepts. A kernel K : X × X → R is a function of two arguments: K(x, y) for x, y ∈ X . The kernel function K is symmetric, i.e., K(x, y) = K(y, x), which means it can be interpreted as a measure of similarity. If the Gram matrix</p><formula xml:id="formula_0">K ∈ R N ×N defined by K(i, j) = K(x i , x j ) for any {x i } N i=1 is positive semidefinite (p.s.d.)</formula><p>, then K is a p.s.d. kernel <ref type="bibr" target="#b25">[26]</ref>. If K(x, y) can be represented as Ψ(x), Ψ(y) , where Ψ : X → R D is a feature mapping function, then K is a valid kernel.</p><p>Graph Kernels. In the graph space G, we denote a graph as G = (V, E), where V is the set of nodes and E is the edge set of G. Given two graphs G i = (V i , E i ) and G j = (V j , E j ) in G, the graph kernel K G (G i , G j ) measures the similarity between them. According to the definition in <ref type="bibr" target="#b30">[31]</ref>, the kernel K G must be p.s.d. and symmetric. The graph kernel K G between G i and G j is defined as:</p><formula xml:id="formula_1">K G (G i , G j ) = vi∈Vi vj ∈Vj k base (f (v i ), f (v j )),<label>(1)</label></formula><p>where k base is the base kernel for any pair of nodes in G i and G j , and f : V → Ω is a function to compute the feature vector associated with each node. However, deriving a new p.s.d. graph kernel is a non-trivial task. Previous methods often implement k base and f as the dot product between hand-crafted graph heuristics <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b3">4]</ref>. There are little learnable parameters in these approaches.</p><p>Representation Learning on Graphs. Although graph kernels have been applied to a wide range of applications, most of them depend on hand-crafted heuristics. In contrast, representation learning aims to automatically learn to encode graph structures into low-dimensional embeddings. Formally, given a graph G = (V, E), we follow <ref type="bibr" target="#b13">[14]</ref> to define representation learning as an encoder-decoder framework, where we minimize the empirical loss L over a set of training node pairs D ⊆ V × V :</p><formula xml:id="formula_2">L = (vi,vj )∈D (ENC-DEC(v i , v j ), s G (v i , v j )).<label>(2)</label></formula><p>Equation <ref type="formula" target="#formula_2">(2)</ref> has three methodological components: ENC-DEC, s G and . Most of the previous methods on representation learning can be distinguished by how these components are defined. The detailed meaning of each component is explained as follows.</p><p>• ENC-DEC : V × V → R is an encoder-decoder function. It contains an encoder which projects each node into a M -dimensional vector to generate the node embedding. This function contains a number of trainable parameters to be optimized during the training phase. It also includes a decoder function, which reconstructs pairwise similarity measurements from the node embeddings generated by the encoder. • s G is a pairwise similarity function defined over the graph G. This function is user-specified, and it is used for measuring the similarity between nodes in G. • : R × R → R is a loss function, which is leveraged to train the model. This function evaluates the quality of the pairwise reconstruction between the estimated value ENC-DEC(v i , v j ) and the true value s G (v i , v j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Method: Learning Kernels for Node Representation</head><p>Given a graph G, as we can see from Eq. (2), the encoder-decoder ENC-DEC aims to approximate the pairwise similarity function s G , which leads to a natural intuition: we can replace ENC-DEC with a kernel function K θ parameterized by θ to measure the similarity between nodes in G, i.e.,</p><formula xml:id="formula_3">L = (vi,vj )∈D (K θ (v i , v j ), s G (v i , v j )).<label>(3)</label></formula><p>However, there exist two technical challenges: (1) designing a valid p.s.d. kernel which captures the node feature is non-trivial; <ref type="bibr" target="#b1">(2)</ref> it is impossible to handcraft a unified kernel to handle all possible graphs with different characteristics <ref type="bibr" target="#b28">[29]</ref>. To tackle these issues, we introduce a novel formulation to replace K θ . Inspired by the graph kernel as defined in Eq. (1) and the mapping kernel framework <ref type="bibr" target="#b35">[36]</ref>, our key idea is to decouple K θ into two components: a base kernel k base which is p.s.d. to maintain the validity, and a learnable feature mapping function g θ to ensure the flexibility of the resulting kernel. Therefore, we rewrite Eq.</p><formula xml:id="formula_4">(3) by K θ (v i , v j ) = k base (g θ (v i ), g θ (v j )) for v i , v j ∈ V of the graph G to optimize the following objective: L = (vi,vj )∈D (k base (g θ (v i ), g θ (v j )), s G (v i , v j )).<label>(4)</label></formula><p>Theorem 1 demonstrates that the proposed formulation, i.e.,</p><formula xml:id="formula_5">K θ (v i , v j ) = k base (g θ (v i ), g θ (v j ))</formula><p>, is still a valid p.s.d. kernel for any feature mapping function g θ parameterized by θ.</p><formula xml:id="formula_6">Theorem 1. Let g θ : V → R M be a function which maps nodes (or their corresponding features) to a M-dimensional Euclidean space. Let k base : R M × R M → R be any valid p.s.d. kernel. Then, K θ (v i , v j ) = k base (g θ (v i ), g θ (v j )) is a valid p.s.d. kernel.</formula><p>Proof. Let Φ be the corresponding feature mapping function of the p.s.d. kernel k base . Then, we</p><formula xml:id="formula_7">have k base (z i , z j ) = Φ(z i ), Φ(z j ) , where z i , z j ∈ R M . Substitute g θ (v i ), g θ (v j ) for z i , z j , and we have k base (g θ (v i ), g θ (v j )) = Φ(g θ (v i )), Φ(g θ (v j )) . Write the new feature mapping Ψ(v) as Ψ(v) = Φ(g θ (v)), and we immediately have that k base (g θ (v i ), g θ (v j )) = Ψ(v i ), Ψ(v j ) . Hence, k base (g θ (v i ), g θ (v j )) is a valid p.s.d. kernel.</formula><p>A natural follow-up question is whether our proposed formulation, in principle, is powerful enough to express any valid p.s.d. kernels? Our answer, in Theorem 2, is yes: if the base kernel has an invertible feature mapping function, then the resulting kernel is able to model any valid p.s.d. kernels.</p><formula xml:id="formula_8">Theorem 2. Let K(v i , v j ) be any valid p.s.d. kernel for node pairs (v i , v j ) ∈ V × V . Let k base : R M × R M → R be a p.s.d</formula><p>. kernel which has an invertible feature mapping function Φ. Then there exists a feature mapping function g θ :</p><formula xml:id="formula_9">V → R M , such that K(v i , v j ) = k base (g θ (v i ), g θ (v j )).</formula><p>Proof. Let Ψ be the corresponding feature mapping function of the p.s.d. kernel K, and then we have</p><formula xml:id="formula_10">K(v i , v j ) = Ψ(v i ), Ψ(v j ) . Similarly, for z i , z j ∈ R M , we have k base (z i , z j ) = Φ(z i ), Φ(z j ) .</formula><p>Substitute g θ (v) for z, and then it is easy to see that</p><formula xml:id="formula_11">g θ (v) = (Φ −1 • Ψ)(v)</formula><p>is the desired feature mapping function when Φ −1 exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation and Learning Criteria</head><p>Theorems 1 and 2 have demonstrated the validity and power of the proposed formulation in Eq. (4).</p><p>In this section, we discuss how to implement and learn g θ , k base , s G and , respectively.</p><p>Implementation of the Feature Mapping Function g θ . The function g θ aims to project the feature vector x v of each node v into a better space for similarity measurement. Our key idea is that in a graph, connected nodes usually share some similar characteristics, and thus changes between nearby nodes in the latent space of nodes should be smooth. Inspired by the concept of kernel smoothing, we consider g θ as a feature smoother which maps x v into a smoothed latent space according to the graph structure. The kernel smoother estimates a function as the weighted average of neighboring observed data. To be specific, given a node v ∈ V , according to Nadaraya-Watson kernel-weighted average <ref type="bibr" target="#b9">[10]</ref>, a feature smoothing function is defined as:</p><formula xml:id="formula_12">g(v) = u∈V k(u, v)p(u) u∈V k(u, v) ,<label>(5)</label></formula><p>where p is a mapping function to compute the feature vector of each node, and here we let p(v) = x v ; k is a pre-defined kernel function to capture pairwise relations between nodes. Note that we omit θ for g here since there are no learnable parameters in Eq. <ref type="bibr" target="#b4">(5)</ref>. In the context of graphs, the natural choice of computing k is to follow the graph structure, i.e., the structural information within the node's h-hop neighborhood.</p><p>To compute g, we let A be the adjacent matrix of the given graph G and I be the identity matrix with the same size. We notice that</p><formula xml:id="formula_13">I + D − 1 2 AD − 1 2 is a valid p.s.d. matrix, where D(i, i) = j A(i, j)</formula><p>. Thus we can employ this matrix to define the kernel function k. However, in practice, this matrix would lead to numerical instabilities and exploding or vanishing gradients when used for training deep neural networks. To alleviate this problem, we adopt the renormalization trick <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_14">I + D − 1 2 AD − 1 2 →Ā =D − 1 2ÃD − 1 2 , whereÃ = A + I andD(i, i) = jÃ (i, j).</formula><p>Then the h-hop neighborhood can be computed directly from the h power ofĀ, i.e.,Ā h . And the kernel k for node pairs</p><formula xml:id="formula_15">v i , v j ∈ V is computed as k(v i , v j ) =Ā h (i, j)</formula><p>. After collecting the feature vector x v of each node v ∈ V into a matrix X V , we rewrite Eq. (5) approximately into its matrix form:</p><formula xml:id="formula_16">g(V ) ≈Ā h X V .<label>(6)</label></formula><p>Next, we enhance the expressive power of Eq. (6) to model any valid p.s.d. kernels by implementing it with deep neural networks based on the following two aspects. First, we make use of multi-layer perceptrons (MLPs) to model and learn the composite function Φ −1 • Ψ in Theorem 2, thanks to the universal approximation theorem <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Second, we add learnable weights to different hops of node neighbors. As a result, our final feature mapping function g θ is defined as:</p><formula xml:id="formula_17">g θ (V ) = h ω h · Ā h M (h) · MLP (l) (X V ),<label>(7)</label></formula><p>where θ means the set of parameters in g θ ; ω h is a learnable parameter for the h-hop neighborhood of each node v; is the Hadamard (element-wise) product; Equation <ref type="formula" target="#formula_17">(7)</ref> can be interpreted as a weighted feature aggregation schema around the given node v and its neighbors, which is employed to compute the node representation. It has a close connection with Graph Neural Networks. We leave it in Section 5 for a more detailed discussion.</p><formula xml:id="formula_18">M (h) is an indicator matrix where M (h) (i, j) equals to 1 if v j is a h-th</formula><p>Implementation of the Base Kernel k base . As we have shown in Theorem 2, in order to model an arbitrary p.s.d. kernel, we require that the corresponding feature mapping function Φ of the base kernel k base must be invertible, i.e., Φ −1 exists. An obvious choice would let Φ be an identity function, then k base will reduce to the dot product between nodes in the latent space. Since g θ maps node representations to a finite dimensional space, the identity function makes our model directly measure the node similarity in this space. On the other hand, an alternative choice of k base is the RBF kernel which additionally projects node representations to an infinite dimensional latent space before comparison. We compare both implementations in the experiments for further evaluation.</p><p>Data-Driven Similarity Metric s G and Criteria . In node classification, each node v i ∈ V is associated with a class label y i ∈ Y . We aim to measure node similarity with respect to their class labels other than hand-designed metrics. Naturally, we define the pairwise similarity s G as:</p><formula xml:id="formula_19">s G (v i , v j ) = 1 if y i = y j −1 o/w<label>(8)</label></formula><p>However, in practice, it is hard to directly minimize the loss between K θ and s G in Eq. <ref type="bibr" target="#b7">(8)</ref>. Instead, we consider a "soft" version of s G , where we require that the similarity of node pairs with the same label is greater than those with distinct labels by a margin. Therefore, we train the kernel</p><formula xml:id="formula_20">K θ (v i , v j ) = k base (g θ (v i ), g θ (v j ))</formula><p>to minimize the following objective function on triplets:</p><formula xml:id="formula_21">L K = (vi,vj ,v k )∈T (K θ (v i , v j ), K θ (v i , v k )),<label>(9)</label></formula><p>where T ⊆ V × V × V is a set of node triplets: v i is an anchor, and v j is a positive of the same class as the anchor while v k is a negative of a different class. The loss function is defined as:</p><formula xml:id="formula_22">(K θ (v i , v j ), K θ (v i , v k )) = [K θ (v i , v k ) − K θ (v i , v j ) + α] + .<label>(10)</label></formula><p>It ensures that given two positive nodes of the same class and one negative node, the kernel value of the negative should be farther away than the one of the positive by the margin α. Here, we present Theorem 3 and its proof to show that minimizing Eq. (9) leads to K θ = s G .</p><formula xml:id="formula_23">Theorem 3. If |K θ (v i , v j )| ≤ 1 for any v i , v j ∈ V , minimizing Eq. (9) with α = 2 yields K θ = s G . Proof. Let (v i , v j , v k ) be all triplets satisfying y i = y j , y i = y k . Suppose that for α = 2, Eq. (10) holds for all (v i , v j , v k ). It means K θ (v i , v k ) + 2 ≤ K θ (v i , v j ) for all (v i , v j , v k ). As |K θ (v i , v j )| ≤ 1, we have K θ (v i , v k ) = −1 for all (v i , v k ) and K θ (v i , v j ) = 1 for all (v i , v j ). Hence, K θ = s G .</formula><p>We note that |K θ (v i , v j )| ≤ 1 can be simply achieved by letting k base be the dot product and normalizing all g θ to the norm ball. In the following sections, the normalized K θ is denoted byK θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inference for Node Classification</head><p>Once the kernel function</p><formula xml:id="formula_24">K θ (v i , v j ) = k base (g θ (v i ), g θ (v j ))</formula><p>has learned how to measure the similarity between nodes, we can leverage the output of the feature mapping function g θ as the node representation for node classification. In this paper, we introduce the following two classifiers.</p><p>Nearest Centroid Classifier. The nearest centroid classifier extends the k-nearest neighbors algorithm by assigning to observations the label of the class of training samples whose centroid is closest to the observation. It does not require additional parameters. To be specific, given a testing node u, for all nodes v i with class label y i ∈ Y in the training set, we compute the per-class average similarity between u and v i :</p><formula xml:id="formula_25">µ y = 1 |Vy| vi∈VyK θ (u, v i ),</formula><p>where V y is the set of nodes belonging to class y ∈ Y . Then the class assigned to the testing node u:</p><formula xml:id="formula_26">y * = arg max y∈Y µ y .<label>(11)</label></formula><p>Softmax Classifier. The idea of the softmax classifier is to reuse the ground truth labels of nodes for training the classifier, so that it can be directly employed for inference. To do this, we add the softmax activation σ after g θ (v i ) to minimize the following objective:</p><formula xml:id="formula_27">L Y = − vi∈V q(y i ) log(σ(g θ (v i ))),<label>(12)</label></formula><p>where q(y i ) is the one-hot ground truth vector. Note that Eq. (12) is optimized together with Eq. <ref type="bibr" target="#b8">(9)</ref> in an end-to-end manner. Let Ψ denote the corresponding feature mapping function of K θ , then</p><formula xml:id="formula_28">we have K θ (v i , v j ) = Ψ(v i ), Ψ(v j ) = k base (g θ (v i ), g θ (v j )).</formula><p>In this case, we use the node feature produced by Ψ for classification since Ψ projects node features into the dot-product space which is a natural metric for similarity comparison. To this end, k base is fixed to be the identity function for the softmax classifier, so that we have Ψ(v i ), Ψ(v j ) = g θ (v i ), g θ (v j ) and thus Ψ(v i ) = g θ (v i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Our feature mapping function g θ proposed in Eq. <ref type="formula" target="#formula_17">(7)</ref> has a close connection with Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b20">[21]</ref> in the way of capturing node latent representations. In GCNs and most of their variants, each layer leverages the following aggregation rule:</p><formula xml:id="formula_29">H (l+1) = ρ Ā H (l) W (l) ,<label>(13)</label></formula><p>where W (l) is a layer-specific trainable weighting matrix; ρ denotes an activation function; H (l) ∈ R N ×D denotes the node features in the l-th layer, and H 0 = X. Through stacking multiple layers, GCNs aggregate the features for each node from its L-hop neighbors recursively, where L is the network depth. Compared with the proposed g θ , GCNs actually interleave two basic operations of g θ : feature transformation and Nadaraya-Watson kernel-weighted average, and repeat them recursively.</p><p>We contrast our approach with GCNs in terms of the following aspects. First, our aggregation function is derived from the kernel perspective, which is novel. Second, we show that aggregating features in a recursive manner is inessential. Powerful h-hop node representations can be obtained by our model where aggregation is performed only once. As a result, our approach is more efficient both in storage and time when handling very large graphs, since no intermediate states of the network have to be kept. Third, our model is flexible and complementary to GCNs: our function g θ can be directly replaced by GCNs and other variants, which can be exploited for future work.</p><p>Time and Space Complexity. We assume the number of features F is fixed for all layers and both GCNs and our method have L ≥ 2 layers. We count matrix multiplications as in <ref type="bibr" target="#b6">[7]</ref>. GCN's time complexity is O(L Ā 0 F + L|V |F 2 ), where Ā 0 is the number of nonzeros ofĀ and |V | is the number of nodes in the graph. While ours is O( Ā h 0 F + L|V |F 2 ), since we do not aggregate features recursively. Obviously, Ā h 0 is constant but L Ā 0 is linear to L. For space complexity, GCNs have to store all the feature matrices for recursive aggregation which needs O(L|V |F + LF 2 ) space, where LF 2 is for storing trainable parameters of all layers, and thus the first term is linear to L. Instead, ours is O(|V |F + LF 2 ) where the first term is again constant to L. Our experiments indicate that we save 20% (0.3 ms) time and 15% space on Cora dataset <ref type="bibr" target="#b23">[24]</ref> than GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate the proposed kernel-based approach on three benchmark datasets: Cora <ref type="bibr" target="#b23">[24]</ref>, Citeseer <ref type="bibr" target="#b10">[11]</ref> and Pubmed <ref type="bibr" target="#b31">[32]</ref>. They are citation networks, where the task of node classification is to classify academic papers of the network (graph) into different subjects. These datasets contain bag-of-words features for each document (node) and citation links between documents.</p><p>We compare our approach to five state-of-the-art methods: GCN <ref type="bibr" target="#b20">[21]</ref>, GAT <ref type="bibr" target="#b39">[40]</ref>, FastGCN <ref type="bibr" target="#b4">[5]</ref>, JK <ref type="bibr" target="#b42">[43]</ref> and KLED <ref type="bibr" target="#b8">[9]</ref>. KLED is a kernel-based method, while the others are based on deep neural networks. We test all methods in the supervised learning scenario, where all data in the training set are used for training. We evaluate the proposed method in two different experimental settings according to FastGCN <ref type="bibr" target="#b4">[5]</ref> and JK <ref type="bibr" target="#b42">[43]</ref>, respectively. The statistics of the datasets together with their data split settings (i.e., the number of samples contained in the training, validation and testing sets, respectively) are summarized in <ref type="table" target="#tab_0">Table 1</ref>. Note that there are more training samples in the data split of JK <ref type="bibr" target="#b42">[43]</ref> than FastGCN <ref type="bibr" target="#b4">[5]</ref>. We report the average means and standard deviations of node classification accuracy which are computed from ten runs as the evaluation metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Variants of the Proposed Method</head><p>As we have shown in Section 4.1, there are alternative choices to implement each component of our framework. In this section, we summarize all the variants of our method employed for evaluation.</p><p>Choices of the Feature Mapping Function g. We implement the feature mapping function g θ according to Eq. <ref type="bibr" target="#b6">(7)</ref>. In addition, we also choose GCN and GAT as the alternative implementations of g θ for comparison, and denote them by g GCN and g GAT , respectively.</p><p>Choices of the Base Kernel k base . The base kernel k base has two different implementations: the dot product which is denoted by k ·,· , and the RBF kernel which is denoted by k RBF . Note that when the softmax classifier is employed, we set the base kernel to be k ·,· .</p><p>Choices of the Loss L and Classifier C. We consider the following three combinations of the loss function and classifier. (1) L K in Eq. (9) is optimized, and the nearest-centroid classifier C K is employed for classification. This combination aims to evaluate the effectiveness of the learned kernel.</p><p>(2) L Y in Eq. (12) is optimized, and the softmax classifier C Y is employed for classification. This combination is used in a baseline without kernel methods. (3) Both Eq. (9) and Eq. (12) are optimized, and we denote this loss by L K+Y . The softmax classifier C Y is employed for classification. This combination aims to evaluate how the learned kernel improves the baseline method.</p><p>In the experiments, we use K to denote kernel-based variants and N to denote ones without the kernel function. All these variants are implemented by MLPs with two layers. Due to the space limitation, we ask the readers to refer to the supplementary material for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results of Node Classification</head><p>The means and standard deviations of node classification accuracy (%) following the setting of FastGCN <ref type="bibr" target="#b4">[5]</ref> are organized in <ref type="table">Table 2</ref>. Our variant of K 3 sets the new state of the art on all datasets. And on Pubmed dataset, all our variants improve previous methods by a large margin. It proves the effectiveness of employing kernel methods for node classification, especially on datasets with large graphs. Interestingly, our non-kernel baseline N 1 even achieves the state-of-the-art performance, which shows that our feature mapping function can capture more flexible structural information than previous GCN-based approaches. For the choice of the base kernel, we can find that K 2 outperforms K 1 on two large datasets: Citeseer and Pubmed. We conjecture that when handling complex datasets, the non-linear kernel, e.g., the RBF kernel, is a better choice than the liner kernel.</p><p>To evaluate the performance of our feature mapping function, we report the results of two variants K * 1 and K * 2 in <ref type="table">Table 2</ref>. They utilize GCN and GAT as the feature mapping function respectively. As expected, our K 1 outperforms K * 1 and K * 2 among most datasets. This demonstrates that the recursive aggregation schema of GCNs is inessential, since the proposed g θ aggregates features only in a single step, which is still powerful enough for node classification. On the other hand, it is also observed that both K * 1 and K * 2 outperform their original non-kernel based implementations, which shows that learning with kernels yields better node representations. <ref type="table">Table 3</ref> shows the results following the setting of JK <ref type="bibr" target="#b42">[43]</ref>. Note that we do not evaluate on Pubmed in this setup since its corresponding data split for training and evaluation is not provided by <ref type="bibr" target="#b42">[43]</ref>. As expected, our method achieves the best performance among all datasets, which is consistent with the results in <ref type="table">Table 2</ref>. For Cora, the improvement of our method is not so significant. We conjecture that the results in <ref type="table">Table 3</ref> involve more training data due to different data splits, which narrows the performance gap between different methods on datasets with small graphs, such as Cora. <ref type="table">Table 2</ref>: Accuracy (%) of node classification following the setting of FastGCN <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cora <ref type="bibr" target="#b23">[24]</ref> Citeseer <ref type="bibr" target="#b10">[11]</ref> Pubmed <ref type="bibr" target="#b31">[32]</ref> KLED <ref type="bibr" target="#b8">[9]</ref> 82.3 -82.3 GCN <ref type="bibr" target="#b20">[21]</ref> 86.0 77.2 86.5 GAT <ref type="bibr" target="#b39">[40]</ref> 85.6 76.9 86.2 FastGCN <ref type="bibr" target="#b4">[5]</ref> 85.0 77.6 88.0 K 1 = {k ·,· , g θ , L K , C K } 86.68 ± 0.17 77.92 ± 0.25 89.22 ± 0.17 K 2 = {k RBF , g θ , L K , C K } 86.12 ± 0.05 78.68 ± 0.38 89.36 ± 0.21 K 3 = {k ·,· , g θ , L K+Y , C Y } 88.40 ± 0.24 80.28 ± 0.03 89.42 ± 0.01</p><formula xml:id="formula_30">N 1 = {g θ , L Y , C Y }</formula><p>87.56 ± 0.14 79.80 ± 0.03 89.24 ± 0.14 K * 1 = {k ·,· , g GCN , L K , C K } 87.04 ± 0.09 77.12 ± 0.23 87.84 ± 0.12 K * 2 = {k ·,· , g GAT , L K , C K } 86.10 ± 0.33 77.92 ± 0.19 - <ref type="table">Table 3</ref>: Accuracy (%) of node classification following the setting of JK <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cora <ref type="bibr" target="#b23">[24]</ref> Citeseer <ref type="bibr" target="#b10">[11]</ref> GCN <ref type="bibr" target="#b20">[21]</ref> 88.20 ± 0.70 77.30 ± 1.30 GAT <ref type="bibr" target="#b39">[40]</ref> 87.70 ± 0.30 76.20 ± 0.80 JK-Concat <ref type="bibr" target="#b42">[43]</ref> 89.10 ± 1.10 78.30 ± 0.80 K 3 = {k ·,· , g θ , L K+Y , C Y } 89.24 ± 0.31 80.78 ± 0.28</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Study on Node Feature Aggregation Schema</head><p>In <ref type="table" target="#tab_1">Table 4</ref>, we implement three variants of K 3 (2-hop and 2-layer with ω h by default) to evaluate the proposed node feature aggregation schema. We answer the following three questions. (1) How does performance change with fewer (or more) hops? We change the number of hops from 1 to 3, and the performance improves if it is larger, which shows capturing long-range structures of nodes is important.</p><p>(2) How many layers of MLP are needed? We show results with different layers ranging from 1 to 3. The best performance is obtained with two layers, while networks overfit the data when more layers are employed. (3) Is it necessary to have a trainable parameter ω h ? We replace ω h with a fixed constant c h , where c ∈ (0, 1]. We can see larger c improves the performance. However, all results are worse than learning a weighting parameter ω h , which shows the importance of it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">t-SNE Visualization of Node Embeddings</head><p>We visualize the node embeddings of GCN, GAT and our method on Citeseer with t-SNE. For our method, we use the embedding of K 3 which obtains the best performance. <ref type="figure" target="#fig_3">Figure 2</ref> illustrates the results. Compared with other methods, our method produces a more compact clustering result. Specifically our method clusters the "red" points tightly, while in the results of GCN and GAT, they are loosely scattered into other clusters. This is caused by the fact that both GCN and GAT minimize the classification loss L Y , only targeting at accuracy. They tend to learn node embeddings driven by those classes with the majority of nodes. In contrast, K 3 are trained with both L K and L Y . Our kernel-based similarity loss L K encourages data within the same class to be close to each other. As a result, the learned feature mapping function g θ encourages geometrically compact clusters.  Due to the space limitation, we ask the readers to refer to the supplementary material for more experiment results, such as the results of link prediction and visualization on other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we introduce a kernel-based framework for node classification. Motivated by the design of graph kernels, we learn the kernel from ground truth labels by decoupling the kernel function into a base kernel and a learnable feature mapping function. More importantly, we show that our formulation is valid as well as powerful enough to express any p.s.d. kernels. Then the implementation of each component in our approach is extensively discussed. From the perspective of kernel smoothing, we also derive a novel feature mapping function to aggregate features from a node's neighborhood. Furthermore, we show that our formulation is closely connected with GCNs but more powerful. Experiments on standard node classification benchmarks are conducted to evaluated our approach. The results show that our method outperforms the state of the art.</p><p>We use different network settings for the combinations of the loss function and inference method in Section 6.1 of the original paper. For Variant (1), we choose the output dimension of the first and second layers to be 512 and 128, respectively. We train this combination with 10 epochs on Cora and Citeseer and 100 epochs on Pubmed.</p><p>For GAT <ref type="bibr" target="#b39">[40]</ref>, due to its large memory cost, its output dimension of the first and second layers is chosen to be 64 and 8, respectively.</p><p>For Variants <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref>, the output dimension of the first layer is chosen to be 16. The output dimension of the second layer is the same as the number of node classes. We train this combination 100 epochs for GAT and 200 epochs for other setups.</p><p>In Eq. (9) of the original paper, we randomly sample 10,000 triplets in each epoch. In Eq. (10) of the original paper, α is set to be 0.1 for all datasets. All methods are optimized using Adam <ref type="bibr" target="#b18">[19]</ref> with the learning rate of 0.01. We use the best model achieved on the validation set for testing. Each result is reported based on an average over 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Results of Link Prediction</head><p>In addition to node classification, we also conduct experiments for link prediction to demonstrate the generalizability of the proposed framework in different graph-based tasks. We train the models using an incomplete version of the three citation datasets (Cora, Citeseer and Pubmed) according to <ref type="bibr" target="#b19">[20]</ref>: the node features remain but parts of the citation links (edges) are missing. The validation and test sets are constructed following the setup of <ref type="bibr" target="#b19">[20]</ref>.</p><p>We choose k base to be the dot product and set g θ to be the feature mapping function. Given graph G = (V, E), for v i , v j ∈ V , the similarity measure is defined as:</p><formula xml:id="formula_31">s G (v i , v j ) = 1 if (v i , v j ) ∈ E 0 o/w<label>(14)</label></formula><p>The feature mapping function g θ can be learned by minimizing the following objective function in a data-driven manner:</p><formula xml:id="formula_32">L K = (vi,vj )∈D (K θ (v i , v j ), s G (v i , v j )),<label>(15)</label></formula><p>where D is the set of training edges, and is the binary cross entropy loss. <ref type="table" target="#tab_2">Table 5</ref> summarizes the link prediction results of our kernel-based method, the variational graph autoencoder (VGAE) <ref type="bibr" target="#b19">[20]</ref> and its non-probabilistic variant (GAE). Our kernel-based method is highly comparable with these state-of-the-art methods, showing the potential of applying the proposed framework in different applications on graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 t-SNE visualization on Cora</head><p>We visualize the node embeddings of GCN <ref type="bibr" target="#b20">[21]</ref>, GAT <ref type="bibr" target="#b39">[40]</ref> and our method on Cora with t-SNE in <ref type="figure" target="#fig_4">Fig. 3</ref>. Our method produces tight and clear clustering embeddings (especially for the "red" points and "violet" points), which shows that compared with GCN and GAT, our method is able to learn more reasonable feature embeddings for nodes.</p><p>(b) GAT (c) Ours (a) GCN </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our kernel-based framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>hop neighbor of v i and 0 otherwise. The hyperparameter l controls the number of layers in the MLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>t-SNE visualization of node embeddings on Citeseer dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>t-SNE visualization of node embeddings on Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of the three evaluation datasets under two different data split settings.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Nodes Edges Classes Features Data split of FastGCN [5] Data split of JK [43]</cell></row><row><cell>Cora [24]</cell><cell>2,708 5,429</cell><cell>7</cell><cell>1,433</cell><cell>1,208 / 500 / 1,000</cell><cell>1,624 / 542 / 542</cell></row><row><cell cols="2">Citeseer [11] 3,327 4,732</cell><cell>6</cell><cell>3,703</cell><cell>1,827 / 500 / 1,000</cell><cell>1,997 / 665 / 665</cell></row><row><cell cols="2">Pubmed [32] 19,717 44,338</cell><cell>3</cell><cell>500</cell><cell>18,217 / 500 / 1,000</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Results of accuracy (%) with different settings of the aggregation schema.</figDesc><table><row><cell>Variants of K 3</cell><cell>Cora [24]</cell><cell>Citeseer [11] Pubmed [32]</cell></row><row><cell>Default</cell><cell cols="2">88.40 ± 0.24 80.28 ± 0.03 89.42 ± 0.01</cell></row><row><cell>1-hop</cell><cell cols="2">85.56 ± 0.02 77.73 ± 0.02 88.98 ± 0.01</cell></row><row><cell>3-hop</cell><cell cols="2">88.25 ± 0.01 80.13 ± 0.01 89.53 ± 0.01</cell></row><row><cell>1-layer</cell><cell cols="2">82.60 ± 0.01 77.63 ± 0.01 85.80 ± 0.01</cell></row><row><cell>3-layer</cell><cell cols="2">86.33 ± 0.04 78.53 ± 0.20 89.46 ± 0.05</cell></row><row><cell>c = 0.25</cell><cell cols="2">69.33 ± 0.09 74.48 ± 0.03 84.68 ± 0.02</cell></row><row><cell>c = 0.50</cell><cell cols="2">76.98 ± 0.10 77.47 ± 0.04 86.45 ± 0.01</cell></row><row><cell>c = 0.75</cell><cell cols="2">84.25 ± 0.01 77.99 ± 0.01 87.45 ± 0.01</cell></row><row><cell>c = 1.00</cell><cell cols="2">87.31 ± 0.01 78.57 ± 0.01 88.68 ± 0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Accuracy (%) of link prediction. ± 0.02 92.0 ± 0.03 89.5 ± 0.04 89.9 ± 0.05 96.4 ± 0.00 96.5 ± 0.00 VGAE [20] 91.4 ± 0.01 92.6 ± 0.01 90.8 ± 0.02 92.0 ± 0.02 94.4 ± 0.02 94.7 ± 0.02 Ours 93.1 ± 0.06 93.2 ± 0.07 90.9 ± 0.08 91.8 ± 0.04 94.5 ± 0.03 94.2 ± 0.01</figDesc><table><row><cell></cell><cell>Cora</cell><cell></cell><cell>Citeseer</cell><cell></cell><cell>Pubmed</cell><cell></cell></row><row><cell>Method</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell></row><row><cell cols="2">GAE [20] 91.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is funded by ARO-MURI-68985NSMUR and NSF 1763523, 1747778, 1733843, 1703883.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Supplementary Materials</head><p>A.1 Implementation Details</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">N-GCN: Multi-scale graph convolution for semi-supervised node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08888</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web (WWW)</title>
		<meeting>the International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>the IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Construct dynamic graphs for hand gesture recognition via spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernels for ordered-neighborhood graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Draief</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vojnovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4051" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental investigation of graph kernels on a collaborative recommendation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Data Mining (ICDM)</title>
		<meeting>the International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="863" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer series in statistics</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Citeseer: An automatic citation indexing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Conference on Digital Libraries</title>
		<meeting>the Third ACM Conference on Digital Libraries</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California at Santa Cruz</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cyclic pattern kernels for predictive graph mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A unifying view of explicit and implicit feature maps for structured data: systematic studies of graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00676</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph matching networks for learning the similarity of graph structured objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-organizing maps for learning the edit costs in graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="503" to="514" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Expressivity versus efficiency of graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Mining Graphs, Trees and Sequences</title>
		<meeting>the International Workshop on Mining Graphs, Trees and Sequences</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast subtree kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1660" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weisfeilerlehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A generalization of haussler&apos;s convolution kernel: mapping kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kuboyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="944" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on World Wide Web (WWW)</title>
		<meeting>the International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quantized Densely Connected U-Nets for Efficient Landmark Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="339" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CR-GAN: learning complete representations for multi-view generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="942" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6410" to="6421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graph node-feature convolution for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00086</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Retgk: Graph kernels based on return probabilities of random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3964" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to forecast and refine residual motion for image-to-video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="387" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A generative adversarial approach for zero-shot learning from noisy texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
