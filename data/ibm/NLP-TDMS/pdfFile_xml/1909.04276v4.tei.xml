<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NISER: Normalized Item and Session Representations to Handle Popularity Bias</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Gupta</surname></persName>
							<email>priyanka.g35@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diksha</forename><surname>Garg</surname></persName>
							<email>diksha.7@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Malhotra</surname></persName>
							<email>malhotra.pankaj@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
							<email>lovekesh.vig@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Shroff</surname></persName>
							<email>gautam.shroff@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">TCS Research</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NISER: Normalized Item and Session Representations to Handle Popularity Bias</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ACM Reference Format: Priyanka Gupta, Diksha Garg, Pankaj Malhotra, Lovekesh Vig, Gautam Shroff. 2019. NISER: Normalized Item and Session Representations to Han-dle Popularity Bias. In Proceedings of 1st International Workshop on Graph Representation Learning and its Applications (CIKM &apos;19). ACM, New York, NY, USA, 6 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Session-based Recommendation</term>
					<term>Graph Neural Networks</term>
					<term>Item and Session Representations</term>
					<term>Popularity Bias</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of session-based recommendation (SR) models is to utilize the information from past actions (e.g. item/product clicks) in a session to recommend items that a user is likely to click next. Recently it has been shown that the sequence of item interactions in a session can be modeled as graph-structured data to better account for complex item transitions. Graph neural networks (GNNs) can learn useful representations for such session-graphs, and have been shown to improve over sequential models such as recurrent neural networks <ref type="bibr" target="#b13">[14]</ref>. However, we note that these GNN-based recommendation models suffer from popularity bias: the models are biased towards recommending popular items, and fail to recommend relevant long-tail items (less popular or less frequent items). Therefore, these models perform poorly for the less popular new items arriving daily in a practical online setting. We demonstrate that this issue is, in part, related to the magnitude or norm of the learned item and session-graph representations (embedding vectors). We propose a training procedure that mitigates this issue by using normalized representations. The models using normalized item and sessiongraph representations perform significantly better: i. for the less popular long-tail items in the offline setting, and ii. for the less popular newly introduced items in the online setting. Furthermore, our approach significantly improves upon existing state-of-the-art on three benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The goal of session-based recommendation (SR) models is to recommend top-K items to a user based on the sequence of items clicked so far. Recently, several effective models for SR based on deep neural networks architectures have been proposed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. These approaches consider SR as a multi-class classification problem where Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CIKM '19, November 3rd-7th, 2019, Beijing, China Â© 2019 Copyright held by the owner/author(s). input is a sequence of items clicked in the past in a session and target classes correspond to available items in the catalog. Many of these approaches use sequential models like recurrent neural networks considering a session as a sequence of item click events <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13]</ref>. On the other hand, approaches like STAMP <ref type="bibr" target="#b7">[8]</ref> consider a session to be a set of items, and use attention models while learning to weigh (attend to) items as per their relevance to predict the next item. Approaches like NARM <ref type="bibr" target="#b5">[6]</ref> and CSRM <ref type="bibr" target="#b12">[13]</ref> use a combination of sequential and attention models. An important building block in most of these deep learning approaches is their ability to learn representations or embeddings for items and sessions. Recently, an alternative approach, namely SR-GNN <ref type="bibr" target="#b13">[14]</ref>, has been proposed to model the sessions as graphstructured data using GNNs <ref type="bibr" target="#b6">[7]</ref> rather than as sequences or sets, noting that users tend to make complex to-and-fro transitions across items within a session: for example, consider a session = 1 , 2 , 1 , 3 , 4 of item clicks by a user. Here, the user clicks on item 1 , then clicks on item 2 and then re-clicks on item 1 . This sequence of clicks induces a graph where nodes and edges correspond to items and transitions across items, respectively. For session in the above example, the fact that 2 and 3 are neighbors of 1 in the induced session-graph, the representation of 1 can be updated using representations of its neighbors, i.e. 2 and 3 , and thus obtain a more context-aware and informative representations. It is worth noting that this way of capturing neighborhood information has also been found to be effective in neighborhood-based SR methods such as SKNN <ref type="bibr" target="#b4">[5]</ref> and STAN <ref type="bibr" target="#b2">[3]</ref>.</p><p>It is well-known that more popular items are presented and interacted-with more often on online platforms. This results in a skewed distribution of items clicked by users <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>, as illustrated in <ref type="figure">Fig. 1</ref>. The models trained using the resulting data tend to have popularity bias, i.e. they tend to recommend more popular items over rarely clicked items.</p><p>We note that SR-GNN (referred to as GNN hereafter) also suffers from popularity bias. This problem is even more severe in a practical online setting where new items are frequently added to the catalog, and are inherently less popular during initial days. To mitigate this problem, we study GNN through the lens of an item and sessiongraph representation learning mechanism, where the goal is to obtain a session-graph representation that is similar to the representation of the item likely to be clicked next. We motivate the advantage of restricting the item and session-graph representations to lie on a unit hypersphere both during training and inference, and propose NISER: Normalized Item and Session Representations model for SR. We demonstrate the enhanced ability of NISER to deal with popularity bias in comparison to a vanilla GNN model in the offline as well as online settings. We also extend NISER to incorporate the sequential nature of a session via position embeddings <ref type="bibr" target="#b10">[11]</ref>, thereby leveraging the benefits of both sequence-aware models (like RNNs) and graph-aware models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recent results in computer vision literature, e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>, indicate the effectiveness of normalizing the final image features during training, and argue in favor of cosine similarity over inner product for learning and comparing feature vectors. <ref type="bibr" target="#b15">[16]</ref> introduces the ring loss for soft feature normalization which eventually learns to constrain the feature vectors on a unit hypersphere. Normalizing words embeddings is also popular in NLP applications, e.g. <ref type="bibr" target="#b8">[9]</ref> proposes penalizing the L 2 norm of word embeddings for regularization. However, to the best of our knowledge, the idea of normalizing item and session-graph embeddings or representations has not been explored.</p><p>In this work, we study the effect of normalizing the embeddings on popularity bias which has not been established and studied so far. Several approaches to deal with popularity bias exist in the collaborative filtering settings, e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>. To deal with popularity bias, <ref type="bibr" target="#b0">[1]</ref> introduces the notion of flexible regularization in a learning-to-rank algorithm. Similarly <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> uses the power-law of popularity where the probability of recommending an item is a smooth function of the items' popularity, controlled by an exponent factor. However, to the best of our knowledge, this is the first attempt to study and address popularity bias in DNN-based SR using SR-GNN <ref type="bibr" target="#b13">[14]</ref> as a working example. Furthermore, SR-GNN does not incorporate the sequential information explicitly to obtain the session-graph representation. We study the effect of incorporating position embeddings <ref type="bibr" target="#b10">[11]</ref> and show that it leads to minor but consistent improvement in recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>Let S denote all past sessions, and I denote the set of items observed in the set S. Any session â S is a chronologically ordered tuple of item-click events: = ( ,1 , ,2 , . . . , , ), where each of the item-click events , ( = 1 . . . ) corresponds to an item in I, and denotes the position of the item , in the session . A session can be modeled as a graph G = (V , E ), where , â V is a node in the graph. Further, ( , , , +1 ) â E is a directed edge from , to , +1 . Given , the goal of SR is to predict the next item , +1 by estimating the -dimensional probability vectorÅ· , +1 corresponding to the relevance scores for the items. The items with highest scores constitute the top-K recommendation list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LEARNING ITEM AND SESSION REPRESENTATIONS</head><p>Each item is mapped to a -dimensional vector from the trainable embedding look-up table or matrix I = [i 1 , i 2 , . . . , i ] â R Ã such that each row i â R is the -dimensional representation or embedding 1 vector corresponding to item â I ( = 1 . . . ). Consider any function (e.g. a neural network as in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>)parameterized by -that maps the items in a session to session embedding s = (I ; ),</p><formula xml:id="formula_0">where 2 I = [i ,1 , i ,2 , . . . , i , ] â R Ã .</formula><p>Along with I as an input which considers as a sequence, we also introduce an adjacency matrix A to incorporate the graph structure. We discuss this in more detail later in Section 5.</p><p>The goal is to obtain s that is close to the embedding i , +1 of the target item = , +1 , such that the estimated index/class for the target item is = arg max i s with = 1 . . . . In a DNN-based model , this is approximated via a differentiable softmax function such that the probability of next item being is given by:</p><formula xml:id="formula_1">(s) =Å· = exp(i s) =1 exp(i s) .<label>(1)</label></formula><p>For this -way classification task, softmax (cross-entropy) loss is used during training for estimating by minimizing the sum of L (Å·) = â =1 y log(Å· ) over all training samples, where y â {0, 1} is a 1-hot vector with y = 1 corresponding to the correct (target) class .</p><p>We next introduce the radial property <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> of softmax loss, and then use it to motivate the need for normalizing the item and session representations in order to reduce popularity bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Radial Property of Softmax Loss</head><p>It is well-known that optimizing for the softmax loss leads to a radial distribution of features for a target class <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>: If = arg max i s, then it is easy to show that</p><formula xml:id="formula_2">exp( i s) â  exp(i s) + exp( i s) &gt; exp(i s) =1 exp(i s)<label>(2)</label></formula><p>for any &gt; 1. This, in turn, implies that softmax loss favors large norm of features for easily classifiable instances. We omit details for brevity and refer the reader to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> for a thorough analysis and proof. This means that a high value ofÅ· can be attained by multiplying vector i by a scalar value &gt; 1; or simply by ensuring a large norm for the item embedding vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Normalizing the Representations</head><p>We note that the radial property has an interesting implication in the context of popularity bias: target items that are easier to predict are likely to have higher L 2 norm. We illustrate this with the help of an example: Items that are popular are likely to be clicked more often, and hence the trained parameters and I should have values that ensure these items get recommended more often. The radial property implies that for a given input and a popular target item , a correct classification decision can be obtained as follows: learn the   embedding vector i with high ||i || 2 such that the inner product i s = ||i || 2 ||s|| 2 (where is the angle between the item and session embeddings) is likely to be high to ensure large value for y (even when is not small enough and || || 2 is not large enough). When analyzing the item embeddings from a GNN model <ref type="bibr" target="#b13">[14]</ref>, we observe that this is indeed the case: As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, items with high popularity have high L 2 norm while less popular items have significantly lower L 2 norm. Further, performance of GNN (depicted in terms of Recall@20) degrades as the popularity of the target item decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NISER</head><p>Based on the above observations, we consider to minimize the influence of embedding norms in the final classification and recommendation decision. We propose optimizing for cosine similarity as a measure of similarity between item and session embeddings instead of the above-stated inner product. Therefore, during training as well as inference, we normalize the item embeddings as i = i | |i | | 2 , and use them to getÄ¨ . The session embedding is then obtained as s = (Ä¨ ; ), and is similarly normalized tos to enforce a unit norm. The normalized item and session embeddings are then used to obtain the relevance score for next clicked item computed as,Å·</p><formula xml:id="formula_3">= exp(Ä©s) =1 exp(Ä©s) .<label>(3)</label></formula><p>Note that the cosine similarityÄ©s is restricted to [â1, 1]. As shown in <ref type="bibr" target="#b11">[12]</ref>, this implies that the softmax loss is likely to get saturated at high values for the training set: a scaling factor &gt; 1 is useful in practice to allow for better convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LEVERAGING NISER WITH GNN</head><p>We consider learning the representations of items and sessiongraphs with GNNs where the session-graph is represented by G as introduced in Section 3. Consider two normalized adjacency matrices A â R , and A â R , corresponding to the incoming and outgoing edges in graph G as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. GNN takes adjacency matrices A and A , and the normalized item embed-dingsÄ¨ as input, and returns an updated set of embeddings after iterations of message propagation across vertices in the graph using gated recurrent units <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_4">[Ä© ,1 ,Ä© ,2 , . . . ,Ä© , ] = (A , A ,Ä¨ ; ), where</formula><p>represents the parameters of the GNN function . For any node in the graph, the current representation of the node and the representations of its neighboring nodes are used to iteratively update the representation of the node times. More specifically, the representation of node , in the -th message propagation step is updated as follows:</p><formula xml:id="formula_5">a , = [A , :Ä¨ â1 H 1 , A , :Ä¨ â1 H 2 ] + b,<label>(4)</label></formula><formula xml:id="formula_6">z , = (W a , + UÄ© â1 , ),<label>(5)</label></formula><formula xml:id="formula_7">r , = (W a , + UÄ© â1 , ),<label>(6)</label></formula><p>i , = tanh(W a , + U (r , âÄ© â1 , )),</p><formula xml:id="formula_8">i , = (1 â z , ) âÄ© â1 , + z , âÃ® , ,<label>(7)</label></formula><p>where A , : , A , : â R 1Ã depicts the -th row of A and A respectively, H 1 , H 2 â R Ã , W (.) and U (.) are trainable parameters, (.) is the sigmoid function, and â is the element-wise multiplication operator.</p><p>To incorporate sequential information of item interactions, we optionally learn position embeddings and add them to item embeddings to effectively obtain position-aware item (and subsequently session) embeddings. The final embeddings for items in a session are computed asÄ© </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL EVALUATION</head><p>Dataset Details: We evaluate NISER on publicly available benchmark datasets: i) Yoochoose (YC), ii) Diginetica (DN), and iii) Re-tailRocket (RR). The YC 3 dataset is from RecSys Challenge 2015, which contains a stream of user clicks on an e-commerce website within six months. Given the large number of sessions in YC, the recent 1/4 and 1/64 fractions of the training set are used to form two datasets: YC-1/4 and YC-1/64, respectively, as done in <ref type="bibr" target="#b13">[14]</ref>. The DN 4 dataset is transactional data from CIKM Cup 2016 challenge. The RR 5 dataset is from an e-commerce personalization company retailrocket, which published dataset with six month of user browsing activities. Offline and Online setting: We consider two evaluation settings: i. offline and ii. online. For evaluation in offline setting, we consider static splits of train and test as used in <ref type="bibr" target="#b13">[14]</ref> for YC and DN. For RR, we consider sessions from last 14 days for testing and remaining 166 days for training. The statistics of datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>. For evaluation in online setting, we re-train the models every day for 2 weeks (number of sessions per day for YC is much larger, so we evaluate for 1 week due to computational constraints) by appending the sessions from that day to the previous train set, and report the test results of the trained model on sessions from the subsequent day. NISER and its variants: We apply our approach over GNN and adapt code 6 from <ref type="bibr" target="#b13">[14]</ref> with suitable modification described later. We found that for sessions with length &gt; 10, considering only the most recently clicked 10 items to make recommendations worked consistently better across datasets. We refer to this variant as GNN+ and use this additional pre-processing step in all our experiments. We consider enhancement over GNN+, and proposed following variants of the embedding normalization approach:</p><p>â¢ Normalized Item Representations (NIR): only item embeddings are normalized and scale factor is not used 7 , â¢ Normalized Item and Session Representations (NISER): both item and session embeddings are normalized, â¢ NISER+: NISER with position embeddings and dropout applied to input item embeddings.</p><p>Hyperparameter Setup: Following <ref type="bibr" target="#b13">[14]</ref>, we use = 100 and learning rate of 0.001 with Adam optimizer. We use 10% of train set which is closer to test test in time as hold-out validation set for hyperparameter tuning including scale factor . We found = 16.0 to work best across most models trained on respective hold-out validation set chosen from {4.0, 9.0, 16.0, 25.0}, and hence, we use the same value across datasets for consistency. We use dropout probability of 0.1 on dimensions of item embeddings in NISER+ across all models. Since the validation set is closer to the test set in time, therefore, it is desirable to use it for training the models. After finding the best epoch via early stopping based on performance on validation set, we re-train the model for same number of epochs on combined train and validation set. We train five models for the best hyperparameters with random initialization, and report average and standard deviation of various metrics for all datasets except for YC-1/4 where we train three models (as it is a large dataset and takes around 15 hours for training one model). Evaluation Metrics: We use same evaluation metrics Recall@K and Mean Reciprocal Rank (MRR@K) as in <ref type="bibr" target="#b13">[14]</ref> with = 20. Re-call@K represents the proportion of test instances which has desired item in the top-K items. MRR@K (Mean Reciprocal Rank) is the average of reciprocal ranks of desired item in recommendation list. Large value of MRR indicates that desired item is in the top of the recommendation list. For evaluating popularity bias, we consider the following metrics as used in <ref type="bibr" target="#b1">[2]</ref>:</p><p>Average Recommendation Popularity (ARP): This measure calculates the average popularity of the recommended items in each list given by:</p><formula xml:id="formula_10">= 1 | | âï¸ â â ( ) ,<label>(9)</label></formula><p>where ( ) is popularity of item , i.e. the number of times item appears in the training set, is the recommended list of items for session , and | | is the number of sessions in the test set. An item belongs to the set Î * of long-tail items or less popular items if ( ) ( ) â¤ * . We evaluate the performance in terms of Recall@20 7Ä© s is not restricted to <ref type="bibr">[-1,1]</ref>, in general | |s | | 2 â  1.  and MRR@20 for the sessions with target item in the set Î * by varying * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results and Observations</head><p>(1) NISER+ reduces popularity bias in GNN+ in : <ref type="table" target="#tab_1">Table 2</ref> shows that ARP for NISER+ is significantly lower than GNN+ indicating that NISER+ is able to recommend less popular items more often than GNN+, thus reducing popularity bias. Furthermore, from <ref type="figure">Fig.  4</ref>, we observe that NISER+ outperforms GNN+ for sessions with less popular items as targets (i.e. when * is small), with gains 13%, 8%, 5%, and 2% for DN, RR, YC-1/64, and YC-1/4 respectively for * = 0.01 in terms of Recall@20. Similarly, gains are 28%, 18%, 6%, and 2% in terms of MRR@20. Gains for DN and RR are high as compared to YC. This is due to the high value of ( ). If instead we consider * = 0.001, gains are as high as 26% and 9% for YC-1/64 and YC-1/4 respectively in terms of Recall@20. Similarly, gains are as high as 34% and 19% in terms of MRR@20. We also note that NISER+ is at least as good as GNN+ even for the sessions with more popular items as targets (i.e. when * is large).</p><p>(2) NISER+ improves upon GNN+ in online setting for newly introduced items in the set of long-tail items Î * . These items have small number of sessions available for training at the end of the day they are launched. From <ref type="figure">Fig. 5</ref>, we observe that for the less popular newly introduced items, NISER+ outperforms GNN+ for sessions where these items are target items on the subsequent day. This proves the ability of NISER+ to recommend new items on very next day, due to its ability to reduce popularity bias. Furthermore, for DN and RR, we observe that during initial days, when training data is less, GNN+ performs poorly while performance of NISER+ is relatively consistent across days indicating potential regularization effect of NISER on GNN models in less data scenarios. Also, as days pass by and more data is available for training, performance of GNN+ improves with time but still stays significantly below NISER+. For YC, as days pass by, the popularity bias becomes more and more severe (as depicted by very small value for , i.e. the fraction of sessions with less popular newly introduced items) such that the performance of both GNN+ and NISER+ degrades with time. However, importantly, NISER+ still performs consistently better than GNN+ on any given day as it better handles the increasing popularity bias.  (3) NISER and NISER+ outperform GNN and GNN+ in offline setting: From <ref type="table" target="#tab_2">Table 3</ref>, we observe that NISER+ shows consistent and significant improvement over GNN in terms of Recall and MRR, establishing a new state-of-the-art in SR.</p><p>We also conduct an ablation study (removing one feature of NISER+ at a time) to understand the effect of each of the following features of NISER+: i. L 2 norm of embeddings, ii. including position embeddings, and iii. applying dropout on item embeddings. As shown in <ref type="table" target="#tab_3">Table 4</ref>, we observe that L 2 norm is the most important factor across datasets while dropout and position embeddings contribute in varying degrees to the overall performance of NISER+.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 )Figure 1 :</head><label>31</label><figDesc>Typical popularity distribution of items depicting the long tail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustrative flow diagram for NISER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Recall@20 and L 2 norm of learned item embeddings decreases with decreasing popularity in GNN<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>,, + W 2Ä© ,,</head><label>2Ä©</label><figDesc>+ p , where p â R is embedding vector for position obtained via a lookup over the position embeddings matrix P = [p 1 , p 2 , . . . , p ] â R Ã , where denotes the maximum length of any input session such that position â¤ .The soft-attention weight of the -th item in session is com-puted as = q sigmoid(W 1Ä© + c), where q, c â R , W 1 , W 2 â R Ã .The s are further normalized using softmax. An intermediate session embedding s â² is computed as: s â² = =1Ä© , , . The session embedding s is a linear transformation over the concatenation of intermediate session embedding s â² and the embedding of most recent itemÄ© , , , s.t. s = W 3 [s â² ;Ä© , , ], where W 3 â R Ã2 . The final recommendation scores for the items are computed as per Eq. 3. Note that while the session-graph embedding is obtained using item embeddingsÄ© , , that are aware of the session-graph and sequence, the normalized item embeddingsÄ© ( = 1 . . . ) independent of a particular session are used to compute the recommendation scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 Figure 4 :Figure 5 :</head><label>445</label><figDesc>Offline setting evaluation: Recall@20 and MRR@20 with varying * indicating larger gains by using NISER+ over GNN+ for less popular items. Online setting evaluation: Recall@20 and MRR@20 for sessions where target item is one of the less popular newly introduced items from the previous day. denotes the fraction of such sessions in the training set for * = 0.01. Standard deviations over five models are shown in lighter-shaded region around the solid lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets used for offline experiments.</figDesc><table><row><cell>Statistics</cell><cell>DN</cell><cell>RR</cell><cell cols="2">YC-1/64 YC-1/4</cell></row><row><cell>#train sessions</cell><cell cols="2">0.7 M 0.7 M</cell><cell>0.4 M</cell><cell>0.6 M</cell></row><row><cell>#test sessions</cell><cell cols="2">60,858 60,594</cell><cell>55,898</cell><cell>55,898</cell></row><row><cell>#items</cell><cell cols="2">43,097 48,759</cell><cell>16,766</cell><cell>29,618</cell></row><row><cell>Average length</cell><cell>5.12</cell><cell>3.55</cell><cell>6.16</cell><cell>5.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Offline setting evaluation: NISER+ versus GNN+ in terms of Average Recommendation Popularity (ARP). Lower values of ARP indicate lower popularity bias.</figDesc><table><row><cell>Method</cell><cell>DN</cell><cell>RR</cell><cell>YC-1/64</cell><cell>YC-1/4</cell></row><row><cell>GNN+</cell><cell>495.25Â±2.52</cell><cell>453.39Â±8.97</cell><cell>4128.54Â±27.80</cell><cell>17898.10Â±126.93</cell></row><row><cell cols="5">NISER+ 487.31Â±0.30 398.53Â±3.09 3972.40Â±41.04 16683.52Â±120.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>NISER+ versus other benchmark methods in offline setting. Numbers after Â± are standard deviation values over five models. 50Â±0.11 31.80Â±0.12 NISER+ 18.72Â±0.06 36.50Â±0.05 31.61Â±0.02 31.77Â±0.10</figDesc><table><row><cell>Method</cell><cell>DN</cell><cell>RR</cell><cell>YC-1/64</cell><cell>YC-1/4</cell></row><row><cell></cell><cell></cell><cell>Recall@20</cell><cell></cell><cell></cell></row><row><cell>SKNN [5]</cell><cell>48.06</cell><cell>56.42</cell><cell>63.77</cell><cell>62.13</cell></row><row><cell>STAN [3]</cell><cell>50.97</cell><cell>59.80</cell><cell>69.45</cell><cell>70.07</cell></row><row><cell>GRU4REC [4]</cell><cell>29.45</cell><cell>-</cell><cell>60.64</cell><cell>59.53</cell></row><row><cell>NARM [6]</cell><cell>49.70</cell><cell>-</cell><cell>68.32</cell><cell>69.73</cell></row><row><cell>STAMP [8]</cell><cell>45.64</cell><cell>53.94</cell><cell>68.74</cell><cell>70.44</cell></row><row><cell>GNN [14]</cell><cell>51.39Â±0.38</cell><cell>57.63Â±0.15</cell><cell>70.54Â±0.14</cell><cell>70.95Â±0.04</cell></row><row><cell>GNN+</cell><cell>51.81Â±0.11</cell><cell>58.59Â±0.10</cell><cell>70.85Â±0.08</cell><cell>71.10Â±0.07</cell></row><row><cell>NIR</cell><cell>52.40Â±0.06</cell><cell>60.67Â±0.08</cell><cell>71.12Â±0.05</cell><cell>71.32Â±0.11</cell></row><row><cell>NISER</cell><cell>52.63Â±0.09</cell><cell>60.85Â±0.06</cell><cell>70.86Â±0.15</cell><cell>71.69Â±0.03</cell></row><row><cell>NISER+</cell><cell cols="4">53.39Â±0.06 61.41Â±0.09 71.27Â±0.05 71.80Â±0.09</cell></row><row><cell></cell><cell></cell><cell>MRR@20</cell><cell></cell><cell></cell></row><row><cell>SKNN [5]</cell><cell>16.95</cell><cell>33.16</cell><cell>25.22</cell><cell>24.82</cell></row><row><cell>STAN [3]</cell><cell>18.48</cell><cell>35.32</cell><cell>28.74</cell><cell>28.89</cell></row><row><cell>GRU4REC [4]</cell><cell>8.33</cell><cell>-</cell><cell>22.89</cell><cell>22.60</cell></row><row><cell>NARM [6]</cell><cell>16.17</cell><cell>-</cell><cell>28.63</cell><cell>29.23</cell></row><row><cell>STAMP [8]</cell><cell>14.32</cell><cell>28.49</cell><cell>29.67</cell><cell>30.00</cell></row><row><cell>GNN [14]</cell><cell>17.79Â±0.16</cell><cell>32.74Â±0.09</cell><cell>30.80Â±0.09</cell><cell>31.37Â±0.13</cell></row><row><cell>GNN+</cell><cell>18.03Â±0.05</cell><cell>33.29Â±0.03</cell><cell>30.84Â±0.10</cell><cell>31.51Â±0.05</cell></row><row><cell>NIR</cell><cell>18.52Â±0.06</cell><cell>35.57Â±0.05</cell><cell>30.99Â±0.10</cell><cell>31.73Â±0.11</cell></row><row><cell>NISER</cell><cell>18.27Â±0.10</cell><cell>36.09Â±0.03</cell><cell>31.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation results for NISER+ indicating that normalization of embeddings (L 2 norm) contributes the most to performance improvement. Here PE: Position Embeddings. 32Â±0.03 31.68Â±0.05 31.71Â±0.06</figDesc><table><row><cell>Method</cell><cell>DN</cell><cell>RR</cell><cell>YC-1/64</cell><cell>YC-1/4</cell></row><row><cell></cell><cell></cell><cell>Recall@20</cell><cell></cell><cell></cell></row><row><cell>NISER+</cell><cell cols="4">53.39Â±0.06 61.41Â±0.09 71.27Â±0.05 71.80Â±0.09</cell></row><row><cell>-L 2 norm</cell><cell>52.23Â±0.10</cell><cell>59.16Â±0.10</cell><cell>71.10Â±0.09</cell><cell>71.46Â±0.19</cell></row><row><cell>-Dropout</cell><cell>52.81Â±0.12</cell><cell>60.99Â±0.09</cell><cell cols="2">71.07Â±0.13 71.90Â±0.03</cell></row><row><cell>-PE</cell><cell cols="2">53.11Â±0.12 61.22Â±0.03</cell><cell>71.13Â±0.04</cell><cell>71.70Â±0.11</cell></row><row><cell></cell><cell></cell><cell>MRR@20</cell><cell></cell><cell></cell></row><row><cell>NISER+</cell><cell cols="3">18.72Â±0.06 36.50Â±0.05 31.61Â±0.02</cell><cell>31.77Â±0.10</cell></row><row><cell>-L 2 norm</cell><cell>18.11Â±0.05</cell><cell>33.78Â±0.04</cell><cell>30.90Â±0.07</cell><cell>31.49Â±0.07</cell></row><row><cell>-Dropout</cell><cell>18.43Â±0.11</cell><cell>35.99Â±0.02</cell><cell cols="2">31.56Â±0.06 31.93Â±0.17</cell></row><row><cell>-PE</cell><cell>18.60Â±0.09</cell><cell>36.</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the terms representation and embedding interchangeably.<ref type="bibr" target="#b1">2</ref> To ensure same dimensions of I â R Ã across sessions, we can pad with a dummy vector â times.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://2015.recsyschallenge.com/challege.html 4 http://cikm2016.cs.iupui.edu/cikm-cup 5 https://www.dropbox.com/sh/dbzmtq4zhzbj5o9/AACldzQWbw-igKjcPTBI6ZPAa?dl=0 6 https://github.com/CRIPAC-DIG/SR-GNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>In this work, we highlighted that the typical item-frequency distribution with long tail leads to popularity bias in state-of-the-art deep learning models such as GNNs <ref type="bibr" target="#b13">[14]</ref> for session-based recommendation. We then argued that this is partially related to the 'radial' property of the softmax loss that, in our setting, implies that the norm for popular items will likely be larger than the norm of less popular items. We showed that learning the representations for items and session-graphs by optimizing for cosine similarity instead of inner product can help mitigate this issue to a large extent. Importantly, this ability to reduce popularity bias is found to be useful in the online setting where the newly introduced items tend to be less popular and are poorly modeled by existing approaches. We observed significant improvements in overall recommendation performance by normalizing the item and session-graph representations and improve upon the existing state-of-the-art results. In future, it would be worth exploring NISER to improve other algorithms like STAMP <ref type="bibr" target="#b7">[8]</ref> that rely on similarity between embeddings for items, sessions, users, etc.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Controlling popularity bias in learning-to-rank recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems</title>
		<meeting>the Eleventh ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="42" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07555</idno>
		<title level="m">Managing Popularity Bias in Recommender Systems with Personalized Re-ranking</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequence and Time Aware Neighborhood for Session-based Recommendations: STAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diksha</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Shroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;19)</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;19)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1069" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BalÃ¡zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Linas Baltrunas, and Domonkos Tikk. ICLR-2016</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">When recurrent neural networks meet the neighborhood for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Ludewig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM Conference on Recommender Systems</title>
		<meeting>the Eleventh ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="306" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-03" />
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">STAMP: shortterm attention/memory priority model for session-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Refuoe</forename><surname>Mokhosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1831" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03721</idno>
		<title level="m">A comparative study on regularization strategies for embedding-based neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Item popularity and recommendation accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM conference on Recommender systems</title>
		<meeting>the fifth ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Collaborative Session-based Recommendation Approach with Parallel Memory Modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;19)</title>
		<meeting>the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;19)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unbiased offline recommender evaluation for missing-not-atrandom implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems</title>
		<meeting>the 12th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ring loss: Convex feature normalization for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dipan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5089" to="5097" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
