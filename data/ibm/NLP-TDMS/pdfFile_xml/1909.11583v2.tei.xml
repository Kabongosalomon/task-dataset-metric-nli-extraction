<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OFF-POLICY ACTOR-CRITIC WITH SHARED EXPERIENCE REPLAY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">Schmitt</forename><surname>Deepmind</surname></persName>
							<email>simonyan@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><forename type="middle">Hessel</forename><surname>Deepmind</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">Simonyan</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">OFF-POLICY ACTOR-CRITIC WITH SHARED EXPERIENCE REPLAY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the combination of actor-critic reinforcement learning algorithms with uniform large-scale experience replay and propose solutions for two challenges: (a) efficient actor-critic learning with experience replay (b) stability of off-policy learning where agents learn from other agents behaviour. We employ those insights to accelerate hyper-parameter sweeps in which all participating agents run concurrently and share their experience via a common replay module. To this end we analyze the bias-variance tradeoffs in V-trace, a form of importance sampling for actor-critic methods. Based on our analysis, we then argue for mixing experience sampled from replay with on-policy experience, and propose a new trust region scheme that scales effectively to data distributions where V-trace becomes unstable. We provide extensive empirical validation of the proposed solution. We further show the benefits of this setup by demonstrating state-of-the-art data efficiency on Atari among agents trained up until 200M environment frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Value-based and actor-critic policy gradient methods are the two leading techniques of constructing general and scalable reinforcement learning agents <ref type="bibr" target="#b42">(Sutton et al., 2018)</ref>. Both have been combined with non-linear function approximation <ref type="bibr" target="#b43">(Tesauro, 1995;</ref><ref type="bibr" target="#b47">Williams, 1992)</ref>, and have achieved remarkable successes on multiple challenging domains; yet, these algorithms still require large amounts of data to determine good policies for any new environment. To improve data efficiency, experience replay agents store experience in a memory buffer (replay) <ref type="bibr" target="#b22">(Lin, 1992)</ref>, and reuse it multiple times to perform reinforcement learning updates <ref type="bibr" target="#b34">(Riedmiller, 2005)</ref>. Experience replay allows to generalize prioritized sweeping <ref type="bibr" target="#b28">(Moore &amp; Atkeson, 1993)</ref> to the non-tabular setting <ref type="bibr" target="#b35">(Schaul et al., 2015)</ref>, and can also be used to simplify exploration by including expert (e.g., human) trajectories <ref type="bibr" target="#b12">(Hester et al., 2017)</ref>. Overall, experience replay can be very effective at reducing the number of interactions with the environment otherwise required by deep reinforcement learning algorithms <ref type="bibr" target="#b35">(Schaul et al., 2015)</ref>. Replay is often combined with the value-based Q-learning , as it is an off-policy algorithm by construction, and can perform well even if the sampling distribution from replay is not aligned with the latest agent's policy. Combining experience replay with actor-critic algorithms can be harder due to their on-policy nature. Hence, most established actor-critic algorithms with replay such as <ref type="bibr" target="#b46">(Wang et al., 2017;</ref><ref type="bibr" target="#b8">Gruslys et al., 2018;</ref><ref type="bibr" target="#b9">Haarnoja et al., 2018)</ref> employ and maintain Q-functions to learn from the replayed off-policy experience.</p><p>In this paper, we demonstrate that off-policy actor-critic learning with experience replay can be achieved without surrogate Q-function approximators using V-trace by employing the following approaches: a) off-policy replay experience needs to be mixed with a proportion of on-policy experience. We show experimentally ( <ref type="figure">Figure 2)</ref> and theoretically that the V-trace policy gradient is otherwise not guaranteed to converge to a locally optimal solution. b) a trust region scheme <ref type="bibr" target="#b5">(Conn et al., 2000;</ref><ref type="bibr" target="#b37">Schulman et al., 2015;</ref> can mitigate bias and enable efficient learning in a strongly off-policy regime, where distinct agents share experience through a commonly shared replay module. Sharing experience permits the agents to benefit from parallel exploration <ref type="bibr" target="#b20">(Kretchmar, 2002)</ref>  <ref type="figure" target="#fig_2">(Figures 1 and 3)</ref>.</p><p>Our paper is structured as follows: In Section 2 we revisit pure importance sampling for actor-critic agents <ref type="bibr" target="#b6">(Degris et al., 2012)</ref> and V-trace, which is notable for allowing to trade off bias and variance in its estimates. We recall that variance reduction is necessary <ref type="figure" target="#fig_1">(Figure 4</ref> left) but is biased in V-trace. We derive proposition 2 stating that off-policy V-trace is not guaranteed to converge to a locally optimal solution -not even in an idealized scenario when provided with the optimal value function. Through theoretical analysis (Section 3) and experimental validation ( <ref type="figure">Figure 2)</ref> we determine that mixing on-policy experience into experience replay alleviates the problem. Furthermore we propose a trust region scheme <ref type="bibr" target="#b5">(Conn et al., 2000;</ref><ref type="bibr" target="#b37">Schulman et al., 2015;</ref> in Section 4 that enables efficient learning even in a strongly off-policy regime, where distinct agents share the experience replay module and learn from each others experience. We define the trust region in policy space and prove that the resulting estimator is correct (i.e. estimates an improved return).</p><p>As a result, we present state-of-the-art data efficiency in Section 5 in terms of median human normalized performance across 57 Atari games <ref type="bibr" target="#b3">(Bellemare et al., 2013)</ref>, as well as improved learning efficiency on DMLab30 <ref type="bibr" target="#b2">(Beattie et al., 2016)</ref>  <ref type="table" target="#tab_0">(Table 1</ref>).</p><p>Figure 1: Sharing experience between agents leads to more efficient hyper-parameter sweeps on 57 Atari games. Prior art results are presented as horizontal lines (with scores cited from <ref type="bibr" target="#b8">Gruslys et al. (2018)</ref>, <ref type="bibr" target="#b10">Hessel et al. (2017)</ref> and <ref type="bibr" target="#b25">Mnih et al. (2013)</ref>). Note that the only previous agent "R2D2" that achieved a score beyond 400% required more than 3,000 million environment steps (see <ref type="bibr" target="#b19">Kapturowski et al. (2019)</ref>, page 14, <ref type="figure">Figure 9</ref>). We present the pointwise best agent from hyper-parameter sweeps with and without experience replay (shared and not shared). Each sweep contains 9 agents with different learning rate and entropy cost combinations. Replay experiment were repeated twice and ran for 50M steps. To report scores at 200M we ran the baseline and one shared experience replay agent for 200M steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE ISSUE WITH IMPORTANCE SAMPLING: BIAS AND VARIANCE IN</head><p>V-TRACE V-trace importance sampling is a popular off-policy correction for actor-critic agents <ref type="bibr" target="#b7">(Espeholt et al., 2018)</ref>. In this section we revisit how V-trace controls the (potentially infinite) variance that arises from naive importance sampling. We note that this comes at the cost of a biased estimate (see Proposition 1) and creates a failure mode (see Proposition 2) which makes the policy gradient biased. We discuss our solutions for said issues in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">REINFORCEMENT LEARNING</head><p>We follow the notation of <ref type="bibr" target="#b42">Sutton et al. (2018)</ref> where an agent interacts with its environment, to collect rewards. On each discrete time-step t, the agent selects an action a t ; it receives in return a reward r t and an observation o t+1 , encoding a partial view of the environment's state s t+1 . In the fully observable case, the RL problem is formalized as a Markov Decision Process <ref type="bibr" target="#b4">(Bellman, 1957)</ref>: a  <ref type="bibr" target="#b48">Xu et al. (2018)</ref> and <ref type="bibr" target="#b11">Hessel et al. (2019)</ref>, the third is our implementation of a pixel control agent from <ref type="bibr" target="#b11">Hessel et al. (2019)</ref>   <ref type="figure">Figure 2</ref>: Left: Learning entirely off-policy from experience replay fails, while combining on-policy data with experience replay leads to improved data efficiency: We present sweeps on DMLab-30 with experience replays of 10M capacity. A ratio of 87.5% implies that there are 7 replayed transitions in the batch for each online transition. Furthermore we consider an agent identical to "LASER 87.5% replay" which however draws all samples from replay. Its batch thus does not contain any online data and we observe a significant performance decrease (see Proposition 2 and 3). The shading represents the point-wise best and worst replica among 3 repetitions. The solid line is the mean. Right: The effect of capacity in experience replay with 87.5% replay data per batch on sweeps on DMLab-30. Data-efficiency improves with larger capacity.</p><p>tuple (S, A, p, γ), where S, A denotes finite sets of states and actions, p models rewards and state transitions (so that r t , s t+1 ∼ p(s t , a t )), and γ is a fixed discount factor. A policy is a mapping π(a|s) from states to action probabilities. The agent seeks an optimal policy π * that maximizes the value, defined as the expectation of the cumulative discounted returns G t = ∞ k=0 γ k r t+k . Off-policy learning is the problem of finding, or evaluating, a policy π from data generated by a different policy µ. This arises in several settings. Experience replay <ref type="bibr" target="#b22">(Lin, 1992)</ref> mixes data from multiple iterations of policy improvement. In large-scale RL, decoupling acting from learning <ref type="bibr" target="#b30">(Nair et al., 2015;</ref><ref type="bibr" target="#b7">Espeholt et al., 2018)</ref> causes the experience to lag behind the latest agent policy. Finally, it is often useful to learn multiple general value functions <ref type="bibr" target="#b40">(Sutton et al., 2011;</ref><ref type="bibr" target="#b23">Mankowitz et al., 2018;</ref><ref type="bibr" target="#b21">Lample &amp; Chaplot, 2016;</ref><ref type="bibr" target="#b24">Mirowski et al., 2017;</ref><ref type="bibr" target="#b16">Jaderberg et al., 2017b)</ref> or options <ref type="bibr" target="#b39">(Sutton et al., 1999;</ref><ref type="bibr" target="#b1">Bacon et al., 2017</ref>) from a single stream of experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NAIVE IMPORTANCE SAMPLING</head><p>On-policy n-step bootstraps give more accurate value estimates in expectation with larger n <ref type="bibr" target="#b42">(Sutton et al., 2018)</ref>. They are used in many reinforcement learning agents <ref type="bibr" target="#b27">(Mnih et al., 2016;</ref><ref type="bibr">Schulman Figure 3</ref>: Left: Naively sharing experience between distinct agents in a hyper-parameter sweep fails (green) and is worse than the no-replay baseline (blue). The proposed trust region estimator mitigates the issue (red). Right: Combining population based training with trust region estimation improves performance further. All replay experiments use a capacity of 10 million observations and 87.5% replay data per batch. <ref type="bibr" target="#b10">Hessel et al., 2017)</ref>. Unfortunately n must be chosen suitably as the estimates variance increases with n too.</p><p>It is desirable to obtain benefits akin to n-step returns in the off-policy case. To this end multi-step importance sampling <ref type="bibr" target="#b17">(Kahn, 1955)</ref> can be used. This however adds another source of (potentially infinite <ref type="bibr" target="#b42">(Sutton et al., 2018)</ref>) variance to the estimate. Importance sampling can estimate the expected return V π from trajectories sampled from µ = π, as long as µ is non-zero whereever π is. We employ a previously estimated value function V as a bootstrap to estimate expected returns. Following <ref type="bibr" target="#b6">Degris et al. (2012)</ref>, a multi-step formulation of the expected return is</p><formula xml:id="formula_0">V π (s t ) = E µ V (s t ) + K−1 k=0 γ k k i=0 π t+i µ t+i δ t+k V<label>(1)</label></formula><p>where E µ denotes the expectation under policy µ up to an episode termination, δ t V = r t + γV (s t+1 ) − V (s t ) is the temporal difference error in consecutive states s t+1 , s t , and π t = π t (a t |s t ). Importance sampling estimates can have high variance. Tree Backup <ref type="bibr" target="#b33">(Precup et al., 2000)</ref>, and Q(λ) <ref type="bibr" target="#b41">(Sutton et al., 2014)</ref> address this, but reduce the number of steps before bootstrapping even when this is undesirable (as in the on-policy case). RETRACE <ref type="bibr" target="#b29">(Munos et al., 2016)</ref> makes use of full returns in the on-policy case, but it introduces a zero-mean random variable at each step, adding variance to empirical estimates in both on-and off-policy cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">BIAS-VARIANCE ANALYSIS &amp; FAILURE MODE OF V-TRACE IMPORTANCE SAMPLING</head><p>V-trace <ref type="bibr" target="#b7">(Espeholt et al., 2018)</ref> reduces the variance of importance sampling by trading off variance for a biased estimate of the return -resulting in a failure mode (see Proposition 2). It uses clipped importance sampling ratios to approximate V π by Vπ(s t ) = V (s t )+</p><formula xml:id="formula_1">K−1 k=0 γ k k−1 i=0 c i ρ t δ t+k V</formula><p>where V is a learned state value estimate used to bootstrap, and ρ t = min [π t /µ t ,ρ], c t = min [π t /µ t ,c] are the clipped importance ratios. Note that, differently from RETRACE, V-trace fully recovers the Monte Carlo return when on policy. It similarly reweights the policy gradient as:</p><formula xml:id="formula_2">∇Vπ(s t ) def = E µ ρ t ∇(log π t )(r t + γVπ(s t+1 ))<label>(2)</label></formula><p>Note that ∇Vπ(s t ) recovers the naively importance sampled policy gradient forρ → ∞. In the literature, it is common to subtract a baseline from the action-value estimate r t + γVπ(s t+1 ) to reduce variance <ref type="bibr" target="#b47">(Williams, 1992)</ref>, omitted here for simplicity. The constantsρ ≥c ≥ 1 (typically chosenρ =c = 1) define the level of clipping, and improve stability by ensuring a bounded variance. For any givenρ, the bias introduced by V-trace in the value and policy gradient estimates increases with the difference between π and µ. We analyze this in the following propositions. (3)</p><p>Proof. See <ref type="bibr" target="#b7">Espeholt et al. (2018)</ref>.</p><p>Note that the biased policyπ µ can be very different from π. Hence the V-trace value estimate Vπ may be very different from V π as well. As an illustrative example, consider two policies over a set of two actions, e.g. "left" and "right" represented as a tuple of probabilities. Let us investigate µ = (φ, 1 − φ) and π = (1 − φ, φ) defined for any suitably small φ ≤ 1. Observe that π and µ share no trajectories (state-action sequences) in the limit as φ → 0 and they get more focused on one action. A practical example of this could be two policies, one almost always taking a left turn and one always taking the right. Given sufficient data of either policy it is possible to estimate the value of the other e.g. with naive importance sampling. However observe that V-trace withρ = 1 will always estimate a biased value -even given infinite data. Observe that min [µ(a|x), π(a|x)] = min [φ, 1 − φ] for both actions. Thusπ µ is uniform rather than resembling π the policy. The V-trace estimate Vπ would thus compute the average value of "left" and "right" -poorly representing the true V π .</p><p>Proposition 2. The V-trace policy gradient is biased: given the the optimal value function V * the V-trace policy gradient does not converge to a locally optimal π * for all off-policy behaviour distributions µ.</p><p>Proof. See Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MIXING ON-AND OFF-POLICY EXPERIENCE</head><p>In Proposition 2 we presented a failure mode in V-trace where the variance reduction biases the value estimate and policy gradient. V-trace computes biased Q-estimates Q ω = Q resulting in a wrong local policy gradient: The question of how biased the resulting policy will be depends on whether the distortion changes the argmax of the Q-function. Little distortions that do not change the argmax will result in the same local fixpoint of the policy improvement. The policy will continue to select the optimal action and it will not be biased at this state. The policy will however be biased if the Q-function is distorted too much. For example consider a ω(s, a) that swaps the argmax for the 2nd largest value, the regret will then be the difference between the maximum and the 2nd largest value. Intuitively speaking the more distorted the Q ω , the larger will be the regret compared to the optimal policy.</p><p>More precisely, the regret of learning a policy that maximizes the distorted Q ω at state s is:</p><formula xml:id="formula_3">R(s) = Q(s, a * ) − Q(s, a actual ) = max b Q(s, b) − Q(s, a actual )</formula><p>where a * = argmax b (Q, b) is the optimal action according to the real Q and a actual = argmax[Q ω (s, a)] = argmax[Q(s, a)ω(s, a)], is the optimal action according to the distorted Q ω . For generality, we denote A * as the set of best actions -covering the case with multiple with identical optimal Q-values.</p><p>Proposition 3 provides a mitigation: Clearly the V-trace policy gradient will converge to the same solution as the true on-policy gradient if the argmax of the Q-function is preserved at all states in a tabular setting. We show that this can be achieved by mixing a sufficient proportion α of on-policy experience into the computation.</p><p>We show in equation 13 in the Appendix that choosing α such that</p><formula xml:id="formula_4">α 1 − α &gt; max b ∈A * Q ω (s, b) − Q ω (s, a * ) Q(s, a * ) − Q(s, b) d µ (s) d π (s) for Q ω (s, a) = Q(s, a)ω(s, a)</formula><p>will result in a policy that correctly chooses the best action at state s. Note that α 1−α → ∞ as α → 1. Intuitively: the larger the action value gap of the real Q-function Q(s, a * ) − Q(s, b) the lower the right hand side and the less on-policy data is required.</p><formula xml:id="formula_5">If max b [(Q(s, b)ω(s, b) − Q(s, a * )ω(s, a * )]</formula><p>is negative, then α may be as small as zero and we enabling even pure off-policy learning. Finally note that the right hand side decreases due to d µ (s)/d π (s) if π visits the state s more often than µ.</p><p>All of those conditions can be computed and checked if an accurate Q-function and state distribution is accessible. How to use imperfect Q-function estimates to adaptively choose such an α remain a question for future research.</p><p>We provide experimental evidence for these results with function approximators in the 3-dimensional simulated environment DMLab-30 with various α ≥ 1/8 in Section 5.3 and <ref type="figure">Figure 2</ref>. We observe that α = 1/8 is sufficient to facilitate stable learning. Furthermore it results in better data-efficiency than pure on-policy learning as it utilizes off-policy replay experience.</p><p>Proposition 3. Mixing on-policy data into the V-trace policy gradient with the ratio α reduces the bias by providing a regularization to the implied state-action values. In the general function approximation case it changes the off-policy V-trace policy gradient from</p><formula xml:id="formula_6">s d µ (s)E π [(Q(s, a)∇ log π(a|s)] to s E π [Q α (s, a)∇ log π(a|s)] where Q α = Qd π (s)α + Q ω d µ (s)(1 − α)</formula><p>is a regularized stateaction estimate and d π , d µ are the state distributions for π and µ. Note that there exists α ≤ 1 such that Q α has the same argmax (i.e. best action) as Q.</p><p>Proof. See Appendix C.</p><p>Mixing online data with replay data has also been argued for by <ref type="bibr" target="#b49">Zhang &amp; Sutton (2017)</ref>, as a heuristic way of reducing the sensitivity of reinforcement learning algorithms to the size of the replay memory. Proposition 3 grounds this in the theoretical properties of V-trace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRUST REGION SCHEME FOR OFF-POLICY V-TRACE</head><p>To mitigate the bias and variance problem of V-trace and importance sampling we propose a trust region scheme that adaptively selects only suitable behaviour distributions when estimating the state-value of π. To this end we introduce a behaviour relevance function that classifies behaviour as relevant. We then define a trust-region estimator that computes expectations (such as expected returns, or the policy gradient) only on relevant transitions. In proposition 4 and 5 we show that this trust region estimator indeed computes new state-value estimates that improve over the current value function. While our analysis and proof is general we propose a suitable behaviour relevance function in section 4.3 that employs the Kullback Leibler divergence between target policy π and implied policyπ µ : KL (π(·|s)||π µ (·|s)). We provide experimental validation in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BEHAVIOUR RELEVANCE FUNCTIONS</head><p>In off-policy learning we often consider a family of behaviour policies either indexed by training iteration t: M T = {µ t |t &lt; T } for experience replay, or by a different agent k: M K = {µ k |k ∈ K} when training multiple agents. In the classic experience replay case we then sample a time t and locate the transition τ that was generated earlier via µ t . This extends naturally to the multiple agent case where we sample an agent index k and then obtain a transition for such agent or tuples of (k, t). Without loss of generality we simplify this notation and index sampled behaviour policies by a random variable z ∼ Z that represents the selection process. While online reinforcement learning algorithms process transitions τ ∼ π, off-policy algorithms process τ ∼ µ z for z ∼ Z. In this notation, given equation (1) and a bootstrap V , the expectation of importance sampled off-policy returns at state s t is described by:</p><formula xml:id="formula_7">V π mix (s t ) = E z E µz|z G π,µz (s t )]<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">G π,µ (s t ) = V (s t ) + ∞ k=0 γ k k i=0</formula><p>πt+i µt+i δ t+k V is a single importance sampled return. Note that the on-policy return G π,π (s t ) = V (s t ) + ∞ k=0 γ k r t+k . Above E µz|z represents the expectation of sampling from a given µ z . The conditioning on z is a notational reminder that this expectation does not sample z or µ z but experience from µ z . For any sampled z we obtain a µ z and observe that the inner expectation wrt. experience of µ z in equation <ref type="formula" target="#formula_7">(4)</ref> recovers the expected on-policy return in expectation:</p><formula xml:id="formula_9">E µz|z [G π,µz (s t )] = E µz|z V (s t ) + ∞ k=0 γ k k i=0 π t+i µ z,t+i δ t+k V = E π V (s t ) + ∞ k=0 γ k k i=0 µ z,t+i µ z,t+i δ t+k V = E π V (s t ) + ∞ k=0 γ k r t+k = E π [G π,π (s t )] = V π (s t )<label>(5)</label></formula><formula xml:id="formula_10">Thus V π mix (s t ) = E z [E π [G π,π (s t )]] = E π [G π,π (s t )] = V π (s t )</formula><p>. This holds provided that µ z is non-zero wherever π is. This fairly standard assumption leads us straight to the core of the problem: it may be that some behaviours µ z are ill-suited for estimating the inner expectation. However, standard importance sampling applied to very off-policy experience divides by small µ resulting in high or even infinite variance. Similarly, V-trace attempts to compute an estimate of the return following π resulting in limited variance at the cost of a biased estimate in turn.</p><p>The key idea of our proposed solution is to compute the return estimate for π at each state only from a subset of suitable behaviours µ z :</p><formula xml:id="formula_11">M β,π (s) = {µ z |z ∈ Z and β(π, µ, s) &lt; b} as determined by a behaviour relevance function β(π, µ, s) : (M Z , M Z , S) → R and a threshold b.</formula><p>The behaviour relevance function decides if experience from a behaviour is suitable to compute an expected return for π. It can be chosen to control properties of V π mix by restricting the expectation on subsets of Z. In particular it can be used to control the variance of an importance sampled estimator: Observe that the inner expectation E µz G π,µ (s t ) z in equation <ref type="formula" target="#formula_7">(4)</ref> already matches the expected return V π . Thus we can condition the expectation on arbitrary subsets of Z without changing the expected value of V π mix . This allows us to reject high variance G π,µ without introducing a bias in V π mix . The same technique can be applied to V-trace where we can reject return estimates with high bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DERIVATION OF TRUST REGION ESTIMATORS</head><p>Using a behaviour relevance function β(s) we can define a trust region estimator for regular importance sampling (IS) and V-trace and show their correctness.</p><p>We define the trust region estimator as the conditional expectation</p><formula xml:id="formula_12">V π trusted (s t ) = E z E µz|z G π,µz,β (s t ) µ z ∈ M β,π (s t )<label>(6)</label></formula><p>with λ-returns G, chosen as G IS for importance sampling and G Vtrace for V-trace:</p><formula xml:id="formula_13">G π,µz IS (s t ) = V (s t ) + ∞ k=0 γ k k i=0 λ π,µz (s t+i ) π t+i µ z,t+i δ t+k V (7) G π,µz Vtrace (s t ) = V (s t ) + ∞ k=0 γ k k−1 i=0 λ π,µz (s t+i )c z,t+i λ π,µz (s t+k )ρ z,t+k δ t+k V<label>(8)</label></formula><p>where λ π,µ (s t ) is designed to constraint Monte-Carlo bootstraps to relevant behaviour: λ π,µ (s t ) = 1 β(π,µ,st)&lt;b and ρ z,t+k = min πt+i µz,t+i ,ρ and c z,t+k are behaviour dependent clipped importance rations. Thus both G π,µz IS and G π,µz</p><p>Vtrace are a multi-step return estimators with adaptive length. Note that only estimators with length ≥ 1 are used in V π trusted . Due to Minkowski's inequality the trust region estimator thus shows at least the same contraction as a 1-step bootstrap, but can be faster due to its adaptive nature: IS be a set of importance sampling estimators as defined in equation 7. Note that they all have the same fix point V π and contract with at least γ. Then the contraction properties carry over to V π trusted . In particular |V π</p><formula xml:id="formula_14">trusted − V π | ∞ ≤ γ |V − V π | ∞ .</formula><p>Proof. See Appendix C.</p><p>Proposition 5. Let G π,µz Vtrace be a set of V-trace estimators (see equation 8) with corresponding fixed points V z (see equation 3) to which they contract at a speed of an algorithm and behaviour specific</p><formula xml:id="formula_15">η z . Then V π trusted moves towards V β = E z|µz∈M β,π (st) [V z ] shrinking the distance as follows V π trusted − V β ∞ &lt; max µz∈M β,π (st) |η z (V − V z )| ∞ ≤ η max max µz∈M β,π (st) |(V − V z )| ∞ with η max = max µz∈M β,π (st) η z . Proof. See Appendix C.</formula><p>Note how the choice of β and thus M β,π enables us to discard ill-suited G π,µz</p><p>Vtrace from the estimation of V π trusted . Recall that V-trace fixed points V z are biased. Thus β allows us to selectively create the V-trace target V β = E z|µz∈M β,π (st) [V z ] and control its bias and the shrinkage max µz∈M β,π (st) |η z (V (s) − V z (s))| ∞ (see Proposition 5). Similarly it can control cases where we can not use the exact importance sampled estimator. The same approach based on nested expectations can be applied to the expectation of the policy gradient estimate and allows to control the bias and greediness (see Proposition 2) there as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">IMPLEMENTATION DETAILS</head><p>In Proposition 5 we have seen that the quality of the trust region V-trace return estimator depends on β. A suitable choice of β can move the return estimate V β closer to V π and improve the shrinkage by reducing max µz∈M β,π (st) |η z (V (s) − V z (s))| ∞ . Hence, we employ a behaviour relevance function β KL that rejects high bias transitions by estimating the Kulback-Leibler divergence between the target policy π and the implied policyπ µz for a sampled behaviour µ z . Recall from Proposition 1 thatπ µz determines the fixed point of the V-trace estimator for behaviour µ z and thus determines the bias in V z .</p><p>β KL (π, µ, s) = KL (π(·|s)||π µ (·|s))</p><p>Note that the behaviour probabilities µ z can be evaluated and saved to the replay when the agent executes the behaviour, similarly the target policy π is represented by the agents neural network. Using both and equation 3,π µ can be computed. For large or infinite action spaces a Monte Carlo estimate of the KL divergence can be computed.</p><p>It is possible to define separate behaviour relevance functions for the policy and value estimate. For simplicity we reject transitions entirely for all estimates and do not consider rejected transitions for the policy gradient and value gradient updates or auxiliary tasks. As described above we stop the Monte-Carlo bootstraps once they reach undesirable state-behaviour pairs. Note that this censoring procedure is computed from state dependent β(π, µ, s) and ensures that the choice of bootstrapping does not depend on the sampled actions. Note that rejection by an action-based criteria such as small π(a|s)/µ(a|s) would introduce an additional bias which we avoid by choosing β KL .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We present experiments to support the following claims:</p><p>• Section 5.2: Uniform experience replay obtains comparable results as prioritized experience replay, while being simpler to implement and tune. • Section 5.3: Using fresh experience before inserting it in experience replay is better than learning purely off-policy from experience replay -in line with Proposition 3. • Section 5.4: Sharing experience without trust region performs poorly as suggested by Proposition 2. Off-Policy Trust-Region V-trace solves this issue. • Section 5.5: Sharing experience can take advantage of parallel exploration and obtains state-of-the-art performance on Atari games, while also saving memory through sharing a single experience replay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENTAL SETUP &amp; METHODOLOGY</head><p>We use the V-trace distributed reinforcement learning agent <ref type="bibr" target="#b7">(Espeholt et al., 2018)</ref> as our baseline. In our experiments we consider two experimental platforms: Atari and DeepMind Lab. On Atari we consider the common single task training regime, where a different agent is trained, from scratch, on each of the tasks. Following <ref type="bibr" target="#b48">Xu et al. (2018)</ref> we use a discount of 0.995. Motivated by recent work by <ref type="bibr" target="#b18">Kaiser et al. (2019)</ref>, we use the IMPALA deep network and increased the number of channels 4×. We use 96% replay data per batch. Differently from <ref type="bibr" target="#b7">Espeholt et al. (2018)</ref>, we do not use gradient clipping by norm <ref type="bibr" target="#b32">(Pascanu et al., 2012)</ref>. Updates are computed on mini-batches of 32 (regular) and 128 (replay) trajectories, each corresponding to 19 steps in the environment. In the context of DeepMind Lab, we consider the multi-task suite DMLab-30 <ref type="bibr" target="#b7">(Espeholt et al., 2018)</ref>, as the visuals and the dynamics are more consistent across tasks. Furthermore the multi-task regime is particularly suitable for the investigation of strongly off-policy data distributions arising from sharing the replay across agents, as concurrently learning agents can easily be stuck in different policy plateaus, generating substantially different data <ref type="bibr" target="#b36">(Schaul et al., 2019)</ref>. As in <ref type="bibr" target="#b7">Espeholt et al. (2018)</ref>, in the multi-task setting each agent trains simultaneously on a uniform mixture of all tasks rather than individually on each game. The score of an agent is thus the median across all 30 tasks. Following <ref type="bibr" target="#b11">Hessel et al. (2019)</ref>, we augment our agent with multi-task Pop-Art normalization and PixelControl. We use a PreCo LSTM <ref type="bibr" target="#b0">(Amos et al., 2018)</ref> instead of the vanilla one <ref type="bibr" target="#b13">(Hochreiter &amp; Schmidhuber, 1997)</ref>. Updates are computed on mini-batches of multiple trajectories chosen as above, each corresponding to 79 steps in the environment. In early experiments we found that computing the entropy cost only on the online data provided slightly better results, hence we have done so throughout our experiments.</p><p>In all our experiments, experience sampled from memory is mixed with online data within each minibatch -following Proposition 3. Episodes are removed in a first in first out order, so that replay always holds the most recent experience. Unless explicitly stated otherwise we consider hyper-parameter sweeps, some of which share experience via replay. In this setting multiple agents start from-scratch, run concurrently at identical speed, and add their new experience into a common replay buffer. All agents will then draw uniform samples from the replay buffer. On DMLab-30 we consider both regular hyper-parameter sweeps and sweeps with population based training (PBT) <ref type="bibr" target="#b15">(Jaderberg et al., 2017a)</ref>. On DMLab-30 sweeps contain 10 agents with hyper-parameters sampled similar as <ref type="bibr" target="#b7">Espeholt et al. (2018)</ref> but fixed RMSProp = 0.1. On Atari sweeps contain 9 agents with different constant learning rate and entropy cost combinations {3 · 10 −4 , 6 · 10 −4 , 1.2 · 10 −3 } × {5 · 10 −3 , 1 · 10 −2 , 2 · 10 −2 } (distributed by factors {1/2, 1, 2} around the initial parameters reported in <ref type="bibr" target="#b7">Espeholt et al. (2018)</ref>). Although our focus is on efficient hyper-parameter sweeps given crude initial parameters, we also present a single-agent LASER experiment using the same tuned schedule as <ref type="bibr" target="#b7">Espeholt et al. (2018)</ref>, a 87.5% replay ratio and a 15M replay. We store the entire episodes in the replay buffer and replay each episode from the beginning, using the most recent network parameters to recompute the LSTM states along the way: this is particularly critical when sharing experience between different agents, which may have arbitrarily different state representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">UNIFORM AND PRIORITIZED EXPERIENCE REPLAY</head><p>Prioritized experience replay has the potential to provide more efficient learning compared to uniform experience replay <ref type="bibr" target="#b35">(Schaul et al., 2015;</ref>. However, it also introduces a number of new hyper-parameters and design choices: the most critical are the priority metric, how strongly to bias the sampling distribution, and how to correct for the resulting bias. Uniform replay is instead almost parameter-free, requires little tuning and can be easily shared between multiple agents. Experiments provided in <ref type="figure" target="#fig_1">Figure 4</ref> in the appendix showed little benefit of actor critic prioritized replay on DMLab-30. Furthermore priorities are typically computed from the agent specific metrics such as the TD-error, which are ill-defined when replay is shared among multiple agents. Hence we used uniform replay for our further investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MIXING ON-AND OFF-POLICY EXPERIENCE AND REPLAY CAPACITY</head><p>Figure 2 (left) shows that performance degrades significantly when online data is not present in the batch. This experimentally validates Propositions 2 and 3 that highlight difficulties of learning purely off-policy. Furthermore <ref type="figure">Figure 2</ref> (right) shows that best results are obtained with experience replay of 10M capacity and 87.5% ratio. A ratio of 87.5% = 7/8 corresponds to 7 replay samples for each online sample. We have considered ratios of 1/2, 3/4, and 7/8 and observed stable training for all of them. Observe that among those values, larger ratios are more data-efficient as they take advantage of more replayed experience per training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">SHARED EXPERIENCE REPLAY WITH OFF-POLICY TRUST REGION V-TRACE</head><p>In line with proposition 2 we observe in <ref type="figure" target="#fig_2">Figure 3</ref> (left) that hyper-parameter sweeps without trustregion are even surpassed by the baseline without experience replay. State-of-the-art results are obtained in <ref type="figure" target="#fig_2">Figure 3</ref> (right) when experience is shared with trust-region in a PBT sweep.</p><p>Observe that this indicates parallel exploration benefits and saves memory at the same time: in our sweep of 10 replay agents the difference between 10 × 10M (separate replays) and 10M (shared replay) is 10-fold. This effect would be even more pronounced with larger sweeps.</p><p>As discussed in section 2.3, the bias in V-trace occurs due to the clipping of importance ratios. A potential solution of reducing the bias would be to increase theρ threshold to clip less aggressively and accept increased variance. <ref type="figure" target="#fig_1">Figure 4</ref> in the appendix shows that this is not a solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">EVALUATION ON ATARI</head><p>We apply our proposed agent to Atari which has been a long established suite to evaluate reinforcement algorithms <ref type="bibr" target="#b3">(Bellemare et al., 2013)</ref>. Since we focus on sample-efficient learning we present our results in comparison to prior work at 200M steps <ref type="figure">(Figure 1</ref>). Shared experience replay obtains even better performance than not shared experience replay. This confirms the efficient use of parallel exploration <ref type="bibr" target="#b20">(Kretchmar, 2002)</ref>. The fastest prior agent to reach 400% is presented by <ref type="bibr" target="#b19">Kapturowski et al. (2019)</ref> requiring more than 3,000M steps. LASER with shared replay achieves 423% in 60M per agent. Given 200M steps it achieves 448%. We also present a single (no sweep) LASER agent that achieves 431% in 200M steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have presented LASER -an off-policy actor-critic agent which employs a large and shared experience replay to achieve data-efficiency. By sharing experience between concurrently running experiments in a hyper-parameter sweep it is able to take advantage of parallel exploration. As a result it achieves state-of-the-art data efficiency on 57 Atari games given 200M environment steps. Furthermore it achieves competitive results on both DMLab-30 and Atari under regular, not shared experience replay conditions.</p><p>To facilitate this algorithm we have proposed two approaches: a) mixing replayed experience and on-policy data and b) a trust region scheme. We have shown theoretically and demonstrated through a series of experiments that they enable learning in strongly off-policy settings, which present a challenge for conventional importance sampling schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A ADDITIONAL EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 REDUCED CLIPPING IN V-TRACE DOES NOT ENABLE SHARED EXPERIENCE REPLAY</head><p>Increasing the clipping constantρ in V-trace reduces bias in favour of increased variance. We investigate if reducing bias in this manner enables sharing experience replay between multiple agents in a hyper-parameter sweep. <ref type="figure" target="#fig_1">Figure 4</ref> (left) shows that this is not a solution, thus motivating our trust region scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PRIORITIZED AND UNIFORM EXPERIENCE REPLAY, LSTM STATES</head><p>With prioritized experience replay each transition τ is sampled with probability P (τ ) ∝ p α τ , for a suitable unnormalized priority score p τ and a global tunable parameter α. It is common <ref type="bibr" target="#b35">(Schaul et al., 2015;</ref><ref type="bibr" target="#b10">Hessel et al., 2017)</ref> to then weight updates computed from that sample by 1/P (τ ) β for 0 &lt; β ≤ 1, where β = 1 fully corrects for the bias introduced in the state distribution. In one step temporal difference methods, typical priorities are based on the immediate TD-error, and are typically recomputed after a transition is sampled from replay. This means low priorities might stay low and get stale -even if the transition suddenly becomes relevant. To alleviate this issue, the sampling distribution is mixed with a uniform, as controlled by a third hyper parameter .</p><p>The performance of agents with prioritized experience replay can be quite sensitive to the hyperparameters α, β, and .</p><p>A critical practical consideration is how to implement random access for recurrent memory agents such as agents using an LSTM. Prioritized agents sample a presumably interesting transition from the past. This transition may be at any position within the episode. To infer the correct recurrent memory-state at this environment-state all earlier environment-states within that episode would need to be replayed. A prioritized agent with a random access pattern would thus require costly LSTM refreshes for each sampled transition. If LSTM states are not recomputed representational missmatch <ref type="bibr" target="#b19">(Kapturowski et al., 2019)</ref> occurs. In fact sharing experience replay in this particular way is worse than pure online learning. This motivates the use of our proposed trust region scheme. On a side note, increased clipping thresholds resulting in worse performance verifies the importance of variance reduction through clipping. Right: Median human normalized performance across 30 tasks for the best agent in a sweep, averaged across 2 replicas. All replay experiments use 50% replay ratio and a capacity of 3 million observations. We investigate if uncorrected LSTM states can be used in combination with different replay modes. We consider uniform sampling and prioritization via the critic's loss, and include both full (β = 1) and partial (β = 0.5) importance corrections Sharing experience between multiple agents amplifies the issue of LSTM state representation missmatch. Here each agent has its own network parameters and the state representations between agents may be arbitrarily different.</p><p>As a mitigation <ref type="bibr" target="#b19">Kapturowski et al. (2019)</ref> use a burn-in window or to initialize with a constant starting state. We note that those solutions can only partially mitigate the fundamental issue and that counter examples such as arbitrarily long T-Mazes <ref type="bibr" target="#b44">(Tolman, 1948;</ref><ref type="bibr" target="#b31">Olton, 1979)</ref> can be constructed easily.</p><p>We thus advocate for uniform sampling. In our implementation we uniformly sample an episode. Then we replay each episode from the beginning, using the most recent network parameters to recompute the LSTM states along the way: this is particularly critical when sharing experience between different agents, which may have arbitrarily different state representations. This solution is exact and cost-efficient as it only requires one additional forward pass for each learning step (forward + backward pass).</p><p>An even more cost efficient approach would be to not refresh LSTM states at all. Naturally this comes at the cost of representational missmatch. However it would allow for an affordable implementation of prioritized experience replay. We investigate this in <ref type="figure" target="#fig_1">Figure 4</ref> (right) and observe that it is not viable. We compare a baseline V-trace agent with no experience replay, one with uniform experience replay, and two different prioritized replay agents. We do not refresh LSTM states for any of the agents.</p><p>The uniform replay agent is more data efficient then the baseline, and also saturates at a higher level of performance. The best prioritized replay agent uses full importance sampling corrections (β = 1). However it performs no higher than with uniform replay. We therefore we used uniform replay with full state correction for all our investigations in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 EVALUATION PROTOCOL</head><p>For evaluation, we average episode returns within buckets of 1M (Atari) and 10M (DMLab) environment steps for each agent instance, and normalize scores on each game by using the scores of a human expert and a random agent <ref type="bibr" target="#b45">(van Hasselt et al., 2016)</ref>. In the multi-task setting, we then define the performance of each agent as the median normalized score of all levels that the agent trains on. Given the use of population based training, we need to perform the comparisons between algorithms at the level of sweeps. We do so by selecting the best performing agent instance within each sweep at any time. Note that for the multi-task setting, our approach of first averaging across many episodes, then taking the median across games, on DMLab further downsampling to 100M env steps, and only finally selecting the maximum within the sweep, results in substantially lower variance than if we were to compute the maximum before the median and smoothing.</p><p>All DMLab-30 sweeps are repeated 3× with the exception of ρ = 2 and ρ = 4 in <ref type="figure" target="#fig_1">Figure 4</ref>. We then plot a shaded area between the point-wise best and worst replica and a solid line for the mean. Atari sweeps having 57 games are summarized and plotted by the median of the human-normalized scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B ALGORITHM PSEUDOCODE</head><p>We present algorithm pseudocode for LASER with trust region (Algorithm 1). For clarity we present a version without LSTM and focus on the single agent case. The multi-agent case is a simple extension where all agents save to the same replay database and also sample from the same replay. Also each agent starts with different network parameters and hyper-parameters. The LSTM state recomputation can be achieved with Replayer Threads (nearly identical to Actor Threads) that sample entire epsiodes from replay, step through them while reevaluating the LSTM state and slice the experience into trajectories of length T . Similar to regular LSTM Actor Threads from <ref type="bibr" target="#b7">Espeholt et al. (2018)</ref> the Replayer Threads send each trajectory together with an LSTM state to the learning thread via a queue. The Learner Thread initializes the LSTM with the transmitted state when the LSTM is unrolled over the trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Single Agent LASER with Trust Region</head><p>Initialize parameter vectors θ. Initialize π 1 = π θ . Actor Thread: while training is ongoing do Sample trajectory unroll u = {τ t } t∈{1,...,T } of length T by acting in the environment using the latest π k where τ t = (s t , a t , r t , µ t = π k (s t |·)). </p><formula xml:id="formula_16">M with M b,t = KL(π k (s b,t |·)||µ b,t ) &lt; b where µ b,t , s b,t are obtained from U b,t . Compute trust-region V-trace return V t,b using 8 where λ π,µ (s b,t ) = M b,t . Let [L V (θ)] t,b = 1 2 (V t,b − V θ (s t,b )) 2 . Let A t,b = V t,b − V θ (s t,b ) and [L P (θ)] t,b = ρ t,b log[π θ (s t,b |a t,b )]A t,b , where ρ is the clipped v-trace importance sampling ratio. Perform gradient update to θ using ∇ θ t,b [L V (θ) + L P (θ)] t,b M t,b</formula><p>, denote the resulting π θ as π k+1 . end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C PROPOSITIONS</head><p>We have stated five propositions in our paper for which we provide proofs below.</p><p>Proposition 1. The V-trace value estimate Vπ is biased: It does not match the expected return of π but the return of a related implied policyπ defined by equation 9 that depends on the behaviour policy µ:π µ (a|x) = min [ρµ(a|x), π(a|x)] b∈A min [ρµ(b|x), π(b|x)]</p><p>Proof. See <ref type="bibr" target="#b7">Espeholt et al. (2018)</ref>.</p><p>Proposition 2. The V-trace policy gradient is biased: given the the optimal value function V * the V-trace policy gradient does not converge to a locally optimal π * for all off-policy behaviour distributions µ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Proof by contradiction:</head><p>Consider a tabular counter example with a single (locally) optimal policy at s t given by π * (s t ) = argmax π a∈A π(a|s t )Q * (a, s t ) that always selects the action argmax a Q * (a, s t ). Even in this ideal tabular setting V-trace policy gradient estimates a differentπ * rather than the optimal π * as follows</p><formula xml:id="formula_18">∇V * ,π (s t ) = E µ [ρ t (r t + γV * (s t+1 )∇ log π(a t |s t )] = E µ [ρ t Q * (s t , a t )∇ log π(a t |s t )] = E µ min π(a t |s t ) µ(a t |s t ) ,ρ Q * (s t , a t )∇ log π(a t |s t ) = E µ π(a t |s t ) µ(a t |s t ) min 1,ρ µ(a t |s t ) π(a t |s t ) Q * (s t , a t )∇ log π(a t |s t ) = E π min 1,ρ µ(a t |s t ) π(a t |s t ) Q * (s t , a t )∇ log π(a t |s t ) = E π [ω(s t , a t )Q * (s t , a t )∇ log π(a t |s t )] = E π [Q * ,ω (s t , a t )∇ log π(a t |s t )]<label>(10)</label></formula><p>Observe how the optimal Q-function Q * is scaled by ω(s t , a t ) = min 1,ρ µ(at|st) π(at|st) ≤ 1 resulting in implied state-action values Q * ,ω . This penalizes actions where µ(a t |s t )ρ &lt; π(a t |s t ) and makes V-trace greedy w.r.t. to the remaining ones. Thus µ can be chosen adversarially to corrupt the optimal state action value. Note thatρ is a constant typically chosen to be 1.</p><p>To prove the lemma consider a counter example such as an MDP with two actions and Q * = (2, 5) and µ = (0.9, 0.1) and initial π = (0.5, 0.5). Here the second action with expected return 5 is clearly favourable. Abusing notation µ/π = (1.8, 0.2). Thus Qπ ,ω = (2 * 1, 5 * 0.2) = (2, 1). Thereforẽ π * = (1, 0) wrongly selects the first action.</p><p>Proposition 3. Mixing on-policy data into the V-trace policy gradient with the ratio α reduces the bias by providing a regularization to the implied state-action values. In the general function approximation case it changes the off-policy V-trace policy gradient from s d µ (s)E π [(Q(s, a)∇ log π(a|s)] to <ref type="figure">α)</ref> is a regularized stateaction estimate and d π , d µ are the state distributions for π and µ. Note that there exists α ≤ 1 such that Q α has the same argmax (i.e. best action) as Q.</p><formula xml:id="formula_19">s E π [Q α (s, a)∇ log π(a|s)] where Q α = Qd π (s)α + Q ω d µ (s)(1 −</formula><p>Proof. Note that the on-policy policy gradient is given by</p><formula xml:id="formula_20">∇J on (π) = s d π (s)E π [Q(s, a)∇ log π(a|s)]</formula><p>Similarly the off-policy V-trace gradient is given by</p><formula xml:id="formula_21">∇J off (π) = s d µ (s)E π [ω(s, a)Q(s, a)∇ log π(a|s)]</formula><p>with the V-trace distortion factor ω(s t , a t ) = min 1,ρ µ(at|st) π(at|st) ≤ 1 that can de-emphasize action values and Q ω (s, a) = ω(s, a)Q(s, a).</p><p>The α-interpolation of both gradients can be transformed as follows: </p><formula xml:id="formula_22">∇ αJ on + (1 − α)J off (π) = α s d π (s)E π [Q(s, a)∇ log π(a|s)] + (1 − α) s d µ (s)E π [ω(s, a)Q(s, a)∇ log π(a|s)] = s d π (s)E π [Q(s, a)α∇ log π(a|s)] + s d µ (s)E π [Q ω (s, a)(1 − α)∇ log π(a|s)] = s E π [Q(s, a)d π (s)α∇ log π(a|s)] + s E π [Q ω (s, a)d µ (s)(1 − α)∇ log π(a|s)] = s E π [Q(s, a)d π (s)α∇ log π(a|s) + Q ω (s, a)d µ (s)(1 − α)∇ log π(a|s)] = s E π Q(s, a)d π (s)α + Q ω (s, a)d µ (s)(1 − α) ∇ log π(a|s) = s E π [Q α (s, a)∇ log π(a|s)] (11) for Q α (s, a) = Q(s, a)d π (s)α + Q ω (s, a)d µ (s)(1 − α)</formula><p>Note that α 1−α → ∞ as α → 1. Mixing-in more online data thus increases the left hand side. Also note that the right hand side decreases due to d µ (s)/d π (s) if π visits the state s more often than µ. Furthermore the larger the action value gap in the real Q-function Q(s, a * ) − Q(s, b) the lower the right hand side. Finally the denominator will be negative if max b ∈A * [Q ω (s, b)] &lt; Q ω (s, a * ) thus enabling correct learning even in the pure off-policy case with α = 0.</p><p>Note that all of those conditions can be computed and checked if an accurate Q-function and state distribution is accessible. How to use imperfect Q-function estimates to adaptively choose such an α remain a question for future research.</p><p>Proposition 4. Let G π,µz IS be a set of importance sampling estimators as defined in equation 7. Note that they all have the same fix point V π and contract with at least γ. Then the contraction properties carry over to V π trusted . In particular |V π trusted − V π | ∞ ≤ γ |V − V π | ∞ .</p><p>Proof. Let us consider the set of importance sampling estimators as defined in 7 and note that they all contract to the same fixed point V π with at least E µz|z [G π,µz IS (s)] − V π (s) ∞ ≤ γ |V (s) − V π (s)| ∞ for any state s.</p><p>By Minkowski's inequality the contraction properties of importance sampled Monte-Carlo bootstraps carry over to V π trusted which is a p(z|µ z ∈ M β,π (s t )) weighted average:</p><formula xml:id="formula_24">|V π trusted (s) − V π (s)| ∞ = E z E µz|z [G π,µz IS (s)] µ z ∈ M β,π (s t ) − V π (s) ∞ = E z E µz|z [G π,µz IS (s)] − V π (s) µ z ∈ M β,π (s t ) ∞ ≤ E z E µz|z [G π,µz IS (s)] − V π (s t ) ∞ µ z ∈ M β,π (s t ) &lt; E z γ |V (s) − V π (s)| ∞ µ z ∈ M β,π (s t ) = γ |V (s) − V π (s)| ∞<label>(14)</label></formula><p>Proposition 5. Let G π,µz Vtrace be a set of V-trace estimators (see equation 8) with corresponding fixed points V z (see equation 3) to which they contract at a speed of an algorithm and behaviour specific η z . Then V π trusted moves towards V β = E z|µz∈M β,π (st) [V z ] shrinking the distance as follows V π trusted − V β ∞ &lt; max µz∈M β,π (st) |η z (V − V z )| ∞ ≤ η max max µz∈M β,π (st) |(V − V z )| ∞ with η max = max µz∈M β,π (st) η z .</p><p>Proof. Recall the contraction properties of a V-trace importance sampled Monte-Carlo bootstraps G π,µz</p><p>Vtrace being E µz|z [G π,µz Vtrace (s)] − V z (s) ∞ &lt; η z |V (s) − V z (s)| ∞ for an algorithm and behaviour specific η z &lt; 1 for a z dependent fixed point V z and for any bootstrap V . We then show that V π trusted moves towards the weighted average of fixed points</p><formula xml:id="formula_25">V β = E z|µz∈M β,π (st) [V z ], since V π trusted (s) − V β (s) ∞ &lt; η max max µz∈M β,π (st) |V (s) − V z (s)| ∞</formula><p>holds for any bootstrap function V as we show below.</p><formula xml:id="formula_26">V π trusted (s) − V β (s) ∞ = E z E µz|z [G π,µz Vtrace (s)] − V z (s) µ z ∈ M β,π (s t ) ∞ ≤ E z E µz|z [G π,µz Vtrace (s)] − V z (s) ∞ µ z ∈ M β,π (s t ) &lt; E z |η z (V (s) − V z (s))| ∞ µ z ∈ M β,π (s t ) ≤ max µz∈M β,π (st) |η z (V (s) − V z (s))| ∞ ≤ η max max µz∈M β,π (st) |V (s) − V z (s)| ∞<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D DETAILED ATARI RESULTS</head><p>We display the Atari per-level performance of various agents at 50M and 200M environment steps in <ref type="table">Table 2</ref>. The scores correspond to the agents presented in <ref type="figure">Figure 1</ref>. The LASER scores are computed by averaging the last 100 episode returns before 50M or respectively 200M environment frames have been experienced. Following the procedure defined by  we initialize the environment with a random number of no-op actions (up to 37 in our case). Again following  episodes are terminated after 30 minutes of gameplay. Note that <ref type="bibr" target="#b48">Xu et al. (2018)</ref> have not published per-level scores. Rainbow scores are obtained from <ref type="bibr" target="#b10">Hessel et al. (2017)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>∇E π(a|s) [Q ω (s, a)] = ∇E π(a|s) [Q(s, a)]. In equation 10 we show that Q ω (s, a) = Q(s, a)ω(s, a) where ω(s, a) = min 1,ρ µ(a|s) π(a|s) ≤ 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Left: Increasing the V-trace clipping constantρ does not enable shared experience replay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Interpretation of Proposition 3</head><label>3</label><figDesc>As discussed in section 3 the V-trace policy gradient will have the correct local fixpoint at state s if the argmax of the state-value function is preserved despite the distortion: i.e. if argmax a [Q(s, a)] = argmax a [Q ω (s, a)]. Respectively when mixing in an α ∈ [0, 1) share of online data the fixpoint will be preserved if argmax a [Q(s, a)] = argmax a [Q α (s, a)](12)Let a * = argmax b (Q, b) be any best action and A * be set of best actions. Then equation 12 is equivalent to:Q α (s, a * ) &gt; Q α (s, b) ∀b ∈ A *Using the definition of Q α this can be rewritten as:Q(s, a * )d π (s)α + Q ω (s, a * )d µ (s)(1 − α) &gt; Q(s, b)d π (s)α + Q ω (s, b)d µ (s)(1 − α) ∀b ∈ A *Which can be rearranged to:[Q(s, a * )d π (s) − Q(s, b)d π (s)]α &gt; [Q ω (s, b)d µ (s) − Q ω (s, a * )d µ (s)](1 − α) ∀b ∈ A * By definition Q(s, a * )d π (s) − Q(s, b)d π (s) &gt; 0 ∀b ∈ A * , hence: α 1 − α &gt; Q ω (s, b) − Q ω (s, a * ) Q(s, a * ) − Q(s, b) d µ (s) d π (s) ∀b ∈ A *It follows that the policy gradient will have the same local fixpoint ifα 1 − α &gt; max b ∈A * Q ω (s, b) − Q ω (s, a * ) Q(s, a * ) − Q(s, b) d µ (s) d π (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of state-of-the-art agents on 57 Atari games trained up until 200M environment steps (per game) and DMLab-30 trained until 10B steps (multi-task; all games combined). The first two rows are quoted from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and the last two rows are our proposed LASER (LArge Scale Experience Replay) agent. All agents use hyper-parameter sweeps expect for the marked.</figDesc><table><row><cell></cell><cell>Atari Median</cell><cell cols="2">DMLab-30 Median DMLab-30 Mean-Capped</cell></row><row><cell>IMPALA Meta-Gradient (no sweep)</cell><cell>287.6% at 200M</cell><cell>-</cell><cell>-</cell></row><row><cell>PopArt-IMPALA</cell><cell>-</cell><cell>-</cell><cell>73.5%</cell></row><row><cell>PopArt-IMPALA+PixelControl</cell><cell>-</cell><cell>85.5%</cell><cell>77.6%</cell></row><row><cell>LASER: Experience Replay (no sweep)</cell><cell>431% at 200M</cell><cell></cell><cell></cell></row><row><cell>LASER: Experience Replay</cell><cell>(233% at 50M)</cell><cell>95.4%</cell><cell>79.6%</cell></row><row><cell>LASER: Shared Experience Replay</cell><cell>(370% at 50M), 448% at 200M</cell><cell>97.2%</cell><cell>81.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Proposition 1. The V-trace value estimate Vπ is biased: It does not match the expected return of π but the return of a related implied policyπ defined by equation 3 that depends on the behaviour policy µ:π</figDesc><table><row><cell>µ (a|x) =</cell><cell>min [ρµ(a|x), π(a|x)] b∈A min [ρµ(b|x), π(b|x)]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Enqueue u into Lerner Queue, wait if full. Add u into Replay Database. Batch size B, online fraction α. for training iteration k do Form training batch U = {u b } b∈{1,...,B} of B trajectories of length T , by dequeuing Bα trajectories from Lerner Queue and sampling B(1 − α) trajectories from Replay Database.Evaluate the target policy π k on the sampled transitions in U : i.e. π k (s b,t |·).</figDesc><table><row><cell>Remove oldest trajectory if database has reached desired capacity limit.</cell></row><row><cell>end while</cell></row><row><cell>Learner Thread:</cell></row><row><cell>Given: Compute behaviour relevance mask</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank Aidan Clark, Will Dabney, Remi Munos, Jeff Stanway, Hubert Soyer, Frank Perbet, Zhe Wang and Andrei Kashin.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nando de Freitas, and Misha Denil. Learning awareness models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Rothörl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Muldal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The option-critic architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Valdés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg</pubPlace>
		</imprint>
	</monogr>
	<note>and Stig Petersen. Deepmind lab. Arxiv, abs/1612.03801</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Arcade Learning Environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A markovian decision process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Mechanics</title>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Conn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">I M</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trust-Region Methods. SIAM</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Off-policy actor-critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymir</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrunas</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<idno>abs/1710.02298</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task deep reinforcement learning with popart</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning from demonstrations for real world reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Vecerik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sendonaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Dulac-Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrunas</forename><surname>Gruslys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Population based training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1711.09846</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Use of different monte carlo sampling techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Kahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
			<pubPlace>Santa Monica, Calif.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-based reinforcement learning for atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Milos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blazej</forename><surname>Osinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Czechowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Kozakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<idno>abs/1903.00374</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent experience replay in distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kretchmar</surname></persName>
		</author>
		<title level="m">Parallel reinforcement learning. SCI2002. The 6th World Conference on Systemics, Cybernetics, and Informatics</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Playing FPS games with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reinforcement Learning for Robots Using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long-Ji</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>UMI Order No. GAX93-22750</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unicorn: Continual learning with a universal, off-policy agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustin</forename><surname>Zídek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schaul</surname></persName>
		</author>
		<idno>abs/1802.08294</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>abs/1312.5602</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><forename type="middle">Puigdomènech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prioritized sweeping: Reinforcement learning with less data and less real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1993-10" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Safe and efficient off-policy reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Bellemare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Massively parallel methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagdas</forename><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rory</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">De</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedavyas</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>abs/1507.04296</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mazes, maps, and memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Olton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem. Arxiv, abs/1211</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5063</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural fitted q iteration -first experiences with a data efficient neural reinforcement learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Ray interference: a source of plateaus in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Borsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0004-3702(99)00052-1</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">M</forename><surname>Pilarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2011)</title>
		<meeting>of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2011)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A new q(λ) with interim forward view and Monte Carlo equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashique</forename><surname>Rupam Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal difference learning and td-gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<idno type="DOI">10.1145/203330.203343</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="1995-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cognitive maps in rats and men. The Psychological Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double Qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Meta-gradient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hado P Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A deeper look at experience replay. Arxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangtong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<idno>abs/1712.01275</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
