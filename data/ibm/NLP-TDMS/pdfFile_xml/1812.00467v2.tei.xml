<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&quot;Double-DIP&quot; : Unsupervised Image Decomposition via Coupled Deep-Image-Priors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Gandelsman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">&quot;Double-DIP&quot; : Unsupervised Image Decomposition via Coupled Deep-Image-Priors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: A unified framework for image decomposition. An image can be viewed as a mixture of "simpler" layers. Decomposing an image into such layers provides a unified framework for many seemingly unrelated vision tasks (e.g., segmentation, dehazing, transparency separation). Such a decomposition can be achieved using "Double-DIP".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Many seemingly unrelated computer vision tasks can be viewed as a special case of image decomposition into separate layers. For example, image segmentation (separation into foreground and background layers); transparent layer separation (into reflection and transmission layers); Image dehazing (separation into a clear image and a haze map), and more. In this paper we propose a unified framework for unsupervised layer decomposition of a single image, based on coupled "Deep-image-Prior" (DIP) networks. It was shown [38] that the structure of a single DIP generator network is sufficient to capture the low-level statistics of a single image. We show that coupling multiple such DIPs provides a powerful tool for decomposing images into their basic components, for a wide variety of applications. This capability stems from the fact that the internal statistics of a mixture of layers is more complex than the statistics of each of its individual components. We show the power of this approach for Image-Dehazing, Fg/Bg Segmentation, Watermark-Removal, Transparency Separation in images and video, and more. These capabilities are achieved in a totally unsupervised way, with no training examples other than the input image/video itself. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Various computer vision tasks aim to decompose an image into its individual components. In image/video segmen- tation, the task is to decompose the image into meaningful sub-regions, such as foreground and background <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b31">31]</ref>. In transparency separation, the task is to separate the image into its superimposed reflection and transmission <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b14">14]</ref>. Such transparency can be a result of accidental physical reflections, or due to intentional transparent overlays (e.g., watermarks). In image dehazing <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b8">8]</ref>, the goal is to separate a hazy/foggy image into its underlying haze-free image and the obscuring haze/fog layers (airlight and transmission map). <ref type="figure">Fig. 1</ref> shows how all these very different tasks can be casted into a single unified framework of layer-decomposition. What is common to all these decompositions is the fact that the distribution of small patches within each separate layer is "simpler" (more uniform) than in the original mixed image, resulting in strong internal self-similarity.</p><p>Small image patches (e.g., 5x5, 7x7) have been shown to repeat abundantly inside a single natural image <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b41">41]</ref>. This strong internal patch recurrence was exploited for solving a large variety of computer vision tasks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b11">11]</ref>. It was also shown that the empirical entropy of patches inside a single image is much smaller than the entropy in a collection of images <ref type="bibr" target="#b41">[41]</ref>. It was further observed by <ref type="bibr" target="#b4">[5]</ref> that the empirical entropy of small image regions composing a segment, is smaller than the empirical cross-entropy of regions across different segments within the same image. This observation has been successfully used for unsupervised image segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">17]</ref>. Finally, it was observed <ref type="bibr" target="#b6">[6]</ref> that the distribution of patches in a hazy image tends to be more diverse (weaker internal patch similarity) than in its underlying haze-free image. This observation was exploited by <ref type="bibr" target="#b6">[6]</ref> for blind image dehazing.</p><p>In this paper we combine the power of the internal patch recurrence (its strength in solving unsupervised tasks), with the power of Deep-Learning. We propose an unsupervised Deep framework for decomposing a single image into its layers, such that the distribution of "image elements" within each layer is "simple". We build on top of the "Deep Image Prior" (DIP) work of Ulyanov et al. <ref type="bibr" target="#b38">[38]</ref>. They showed that the structure of a single DIP generator network is sufficient to capture the low-level statistics of a single natural image. The input to the DIP network is random noise, and it trains to reconstruct a single image (which serves as its sole output training example). This network was shown to be quite powerful for solving inverse problems like denoising, super-resolution and inpainting, in an unsupervised way.</p><p>We observe that when employing a combination of multiple DIPs to reconstruct an image, those DIPs tend to "split" the image, such that the patch distribution of each DIP output is "simple". Our approach for unsupervised multi-task layer decomposition is thus based on a combination of multiple (two or more) DIPs which we coin "Double-DIP". We demonstrate the applicability of this approach to a wide range of computer vision tasks, including Image-Dehazing, Fg/Bg Segmentation of images and videos, Watermark Removal, and Transparency Separation in images and videos.</p><p>Double-DIP is general-purpose and caters many different applications. Special-purpose methods designed for one specific task may outperform Double-DIP on their own challenge. However, to the best of our knowledge, this is the first framework that is able to handle well such a large variety of image-decomposition tasks. Moreover, in some tasks (e.g., image dehazing), Double-DIP achieves comparable and even better results than leading methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of the Approach</head><p>Observe the illustrative example in <ref type="figure" target="#fig_1">Fig. 3a</ref>. Two different textures, X and Y , are mixed to form a more complex image Z which exhibits layer transparency. The distribution of small patches and colors inside each pure texture is simpler than the distribution of patches and colors in the combined image. Moreover, the similarity of patches across the two textures is very weak. It is well known <ref type="bibr" target="#b12">[12]</ref> that if X and Y are two independent random variables, the entropy of their sum Z = X + Y is larger than their individual entropies: max{H(X), H(Y )} ≤ H(Z). We leverage this fact to separate the image into its natural "simpler" components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single DIP vs. Coupled DIPs</head><p>Let's see what happens when a DIP network is used to learn pure images versus mixed images. The graph in <ref type="figure" target="#fig_1">Fig. 3</ref>.c shows the MSE Reconstruction Loss of a single DIP network, as a function of time (training iterations), for each of the 3 images in <ref type="figure" target="#fig_1">Fig. 3</ref>.a: (i) the orange plot is the loss of a DIP trained to reconstruct the texture image X, (ii) the blue plot -a DIP trained to reconstruct the texture Y, and (iii) the green plot -a DIP trained to reconstruct their superimposed mixture (image transparency). Note the larger loss and longer convergence time of the mixed image, compared to the loss of its individual components. In fact, the loss of the mixed image is larger than the sum of the two individual losses. We attribute this behavior to the fact that the distribution of patches in the mixed image is more complex and diverse (larger entropy; smaller internal self-similarity) than in any of its individual components.</p><p>While these are pure textures, the same behavior holds also for mixtures of natural images. The internal selfsimilarity of patches inside a single natural image tends to be much stronger than the patch similarity across different images <ref type="bibr" target="#b41">[41]</ref>. We repeated the above experiment for a large collection of natural images: We randomly sampled 100 pairs of images from the BSD100 dataset <ref type="bibr" target="#b27">[27]</ref>, and mixed each pair. For each image pair we trained a DIP to learn the mixed image and each of the individual images. The same behavior exhibited in the graph of <ref type="figure" target="#fig_1">Fig. 3</ref>.c repeated also in the case of natural images -interestingly, with an even larger gap between the loss of the mixed image and its individual components (see graph in the project website).</p><p>We performed a similar experiment for non-overlapping image segments. It was observed <ref type="bibr" target="#b4">[5]</ref> that the empirical entropy of small regions composing an image segment is smaller than their empirical cross-entropy across different segments in the same image. We randomly sampled 100 pairs of images from the BSD100 dataset. For each pair we generated a new image, whose left side is the left side of one image, and whose right side is the right side of the second image. We trained a DIP to learn the mixed image and each of the individual components. The graph behavior of <ref type="figure" target="#fig_1">Fig.3</ref>.c repeated also in this case (see project website).</p><p>We further observe that when multiple DIPs train to jointly reconstruct a single input image, they tend to "split" the image patches among themselves. Namely, similar small patches inside the image tend to all be generated by a single DIP network. In other words, each DIP captures different components of the internal statistics of the image. We explain this behavior by the fact that a single DIP network is fully convolutional, hence its filter weights are shared across the entire spatial extent of the image. This promotes selfsimilarity of patches in the output of each DIP.</p><p>The simplicity of the patch distribution in the output of a single DIP is further supported by the denoising experiments reported in <ref type="bibr" target="#b38">[38]</ref>. When a DIP was trained to reconstruct a noisy image (high patch diversity/entropy), it was shown to generate along the way an intermediate clean version of the image, before overfitting the noise. The clean image has higher internal patch similarity (smaller patch diversity/entropy), hence is simpler for the DIP to reconstruct.</p><p>Building on these observations, we propose to decompose an image into its layers by combining multiple (two or more) DIPs, which we call "Double-DIP". Figs. 3.a,b show that when training 2 DIP networks to jointly recover the mixed texture transparency image (as the sum of their outputs), each DIP outputs a coherent layer on its own.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Unified Multi-Task Decomposition Architecture</head><p>What is a good image decomposition? There are infinitely many possible decompositions of an image into layers. However, we suggest that a meaningful decomposition satisfies the following criteria: These criteria form the basis of our general-purpose Double-DIP architecture, illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The first criterion is enforced via a "Reconstruction Loss", which measures the error between the constructed image and the input image (see <ref type="figure" target="#fig_0">Fig. 2</ref>). The second criterion is obtained by employing multiple DIPs (one per layer). The third criterion is enforced by an "Exclusion Loss" between the outputs of the different DIPs (minimizing their correlation).</p><p>Each DIP network (DIP i ) reconstructs a different layer y i of the input image I. The input to each DIP i is randomly sampled uniform noise, z i . The DIP outputs, y i =DIP i (z i ), are mixed using a weight mask m, to form a reconstructed imageÎ = m · y 1 + (1 − m) · y 2 , which should be as close as possible to the input image I.</p><p>In some tasks the weight mask m is simple and known, in other cases it needs to be learned (using an additional DIP). The learned mask m may be uniform or spatially varying, continuous or binary. These constraints on m are taskdependant, and are enforced using a task-specific "Regularization Loss". The optimization loss is therefore: <ref type="formula">(1)</ref> where Loss Reconst = I −Î , and Loss Excl (the Exclusion loss) minimizes the correlation between the gradients of y 1 and y 2 (as defined in <ref type="bibr" target="#b40">[40]</ref>). Loss Reg is a task-specific mask regularization (e.g., in the segmentation task the mask m has to be as close as possible to a binary image, while in the dehazing task the t-map is continuous and smooth). We further apply guided filtering <ref type="bibr" target="#b22">[22]</ref> on the learned mask m to obtain a refined mask.</p><formula xml:id="formula_0">Loss = Loss Reconst + α · Loss Excl + β · Loss Reg</formula><p>Optimization: The architecture of the individual DIPs is similar to that used in <ref type="bibr" target="#b38">[38]</ref>. As in the basic DIP, we found that adding extra non-constant noise perturbations to the input noise adds stability in the reconstruction. We gradually increase noise perturbations with iterations. We further enrich the training set by transforming the input image I and the corresponding random noise inputs of all the DIPs using 8 transformations (4 rotations by 90 • combined with 2 mirror reflections -vertical and horizontal). Such an augmentation was also found to be useful in the unsupervised internal learning of <ref type="bibr" target="#b35">[35]</ref>. The optimization process is done using ADAM optimizer <ref type="bibr" target="#b25">[25]</ref>, and takes a few minutes per image on Tesla V100 GPU. Inherent Layer Ambiguities: Separating a superposition of 2 pure uncorrelated textures is relatively simple (see <ref type="figure" target="#fig_1">Fig. 3</ref>.a). There are no real ambiguities other than a constant global color ambiguity c : I = (y 1 + c) + (y 2 − c). Similarly, pure non-overlapping textures are relatively easy to segment. However, when a single layer contains multiple independent regions, as in <ref type="figure" target="#fig_1">Fig. 3</ref>.b, the separation becomes (Please see many more results in the project website) ambiguous (note the switched textures in the recovered output layers of <ref type="figure" target="#fig_1">Fig. 3.b)</ref>. Unfortunately, such ambiguities exist in almost any natural indoor/outdoor image.</p><p>To overcome this problem, initial "hints" are often required to guide the Double-DIP. These hints are provided automatically in the form of very crude image saliency <ref type="bibr" target="#b20">[20]</ref>. Namely, in the first few iterations, DIP 1 is encouraged to train more on the salient image regions, whereas DIP 2 is guided to train more on the non-salient image regions. This guidance is relaxed after a few iterations.</p><p>When more than one image is available, this ambiguity is often resolved on its own, without requiring any initial hints. For example, in video transparency, the superposition of 2 video layers changes from frame to frame, resulting in different mixtures. The statistics of each layer, however, remains the same throughout the video (despite its dynamics) <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b33">33]</ref>. This means that a single DIP suffices to represent all the frames of a single video layer. Hence, Double-DIP can be used to separate video sequences into 2 dynamic layers, and can often do so with no initial hints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Segmentation</head><p>Fg/Bg segmentation can be viewed as decomposing an image I into a foreground layer y 1 and background layer y 2 , combined by a binary mask m(x) at every pixel x:</p><formula xml:id="formula_1">I(x) = m(x)y 1 (x) + (1 − m(x))y 2 (x)<label>(2)</label></formula><p>This formulation naturally fits our framework, subject to y 1 and y 2 complying to natural image priors and each being 'simpler' to generate than I. This requirement is verified by [5] that defines a 'good image segment' as one which can be easily composed using its own pieces, but is difficult to compose using pieces from other parts of the image. The Zebra image in the top row of <ref type="figure">Fig. 1</ref> demonstrates the decomposition specified in Eq. 2. It is apparent that the layers y 1 and y 2 , generated by DIP 1 and DIP 2 , each complies with the definition of <ref type="bibr" target="#b4">[5]</ref>, thus allowing to obtain also a good segmentation mask m. Note that DIP 1 and DIP 2 automatically filled-in the 'missing' image parts in each of their output layers.</p><p>In order to encourage the learned segmentation mask m(x) to be binary, we use the following regularization loss:</p><formula xml:id="formula_2">Loss Reg (m) = ( x |m(x) − 0.5|) −1<label>(3)</label></formula><p>While Double-DIP does not capture any semantics, it is able to obtain high quality segmentation based solely on unsupervised layer separation, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Please see many more results in the project website. Other approaches to segmentation, such as semantic-segmentation (eg., <ref type="bibr" target="#b21">[21]</ref>) may outperform Double-DIP, but these are supervised and trained on many labeled examples.</p><p>Video segmentation: The same approach can be used for Fg/Bg video segmentation, by exploiting the fact that sequential video frames share internal patch statistics <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b33">33]</ref>. Video segmentation is cast as 2-layer separation as follows: <ref type="formula">(4)</ref> where i is the frame number. <ref type="figure" target="#fig_4">Fig. 5</ref> depicts how a single DIP is shared by all frames of a separated video layer:  are generated by DIP 2 , m (1) , ..., m (n) are all generated the mask DIP. The similarity across frames in each separated video-layer strengthens the tendency of a single DIP to generate a consistently segmented sequence. <ref type="figure" target="#fig_5">Fig. 6</ref>.b shows example frames from 2 different segmented videos (full videos can be found in the project website).</p><formula xml:id="formula_3">I (i) (x) = m (i) (x)y (i) 1 (x) + (1 − m (i) (x))y (i) 2 (x) ∀i</formula><p>We implicitly enforce temporal consistency in the segmentation mask, by imposing temporal consistency on the random noise inputted to the mask DIP in successive frames:</p><formula xml:id="formula_4">z m (i+1) (x) = z m (i) (x) + ∆z (i+1) m (x)<label>(5)</label></formula><p>where z (i) m is the noise input at frame i to the DIP that generates the mask. These noises change gradually from frame to frame by ∆z </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Transparent Layers Separation</head><p>In the case of image reflection, each pixel value in image I(x) is a convex combination of a pixel from the transmission layer y 1 (x) and the corresponding pixel in the reflection layer y 2 (x). This again can be formulated as in Eq. 2, where m(x) is the reflective mask. In most practical cases, it is safe to assume that m(x) ≡ m is a uniform mask (with <ref type="figure">Figure 7</ref>: Watermark removal from a single image. Ambiguity is resolved by a rough bounding-box around the watermark. (Tennis image was provided in <ref type="bibr" target="#b14">[14]</ref> as an example of an image immune to their watermark-removal method). an unknown constant 0 &lt; m &lt; 1). Double-DIP can be used to decompose an image I to its transparent layers. Each layer is again constructed by a separate DIP. The constant m is calculated by a third DIP. The Exclusion loss encourages minimal correlation between the recovered layers. <ref type="figure" target="#fig_1">Fig. 3</ref>.a shows a successful separation in a simple case, where each transparent layer has a relatively uniform patch distribution. This however does not hold in general. Because each pixel in I is a mixture of 2 values, the inherent layer ambiguity (Sec. 2.2) in a single transparent image is much greater than in the binary segmentation case.</p><p>Ambiguity can be resolved using external training, as in <ref type="bibr" target="#b40">[40]</ref>. However, since Double-DIP is unsupervised, we resolve this ambiguity when 2 different mixtures of the same layers are available. This gives rise to coupled equations:</p><formula xml:id="formula_5">I (1) (x) = m (1) y 1 (x) + (1 − m (1) )y 2 (x) I (2) (x) = m (2) y 1 (x) + (1 − m (2) )y 2 (x)<label>(6)</label></formula><p>Since the layers y 1 , y 2 are shared by both mixtures, one Double-DIP suffices to generate these layers using I <ref type="bibr" target="#b0">(1)</ref> , I <ref type="bibr" target="#b1">(2)</ref> simultaneously. The different coefficients m <ref type="bibr" target="#b0">(1)</ref> , m <ref type="bibr" target="#b1">(2)</ref> are generated by the same DIP using 2 random noises, z</p><formula xml:id="formula_6">(1) m , z (2) m</formula><p>See such an example in <ref type="figure" target="#fig_8">Fig. 8 (real transparent images)</ref>.</p><p>Video Transparency Separation: The case of a static reflection and dynamic transmission can be solved in a similar way. This case can be formulated as a set of equations:</p><formula xml:id="formula_7">I (i) (x) = m (i) y (i) 1 (x) + (1 − m (i) )y 2 (x)<label>(7)</label></formula><p>where i is the frame number, and y 2 is the static reflection (hence has no frame index i, but could have varying intensity over time, captured by m (i) ). Applying Double-DIP to a separate transparent video layers is done similarly to video segmentation (Sec. 3). We employ one DIP for each video layer, y 1 , y 2 and one more DIP to generate m (i) (but with a modified noise input per each frame, as in the video segmentation). <ref type="figure" target="#fig_5">Fig. 6</ref>.a shows examples of video separation. For full videos see the project website.</p><p>Watermark removal: Watermarks are widely-used for copyright protection of photos and videos. Dekel et al. <ref type="bibr" target="#b14">[14]</ref> presented a watermark removal algorithm, based on recurrence of the same watermark in many different images. Double-DIP is able to remove watermarks shared by very few images, often only one. We model watermarks as a special case of image reflection, where layers y 1 and y 2 are the clean image and the watermark, respectively. This time, however, the mask is not a constant m. The inherent transparent layer ambiguity is resolved by one of two practical ways: (i) when only one watermarked image is available, the user provides a crude hint (bounding box) around the location of the watermark; (ii) given a few images which share the same watermark (2-3 typically suffice), the ambiguity is resolved on its own. When a single image and a bounding box are provided, the learned mask m(x) is constrained to be zero outside the bounding box. This hint suffices for Double-DIP to perform reasonably well on this task. See examples in <ref type="figure">Fig. 7</ref>. In fact, the Tennis image in <ref type="figure">Fig. 7</ref> was provided by <ref type="bibr" target="#b14">[14]</ref> as an example image immune to their watermark-removal method.</p><p>When multiple images contain the same watermark are available, no bounding-box is needed. E.g., if 3 images share a watermark, we use 3 Double-DIPs, which share DIP 2 to output the common watermark layer y 2 . Independent layers y (i) <ref type="figure">Figure 9</ref>: Multi-image Watermark removal. Since the 3 images share the same watermark, the layer ambiguity is resolved.</p><p>opacity mask m, also common to the 3 images, is generated by another shared DIP. Example is shown in <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Image Dehazing</head><p>Images of outdoor scenes are often degraded by a scattering medium (e.g., haze, fog, underwater scattering). The degradation in such images grows with scene depth. Typically, a hazy image I(x) is modeled <ref type="bibr" target="#b23">[23]</ref>:</p><formula xml:id="formula_8">I(x) = t(x)J(x) + (1 − t(x))A(x)<label>(8)</label></formula><p>where A(x) is the Airlight map (A-map), J(x) is the hazefree image, and t(x) is the transmission (t-map), which exponentially decays with scene depth. The goal of image dehazing is to recover from a hazy image I(x) its underlying haze-free image J(x) (i.e., the image that would have been captured on a clear day with good visibility conditions).</p><p>We treat the dehazing problem as a layer separation problem, where one layer is the haze-free image (y 1 (x)=J(x)), the second layer is the A-map (y 2 (x)=A(x)), and the mixing mask is the t-map (m(x)=t(x)).</p><p>Handling non-uniform airlight: Most single-image blind dehazing methods (e.g. <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8]</ref>) assume a uniform airlight color A for the entire image (i.e., A(x) ≡ A). This is true also for deep network based dehazing methods <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b30">30]</ref>, which train on synthesized datasets of hazy/non-hazy image pairs. The uniform airlight assumption, however, is only an approximation. It tends to break, e.g. in outdoor images captured at dawn or dusk, when the sun is positioned close to the horizon. The airlight color is affected by the non-isotropic scattering of sun rays by haze particles, which causes the airlight color to vary across the image. When the uniform airlight assumption holds, estimating a single uniform airlight color A from the hazy I is relatively easy (e.g., using the dark channel prior of <ref type="bibr" target="#b23">[23]</ref>, or the patch-based prior of <ref type="bibr" target="#b6">[6]</ref>), and the challenging part remains the t-map estimation. However, when the uniform <ref type="figure">Figure 10</ref>: Uniform vs. Non-Uniform Airlight recovery. The uniform airlight assumption is often violated (e.g., in dusk and dawn). Double-DIP allows to recover a nonuniform airlight map, yielding higher-quality dehazing.</p><p>airlight assumption breaks, this produces dehazing results with distorted colors. Estimating a varying airlight, on the other hand, is a very challenging and ill-posed problem. Our Double-DIP framework allows simultaneous estimation of a varying airlight-map and varying t-map, by treating the Amap as another layer, and the t-map as a mask. This results in higher-quality image dehazing. The effect of estimating a uniform vs. varying airlight is exemplified in <ref type="figure">Fig. 10</ref>.</p><p>In dehazing, Loss Reg forces the mask t(x) to be smooth  (by minimizing the norm of its Laplacian). The internal self-similarity of patches in a hazy image I(x) is weaker than in its underlying haze-free image J(x) <ref type="bibr" target="#b6">[6]</ref>. This drives the first DIP to converge to a haze-free image. The A-map, however, is not a typical natural image. While it satisfies the strong internal self-similarity requirement, it tends to be much smoother than a natural image, and should not deviate much from a global airlight color. Hence, we apply an extra regularization loss on the airlight layer: A(x) − A 2 , where A is a single initial airlight color estimated from the hazy image I using one of the standard methods (we used the method of <ref type="bibr" target="#b6">[6]</ref>). Although the deviations from the initial airlight A are subtle, they are quite crucial to the quality of the recovered haze-free image (see <ref type="figure">Fig. 10</ref>).</p><p>We evaluated our framework on the O-HAZE dataset <ref type="bibr" target="#b3">[4]</ref> and compared it to unsupervised and self-supervised dehazing algorithms. Results are presented in <ref type="table" target="#tab_0">Table 1</ref>. Numerically, on this dataset, we ranked second of all dehazing methods. However, visually, on images outside this dataset, our results seem to surpass all dehazing methods (see the project website). We further wanted to compare to the winning methods of NTIRE'2018 Dehazing Challenge <ref type="bibr" target="#b1">[2]</ref>, but only one of them had code available <ref type="bibr" target="#b39">[39]</ref>. Our experiments show that while these methods obtained state-of-the-art results on the tiny test-set of the challenge (5 test images only!), they seem to severely overfit the challenge training-set. In fact, they perform very poorly on any hazy image outside this dataset (see <ref type="figure" target="#fig_9">Fig. 11</ref> and the project website). A visual comparison to many more methods and on many more images is found in the project website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>"Double-DIP" is a unified framework for unsupervised layer decomposition, applicable for a wide variety of tasks. It needs no training examples other than the input image/video. Although general-purpose, in some tasks (e.g., dehazing) it achieves results comparable or even better than leading methods in the field. We believe that augmenting Double-DIP with semantic/perceptual cues, may lead to advancements also in semantic segmentation and in other high-level tasks. This is part of our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Double-DIP Framework. Two Deep-Image-Prior networks (DIP 1 &amp; DIP 2 ) jointly decompose an input image I into its layers (y 1 &amp; y 2 ). Mixing those layers back according to a learned mask m, reconstructs an imageÎ≈I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The complexity of mixtures of layers vs. the simplicity of the individual components. (See text for explanation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(i) the recovered layers, when recombined, should yield the input image. (ii) Each of the layers should be as "simple" as possible, namely, it should have a strong internal self-similarity of "image elements". (iii) The recovered layers should be as independent of each other (uncorrelated) as possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Foreground/Background Image Segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Video Decomposition using Double-DIP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Video Layer Separation via Double-DIP. Double-DIP exploits the fact that all frames of a single dynamic video layer share the same patches. This promotes: (a) video transparency separation, and (b) Fg/Bg video segmentation. (See full videos in the project website).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>is a random uniform noise with variance significantly lower than that of z (i) m ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Layer ambiguity is resolved when two different mixtures of the same layers are available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Comparing Double-DIP's dehazing to specialized dehazing methods. (many more results in the project page)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of dehazing methods on O-Haze Dataset.</figDesc><table><row><cell cols="8">He [23] Meng [28] Fattal [18] Cai [10] Ancuti [3] Berman [8] Ren [30] Bahat [6]</cell><cell>Ours</cell></row><row><cell>PSNR 16.586</cell><cell>17.444</cell><cell>15.640</cell><cell>16.208</cell><cell>16.855</cell><cell>16.610</cell><cell>19.071</cell><cell>18.640</cell><cell>18.815</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code will be made publicly available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">, i=1, 2, 3, provide the 3 clean images. The</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image segmentation by probabilistic bottom-up aggregation and cue integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on image dehazing: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="891" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Night-time dehazing by fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2256" to="2260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">O-haze: a dehazing benchmark with real hazy and haze-free outdoor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Vleeschouwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, NTIRE Workshop, NTIRE CVPR&apos;18</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What is a good image segment? a unified approach to segment extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<editor>D. Forsyth, P. Torr, and A. Zisserman</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">5305</biblScope>
			<biblScope unit="page" from="30" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blind dehazing using internal patch recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Goldman. Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/1601.07661</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The patch transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Series in Telecommunications and Signal Processing)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the effectiveness of visible watermarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-segmentation by composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transaction on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision: Part I, ECCV&apos;10</title>
		<meeting>the 11th European Conference on Computer Vision: Part I, ECCV&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time foregroundbackground segmentation using codebook model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Chalidabhongse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Issue on Video Object Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="172" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">User assisted separation of reflections from a single image using a sparsity prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1647" to="1654" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int&apos;l Conf. Computer Vision</title>
		<meeting>8th Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision, ICCV &apos;13</title>
		<meeting>the 2013 IEEE International Conference on Computer Vision, ICCV &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shift-map image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kav-Venaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">grabcut&quot;: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Separating transparent layers through layer information exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<editor>T. Pajdla and J. Matas, editors</editor>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="328" to="341" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Space-time superresolution from a single video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition 2007 (CVPR&apos;07)</title>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">zero-shot&quot; superresolution using deep internal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Layer extraction from multiple images containing reflections and transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">1246</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-scale single image dehazing using perceptual pyramid deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Single image reflection separation with perceptual losses. CoRR, abs/1806.05376</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Internal statistics of a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zontak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
