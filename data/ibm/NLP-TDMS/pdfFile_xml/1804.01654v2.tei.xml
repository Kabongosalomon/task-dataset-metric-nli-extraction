<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
							<email>nywang16@fudan.edu.cnyindaz@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
							<email>lzhuwen@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
							<email>yanweifu@fudan.edu.cnwl2223@columbia.edu</email>
							<affiliation key="aff3">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>3D shape generation · Graph convolutional neural network · Mesh reconstruction · Coarse-to-fine · End-to-end framework</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inferring 3D shape from a single perspective is a fundamental human vision functionality but is extremely challenging for computer vision. Recently, great success has been achieved for 3d shape generation from a single color image using deep learning techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. Taking advantage of convolutional layers on regular grids or multi-layer perception, the estimated 3D shape, as the output of the neural network, is represented as either a volume <ref type="bibr" target="#b5">[6]</ref> or point cloud <ref type="bibr" target="#b8">[9]</ref>. However, both representations lose important surface details, and is non-trivial to reconstruct a surface model ( <ref type="figure">Fig. 1)</ref>, i.e. a mesh, which is more desirable for many real applications since it is lightweight, capable of modelling shape details, easy to deform for animation, to name a few.</p><p>In this paper, we push along the direction of single image reconstruction, and propose an algorithm to extract a 3D triangular mesh from a single color image. Rather  <ref type="figure">Fig. 1</ref>. Given a single color image and an initial mesh, our method can produce a high-quality mesh that contains details from the example. than directly synthesizing, our model learns to deform a mesh from a mean shape to the target geometry. This benefits us from several aspects. First, deep network is better at predicting residual, e.g. a spatial deformation, rather than structured output, e.g. a graph. Second, a series of deformations can be added up together, which allows shape to be gradually refined in detail. It also enables the control of the trade-off between the complexity of the deep learning model and the quality of the result. Lastly, it provides the chance to encode any prior knowledge to the initial mesh, e.g. topology. As a pioneer study, in this work, we specifically work on objects that can be approximated using 3D mesh with genus 0 by deforming an ellipsoid with a fixed size. In practice, we found most of the commonly seen categories can be handled well under this setting, e.g. car, plane, table, etc. To achieve this goal, there are several inherent challenges.</p><p>The first challenge is how to represent a mesh model, which is essentially an irregular graph, in a neural network and still be capable of extracting shape details effectively from a given color image represented in a 2D regular grid. It requires the integration of the knowledge learned from two data modalities. On the 3D geometry side, we directly build a graph based fully convolutional network (GCN) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18</ref>] on the mesh model, where the vertices and edges in the mesh are directly represented as nodes and connections in a graph. Network feature encoding information for 3D shape is saved on each vertex. Through forward propagation, the convolutional layers enable feature exchanging across neighboring nodes, and eventually regress the 3D location for each vertex. On the 2D image side, we use a VGG-16 like architecture to extract features as it has been demonstrated to be successful for many tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>. To bridge these two, we design a perceptual feature pooling layer which allows each node in the GCN to pool image features from its 2D projection on the image, which can be readily obtained by assuming known camera intrinsic matrix. The perceptual feature pooling is enabled once after several convolutions (i.e. a deformation block described in Sec. 3.4) using updated 3D locations, and hence the image features from correct locations can be effectively integrated with 3D shapes.</p><p>Given the graph representation, the next challenge is how to update the vertex location effectively towards ground truth. In practice, we observe that network trained to directly predict mesh with a large number of vertices is likely to make mistake in the beginning and hard to fix later. One reason is that a vertex cannot effectively retrieve features from other vertices with a number of edges away, i.e. the limited receptive field.</p><p>To solve this problem, we design a graph unpooling layer, which allows the network to initiate with a smaller number of vertices and increase during the forward propagation. With fewer vertices at the beginning stages, the network learns to distribute the vertices around to the most representative location, and then add local details as the number of vertices increases later. Besides the graph unpooling layer, we use a deep GCN enhanced by shortcut connections <ref type="bibr" target="#b12">[13]</ref> as the backbone of our architecture, which enables large receptive fields for global context and more steps of movements.</p><p>Representing the shape in graph also benefits the learning procedure. The known connectivity allows us to define higher order loss functions across neighboring nodes, which are important to regularize 3D shapes. Specifically, we define a surface normal loss to favor smooth surface; an edge loss to encourage uniform distribution of mesh vertices for high recall; and a laplacian loss to prevent mesh faces from intersecting each other. All of these losses are essential to generate quality appealing mesh model, and none of them can be trivially defined without the graph representation.</p><p>The contributions of this paper are mainly in three aspects. First, we propose a novel end-to-end neural network architecture that generates a 3D mesh model from a single RGB image. Second, we design a projection layer which incorporates perceptual image features into the 3D geometry represented by GCN. Third, our network predict 3D geometry in a coarse to fine fashion, which is more reliable and easy to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>3D reconstruction has been well studied based on the multi-view geometry (MVG) <ref type="bibr" target="#b11">[12]</ref> in the literature. The major research directions include structure from motion (SfM) <ref type="bibr" target="#b26">[27]</ref> for large-scale high-quality reconstruction and simultaneous localization and mapping (SLAM) <ref type="bibr" target="#b3">[4]</ref> for navigation. Though they are very successful in these scenarios, they are restricted by 1) the coverage that the multiple views can give and 2) the appearance of the object that wants to reconstruct. The former restriction means MVG cannot reconstruct unseen parts of the object, and thus it usually takes a long time to get enough views for a good reconstruction; the latter restriction means MVG cannot reconstruct non-lambertian (e.g. reflective or transparent) or textureless objects. These restrictions lead to the trend of resorting to learning based approaches.</p><p>Learning based approaches usually consider single or few images, as it largely relies on the shape priors that it can learn from data. Early works can be traced back to Hoiem et al. <ref type="bibr" target="#b13">[14]</ref> and Saxena et al. <ref type="bibr" target="#b24">[25]</ref>. Most recently, with the success of deep learning architectures and the release of large-scale 3D shape datasets such as ShapeNet <ref type="bibr" target="#b4">[5]</ref>, learning based approaches have achieved great progress. Huang et al. <ref type="bibr" target="#b14">[15]</ref> and Su et al. <ref type="bibr" target="#b28">[29]</ref> retrieve shape components from a large dataset, assemble them and deform the assembled shape to fit the observed image. However, shape retrieval from images itself is an illposed problem. To avoid this problem, Kar et al. <ref type="bibr" target="#b15">[16]</ref> learns a 3D deformable model for each object category and capture the shape variations in different images. However, the reconstruction is limited to the popular categories and its reconstruction result is usually lack of details. Another line of research is to directly learn 3D shapes from single images. Restricted by the prevalent grid-based deep learning architectures, most works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> outputs 3D voxels, which are usually with low resolutions due to the memory constraint on a modern GPU. Most recently, Tatarchenko et al. <ref type="bibr" target="#b29">[30]</ref> have proposed an octree representation, which allows to reconstructing higher resolution outputs with a limited memory budget. However, a 3D voxel is still not a popular shape representation in game and movie industries. To avoid drawbacks of the voxel representation, Fan et al. <ref type="bibr" target="#b8">[9]</ref> propose to generate point clouds from single images. The point cloud representation has no local connections between points, and thus the point positions have a very large degree of freedom. Consequently, the generated point cloud is usually not close to a surface and cannot be used to recover a 3D mesh directly. Besides these typical 3D representations, there is an interesting work <ref type="bibr" target="#b27">[28]</ref> which uses a so-called "geometry image" to represent a 3D shape. Thus, their network is a 2D convolutional neural network which conducts an image to image mapping. Our works are mostly related to the two recent works <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b23">[24]</ref>. However, the former adopts simple silhouette supervision, and hence does not perform well for complicated objects such as car, lamp, etc; the latter needs a large model repository to generate a combined model.</p><p>Our base network is a graph neural network <ref type="bibr" target="#b25">[26]</ref>; this architecture has been adopted for shape analysis <ref type="bibr" target="#b30">[31]</ref>. In the meanwhile, there are charting-based methods which directly apply convolutions on surface manifolds <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> for shape analysis. As far as we know, these architectures have never been adopted for 3D reconstruction from single images, though graph and surface manifold are natural representations for meshed objects. For a comprehensive understanding of the graph neural network, the chartingbased methods and their applications, please refer to this survey <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary: Graph-based Convolution</head><p>We first provide some background about graph based convolution; more detailed introduction can be found in <ref type="bibr" target="#b2">[3]</ref>. A 3D mesh is a collection of vertices, edges and faces that defines the shape of a 3D object; it can be represented by a graph M = (V, E, F),</p><formula xml:id="formula_0">where V = {v i } N i=1 is the set of N vertices in the mesh, E = {e i } E i=1</formula><p>is the set of E edges with each connecting two vertices, and F = {f i } N i=1 are the feature vectors attached on vertices. A graph based convolutional layer is defined on irregular graph as:</p><formula xml:id="formula_1">f l+1 p = w 0 f l p + q∈N (p) w 1 f l q (1) where f l p ∈ R d l , f l+1 p ∈ R d l+1</formula><p>are the feature vectors on vertex p before and after the convolution, and N (p) is the neighboring vertices of p; w 0 and w 1 are the learnable parameter matrices of d l × d l+1 that are applied to all vertices. Note that w 1 is shared for all edges, and thus (1) works on nodes with different vertex degrees. In our case, the attached feature vector f p is the concatenation of the 3D vertex coordinate, feature encoding 3D shape, and feature learned from the input color image (if they exist). Running convolutions updates the features, which is equivalent as applying a deformation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Overview</head><p>Our model is an end-to-end deep learning framework that takes a single color image as input and produces a 3D mesh model in camera coordinate. The overview of our framework is illustrated in <ref type="figure">Fig. 2</ref>. The whole network consists an image feature network and a cascaded mesh deformation network. The image feature network is a 2D CNN that extract perceptual feature from the input image, which is leveraged by the mesh deformation network to progressively deform an ellipsoid mesh into the desired 3D model. The cascaded mesh deformation network is a graph-based convolution network (GCN), which contains three deformation blocks intersected by two graph unpooling layers. Each deformation block takes an input graph representing the current mesh model with the 3D shape feature attached on vertices, and produces new vertices locations and features. Whereas the graph unpooling layers increase the number of vertices to increase the capacity of handling details, while still maintain the triangular mesh topology. Starting from a smaller number of vertices, our model learns to gradually deform and add details to the mesh model in a coarse-to-fine fashion. In order to train the network to produce stable deformation and generate an accurate mesh, we extend the Chamfer Distance loss used by Fan et al. <ref type="bibr" target="#b8">[9]</ref> with three other mesh specific loss -Surface normal loss, Laplacian regularization loss, and Edge length loss. The remaining part of this section describes details of these components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Initial ellipsoid</head><p>Our model does not require any prior knowledge of the 3D shape, and always deform from an initial ellipsoid with average size placed at the common location in the camera coordinate. The ellipsoid is centered at 0.8m in front of the camera with 0.2m, 0.2m, 0.4m as the radius of three axis. The mesh model is generated by implicit surface algorithm in Meshlab <ref type="bibr" target="#b6">[7]</ref> and contains 156 vertices. We use this ellipsoid to initialize our input graph, where the initial feature contains only the 3D coordinate of each vertex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mesh deformation block</head><p>The architecture of mesh deformation block is shown in <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>. In order to generate 3D mesh model that is consistent with the object shown in the input image, the deformation block need to pool feature (P) from the input image. This is done in conjunction with the image feature network and a perceptual feature pooling layer given the location of vertex (C i−1 ) in the current mesh model. The pooled perceptual feature is then concatenated with the 3D shape feature attached on the vertex from the input graph (F i−1 ) and fed into a series of graph based ResNet (G-ResNet). The G-ResNet produces, also as the output of the mesh deformation block, the new coordinates (C i ) and 3d shape feature (F i ) for each vertex.</p><formula xml:id="formula_2">Perceptual Feature Pooling G-ResNet … "#$ "#$ " " x 14 (a) Mesh Deformation Block (b) Perceptual Feature Pooling</formula><p>Perceptual feature pooling layer We use a VGG-16 architecture up to layer conv5 3 as the image feature network as it has been widely used. Given the 3D coordinate of a vertex, we calculate its 2D projection on input image plane using camera intrinsics, and then pool the feature from four nearby pixels using bilinear interpolation. In particular, we concatenate feature extracted from layer 'conv3 3', 'conv4 3', and 'conv5 3', which results in a total dimension of 1280. This perceptual feature is then concatenated with the 128-dim 3D feature from the input mesh, which results in a total dimension of 1408. This is illustrated in <ref type="figure" target="#fig_1">Fig. 3 (b)</ref>. Note that in the first block, the perceptual feature is concatenated with the 3-dim feature (coordinate) since there is no learnt shape feature at the beginning.</p><p>G-ResNet After obtaining 1408-dim feature for each vertex representing both 3D shape and 2D image information, we design a graph based convolutional neural network to predict new location and 3D shape feature for each vertex. This requires efficient exchange of the information between vertices. However, as defined in <ref type="formula">(1)</ref>, each convolution only enables the feature exchanging between neighboring pixels, which severely impairs the efficiency of information exchanging. This is equivalent as the small receptive field issue on 2D CNN.</p><p>To solve this issue, we make a very deep network with shortcut connections <ref type="bibr" target="#b12">[13]</ref> and denote it as G-ResNet ( <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>). In this work, the G-ResNet in all blocks has the same structure, which consists of 14 graph residual convolutional layers with 128 channels. The serial of G-ResNet block produces a new 128-dim 3D feature. In addition to the feature output, there is a branch which applies an extra graph convolutional layer to the last layer features and outputs the 3D coordinates of the vertex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Graph unpooling layer</head><p>The goal of unpooling layer is to increase the number of vertex in the GCNN. It allows us to start from a mesh with fewer vertices and add more only when necessary, which reduces memory costs and produces better results. A straightforward approach is to add one vertex in the center of each triangle and connect it with the three vertices of the triangle <ref type="figure" target="#fig_2">(Fig. 4 (b)</ref> Face-based). However, this causes imbalanced vertex degrees, i.e. number of edges on vertex. Inspired by the vertex adding strategy of the mesh subdivision algorithm prevalent in computer graphics, we add a vertex at the center of each edge and connect it with the two end-point of this edge ( <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>). The 3D feature for newly added vertex is set as the average of its two neighbors. We also connect three vertices if they are added on the same triangle (dashed line.) Consequently, we create 4 new triangles for each triangle in the original mesh, and the number of vertex is increased by the number of edges in the original mesh. This edge-based unpooling uniformly upsamples the vertices as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (b) Edge-based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Losses</head><p>We define four kinds of losses to constrain the property of the output shape and the deformation procedure to guarantee appealing results. We adopt the Chamfer loss <ref type="bibr" target="#b8">[9]</ref> to constrain the location of mesh vertices, a normal loss to enforce the consistency of surface normal, a laplacian regularization to maintain relative location between neighboring vertices during deformation, and an edge length regularization to prevent outliers. These losses are applied with equal weight on both the intermediate and final mesh. Unless otherwise stated, we use p for a vertex in the predicted mesh, q for a vertex in the ground truth mesh, N (p) for the neighboring pixel of p, till the end of this section.</p><p>Chamfer loss The Chamfer distance measures the distance of each point to the other set: l c = p min q p − q 2 2 + q min p p − q 2 2 . It is reasonably good to regress the vertices close to its correct position, however is not sufficient to produce nice 3D mesh (see the result of Fan et al. <ref type="bibr" target="#b8">[9]</ref> in <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal loss</head><p>We further define loss on surface normal to characterize high order properties:</p><formula xml:id="formula_3">l n = p q=arg minq( p−q 2 2 ) p − k, n q 2 2 , s.t. k ∈ N (p)</formula><p>where q is the closest vertex for p that is found when calculating the chamfer loss, k is the neighboring pixel of p, ·, · is the inner product of two vectors, and n q is the observed surface normal from ground truth.</p><p>Essentially, this loss requires the edge between a vertex with its neighbors to perpendicular to the observation from the ground truth. One may find that this loss does not equal to zero unless on a planar surface. However, optimizing this loss is equivalent as forcing the normal of a locally fitted tangent plane to be consistent with the observation, which works practically well in our experiment. Moreover, this normal loss is fully differentiable and easy to optimize.</p><p>Regularization Even with the Chamfer loss and Normal loss, the optimization is easily stucked in some local minimum. More specifically, the network may generate some super large deformation to favor some local consistency, which is especially harmful at the beginning when the estimation is far from ground truth, and causes flying vertices ( <ref type="figure" target="#fig_3">Fig. 5</ref>).</p><p>Laplacian regularization To handle these problem, we first propose a Laplacian term to prevent the vertices from moving too freely, which potentially avoids mesh selfintersection. The laplaician term serves as a local detail preserving operator, that encourages neighboring vertices to have the same movement. In the first deformation block, it acts like a surface smoothness term since the input to this block is a smootheverywhere ellipsoid; starting from the second block, it prevents the 3D mesh model from deforming too much, so that only fine-grained details are added to the mesh model. To calculate this loss, we first define a laplacian coordinate for each vertex p as δ p = p − k∈N (p) 1 N (p) k, and the laplacian regularization is defined as: l lap = p δ p −δ p 2 2 , where δ p and δ p are the laplacian coordinate of a vertex after and before a deformation block.</p><p>Edge length regularization. To penalize flying vertices, which ususally cause long edge, we add an edge length regularization loss: l loc = p k∈N (p) p − k 2 2 . The overall loss is a weighted sum of all four losses, l all = l c +λ 1 l n +λ 2 l lap +λ 3 l loc , where λ 1 = 1.6e − 4, λ 2 = 0.3 and λ 3 = 0.1 are the hyperparameters which balance the losses and fixed for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we perform an extensive evaluation on our model. In addition to comparing with previous 3D shape generation works for evaluating the reconstruction accuracy, we also analyse the importance of each component in our model. Qualitative results on both synthetic and real-world images further show that our model produces triangular meshes with smooth surfaces and still maintains details depicted in the input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>Data. We use the dataset provided by Choy et al. <ref type="bibr" target="#b5">[6]</ref>. The dataset contains rendering images of 50k models belonging to 13 object categories from ShapeNet <ref type="bibr" target="#b4">[5]</ref>, which is a collection of 3D CAD models that are organized according to the WordNet hierarchy. A model is rendered from various camera viewpoints, and camera intrinsic and extrinsic matrices are recorded. For fair comparison, we use the same training/testing split as in Choy et. al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>Evaluation Metric. We adopt the standard 3D reconstruction metric. We first uniformly sample points from our result and ground truth. We calculate precision and recall by checking the percentage of points in prediction or ground truth that can find a nearest neighbor from the other within certain threshold τ . A F-score <ref type="bibr" target="#b18">[19]</ref> as the harmonic mean of precision and recall is then calculated. Following Fan et. al. <ref type="bibr" target="#b8">[9]</ref>, we also report the Chamfer Distance (CD) and Earth Mover's Distance (EMD). For F-Score, larger is better. For CD and EMD, smaller is better.</p><p>On the other hand, we realize that the commonly used evaluation metrics for shape generation may not thoroughly reflect the shape quality. They often capture occupancy or point-wise distance rather than surface properties, such as continuity, smoothness, high-order details, for which a standard evaluation metric is barely missing in literature. Thus, we recommend to pay attention on qualitative results for better understanding of these aspects.</p><p>Baselines. We compare the presented approach to the most recent single image reconstruction approaches. Specifically, we compare with two state-of-the-art methods -Choy et. al. <ref type="bibr" target="#b5">[6]</ref> (3D-R2N2) producing 3D volume, and Fan et. al. <ref type="bibr" target="#b8">[9]</ref> (PSG) producing point cloud. Since the metrics are defined on point cloud, we can evaluate PSG directly on its output, our method by uniformly sampling point on surface, and 3D-R2N2 by uniformly sampling point from mesh created using the Marching Cube <ref type="bibr" target="#b20">[21]</ref> method.</p><p>We also compare to Neural 3D Mesh Renderer (N3MR) <ref type="bibr" target="#b16">[17]</ref> which is so far the only deep learning based mesh generation model with code public available. For fair comparison, the models are trained with the same data using the same amount of time.</p><p>Training and Runtime. Our network receives input images of size 224 × 224, and initial ellipsoid with 156 vertices and 462 edges. The network is implemented in Tensorflow and optimized using Adam with weight decay 1e-5. The batch size is 1; the total number of training epoch is 50; the learning rate is initialized as 3e-5 and drops to 1e-5 after 40 epochs. The total training time is 72 hours on a Nvidia Titan X. During testing, our model takes 15.58ms to generate a mesh with 2466 vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to state of the art</head><p>Tab. 1 shows the F-score with different thresholds of different methods. Our approach outperforms the other methods in all categories except watercraft. Notably, our results are significantly better than the others in all categories under a smaller threshold τ , showing at least 10% F-score improvement. N3MR does not perform well, and its result is about 50% worse than ours, probably because their model only learns from limited silhouette signal in images and lacks of explicit handling of the 3D mesh.</p><p>We also show the CD and EMD for all categories in Tab. 2. Our approach outperforms the other methods in most categories and achieves the best mean score. The major competitor is PSG, which produces a point cloud and has the most freedom; this freedom leads to smaller CD and EMD, however does not necessarily leads to a better mesh model without proper regularization. To demonstrate this, we show the qualitative results to analyze why our approach outperforms the others. <ref type="figure" target="#fig_6">Fig. 8</ref> shows the visual results. To compare the quality of mesh model, we convert volumetric and point cloud to mesh using standard approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1]</ref>. As we can see, the 3D volume results produced by 3D-R2N2 lack of details due to the low resolution, e.g., the legs are missing in the chair example as shown in the 4-th row of <ref type="figure" target="#fig_6">Fig. 8</ref>. We tried octree based solution <ref type="bibr" target="#b29">[30]</ref> to increase the volume resolution, but found it still hard to recover surface level details as much as our model. PSG produces sparse 3D point clouds, and it is non-trivial to recover meshes from them. This is due to the applied Chamfer loss acting like a regression loss which gives too much degree of freedom to the point cloud. N3MR produces very rough shape, which might be sufficient for some rendering tasks, however cannot recover complicated objects such as chairs and tables. In contrast, our model does not suffer from these issues by leveraging a mesh representation, integration of perceptual feature, and carefully defined losses during the training. Our result is not restricted by the resolution due to the limited memory budget and contains both smooth continuous surface and local details.  <ref type="table">Table 3</ref>. Ablation study that evaluates the contribution of different ideas to the performance of the presented model. The table reports all 4 measurements. For F-score, larger is better. For CD and EMD, small is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Now we conduct controlled experiments to analyse the importance of each component in our model. Tab. 3 reports the performance of each model by removing one component from the full model. Again, we argue that these commonly used evaluation metrics does not necessarily reflect the quality of the recovered 3D geometry. For example, the model with no edge length regularization achieves the best performance across all, however, in fact produces the worst mesh <ref type="figure" target="#fig_3">(Fig. 5, the last 2nd column)</ref>. As such, we use qualitative result <ref type="figure" target="#fig_3">Fig. 5</ref> to show the contribution of each component in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Unpooling</head><p>We first remove the graph unpooling layers, and thus each block has the same number of vertices as in the last block of our full model. It is observed that the deformation makes mistake easier at beginning, which cannot be fixed later on.</p><p>Consequently, there are some obvious artifacts in some parts of the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G-ResNet</head><p>We then remove the shortcut connections in G-ResNet, and make it regular GCN. As can be seen from Tab. 3, there is a huge performance gap in all four measurement metrics, which means the failure of optimizing Chamfer distance. The main reason is the degradation problem observed in the very deep 2D convolutional neural network. Such problem leads to a higher training error (and thus higher testing error) when adding more layers to a suitably deep model <ref type="bibr" target="#b12">[13]</ref>. Essetially, our network has 42 graph convolutional layers. Thus, this phenomenon has also been observed in our very deep graph neural network experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss terms</head><p>We evaluate the function of each additional terms besides the Chamfer loss.</p><p>As can be seen in <ref type="figure" target="#fig_3">Fig. 5</ref>, removing normal loss severely impairs the surface smoothness and local details, e.g. seat back; removing Laplacian term causes intersecting geometry because the local topology changes, e.g. the hand held of the chair; removing edge length term causes flying vertices and surfaces, which completely ruins the surface characteristics. These results demonstrate that all the components presented in this work contribute to the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Deformation Blocks</head><p>We now analyze the effects of the number of blocks. <ref type="figure">Figure Fig. 6 (left)</ref> shows the mean F-score(τ ) and CD with regard to the number of blocks. The results indicate that increasing the number of blocks helps, but the benefit is getting saturated with more blocks, e.g. from 3 to 4. In our experiment, we found that 4 blocks results in too many vertices and edges, which slow down our approach dramatically even though it provides better accuracy on evaluation metrics. Therefore, we use 3 blocks in all our experiment for the best balance of performance and efficiency. <ref type="figure">Fig. 6 (right)</ref> shows the output of our model after each deformation block. Notice how mesh is densified with more vertices and new details are added.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Reconstructing Real-World images</head><p>Following Choy et. al. <ref type="bibr" target="#b5">[6]</ref>, we test our network on the Online Products dataset and Internet images for qualitative evaluation on real images. We use the model trained from ShapeNet dataset and directly run on real images without finetuning, and show results in <ref type="figure" target="#fig_5">Fig. 7</ref>. As can be seen, our model trained on synthetic data generalizes well to the real-world images across various categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented an approach to extract 3D triangular meshes from singe images. We exploit the key advantages the mesh presentation can bring to us, and the key issues required to solve for success. The former includes surface normal constraints and information propagation along edges; the latter includes perceptual features extracted from images as a guidance. We carefully design our network structure and propose a very deep cascaded graph convolutional neural network with "shortcut" connections. Meshes are progressively refined by our network trained end-to-end with the chamfer , converted using Marching Cube <ref type="bibr" target="#b20">[21]</ref>; (c) Point cloud from PSG <ref type="bibr" target="#b8">[9]</ref>, converted using ball pivoting <ref type="bibr" target="#b0">[1]</ref>; (d) N3MR <ref type="bibr" target="#b16">[17]</ref>; (e) Ours; (f) Ground truth.</p><p>loss and normal loss. Our results are significantly better than the previous state-of-theart using other shape representations such as 3D volume or 3D point cloud. Thus, we believe mesh representation is the next big thing in this direction, and we hope that the key components discovered in our work can support follow-up works that will further advance direct 3D mesh reconstruction from single images.</p><p>Future work Our method only produces meshes with the same topology as the initial mesh. In the future, we will extend our approach to more general cases, such as scene level reconstruction, and learn from multiple images for multi-view reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>indicates equal contributions. † indicates corresponding author. arXiv:1804.01654v2 [cs.CV] 3 Aug 2018 Input Image Volume from [Choy et al.] Convert using [Lorensen et al.] Pixel2Mesh (Ours) Point cloud from [Fan et al.] Convert using [Bernardini et al.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a) The vertex locations Ci are used to extract image features, which are then combined with vertex features Fi and fed into G-ResNet. means a concatenation of the features. (b) The 3D vertices are projected to the image plane using camera intrinsics, and perceptual feature is pooled from the 2D-CNN layers using bilinear interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Black vertices and dashed edges are added in the unpooling layer. (b) The face based unpooling leads to imbalanced vertex degrees, while the edge-based unpooling remains regular.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results for ablation study. This figure truly reflects the contribution of each components especially for the regularization ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 CDFig. 6 .</head><label>26</label><figDesc>Left: Effect of number of blocks. Each curve shows the mean F-score (τ ) and CD for different number of blocks. Right: Sample examples showing the output after each block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative results of real-world images from the Online Products dataset and Internet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results. (a) Input image; (b) Volume from 3D-R2N2 [6]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>68.20 62.10 71.12 63.23 81.22 77.15 81.38 bench 34.09 49.29 35.84 57.57 48.89 69.17 49.58 71.86 cabinet 49.88 39.93 21.04 60.39 64.83 67.03 35.16 77.19 car 37.80 50.70 36.66 67.86 54.84 77.79 53.93 84.15 chair 40.22 41.60 30.25 54.38 55.20 63.70 44.59 70.42 monitor 34.38 40.53 28.77 51.39 48.23 63.64 42.76 67.01 lamp 32.35 41.40 27.97 48.15 44.37 58.84 39.41 61.50 speaker 45.30 32.61 19.46 48.84 57.86 56.79 32.20 65.61 firearm 28.34 69.96 52.22 73.20 46.87 82.65 63.28 83.47 couch 40.01 36.59 25.04 51.90 53.42 62.95 39.90 69.83 table 43.79 53.44 28.40 66.30 59.49 73.10 41.73 79.20 cellphone 42.31 55.95 27.96 70.24 60.88 79.63 41.83 82.86 watercraft 37.10 51.28 43.71 55.12 52.19 70.63 58.85 69.99 mean 39.01 48.58 33.80 59.72 54.62 69.78 47.72 74.19 F-score (%) on the ShapeNet test set at different thresholds, where τ = 10 −4 . Larger is better. Best results under each threshold are bolded.</figDesc><table><row><cell>Threshold</cell><cell>τ</cell><cell>2τ</cell></row><row><cell cols="3">Category 3D-R2N2 PSG N3MR Ours 3D-R2N2 PSG N3MR Ours</cell></row><row><cell>plane</cell><cell>41.46</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>CD and EMD on the ShapeNet test set. Smaller is better. Best results under each threshold are bolded.</figDesc><table><row><cell>Category</cell><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell>EMD</cell><cell></cell></row><row><cell></cell><cell cols="6">3D-R2N2 PSG N3MR Ours 3D-R2N2 PSG N3MR Ours</cell></row><row><cell>plane</cell><cell cols="6">0.895 0.430 0.450 0.477 0.606 0.396 7.498 0.579</cell></row><row><cell>bench</cell><cell cols="6">1.891 0.629 2.268 0.624 1.136 1.113 11.766 0.965</cell></row><row><cell>cabinet</cell><cell cols="6">0.735 0.439 2.555 0.381 2.520 2.986 17.062 2.563</cell></row><row><cell>car</cell><cell cols="6">0.845 0.333 2.298 0.268 1.670 1.747 11.641 1.297</cell></row><row><cell>chair</cell><cell cols="6">1.432 0.645 2.084 0.610 1.466 1.946 11.809 1.399</cell></row><row><cell>monitor</cell><cell cols="6">1.707 0.722 3.111 0.755 1.667 1.891 14.097 1.536</cell></row><row><cell>lamp</cell><cell cols="6">4.009 1.193 3.013 1.295 1.424 1.222 14.741 1.314</cell></row><row><cell>speaker</cell><cell cols="6">1.507 0.756 3.343 0.739 2.732 3.490 16.720 2.951</cell></row><row><cell>firearm</cell><cell cols="6">0.993 0.423 2.641 0.453 0.688 0.397 11.889 0.667</cell></row><row><cell>couch</cell><cell cols="6">1.135 0.549 3.512 0.490 2.114 2.207 14.876 1.642</cell></row><row><cell>table</cell><cell cols="6">1.116 0.517 2.383 0.498 1.641 2.121 12.842 1.480</cell></row><row><cell>cellphone</cell><cell cols="6">1.137 0.438 4.366 0.421 0.912 1.019 17.649 0.724</cell></row><row><cell cols="7">watercraft 1.215 0.633 2.154 0.670 0.935 0.945 11.425 0.814</cell></row><row><cell>mean</cell><cell cols="6">1.445 0.593 2.629 0.591 1.501 1.653 13.386 1.380</cell></row><row><cell cols="7">Category -ResNet -Laplacian -Unpooling -Normal -Edge length Full model</cell></row><row><cell>F (τ )↑</cell><cell>55.308</cell><cell>60.801</cell><cell cols="2">60.222 58.668</cell><cell>60.101</cell><cell>59.728</cell></row><row><cell>F (2τ )↑</cell><cell>71.567</cell><cell>75.202</cell><cell cols="2">76.231 74.276</cell><cell>76.053</cell><cell>74.191</cell></row><row><cell>CD↓</cell><cell>0.644</cell><cell>0.596</cell><cell>0.561</cell><cell>0.598</cell><cell>0.552</cell><cell>0.591</cell></row><row><cell>EMD↓</cell><cell>1.583</cell><cell>1.350</cell><cell>1.656</cell><cell>1.445</cell><cell>1.479</cell><cell>1.380</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by two projects from NSFC (#61622204 and #61572134), two projects from STCSM (#16JC1420401 and #16QA1400500), Eastern Scholar (TP2017006), and The Thousand Talents Plan of China (for young professionals, D1410009).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The ball-pivoting algorithm for surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mittleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="359" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Past, present, and future of simultaneous localization and mapping: Towards the robustperception age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1332" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ShapeNet: An Information-Rich 3D Model Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>cs.GR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meshlab: an open-source mesh processing tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cignoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Callieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dellepiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ganovelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ranzuglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Italian Chapter Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="3837" to="3845" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single-view reconstruction via joint analysis of image and shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="87" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Category-specific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tanks and temples: benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>78:1-78:13</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Marching cubes: A high resolution 3d surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<title level="m">Geodesic convolutional neural networks on riemannian manifolds. In: ICCV Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Image2mesh: A learning framework for single image 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10669[cs.CV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Surfnet: Generating 3d shape surfaces using deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Unmesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating image depth using shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Syncspeccnn: Synchronized spectral CNN for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
