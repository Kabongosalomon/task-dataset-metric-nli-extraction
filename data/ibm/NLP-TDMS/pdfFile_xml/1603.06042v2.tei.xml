<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Globally Normalized Transition-Based Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-08">8 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
							<email>andor@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
							<email>chrisalberti@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
							<email>djweiss@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
							<email>severyn@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
							<email>apresta@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
							<email>kuzman@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<email>mjcollins@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc New York</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Globally Normalized Transition-Based Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-08">8 Jun 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-ofspeech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network approaches have taken the field of natural language processing (NLP) by storm.</p><p>In particular, variants of long short-term memory (LSTM) networks <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref> have produced impressive results on some of the classic NLP tasks such as part-of-speech tagging , syntactic parsing <ref type="bibr" target="#b40">(Vinyals et al., 2015)</ref> and semantic role labeling <ref type="bibr" target="#b47">(Zhou and Xu, 2015)</ref>. One might speculate that it is the recurrent nature of these models that enables these results.</p><p>In this work we demonstrate that simple feed-forward networks without any recurrence can achieve comparable or better accuracies than LSTMs, as long as they are globally normalized. Our model, described in detail in Section 2, uses a transition system <ref type="bibr" target="#b33">(Nivre, 2006)</ref> and feature embeddings as introduced by * On leave from Columbia University. <ref type="bibr">Chen and Manning (2014)</ref>. We do not use any recurrence, but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field (CRF) objective <ref type="bibr" target="#b7">(Bottou et al., 1997;</ref><ref type="bibr" target="#b27">Le Cun et al., 1998;</ref><ref type="bibr" target="#b26">Lafferty et al., 2001;</ref><ref type="bibr" target="#b12">Collobert et al., 2011)</ref> to overcome the label bias problem that locally normalized models suffer from. Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates <ref type="bibr" target="#b10">(Collins and Roark, 2004;</ref>. We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss.</p><p>In Section 3 we revisit the label bias problem and the implication that globally normalized models are strictly more expressive than locally normalized models. Lookahead features can partially mitigate this discrepancy, but cannot fully compensate for it-a point to which we return later. To empirically demonstrate the effectiveness of global normalization, we evaluate our model on part-of-speech tagging, syntactic dependency parsing and sentence compression (Section 4). Our model achieves state-of-the-art accuracy on all of these tasks, matching or outperforming LSTMs while being significantly faster. In particular for dependency parsing on the Wall Street Journal we achieve the best-ever published unlabeled attachment score of 94.61%.</p><p>As discussed in more detail in Section 5, we also outperform previous structured training approaches used for neural network transitionbased parsing.</p><p>Our ablation experiments show that we outperform  and  because we do global backpropagation training of all model parameters, while they fix the neural network parameters when training the global part of their model. We also outperform  despite using a smaller beam. To shed additional light on the label bias problem in practice, we provide a sentence compression example where the local model completely fails. We then demonstrate that a globally normalized parsing model without any lookahead features is almost as accurate as our best model, while a locally normalized model loses more than 10% absolute in accuracy because it cannot effectively incorporate evidence as it becomes available.</p><p>Finally, we provide an open-source implementation of our method, called SyntaxNet, 1 which we have integrated into the popular TensorFlow 2 framework.</p><p>We also provide a pre-trained, state-of-the art English dependency parser called "Parsey McParseface," which we tuned for a balance of speed, simplicity, and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>At its core, our model is an incremental transitionbased parser <ref type="bibr" target="#b33">(Nivre, 2006)</ref>. To apply it to different tasks we only need to adjust the transition system and the input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transition System</head><p>Given an input x, most often a sentence, we define:</p><p>• A set of states S(x).</p><p>• A special start state s † ∈ S(x).</p><p>• A set of allowed decisions A(s, x) for all s ∈ S(x). • A transition function t(s, d, x) returning a new state s ′ for any decision d ∈ A(s, x). We will use a function ρ <ref type="bibr">(s, d, x; θ)</ref> to compute the score of decision d in state s for input x. The vector θ contains the model parameters and we assume that ρ(s, d, x; θ) is differentiable with respect to θ.</p><p>In this section, for brevity, we will drop the dependence of x in the functions given above, simply writing S, A(s), t(s, d), and ρ <ref type="bibr">(s, d; θ)</ref>.</p><p>Throughout this work we will use transition systems in which all complete structures for the same input x have the same number of decisions n(x) (or n for brevity). In dependency parsing for example, this is true for both the arc-standard and arc-eager transition systems <ref type="bibr" target="#b33">(Nivre, 2006)</ref>, where for a sentence x of length m, the number of deci-sions for any complete parse is n(x) = 2 × m. 3 A complete structure is then a sequence of decision/state pairs (s 1 , d 1 ) . . . (s n , d n ) such that s 1 = s † , d i ∈ S(s i ) for i = 1 . . . n, and s i+1 = t(s i , d i ). We use the notation d 1:j to refer to a decision sequence d 1 . . . d j .</p><p>We assume that there is a one-to-one mapping between decision sequences d 1:j−1 and states s j : that is, we essentially assume that a state encodes the entire history of decisions. Thus, each state can be reached by a unique decision sequence from s † . <ref type="bibr">4</ref> We will use decision sequences d 1:j−1 and states interchangeably: in a slight abuse of notation, we define ρ(d 1:j−1 , d; θ) to be equal to ρ <ref type="bibr">(s, d; θ)</ref> where s is the state reached by the decision sequence d 1:j−1 .</p><p>The scoring function ρ(s, d; θ) can be defined in a number of ways. In this work, following Chen and Manning (2014), , and , we define it via a feedforward neural network as</p><formula xml:id="formula_0">ρ(s, d; θ) = φ(s; θ (l) ) · θ (d) .</formula><p>Here θ (l) are the parameters of the neural network, excluding the parameters at the final layer. θ (d) are the final layer parameters for decision d. φ(s; θ (l) ) is the representation for state s computed by the neural network under parameters θ <ref type="bibr">(l)</ref> . Note that the score is linear in the parameters θ <ref type="bibr">(d)</ref> . We next describe how softmax-style normalization can be performed at the local or global level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global vs. Local Normalization</head><p>In the Chen and Manning (2014) style of greedy neural network parsing, the conditional probability distribution over decisions d j given context d 1:j−1 is defined as Each Z L (d 1:j−1 ; θ) is a local normalization term. The probability of a sequence of decisions d 1:n is</p><formula xml:id="formula_1">p L (d 1:n ) = n j=1 p(d j |d 1:j−1 ; θ) = exp n j=1 ρ(d 1:j−1 , d j ; θ) n j=1 Z L (d 1:j−1 ; θ)</formula><p>.</p><p>(</p><p>Beam search can be used to attempt to find the maximum of Eq.</p><p>(2) with respect to d 1:n . The additive scores used in beam search are the logsoftmax of each decision, ln p(d j |d 1:j−1 ; θ), not the raw scores ρ(d 1:j−1 , d j ; θ).</p><p>In contrast, a Conditional Random Field (CRF) defines a distribution p G (d 1:n ) as follows:</p><formula xml:id="formula_3">p G (d 1:n ) = exp n j=1 ρ(d 1:j−1 , d j ; θ) Z G (θ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">Z G (θ) = d ′ 1:n ∈Dn exp n j=1 ρ(d ′ 1:j−1 , d ′ j ; θ)</formula><p>and D n is the set of all valid sequences of decisions of length n. Z G (θ) is a global normalization term. The inference problem is now to find Beam search can again be used to approximately find the argmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>Training data consists of inputs x paired with gold decision sequences d * 1:n . We use stochastic gradient descent on the negative log-likelihood of the data under the model. Under a locally normalized model, the negative log-likelihood is</p><formula xml:id="formula_5">L local (d * 1:n ; θ) = − ln p L (d * 1:n ; θ) = (4) − n j=1 ρ(d * 1:j−1 , d * j ; θ) + n j=1 ln Z L (d * 1:j−1 ; θ),</formula><p>whereas under a globally normalized model it is</p><formula xml:id="formula_6">L global (d * 1:n ; θ) = − ln p G (d * 1:n ; θ) = − n j=1 ρ(d * 1:j−1 , d * j ; θ) + ln Z G (θ).<label>(5)</label></formula><p>A significant practical advantange of the locally normalized cost Eq. (4) is that the local partition function Z L and its derivative can usually be computed efficiently. In contrast, the Z G term in Eq. (5) contains a sum over d ′ 1:n ∈ D n that is in many cases intractable.</p><p>To make learning tractable with the globally normalized model, we use beam search and early updates <ref type="bibr" target="#b10">(Collins and Roark, 2004;</ref>. As the training sequence is being decoded, we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step j, a stochastic gradient step is taken on the following objective:</p><formula xml:id="formula_7">L global−beam (d * 1:j ; θ) = − j i=1 ρ(d * 1:i−1 , d * i ; θ) + ln d ′ 1:j ∈B j exp j i=1 ρ(d ′ 1:i−1 , d ′ i ; θ).(6)</formula><p>Here the set B j contains all paths in the beam at step j, together with the gold path prefix d * 1:j . It is straightforward to derive gradients of the loss in Eq. (6) and to back-propagate gradients to all levels of a neural network defining the score ρ(s, d; θ). If the gold path remains in the beam throughout decoding, a gradient step is performed using B n , the beam at the end of decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Label Bias Problem</head><p>Intuitively, we would like the model to be able to revise an earlier decision made during search, when later evidence becomes available that rules out the earlier decision as incorrect. At first glance, it might appear that a locally normalized model used in conjunction with beam search or exact search is able to revise earlier decisions. However the label bias problem (see <ref type="bibr" target="#b8">Bottou (1991)</ref>, <ref type="bibr" target="#b11">Collins (1999)</ref> pages 222-226, <ref type="bibr" target="#b26">Lafferty et al. (2001)</ref>, <ref type="bibr">Bottou and LeCun (2005)</ref>, Smith and Johnson <ref type="formula" target="#formula_14">(2007)</ref>) means that locally normalized models often have a very weak ability to revise earlier decisions.</p><p>This section gives a formal perspective on the label bias problem, through a proof that globally normalized models are strictly more expressive than locally normalized models. The theorem was originally proved 5 by <ref type="bibr">Smith and Johnson (2007)</ref>.</p><p>The example underlying the proof gives a clear illustration of the label bias problem. 6 Global Models can be Strictly More Expressive than Local Models Consider a tagging problem where the task is to map an input sequence x 1:n to a decision sequence d 1:n . First, consider a locally normalized model where we restrict the scoring function to access only the first i input symbols x 1:i when scoring decision d i . We will return to this restriction soon. The scoring function ρ can be an otherwise arbitrary function of the tuple d 1:i−1 , d i , x 1:i :</p><formula xml:id="formula_8">p L (d 1:n |x 1:n ) = n i=1 p L (d i |d 1:i−1 , x 1:i ) = exp n i=1 ρ(d 1:i−1 , d i , x 1:i ) n i=1 Z L (d 1:i−1 , x 1:i )</formula><p>.</p><p>Second, consider a globally normalized model</p><formula xml:id="formula_9">p G (d 1:n |x 1:n ) = exp n i=1 ρ(d 1:i−1 , d i , x 1:i ) Z G (x 1:n ) .</formula><p>This model again makes use of a scoring function</p><formula xml:id="formula_10">ρ(d 1:i−1 , d i , x 1:i ) restricted to the first i input sym- bols when scoring decision d i .</formula><p>Define P L to be the set of all possible distributions p L (d 1:n |x 1:n ) under the local model obtained as the scores ρ vary. Similarly, define P G to be the set of all possible distributions p G (d 1:n |x 1:n ) under the global model. Here a "distribution" is a function from a pair (x 1:n , d 1:n ) to a probability p(d 1:n |x 1:n ). Our main result is the following: Theorem 3.1 See also <ref type="bibr">Smith and Johnson (2007)</ref>.</p><formula xml:id="formula_11">P L is a strict subset of P G , that is P L P G .</formula><p>To prove this we will first prove that P L ⊆ P G . This step is straightforward. We then show that P G P L ; that is, there are distributions in P G that are not in P L . The proof that P G P L gives a clear illustration of the label bias problem.</p><p>Proof that P L ⊆ P G : We need to show that for any locally normalized distribution p L , we can 6 Smith and Johnson (2007) cite Michael Collins as the source of the example underlying the proof. Note that the theorem refers to conditional models of the form p(d1:n|x1:n) with global or local normalization. Equivalence (or non-equivalence) results for joint models of the form p(d1:n, x1:n) are quite different: for example results from <ref type="bibr" target="#b9">Chi (1999)</ref> and <ref type="bibr" target="#b0">Abney et al. (1999)</ref> imply that weighted context-free grammars (a globally normalized joint model) and probabilistic context-free grammars (a locally normalized joint model) are equally expressive.</p><p>construct a globally normalized model p G such that p G = p L . Consider a locally normalized model with scores ρ(d 1:i−1 , d i , x 1:i ). Define a global model p G with scores</p><formula xml:id="formula_12">ρ ′ (d 1:i−1 , d i , x 1:i ) = log p L (d i |d 1:i−1 , x 1:i ).</formula><p>Then it is easily verified that</p><formula xml:id="formula_13">p G (d 1:n |x 1:n ) = p L (d 1:n |x 1:n )</formula><p>for all x 1:n , d 1:n .</p><p>In proving P G P L we will use a simple problem where every example seen in training or test data is one of the following two tagged sentences:</p><formula xml:id="formula_14">x 1 x 2 x 3 = a b c, d 1 d 2 d 3 = A B C x 1 x 2 x 3 = a b e, d 1 d 2 d 3 = A D E<label>(7)</label></formula><p>Note that the input x 2 = b is ambiguous: it can take tags B or D. This ambiguity is resolved when the next input symbol, c or e, is observed. Now consider a globally normalized model, where the scores ρ(d 1:i−1 , d i , x 1:i ) are defined as follows.</p><p>Define T as the set</p><formula xml:id="formula_15">{(A, B), (B, C), (A, D), (D, E)} of bigram tag transitions seen in the data. Similarly, define E as the set {(a, A), (b, B), (c, C), (b, D)</formula><p>, (e, E)} of (word, tag) pairs seen in the data. We define</p><formula xml:id="formula_16">ρ(d 1:i−1 , d i , x 1:i ) (8) = α × (d i−1 , d i ) ∈ T + α × (x i , d i ) ∈ E</formula><p>where α is the single scalar parameter of the model, and π = 1 if π is true, 0 otherwise.</p><p>Proof that P G P L : We will construct a globally normalized model p G such that there is no locally normalized model such that p L = p G .</p><p>Under the definition in Eq. <ref type="formula">(8)</ref>, it is straightforward to show that</p><formula xml:id="formula_17">lim α→∞ p G (A B C|a b c) = lim α→∞ p G (A D E|a b e) = 1.</formula><p>In contrast, under any definition for</p><formula xml:id="formula_18">ρ(d 1:i−1 , d i , x 1:i ), we must have p L (A B C|a b c) + p L (A D E|a b e) ≤ 1 (9) This follows because p L (A B C|a b c) = p L (A|a) × p L (B|A, a b) × p L (C|A B, a b c) and p L (A D E|a b e) = p L (A|a) × p L (D|A, a b) × p L (E|A D, a b e).</formula><p>The inequality p L (B|A, a b) + p L (D|A, a b) ≤ 1 then immediately implies Eq. (9).</p><p>It follows that for sufficiently large values of α, we have p G (A B C|a b c) + p G (A D E|a b e) &gt; 1, and given Eq. (9) it is impossible to define a locally normalized model with</p><formula xml:id="formula_19">p L (A B C|a b c) = p G (A B C|a b c) and p L (A D E|a b e) = p G (A D E|a b e).</formula><p>Under the restriction that scores ρ(d 1:i−1 , d i , x 1:i ) depend only on the first i input symbols, the globally normalized model is still able to model the data in Eq. <ref type="formula" target="#formula_14">(7)</ref>, while the locally normalized model fails (see Eq. 9). The ambiguity at input symbol b is naturally resolved when the next symbol (c or e) is observed, but the locally normalized model is not able to revise its prediction.</p><p>It is easy to fix the locally normalized model for the example in Eq. (7) by allowing scores ρ(d 1:i−1 , d i , x 1:i+1 ) that take into account the input symbol x i+1 . More generally we can have a model of the form ρ(d 1:i−1 , d i , x 1:i+k ) where the integer k specifies the amount of lookahead in the model. Such lookahead is common in practice, but insufficient in general. For every amount of lookahead k, we can construct examples that cannot be modeled with a locally normalized model by duplicating the middle input b in (7) k + 1 times. Only a local model with scores ρ(d 1:i−1 , d i , x 1:n ) that considers the entire input can capture any distribution p(d 1:n |x 1:n ): in this case the decomposition p L (d 1:n |x 1:n ) = n i=1 p L (d i |d 1:i−1 , x 1:n ) makes no independence assumptions.</p><p>However, increasing the amount of context used as input comes at a cost, requiring more powerful learning algorithms, and potentially more training data. For a detailed analysis of the tradeoffs between structural features in CRFs and more powerful local classifiers without structural constraints, see <ref type="bibr" target="#b29">Liang et al. (2008)</ref>; in these experiments local classifiers are unable to reach the performance of CRFs on problems such as parsing and named entity recognition where structural constraints are important. Note that there is nothing to preclude an approach that makes use of both global normalization and more powerful scoring functions ρ(d 1:i−1 , d i , x 1:n ), obtaining the best of both worlds. The experiments that follow make use of both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To demonstrate the flexibility and modeling power of our approach, we provide experimental results on a diverse set of structured prediction tasks. We apply our approach to POS tagging, syntactic dependency parsing, and sentence compression.</p><p>While directly optimizing the global model defined by Eq. (5) works well, we found that training the model in two steps achieves the same precision much faster: we first pretrain the network using the local objective given in Eq. (4), and then perform additional training steps using the global objective given in Eq. (6). We pretrain all layers except the softmax layer in this way. We purposefully abstain from complicated hand engineering of input features, which might improve performance further <ref type="bibr" target="#b16">(Durrett and Klein, 2015)</ref>.</p><p>We use the training recipe from  for each training stage of our model. Specifically, we use averaged stochastic gradient descent with momentum, and we tune the learning rate, learning rate schedule, momentum, and early stopping time using a separate held-out corpus for each task. We tune again with a different set of hyperparameters for training with the global objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Part of Speech Tagging</head><p>Part of speech (POS) tagging is a classic NLP task, where modeling the structure of the output is important for achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data &amp; Evaluation.</head><p>We conducted experiments on a number of different datasets: (1) the English Wall Street Journal (WSJ) part of the Penn Treebank <ref type="bibr" target="#b31">(Marcus et al., 1993)</ref> with standard POS tagging splits; (2) the English "Treebank Union" multi-domain corpus containing data from the OntoNotes corpus version 5 <ref type="bibr" target="#b24">(Hovy et al., 2006)</ref>, the English Web Treebank <ref type="bibr">(Petrov and McDonald, 2012)</ref>, and the updated and corrected Question Treebank <ref type="bibr">(Judge et al., 2006)</ref> with identical setup to ; and (3) the CoNLL '09 multi-lingual shared task <ref type="bibr" target="#b20">(Hajič et al., 2009)</ref>. Model Configuration. Inspired by the integrated POS tagging and parsing transition system of <ref type="bibr" target="#b4">Bohnet and Nivre (2012)</ref>, we employ a simple transition system that uses only a SHIFT action and predicts the POS tag of the current word on the buffer as it gets shifted to the stack. We extract the following features on a window ±3 tokens centered at the current focus token: word, cluster, character n-gram up to length 3. We also extract the tag predicted for the previous 4 tokens. The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>In <ref type="table">Table 1</ref> we compare our model to a linear CRF and to the compositional characterto-word LSTM model of . The CRF is a first-order linear model with exact inference and the same emission features as our model. It additionally also has transition features of the word, cluster and character n-gram up to length 3 on both endpoints of the transition. The results for  were solicited from the authors. Our local model already compares favorably against these methods on average. Using beam search with a locally normalized model does not help, but with global normalization it leads to a 7% reduction in relative error, empirically demonstrating the effect of label bias. The set of character ngrams feature is very important, increasing average accuracy on the CoNLL'09 datasets by about 0.5% absolute. This shows that characterlevel modeling can also be done with a simple feed-forward network without recurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dependency Parsing</head><p>In dependency parsing the goal is to produce a directed tree representing the syntactic structure of the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data &amp; Evaluation.</head><p>We use the same corpora as in our POS tagging experiments, except that we use the standard parsing splits of the WSJ. To avoid over-fitting to the development set (Sec. 22), we use Sec. 24 for tuning the hyperparameters of our models. We convert the English constituency trees to Stanford style dependencies <ref type="bibr" target="#b14">(De Marneffe et al., 2006)</ref> using version 3.3.0 of the converter. For English, we use predicted POS tags (the same POS tags are used for all models) and exclude punctuation from the evaluation, as is standard. For the CoNLL '09 datasets we follow standard practice and include all punctuation in the evaluation. We follow  and use our own predicted POS tags so that we can include a k-best tag feature (see below) but use the supplied predicted morphological features. We report unlabeled and labeled attachment scores (UAS/LAS).</p><p>Model Configuration. Our model configuration is basically the same as the one originally proposed by <ref type="bibr">Chen and Manning (2014)</ref> and then refined by . In particular, we use the arc-standard transition system and extract the same set of features as prior work: words, part of speech tags, and dependency arcs and labels in the surrounding context of the state, as well as k-best tags as proposed by . We use two hidden layers of 1,024 dimensions each. <ref type="table" target="#tab_2">Tables 2 and 3</ref> show our final parsing results and a comparison to the best systems from the literature. We obtain the best ever published results on almost all datasets, including the WSJ. Our main results use the same pretrained word embeddings as  and , but no tri-training. When we artificially restrict ourselves to not use pretrained word embeddings, we observe only a modest drop of ∼0.5% UAS; for example, training only on the WSJ yields 94.08% UAS and 92.15% LAS for our global model with a beam of size 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>Even though we do not use tri-training, our model compares favorably to the 94.26% LAS and 92.41% UAS reported by  with tri-training. As we show in Sec. 5, these gains can be attributed to the full backpropagation training that differentiates our approach from that of  and . Our results also significantly outperform the LSTM-based approaches of  and    .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentence Compression</head><p>Our final structured prediction task is extractive sentence compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data &amp; Evaluation.</head><p>We follow <ref type="bibr" target="#b18">Filippova et al. (2015)</ref>, where a large news collection is used to heuristically generate compression instances. Our final corpus contains about 2.3M compression instances: we use 2M examples for training, 130k for development and 160k for the final test. We report per-token F1 score and per-sentence accuracy (A), i.e. percentage of instances that fully match the golden compressions. Following <ref type="bibr" target="#b18">Filippova et al. (2015)</ref> we also run a human evaluation on 200 sentences where we ask the raters to score compressions for readability (read) and informativeness (info) on a scale from 0 to 5.</p><p>Model Configuration. The transition system for sentence compression is similar to POS tagging: we scan sentences from left-to-right and label each token as keep or drop. We extract features from words, POS tags, and dependency labels from a window of tokens centered on the in-  put, as well as features from the history of predictions. We use a single hidden layer of size 400.</p><p>Results. <ref type="table" target="#tab_5">Table 4</ref> shows our sentence compression results. Our globally normalized model again significantly outperforms the local model. Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5. We also compare to the sentence compression system from <ref type="bibr" target="#b18">Filippova et al. (2015)</ref>, a 3-layer stacked LSTM which uses dependency label information. The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings, but our model is roughly 100× faster.</p><p>All compressions kept approximately 42% of the tokens on average and all the models are significantly better than the automatic extractions (p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We derived a proof for the label bias problem and the advantages of global models. We then emprirically verified this theoretical superiority by demonstrating state-of-the-art performance on three different tasks. In this section we situate and compare our model to previous work and provide two examples of the label bias problem in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Related Neural CRF Work</head><p>Neural network models have been been combined with conditional random fields and globally normalized models before. <ref type="bibr" target="#b7">Bottou et al. (1997)</ref> and Le <ref type="bibr" target="#b27">Cun et al. (1998)</ref> describe global training of neural network models for structured prediction problems. <ref type="bibr" target="#b35">Peng et al. (2009)</ref> add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures. <ref type="bibr" target="#b43">Yao et al. (2014)</ref> and <ref type="bibr" target="#b46">Zheng et al. (2015)</ref> introduce recurrence into the model and  finally combine CRFs and LSTMs. These neural CRF models are limited to sequence labeling tasks where exact inference is possible, while our model works well when exact inference is intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Related Transition-Based Parsing Work</head><p>For early work on neural-networks for transition-based parsing, see <ref type="bibr" target="#b21">Henderson (2003;</ref><ref type="bibr" target="#b22">2004)</ref>. Our work is closest to the work of ,  and <ref type="bibr" target="#b41">Watanabe and Sumita (2015)</ref>; in these approaches global normalization is added to the local model of <ref type="bibr">Chen and Manning (2014)</ref>.</p><p>Empirically,  achieves the best performance, even though their model keeps the parameters of the locally normalized neural network fixed and only trains a perceptron that uses the activations as features. Their model is therefore limited in its ability to revise the predictions of the locally normalized model. In <ref type="table" target="#tab_7">Table 5</ref> we show that full backpropagation training all the way to the word embeddings is very important and significantly contributes to the performance of our model. We also compared training under the CRF objective  with a Perceptron-like hinge loss between the gold and best elements of the beam. When we limited the backpropagation depth to training only the top layer θ (d) , we found negligible differences in accuracy: 93.20% and 93.28% for the CRF objective and hinge loss respectively. However, when training with full backpropagation the CRF accuracy is 0.2% higher and training converged more than 4× faster.  perform full backpropagation training like us, but even with a much larger beam, their performance is significantly lower than ours. We also apply our model to two additional tasks, while they experiment only with dependency parsing. Finally, <ref type="bibr" target="#b41">Watanabe and Sumita (2015)</ref> introduce recurrent components and additional techniques like maxviolation updates for a corresponding constituency parsing model. In contrast, our model does not require any recurrence or specialized training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Label Bias in Practice</head><p>We observed several instances of severe label bias in the sentence compression task. Although using beam search with the local model outperforms greedy inference on average, beam search leads the local model to occasionally produce empty compressions <ref type="table" target="#tab_9">(Table 6</ref>). It is important to note that these are not search errors: the empty compression has higher probability under p L than the prediction from greedy inference. However, the more expressive globally normalized model does not suffer from this limitation, and correctly gives the empty compression almost zero probability.</p><p>We also present some empirical evidence that the label bias problem is severe in parsing. We trained models where the scoring functions in parsing at position i in the sentence are limited to  A number of authors have considered modified training procedures for greedy models, or for locally normalized models. <ref type="bibr" target="#b13">Daumé III et al. (2009)</ref> introduce Searn, an algorithm that allows a classifier making greedy decisions to become more robust to errors made in previous decisions. <ref type="bibr" target="#b19">Goldberg and Nivre (2013)</ref> describe improvements to a greedy parsing approach that makes use of methods from imitation learning <ref type="bibr" target="#b37">(Ross et al., 2011)</ref> to augment the training set. Note that these methods are focused on greedy models: they are unlikely to solve the label bias problem when used in conjunction with beam search, given that the problem is one of expressivity of the underlying model. More recent work <ref type="bibr" target="#b44">(Yazdani and Henderson, 2015;</ref><ref type="bibr" target="#b39">Vaswani and Sagae, 2016)</ref> has augmented locally normalized models with correctness probabilities or error states, effectively adding a step after every decision where the probability of correctness of the resulting structure is evaluated. This gives con-siderable gains over a locally normalized model, although performance is lower than our full globally normalized approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a simple and yet powerful model architecture that produces state-of-the-art results for POS tagging, dependency parsing and sentence compression. Our model combines the flexibility of transition-based algorithms and the modeling power of neural networks. Our results demonstrate that feed-forward network without recurrence can outperform recurrent models such as LSTMs when they are trained with global normalization. We further support our empirical findings with a proof showing that global normalization helps the model overcome the label bias problem from which locally normalized models suffer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1:j−1 , d j ; θ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.</figDesc><table><row><cell></cell><cell>Catalan</cell><cell>Chinese</cell><cell>Czech</cell><cell>English</cell><cell>German</cell><cell>Japanese</cell><cell>Spanish</cell></row><row><cell>Method</cell><cell>UAS LAS</cell><cell>UAS LAS</cell><cell>UAS LAS</cell><cell>UAS LAS</cell><cell>UAS LAS</cell><cell>UAS LAS</cell><cell>UAS LAS</cell></row><row><cell>Best Shared Task Result</cell><cell>-87.86</cell><cell>-79.17</cell><cell>-80.38</cell><cell>-89.88</cell><cell>-87.48</cell><cell>-92.57</cell><cell>-87.64</cell></row><row><cell>Ballesteros et al. (2015)</cell><cell cols="7">90.22 86.42 80.64 76.52 79.87 73.62 90.56 88.01 88.83 86.10 93.47 92.55 90.38 86.59</cell></row><row><cell cols="8">Zhang and McDonald (2014) 91.41 87.91 82.87 78.57 86.62 80.59 92.69 90.01 89.88 87.38 92.82 91.87 90.82 87.34</cell></row><row><cell>Lei et al. (2014)</cell><cell cols="7">91.33 87.22 81.67 76.71 88.76 81.77 92.75 90.00 90.81 87.81 94.04 91.84 91.16 87.38</cell></row><row><cell>Bohnet and Nivre (2012)</cell><cell cols="7">92.44 89.60 82.52 78.51 88.82 83.73 92.87 90.60 91.37 89.38 93.67 92.63 92.24 89.60</cell></row><row><cell>Alberti et al. (2015)</cell><cell cols="7">92.31 89.17 83.57 79.90 88.45 83.57 92.70 90.56 90.58 88.20 93.99 93.10 92.26 89.33</cell></row><row><cell>Our Local (B=1)</cell><cell cols="7">91.24 88.21 81.29 77.29 85.78 80.63 91.44 89.29 89.12 86.95 93.71 92.85 91.01 88.14</cell></row><row><cell>Our Local (B=16)</cell><cell cols="5">91.91 88.93 82.22 78.26 86.25 81.28 92.16 90.05 89.53 87.4</cell><cell cols="2">93.61 92.74 91.64 88.88</cell></row><row><cell>Our Global (B=16)</cell><cell cols="7">92.67 89.83 84.72 80.85 88.94 84.56 93.22 91.23 90.91 89.15 93.65 92.84 92.62 89.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Final CoNLL '09 dependency parsing test set results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Sentence compression results on News data. Automatic refers to application of the same automatic extraction rules used to generate the News training corpus.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>WSJ dev set scores for successively deeper levels of backpropagation. The full parameter set corresponds to backpropagation all the way to the embeddings. Wi: hidden layer i weights.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>In Pakistan, former leader Pervez Musharraf has appeared in court for the first time, on treason charges. 0.13 0.05 Local (B=8) In Pakistan, former leader Pervez Musharraf has appeared in court for the first time, on treason charges. 0.16 &lt;10 −4 Global (B=8) In Pakistan, former leader Pervez Musharraf has appeared in court for the first time, on treason charges. 0.06 0.07</figDesc><table><row><cell>Method</cell><cell>Predicted compression</cell><cell>pL pG</cell></row><row><cell>Local (B=1)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Example sentence compressions where the label bias of the locally normalized model leads to a breakdown during beam search. The probability of each compression under the local (pL) and global (pG) models shows that only the global model can properly represent zero probability for the empty compression.considering only tokens x 1:i ; hence unlike the full parsing model, there is no ability to look ahead in the sentence when making a decision.7  The result for a greedy model under this constraint is 76.96% UAS; for a locally normalized model with beam search is 81.35%; and for a globally normalized model is 93.60%. Thus the globally normalized model gets very close to the performance of a model with full lookahead, while the locally normalized model with a beam gives dramatically lower performance. In our final experiments with full lookahead, the globally normalized model achieves 94.01% accuracy, compared to 93.07% accuracy for a local model with beam search. Thus adding lookahead allows the local model to close the gap in performance to the global model; however there is still a significant difference in accuracy, which may in large part be due to the label bias problem.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://github.com/tensorflow/models/tree/master/syntaxnet 2 http://www.tensorflow.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5"> More precisely Smith and Johnson (2007)  prove the theorem for models with potential functions of the form ρ(di−1, di, xi); the generalization to potential functions of the form ρ(d1:i−1, di, x1:i) is straightforward.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">This setting may be important in some applications, where for example parse structures for sentence prefixes are required, or where the input is received one word at a time and online processing is beneficial.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Ling Wang for training his C2W part-of-speech tagger on our setup, and Emily Pitler, Ryan McDonald, Greg Coppola and Fernando Pereira for tremendously helpful discussions. Finally, we are grateful to all members of the Google Parsing Team.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Relating probabilistic grammars and automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="131" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing and tagging with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Alberti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1354" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved transitionbased parsing by modeling characters instead of words with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballesteros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Bohnet and Nivre2012</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph transformer networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bulletin of the International Statistical Institute (ISI)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Bottou and LeCun2005</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global training of document processing systems using graph transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="489" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Une approche théorique de lapprentissage connexionniste: Applicationsà la reconnaissance de la parole</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Manning2014] Danqi Chen and Christopher D. Manning</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
		<respStmt>
			<orgName>Universite de Paris XI</orgName>
		</respStmt>
	</monogr>
	<note>Doctoral dissertation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical properties of probabilistic context-free grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="131" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roark2004] Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04)</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal (MLJ)</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>[de Marneffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fifth International Conference on Language Resources and Evaluation</title>
		<meeting>Fifth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural conditional random fields</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>Artires2010] Trinh Minh Tri Do and Thierry Artires</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural crf parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
	<note>Durrett and Klein2015</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Katja Filippova, Enrique Alfonseca, Carlos A. Colmenares, Łukasz Kaiser, and Oriol Vinyals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
	<note>Sentence compression by deletion with lstms</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hajič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2009-01" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inducing history representations for broad coverage statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04)</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ontonotes: The 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Short Papers</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Questionbank: Creating a corpus of parse-annotated questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Judge et al.2006] John Judge, Aoife Cahill, and Josef van Genabith</editor>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bidirectional LSTM-CRF models for sequence tagging</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gradient based learning applied to document recognition. Proceedings of IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>[le Cun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure compilation: Trading structure for features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="592" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order non-projective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Inductive Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, Inc</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing in expected linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1419" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcdonald2012] Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mc-Donald</surname></persName>
		</author>
		<title level="m">Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">No-regret reductions for imitation learning and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weighted and probabilistic context-free grammars are equally expressive</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Johnson2007] Noah Smith and Mark Johnson</editor>
		<imprint>
			<biblScope unit="page" from="477" to="491" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient structured inference for transition-based parsing with neural networks and error states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="183" to="196" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Vaswani and Sagae2016</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Transition-based neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumita2015] Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1169" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recurrent conditional random field for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICASSP &apos;14</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Incremental recurrent neural network dependency parser with searchbased discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="142" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Enforcing structural diversity in cube-pruned dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Mcdonald2014</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mc-Donald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="656" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Endto-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A neural probabilistic structuredprediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
