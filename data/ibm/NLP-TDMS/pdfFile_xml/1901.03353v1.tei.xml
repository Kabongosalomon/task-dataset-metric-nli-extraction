<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
							<email>cyfu@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department of UNC at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhailo</forename><surname>Shvets</surname></persName>
							<email>mshvets@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department of UNC at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
							<email>aberg@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department of UNC at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is at: https://github.com/chengyangfu/ retinamask</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently two-stage detectors have surged ahead of single-shot detectors in the accuracy-vs-speed trade-off. Nevertheless single-shot detectors are immensely popular in embedded vision applications. This paper brings singleshot detectors up to the same level as current two-stage techniques. We do this by improving training for the stateof-the-art single-shot detector, RetinaNet, in three ways: integrating instance mask prediction for the first time, making the loss function adaptive and more stable, and including additional hard examples in training. We call the resulting augmented network RetinaMask. The detection component of RetinaMask has the same computational cost as the original RetinaNet, but is more accurate. COCO test-dev results are up to 41.4 mAP for RetinaMask-101 vs 39.1mAP for RetinaNet-101, while the runtime is the same during evaluation. Adding Group Normalization increases the performance of RetinaMask-101 to 41.7 mAP. Code is at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single-shot detectors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref> have become extremely popular in applications where speed and computational resources are important design considerations. These include embedded vision applications, self-driving cars, and mobile phone vision. Despite this intense usage, there has been little improvement in state-of-the-art performance for single-shot detectors, e.g. RetinaNet <ref type="bibr" target="#b24">[25]</ref>. When published in 2017, RetinaNet effectively cleaned up a range of work on single-shot detection, building on <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b27">[28]</ref> and introducing innovations in training that resulted in state-of-the-art performance in terms of the speed-versusaccuracy trade-off (sharing the frontier with YOLOv3 <ref type="bibr" target="#b30">[31]</ref> on the faster, lower-accuracy, end of the spectrum, and Mask R-CNN on the high end). While there have not been significant improvements in performance on top of Reti- Accuracy [mAP] <ref type="bibr" target="#b24">[25]</ref>RetinaNet-50 <ref type="bibr" target="#b24">[25]</ref>RetinaNet-101 <ref type="bibr" target="#b30">[31]</ref>YOLOv3 <ref type="bibr" target="#b17">[18]</ref>Mask R-CNN-101</p><p>RetinaMask-50</p><p>RetinaMask-101 <ref type="figure">Figure 1</ref>: Accuracy versus inference time on COCO test-dev. For fair comparison with RetinaNet, we train our RetinaMask models at {400, 500, 600, 700, 800} resolution without using multi-scale augmentation during training. Our best model, which achieves 42.6 mAP, can be found in <ref type="table" target="#tab_7">Table 5</ref>. Our improved versions, RetinaMask-50/101 respectively, are shown with blue/red square markers. The original RetinaNet-50/101 results are shown with blue/red circle markers. The state-of-the-art two-stage detector Mask R-CNN has improved since publication. We show three versions with green circle markers: original paper 38.5/195 detection+mask/M40 (mAP/ms/GPU), Detectron <ref type="bibr" target="#b0">[1]</ref> Caffe2 (40.0/119 detection only/P100), and Py-Torch <ref type="bibr" target="#b1">[2]</ref> (40.1/143 detection only/V100).</p><p>naNet, two-stage detectors have advanced over the intervening time, and now outperform RetinaNet on the speedvs-accuracy trade-off. Part of this improvement has been due to architectures like Mask R-CNN that allow training multiple prediction heads on top of the region proposal and bounding box prediction stages of the detector <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>In this paper we show how to improve the accuracy of state-of-the-art single-shot detectors (e.g. RetinaNet <ref type="bibr" target="#b24">[25]</ref>, SSD <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13]</ref>) largely by adding the task of instance mask prediction during training, but also by introducing an adaptive loss that improves robustness to parameter choice during training, and including more difficult examples in training. We call the resulting system RetinaMask to make clear that its training included the additional task of instance mask prediction, which in the past had been added to twostage but not to single-shot detectors. These modifications involve more work during training, but it is possible to evaluate just the detection part of RetinaMask, which has exactly the same computational cost as the original RetinaNet detector-our modifications to training result in detectors with better accuracy at the same computational cost.</p><p>Because we keep the structure of the detector at test time unchanged, our approach can be directly applied to the wide range of embedded applications that choose single-shot detectors over two-stage detectors. This is a subtle point, but today (before this paper), two-stage detectors significantly beat single stage detectors on the speed-vs-accuracy tradeoff on a standard desktop/workstation + high-end GPU configurations. However, when adapting these implementations to lower-power embedded devices the cost of resampling (e.g. ROI-Align) for the second stage is often exacerbated by communications between the hardware that is used to accelerate convolutions and the CPU. Single-shot approaches avoid this extra implementation challenge, which makes them popular in many implementations.</p><p>This allusion to implementation overhead for a variety of special purpose architectures brings up one of the difficulties of keeping track of progress on the speed-vs-accuracy trade-off in detection. In addition to the algorithms, the hardware itself and software libraries are constantly evolving year over year. These complexities are small relative to the variety of software and architectures for embedded systems. The main point of our work is to show a significant improvement in the accuracy of single-shot detectors with keeping the same computational cost as the original network during inference. Note the the plot in <ref type="figure">Figure 1</ref> shows an improvement in speed as well, but this is partly due to the next generation of libraries and slightly different hardware. We expect the improvements demonstrated in this paper to be applicable to a wide range of single-shot detector implementations.</p><p>In summary, this paper shows a significant increase in the accuracy of state-of-the-art single-shot detectors by introducing three new techniques that improve training:</p><p>1. Adding a novel instance mask prediction head to the single-shot RetinaNet detector during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A new self-adjusting loss function that improves robustness during training.</p><p>3. Including more of the positive examples in training, even those with low overlap.</p><p>Each of these contributions is analyzed with ablation studies, and together they provide a large boost to the accuracy of RetinaNet, bringing it up to state-of-the-art accuracy again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review recent developments in object detection using two-stage and single-shot techniques, general techniques for improving detection, and work on integrating instance mask prediction with detection. Two-Stage Detectors: Two-stage detectors follow a long line of reasoning in computer vision about grouping and perception. They first propose potential object locations in an image-region proposals-and then apply a classifier to these regions to score potential detections. Earlier slidingwindow approaches ran into scaling problems as the number of potential windows combined with the number of models became unmanageable <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. Selective Search <ref type="bibr" target="#b36">[37]</ref> allowed more expensive and accurate bag-of-visual-words (BoVW) features to be considered by using low-level vision to identify a smaller number of potential locations that needed to be evaluated. A transition to deep learning models was done with R-CNN <ref type="bibr" target="#b14">[15]</ref>, which used a convolutional neural network to replace the BoVW, resulting in a significant accuracy improvement. SPPNet <ref type="bibr" target="#b22">[23]</ref> sped up this process by direct region pooling on the feature layers instead of repetitive image cropping. Then Fast R-CNN <ref type="bibr" target="#b13">[14]</ref>, and Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> sped up detection even further and increased accuracy by replacing Selective Search with a Regional Proposal Network. Faster R-CNN was also the first end-to-end trainable deep learning model for object detection. R-FCN <ref type="bibr" target="#b6">[7]</ref> used position-sensitive features and ROIpooling to make a fully convolutional network design. Single-Shot Detectors: In contrast to two-stage detectors, single-shot detectors avoid image or feature re-sampling. OverFeat <ref type="bibr" target="#b33">[34]</ref> and DeepMultiBox <ref type="bibr" target="#b9">[10]</ref> were early examples. Then, YOLO <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> and SSD <ref type="bibr" target="#b27">[28]</ref> popularized the single-shot approach by demonstrating models that ran in real-time with good accuracy. More recently, Reti-naNet <ref type="bibr" target="#b24">[25]</ref> proposed Focal Loss to address the extreme class imbalance problem between target and background, and cleaned up a number of design aspects for single-shot detection. Techniques for Improving Detectors: Several techniques for improving detection apply to both Single-Shot and Two-Stage Detectors. First, cleaner training data often helps achieve faster convergence and higher final accuracy. Online Hard Negative Sampling <ref type="bibr" target="#b34">[35]</ref> uses non-maximum suppression (nms) during training to provide negative examples diversity. Second, certain model modifications add context information to predictions. SSD <ref type="bibr" target="#b27">[28]</ref> and MS-CNN <ref type="bibr" target="#b4">[5]</ref> both predict instances across features layers of different resolutions. DSSD <ref type="bibr" target="#b12">[13]</ref>, Feature Pyramid Network <ref type="bibr" target="#b23">[24]</ref> and TDN <ref type="bibr" target="#b35">[36]</ref> combine feature layers in a top-down manner to enrich the context of coarser features, strengthening the feature representation for better detection. Also, additional training information is beneficial for detectors. BlitzNet <ref type="bibr" target="#b8">[9]</ref> augments SSD with a semantic segmentation prediction branch, combining these two tasks in a single network, which results in higher detection accuracy. Instance Segmentation: As object detection matured and demonstrated strong accuracy and high speed, the research community started shifting attention to the more detailed task of instance segmentation. In addition to generating a tight bounding box for each object, instance segmentation requires a pixel-level mask for that object. The COCO <ref type="bibr" target="#b25">[26]</ref> dataset established a recognized benchmark for this task by holding the Instance Segmentation Challenge starting in 2015. Current state-of-the-art instance segmentation approaches are based on two-stage detectors, adding an instance mask prediction module after detection. MNC <ref type="bibr" target="#b5">[6]</ref> breaks down the Instance Segmentation into three stages, namely object detection, class-agnostic mask prediction, and mask categorization. FCIS <ref type="bibr" target="#b39">[40]</ref> extends the idea of R-FCN <ref type="bibr" target="#b6">[7]</ref> by using position-sensitive score maps for mask prediction. The recent Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> identifies the core issue for mask prediction in ROI pooling box misalignment, which arises from pooling box quantization over the coarse feature scale. Bilinear interpolation is introduced in their ROI-Align module to fix this issue. Mask R-CNN has been further improved in the Path Aggregation Network <ref type="bibr" target="#b26">[27]</ref>, using the ROI-Align operation on multiple feature layers to aggregate better features for instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>We start with the RetinaNet settings in Detectron 1 and rebuild it in PyTorch to form our baseline. Then, we introduce the following modifications to the baseline settings: best matching policy (Sec. 3.1), and modified bounding box regression loss (Sec. 3.2). Finally, we describe how to add the mask prediction module on top of RetinaNet (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Best Matching Policy</head><p>In the bounding box matching stage, the RetinaNet policy is as follows. All anchor boxes that have an intersectionover-union (IOU) overlap with a ground truth object greater than 0.5, are considered positive examples. If the overlap is less than 0.4, the anchor boxes are assigned a negative label. All anchors for which the overlap falls between 0.4 and 0.5 are not used in the training. However, there exists an exceptional case for which the assignment can be improved. Specifically, some of the ground truth objects' aspect ratios are outliers, with one side much longer than the other. Thus, no anchor box can be matched to those according to the RetinaNet strategy. For each of these ground 1 https://github.com/facebookresearch/Detectron truth boxes we propose to find its best matching anchor box, relaxing the overlapping IOU threshold. We provide an ablation study using different thresholds on the best matching anchors. The results suggests that using best matching anchor with any nonzero overlap gives the best accuracy (notice that such anchors always exists, because single-shot anchors are densely sampled). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-Adjusting Smooth L1 Loss</head><p>Smooth L1 Loss for object detection was originally proposed in Fast R-CNN <ref type="bibr" target="#b13">[14]</ref> to make bounding box regression more robust by replacing the excessively strict L2 Loss. Current state-of-the-art methods such as Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>, R-FCN <ref type="bibr" target="#b6">[7]</ref> and SSD <ref type="bibr" target="#b27">[28]</ref> still use this loss.</p><p>In Smooth L1 Loss described in Equation 1, a point β splits the positive axis range into two parts: L2 loss is used for targets in range [0, β], and L1 loss is used beyond β to avoid over-penalizing outliers. The overall function is smooth (continuous, together with its derivative), as illustrated in <ref type="figure">Figure 1</ref>. However, the choice of control point(β) is heuristic and is usually done by hyper parameter search.</p><formula xml:id="formula_0">f (x) = 0.5 x 2 β , if |x| &lt; β |x| − 0.5β, otherwise<label>(1)</label></formula><p>We propose an improved version of Smooth L1 called Self-Adjusting Smooth L1 Loss. Inside the loss function, the running mean and variance of the absolute loss are recorded. We use the running minibatch mean and variance with momentum=0.9 to update these two parameters.</p><p>Then, the parameters are used to calculate the control point. Specifically, the control point is chosen to be equal to the difference between the running mean and running variance (µ R − σ 2 R ), and the value is clipped to a range [0,β], as can be seen in Equation 2. Clipping is used because running mean is unstable during training, as the number of positive examples in each batch is different. <ref type="figure" target="#fig_2">Figure 3</ref> shows the running mean of L1 loss for the x offset and for width adjustment prediction in bounding box regression. We observe a decreasing trend for both during training. </p><formula xml:id="formula_1">µ B = 1 n n i=1 |x i |, σ 2 B = 1 n n i=1 (|x i | − µ B ) 2 µ R = µ R * m + µ B * (1 − m) σ 2 R = σ 2 R * m + σ 2 B * (1 − m) β = max(0, min(β, µ R − σ 2 R ))<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mask Prediction Module</head><p>In order to add the mask prediction module, single-shot detection predictions are treated as mask proposals. After running RetinaNet for bounding box predictions, we extract the top N scored predictions. Then, we distribute these mask proposals to sample features from the appropriate layers of the FPN according to <ref type="bibr">Equation 3</ref> proposed in FPN <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the assignment process. We use the following equation to determine which feature map, P k to sample from for predicting the instance mask:</p><formula xml:id="formula_2">k = k 0 + log 2 √ wh/224 ,<label>(3)</label></formula><p>where k 0 = 4, and w, h are the width and height of the detection. If the size is smaller than 224 2 , it will be assigned to feature layer P 3 , between 224 2 to 448 2 is assigned to P 4 , and larger than 448 2 is assigned to P 5 . In our final model, we use the {P 3 , P 4 , P 5 , P 6 , P 7 } 2 feature layers for bounding box predictions and {P 3 , P 4 , P 5 } feature layers for mask prediction. In our ablation study, we analyze the impact of using more feature layers for mask proposals assignment, showing that this does not give any performance boost. Network: <ref type="figure" target="#fig_4">Figure 4</ref> shows a high-level overview of the model. We use ResNet-50 and ResNet-101 as backbone models in our experiments, freezing all of the Batch Normalization layers. Following the Feature Pyramid Network <ref type="bibr" target="#b23">[24]</ref> setting, we add extra layers (P 6 and P 7 ) and form top-down connections (P 5 , P 4 , and P 3 ). The dimensionality of each of the Feature Pyramid layers (P 3 , . . . , P 7 ) is set to 256. The bounding box classification head consists of 4 convolutional layers (conv3x3(256) + ReLU) and uses 1 convolution (conv3x3(number of anchors * number of classes)) with point-wise sigmoid nonlinearities. For bounding box regression, we adopt the class-agnostic setting. We also run 4 convolutional layers (conv3x3(256) + ReLU) and 1 output layer (conv3x3(number of anchors * 4)) to refine the anchors. Once the bounding boxes are predicted, we aggregate them and distribute to the Feature Pyramid layers, as discussed above. The ROI-Align operation is performed at the assigned feature layers, yielding 14x14 resolution features, which are fed into 4 consequent convolutional layers (conv3x3), and a single transposed convolutional layer (convtranspose2d 2x2) that upsamples the map to 28x28 resolution. Finally, a prediction convolutional layer (conv1x1) is applied. We predict classspecific masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>To train RetinaNet, we follow the settings in the original paper. Images are resized to make the shorter side equal to 800 pixels, while limiting the longer side to 1333 pixels.</p><p>We use batch size of 16 images, weight decay 10 −4 , momentum 0.9, and train for 90k iterations with the base learning rate of 0.01, dropped to 0.001 and 0.0001 at iterations 60k and 80k. We train our models on servers with 4 1080Ti GPUs. For some models (e.g. a ResNet-101-FPN backbone), there is not enough GPU memory for this batch size. If this is the case, we follow the Linear Scaling policy proposed in <ref type="bibr" target="#b15">[16]</ref> and reduce the batch size (increasing the number of training iterations and reducing the learning rate accordingly). In order to train with the Mask Prediction Module, we extend the number of training iterations by a factor of 1.5x, or 2x, during multi-scale training. The multi-scale training is done at scales {640, 800, 1200}.</p><p>Thus, for 1.5x training, the number of iterations is set to 135k, and learning rate drops occur at iterations 90k, and 120k. For 2x, we train the network 180k iterations, and drop the learning rate at 120k, 160k. The training time is ≈ 56 hours when using ResNet-50 with 1.5x train iteration, while for ResNet-101 it goes up to ≈ 75 hours, for the same number of iterations.</p><p>The anchor boxes span 5 scales and 9 combinations (3 aspect ratios [0.5, 1, 2] and 3 sizes [2 0 , 2 1/3 , 2 2/3 ]), following <ref type="bibr" target="#b24">[25]</ref>. The base anchor sizes range from 32 2 to 512 2 on Feature Pyramid levels P 3 to P 7 . Each anchor box is matched to no more than one ground truth bounding box. The anchors that have intersection-over-union overlap with a ground truth box larger than 0.5 are considered positive  </p><formula xml:id="formula_3">FL = −α t (1 − p t ) γ log(p t )<label>(4)</label></formula><p>For each image during training, we also run suppression and top-100 selection of the predicted boxes (the same processing as single-shot detectors apply during inference). Then, we add ground truth boxes to the proposals set, and run the mask prediction module. Thus, the number of mask proposals is (100+Gt) during training. The final loss function is a sum of the three losses: Loss boxCls + Loss boxReg + Loss mask .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference</head><p>First, during the bounding box inference we use a confidence threshold of 0.05 to filter out predictions with low confidence. Second, we select the top 1000 scoring boxes from each prediction layer. Third, we apply non-maximum suppression (nms) with threshold 0.4 for each class separately. Finally, the top-100 scoring predictions are selected for each image. For mask inference, we use the top 50 bounding box predictions as mask proposals. Although there are more intelligent ways to perform post-processing, such as SoftNMS <ref type="bibr" target="#b3">[4]</ref> or test-time image augmentations, in order to fairly compare against the baseline models in speed and accuracy, we intentionally do not use those here.  <ref type="table">Table 1</ref>: Ablation study of different thresholds used in the best matching case on COCO minival. The selected threshold relaxes the regular intersection-over-union threshold of 0.5 for assigning at least one anchor box to each ground truth box. The base threshold is kept at 0.5, so the modification only affects previously unmatched ground truth objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset: In this paper, we use the COCO <ref type="bibr" target="#b25">[26]</ref> dataset, which provides bounding box and segmentation mask annotations. We follow common practice <ref type="bibr" target="#b2">[3]</ref>, using the COCO trainval135k split (union of 2014train 80k and a subset of 35k images from 2014val 40k) for training and the minival (remaining 5k images from 2014val 40k) for evaluation 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Best Matching Policy: In <ref type="table">Table 1</ref>, we test the effectiveness of using Best Matching Policy for all ground truth objects, as described in Section 3.1. Threshold 0.5 corresponds to regular matching. We then gradually lower the threshold for the best matching policy, going down all the way to 0 (completely relaxing the threshold). According to <ref type="table">Table 1</ref>, using best matching anchors with any positive overlap to ground truth gives the best performance.   Qualitative results suggest that our matching strategy reduces the number of duplicate detections. Indeed, best matching enforces larger changes to anchor boxes (but not too large to destabilize the training process), so different anchors shrink tighter to the ground truth object, and only one survives during non-maximum suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Adjusting Smooth L1 Loss:</head><p>We first ran the Smooth L1 with fixed values (1.0 and 0.11). The choice of β is not specificed in RetinaNet <ref type="bibr" target="#b24">[25]</ref>. According to the released implementation, Detectron, 0.11 is used. The upper part of <ref type="table" target="#tab_2">Table 2</ref> shows that setting β is set to 1.0 will favor will favor AP 0.5 which is widely used in datasets which adopt IOU=0.5 as the evaluation metric such as Pascal VOC <ref type="bibr" target="#b10">[11]</ref>. In contrast, the smaller value, 0.11, will favor more restrictive metrics such as IOU=.50:.05:.95 <ref type="bibr" target="#b25">[26]</ref>.</p><p>The bottom part of <ref type="table" target="#tab_2">Table 2</ref> shows the results of using Self-Adjusting Smooth L1 loss. First, we can see that our Self-Adjusting loss with setting 0.11, gives the best results for every metric. It is clear that this method is not dataset dependent. Second, the Self-Adjusting Smooth L1 is robust. When we change the bounding region from 0.11 to 1.0, the decrease of results is minor compared to the original Smooth L1 method. We also tried to share the running mean and variance across channels. The result (36.4 mAP) is slightly worse than the separate channel version. <ref type="table" target="#tab_3">Table 3</ref> illustrates bounding box accuracy improvement when running multi-task training with instance segmentation. When training with mask prediction using {P 3 , P 4 , P 5 }, we see 0.7 mAP improvement. If we train with 1.5x schedule, the improvement is 0.9 mAP. If we add the feature layer with higher resolution, will it be helpful for the prediction? We follow Mask R-CNN to use {P 2 , P 3 , P 4 , P 5 } for mask prediction. The results are slightly better on mask prediction but worse on detection. In conclusion, adding mask prediction consistently improves detection results, but requires longer training. It is also worth noting that in the Mask R-CNN ablation study, the authors also show 0.9 mAP improvement on bbox prediction from multi-tasking training.  2). The figure shows all detection results (no confidence threshold applied) only for selected categories (tie for (a), skis for (b), sports ball for (c), and toaster for (d)). Prediction scores are also shows, where they do not clutter the image. Our model shows improvement in classes with large aspect ratios (no multiple detections for tie, and better recall for skis); In (a) our model demonstrates no false negatives (e.g. note false negative sports ball with 0.14 score for the baseline model); (d) shows the failure toaster case, that accounts for the decrease in <ref type="figure">Figure 7</ref> (only 9 toasters in COCO minival). Better viewed electronically, enlarged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Training with Mask Prediction:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to RetinaNet</head><p>Following <ref type="bibr" target="#b19">[20]</ref> we give an explanation of our model's improvement over the RetinaNet baseline. The model evaluated in this section incorporates all three components described in Section 3: Best Matching policy, Self-Adjusting Smooth L1, and Mask Prediction head. ResNet-50 is used as the backbone architecture, and images are resized to a shorter side of 800 pixels. No data augmentation is used.</p><p>First, we look at per-class difference of the mean Average Precision in <ref type="figure">Figure 7</ref>, showing improvement in most of the classes. Note that the toaster class, whose mAP decreases by 7.9 points (from 28.9 to 21.0), has only 9 ground truth objects in the validation set. On the other hand, hair drier shows a significant increase from 0.9 to 7.1 mAP points. The classes that improve most also include snowboard, sports ball, kite, refrigerator, and scissors (mAP difference ≥ 5). See some qualitative examples in <ref type="figure" target="#fig_6">Figure 5</ref>.  <ref type="figure" target="#fig_8">Figure 8</ref> shows the difference in class-agnostic weak detection at low IoU threshold of 0.1, which ignores localization errors. Moreover, foreground object mis-classification is also ignored, which does not count for errors of attributing an object of one category to a different category. High correlation in these two relative differences for the improved classes (snowboard, sports ball, etc.) suggest that a large portion of network improvement comes from better localization, rather than better confidence prediction (otherwise class-agnostic weak detection would not improve).   <ref type="table" target="#tab_5">Table 4</ref> shows comparisons of our model to RetinaNet on different backbone networks and input resolutions. RetinaNet results come from the <ref type="table">Table 1</ref>(e) of the Reti-naNet <ref type="bibr" target="#b24">[25]</ref> paper. Our model shows better accuracy for all combinations of backbone network choices and resolutions. We report the speed number evaluated on Nvidia 1080 Ti. We re-implement the network in PyTorch (1080Ti), while RetinaNet is implemented in Caffe2 (M40). Note that speed numbers are reported for different GPU architectures, and thus should not be directly compared. Our network is very similar in inference settings to the original RetinaNet, so most speed performance gains are attributed to better framework implementation.</p><p>In <ref type="figure">Figure 1</ref>, we show our results compared with stateof-the-art single-shot and two-stage detectors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b17">18]</ref>.  Note that the speed of our implementation on the short side of 400 is surprisingly slow. We think this is an idiosyncrasy of the libraries used, and note that as with all the other resolutions we do see an improvement in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons to the state-of-the-art methods</head><p>We use ResNet-50-FPN, ResNet-101-FPN, and ResNeXt32x8d-101-FPN <ref type="bibr" target="#b38">[39]</ref> as the backbones in our final models. We train with the multi-scale {640, 800, 1200} and 2x iterations schedule. For the ResNet-101-FPN model, we also train a version using Group Normalization(GN) <ref type="bibr" target="#b37">[38]</ref>, which is applied only on the extra layers (FPN, localization, and classification). Replacing all the Batch Normalization <ref type="bibr" target="#b21">[22]</ref> in ResNet-101 would cause a significant slowdown. The speed of ResNet-101-FPN-GN model is 0.158 s/im (compared to 0.145 s/im without GN). Using ResNeXt32x8d-101-FPN <ref type="bibr" target="#b38">[39]</ref> as backbone further improves results by 0.9 mAP and achieves 42.6 mAP on COCO. We provide the quantitative comparison in <ref type="table" target="#tab_7">Table 5</ref> and show some detection examples in <ref type="figure" target="#fig_7">Figure 6</ref>.</p><p>We also acknowledge the recent new architectures for better object detection such as NASNET <ref type="bibr" target="#b41">[42]</ref> or efficient networks (MobileNet <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>, ShuffleNet <ref type="bibr" target="#b40">[41]</ref>), but their evaluation is beyond the scope of this work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Mask R-CNN on instance mask prediction</head><p>In <ref type="table" target="#tab_9">Table 6</ref>, we compare our mask (instance segmentation) results to Mask R-CNN. The Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> results are from <ref type="table">Table 8</ref> of Mask R-CNN <ref type="bibr" target="#b17">[18]</ref>. All the results are using ResNet-101 and Feature Pyramid Network <ref type="bibr" target="#b23">[24]</ref> as the backbone model. Our models are trained in a very similar fashion to the +e2e training in <ref type="bibr" target="#b17">[18]</ref>. Mask R-CNN still shows better accuracy on mask prediction, but the difference is only around 1.2 mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed three components to train a more accurate Single-Shot detector, RetinaMask. Our ablations show improvements for each module and our final model shows better accuracy without any network architecture change during inference. The proposed Self-Adjusting Smooth L1 loss can be used beyond the tasks of object detection and instance segmentation.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Smooth L1 and Self-Adjusting Smooth L1. In Smooth L1 Loss (a) β if a fixed threshold that separates loss into L2 and L1 regions. In the proposed Self-Adjusting Smooth L1 (b), the β is calculated as the difference between running mean and running variance of L1 loss and the value is clamped to the [0,β] range. The β is approaching 0 during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Running mean of the L1 loss applied to bounding box regression variables: x offset prediction d x and width prediction d w . Similar plots for y offset prediction d y and height prediction d h are omitted for readability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>RetinaMask network architecture. This figure demonstrates a single-shot detector extended with the mask prediction module. Left part shows the RetinaNet on the Feature Pyramid Network. The classification and bounding box regression module is applied on the feature layers, {P 3 , P 4 , P 5 , P 6 , P 7 }. After the predicted bounding boxes are processed and aggregated, they are distributed to {P 3 , P 4 , P 5 } for mask predictions according to the size. This results in P 5 predicting masks for larger objects, and P 3 predicting smaller objects. examples. On the other hand, if the overlap is less than 0.4, such anchors are treated as negative examples. Then, we use the proposed best matching policy, as described in Section 3, which can only add positive examples. For the Focal Loss function 4 used in classification, we set α = 0.25, γ = 2.0, and initialize the prediction logits according to N (0, 0.01) distribution. For the bounding box regression we add the proposed Self-Adjusting Smooth L1 and limit the control point to the range [0, 0.11] (β = 0.11).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison to RetinaNet baseline (left column). RetinaMask (right column) includes Best Matching policy, Self-Adjusting L1 loss, and Mask Prediction (see Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of RetinaMask with ResNet-101-FPN-GN Model(BBox=41.7 mAP, Mask=36.7 mAP result on COCO test-dev).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>RetinaMask class-agnostic detection improvement over RetinaNet baseline. Localization errors are ignored by setting a low IoU threshold of 0.1, foreground object mis-classification is ignored as well. ResNet-50 backbone results are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>AP AP 50 AP 75 AP S AP M AP L Fixed β = 1.0 35.3 55.6 37.8 19.4 38.9 46.9 β = 0.11 36.2 55.0 38.7 19.7 39.5 48.6 Self-Adj β ≤ 1.0 36.4 55.4 39.0 19.9 39.9 48.1 β ≤ 0.11 36.6 55.7 39.0 20.3 40.0 48.8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of Self-Adjusting Smooth L1 with different β on COCO minival.</figDesc><table><row><cell cols="3">Method Train AP bb AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">AP m AP m 50</cell><cell>AP m 75</cell></row><row><cell>Base</cell><cell>1x</cell><cell cols="2">36.2 55.0 38.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>P3-P5</cell><cell>1x</cell><cell cols="5">36.9 55.3 39.7 32.7 52.2 34.9</cell></row><row><cell>P3-P5</cell><cell>1.5x</cell><cell cols="5">37.1 55.9 39.5 33.0 52.9 35.0</cell></row><row><cell>P2-P5</cell><cell>1x</cell><cell cols="5">36.7 54.9 39.7 32.8 52.2 35.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of different settings for adding mask prediction module on COCO minival.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Comparison to RetinaNet with different input reso-</cell></row><row><cell>lutions on COCO test-dev (Also see Figure 1). For each</cell></row><row><cell>(D/S) depth/scale, the upper part (R) is the RetinaNet per-</cell></row><row><cell>formance from Table 1(e) in RetinaNet [25], our results are</cell></row><row><cell>in the bottom part. We also report mask prediction accura-</cell></row><row><cell>cies. For Depth, 50:ResNet-50-FPN, 101:ResNet-101-FPN.</cell></row><row><cell>For Method, R:RetinaNet, O(B): Our Method of BBox pre-</cell></row><row><cell>diction, O(M): Our Method of Mask prediction. Our speed</cell></row><row><cell>number is evaluated on Nvidia 1080 Ti / PyTorch1.0 and</cell></row><row><cell>FocalLoss results are evaluated on Nvidia M40 / Caffe2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art methods on COCO test-dev. Compared to RetinaNet<ref type="bibr" target="#b24">[25]</ref>, our model based on ResNet-101-FPN is better by 2.6 mAP. Compared to Mask R-CNN<ref type="bibr" target="#b17">[18]</ref>, our model shows 3.5 mAP improvement. 101 for the backbone model. All the numbers forFigure 1can be found inTable 4. Our model shows 1.84 mAP and 1.52 mAP improvement on ResNet-50 and ResNet-101 compared to RetinaNet. Our detection result is better than the original numbers from Mask R-CNN and very close to recent implementation results.</figDesc><table><row><cell>Note that YOLOv3 [31] is trained with multi-scale train-</cell></row><row><cell>ing but ours and ReinaNet [25] are not. Our results show</cell></row><row><cell>that the detector in RetinaMask has a higher envelop for</cell></row><row><cell>accuracy-vs-time than RetinaNet when using ResNet-50</cell></row><row><cell>and ResNet-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison with Mask R-CNN on mask prediction using ResNet-101 on COCO minival. The Mask R-CNN results are fromTable 8in the appendix of Mask R-CNN<ref type="bibr" target="#b17">[18]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>RetinaMask mAP detection improvement over RetinaNet baseline ResNet-50 backbone results are shown. mAP is computed across top 100 detections, and averaged for thresholds in range .50:.05:.95, according to COCO<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell></cell><cell>7.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mAP difference</cell><cell>−5 −2.5 0 2.5 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>−7.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>person bicycle</cell><cell>car motorcycle airplane</cell><cell>bus train truck boat traffic light fire hydrant stop sign parking meter</cell><cell>bench bird cat dog</cell><cell>horse sheep</cell><cell>cow elephant</cell><cell>bear zebra giraffe backpack</cell><cell>umbrella handbag</cell><cell>tie suitcase frisbee skis snowboard sports ball</cell><cell>kite baseball bat baseball glove</cell><cell>skateboard surfboard tennis racket bottle wine glass cup</cell><cell>fork knife spoon</cell><cell>bowl banana apple</cell><cell>sandwich orange</cell><cell>broccoli carrot hot dog pizza</cell><cell>donut cake chair couch potted plant</cell><cell>bed dining table</cell><cell>toilet tv laptop</cell><cell>mouse remote keyboard cell phone</cell><cell>microwave oven toaster sink refrigerator</cell><cell>book clock vase scissors teddy bear</cell><cell>hair drier toothbrush</cell></row><row><cell cols="2">Figure 7: person bicycle</cell><cell>car motorcycle airplane</cell><cell>bus train truck boat traffic light fire hydrant stop sign parking meter</cell><cell>bench bird cat dog</cell><cell>horse sheep</cell><cell>cow elephant</cell><cell>bear zebra giraffe backpack</cell><cell>umbrella handbag</cell><cell>tie suitcase frisbee skis snowboard sports ball</cell><cell>kite baseball bat baseball glove</cell><cell>skateboard surfboard tennis racket bottle wine glass cup</cell><cell>fork knife spoon</cell><cell>bowl banana apple</cell><cell>sandwich orange</cell><cell>broccoli carrot hot dog pizza</cell><cell>donut cake chair couch potted plant</cell><cell>bed dining table</cell><cell>toilet tv laptop</cell><cell>mouse remote keyboard cell phone</cell><cell>microwave oven toaster sink refrigerator</cell><cell>book clock vase scissors teddy bear</cell><cell>hair drier toothbrush</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the same definition as in FPN<ref type="bibr" target="#b23">[24]</ref> and RetinaNet<ref type="bibr" target="#b24">[25]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">COCO trainval135k is also called COCO 2017 train and the minival is COCO 2017 val.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Tamara Berg, and Phil Ammirato for their helpful suggestions, and we acknowledge support from NSF 1452851, 1533771, 1526367.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Detectron model zoo and baselines</title>
		<ptr target="https://github.com/facebookresearch/Detectron/blob/master/MODEL_ZOO.md" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/MODEL_ZOO.md" />
		<title level="m">PyTorch 1.0: Model zoo and baselines</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inside-Outside Net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft-NMS -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BlitzNet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part based models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">DSSD : Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DensePose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Beyond skip connections: Top-down modulation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D X J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
