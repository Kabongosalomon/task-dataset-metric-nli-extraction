<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Few-Shot Visual Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
							<email>pbateni@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
							<email>rgoyal14@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
							<email>fwood@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">MILA</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<email>lsigal@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Few-Shot Visual Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning is a fundamental task in computer vision that carries the promise of alleviating the need for exhaustively labeled data. Most few-shot learning approaches to date have focused on progressively more complex neural feature extractors and classifier adaptation strategies, and the refinement of the task definition itself. In this paper, we explore the hypothesis that a simple class-covariance-based distance metric, namely the Mahalanobis distance, adopted into a state of the art few-shot learning approach (CNAPS [30]) can, in and of itself, lead to a significant performance improvement. We also discover that it is possible to learn adaptive feature extractors that allow useful estimation of the high dimensional feature covariances required by this metric from surprisingly few samples. The result of our work is a new "Simple CNAPS" architecture which has up to 9.2% fewer trainable parameters than CNAPS and performs up to 6.1% better than state of the art on the standard few-shot image classification benchmark dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning successes have led to major computer vision advances <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref>. However, most methods behind these successes have to operate in fully-supervised, high data availability regimes. This limits the applicability of these methods, effectively excluding domains where data is fundamentally scarce or impossible to label en masse. This inspired the field of few-shot learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref> which aims to computationally mimic human reasoning and learning from limited data.</p><p>The goal of few-shot learning is to automatically adapt models such that they work well on instances from classes not seen at training time, given only a few labelled examples for each new class. In this paper, we focus on few-shot image classification where the ultimate aim is to develop a classification methodology that automatically adapts to new classification tasks at test time, and particularly in the case where only a very small number of labelled "support" images are available per class.  <ref type="figure">Figure 1</ref>: Class-covariance metric: Two-dimensional illustration of the embedded support image features output by a task-adapted feature extractor (points), per-class embedding means (inset icons), explicit (left) and implied class decision boundaries (right), and test query instance (gray point and inset icon) for two classifiers: standard L 2 2 -based (left) and ours, class-covariance-based (Mahalanobis distance, right). An advantage of using a class-covariancebased metric during classification is that taking into account the distribution in feature space of each class can result in improved non-linear classifier decision boundaries. What cannot explicitly appear in this figure, but we wish to convey here regardless, is that the task-adaptation mechanism used to produce these embeddings is trained endto-end from the Mahalanobis-distance-based classification loss. This means that, in effect, the task-adaptation feature extraction mechanism learns to produce embeddings that result in informative task-adapted covariance estimates.</p><p>Few-shot learning approaches typically take one of two forms: 1) nearest neighbor approaches and their variants, including matching networks <ref type="bibr" target="#b39">[40]</ref>, which effectively apply nearest-neighbor or weighted nearest neighbor classification on the samples themselves, either in a feature <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref> or a semantic space <ref type="bibr" target="#b4">[5]</ref>; or 2) embedding methods that effectively distill all of the examples to a single prototype per class, where a prototype may be learned <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref> or implicitly derived from the samples <ref type="bibr" target="#b35">[36]</ref> (e.g. mean embedding). The prototypes are often defined in feature or semantic space (e.g. word2vec <ref type="bibr" target="#b43">[44]</ref>). Most research in this domain has focused on learning non-linear mappings, often expressed as neural nets, from images to the embed-1 arXiv:1912.03432v3 [cs.CV] 11 Jun 2020 <ref type="figure">Figure 2</ref>: Approaches to few-shot image classification: organized by image feature extractor adaptation scheme (vertical axis) versus final classification method (horizontal axis). Our method (Simple CNAPS) partially adapts the feature extractor (which is architecturally identical to CNAPS) but is trained with, and uses, a fixed, rather than adapted, Mahalanobis metric for final classification.</p><p>ding space subject to a pre-defined metric in the embedding space used for final nearest class classification; usually cosine similarity between query image embedding and class embedding. Most recently, CNAPS <ref type="bibr" target="#b29">[30]</ref> achieved state of the art (SoTA) few-shot visual image classification by utilizing sparse FiLM <ref type="bibr" target="#b26">[27]</ref> layers within the context of episodic training to avoid problems that arise from trying to adapt the entire embedding network using few support samples.</p><p>Overall much less attention has been given to the metric used to compute distances for classification in the embedding space. Presumably this is because common wisdom dictates that flexible non-linear mappings are ostensibly able to adapt to any such metric, making the choice of metric apparently inconsequential. In practice, as we find in this paper, the choice of metric is quite important. In <ref type="bibr" target="#b35">[36]</ref> the authors analyze the underlying distance function used in order to justify the use of sample means as prototypes. They argue that Bregman divergences <ref type="bibr" target="#b0">[1]</ref> are the theoretically sound family of metrics to use in this setting, but only utilize a single instance within this class squared Euclidean distance, which they find to perform better than the more traditional cosine metric. However, the choice of Euclidean metric involves making two flawed assumptions: 1) that feature dimensions are un-correlated and 2) that they have uniform variance. Also, it is insensitive to the distribution of within-class samples with respect to their prototype and recent results <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref> suggest that this is problematic. Modeling this distribution (in the case of <ref type="bibr" target="#b0">[1]</ref> using extreme value theory) is, as we find, a key to better performance.</p><p>Our Contributions: Our contributions are four-fold: 1) A robust empirical finding of a significant 6.1% improvement, on average, over SoTA (CNAPS <ref type="bibr" target="#b29">[30]</ref>) in few-shot image classification, obtained by utilizing a test-time-estimated class-covariance-based distance metric, namely the Mahalanobis distance <ref type="bibr" target="#b5">[6]</ref>, in final, task-adapted classification. 2) The surprising finding that we are able to estimate such a metric even in the few shot classification setting, where the number of available support examples, per class, is far too few in theory to estimate the required class-specific covariances. 3) A new "Simple CNAPS" architecture that achieves this performance despite removing 788,485 parameters (3.2%-9.2% of the total) from original CNAPS architecture, replacing them with fixed, not-learned, deterministic covariance estimation and Mahalanobis distance computations. 4) Evidence that should make readers question the common understanding that CNN feature extractors of sufficient complexity can adapt to any final metric (be it cosine similarity/dot product or otherwise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most of last decade's few-shot learning works <ref type="bibr" target="#b42">[43]</ref> can be differentiated along two main axes: 1) how images are transformed into vectorized embeddings, and 2) how "distances" are computed between vectors in order to assign labels. This is shown in <ref type="figure">Figure 2</ref>.</p><p>Siamese networks <ref type="bibr" target="#b15">[16]</ref>, an early approach to few-shot learning and classification, used a shared feature extractor to produce embeddings for both the support and query images. Classification was then done by picking the smallest weighted L1 distance between query and labelled image embeddings. Relation networks <ref type="bibr" target="#b37">[38]</ref>, and recent GCNN variants <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>, extended this by parameterizing and learning the classification metric using a Multi-Layer Perceptron (MLP). Matching networks <ref type="bibr" target="#b39">[40]</ref> learned distinct feature extractors for support and query images which were then used to compute cosine similarities for classification.</p><p>The feature extractors used by these models were, notably, not adapted to test-time classification tasks. It has become established that adapting feature extraction to new tasks at test time is generally a good thing to do. Fine tuning transfer-learned networks <ref type="bibr" target="#b44">[45]</ref> did this by fine-tuning the feature extractor network using the task-specific support images but found limited success due to problems related to overfitting to, the generally very few, support examples. MAML <ref type="bibr" target="#b2">[3]</ref> (and its many extensions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>) mitigated this issue by learning a set of meta-parameters that specifically enabled the feature extractors to be adapted to new tasks given few support examples using few gradient steps.</p><p>The two methods most similar to our own are CNAPS <ref type="bibr" target="#b29">[30]</ref> (and the related TADAM <ref type="bibr" target="#b25">[26]</ref>) and Prototypical networks <ref type="bibr" target="#b35">[36]</ref>. CNAPS is a few-shot adaptive classifier based on conditional neural processes (CNP) <ref type="bibr" target="#b6">[7]</ref>. It is the state of the art approach for few-shot image classification <ref type="bibr" target="#b29">[30]</ref>. It uses a pre-trained feature extractor augmented with FiLM  <ref type="figure">Figure 3</ref>: Overview of the feature extractor adaptation methodology in CNAPS: task encoder g φ (·) provides the adaptation network ψ i φ at each block i with the task representations (g φ (S τ ) to produce FiLM parameters (γ j , β j ). For details on the auto-regressive variant (AR-CNAPS), architectural implementations, and FiLM layers see Appendix B. For an in-depth explanation, refer to the original paper <ref type="bibr" target="#b29">[30]</ref>.</p><p>layers <ref type="bibr" target="#b26">[27]</ref> that are adapted for each task using the support images specific to that task. CNAPS uses a dot-product distance in a final linear classifier; the parameters of which are also adapted at test-time to each new task. We describe CNAPS in greater detail when describing our method.</p><p>Prototypical networks <ref type="bibr" target="#b35">[36]</ref> do not use a feature adaptation network; they instead use a simple mean pool operation to form class "prototypes." Squared Euclidean distances to these prototypes are then subsequently used for classification. Their choice of the distance metric was motivated by the theoretical properties of Bregman divergences <ref type="bibr" target="#b0">[1]</ref>, a family of functions of which the squared Euclidean distance is a member of. These properties allow for a mathematical correspondence between the use of the squared Euclidean distance in a Softmax classifier and performing density estimation. Expanding on <ref type="bibr" target="#b35">[36]</ref> in our paper, we also exploit similar properties of the squared Mahalanobis distance as a Bregman divergence <ref type="bibr" target="#b0">[1]</ref> to draw theoretical connections to multi-variate Gaussian mixture models.</p><p>Our work differs from CNAPS <ref type="bibr" target="#b29">[30]</ref> and Prototypical networks <ref type="bibr" target="#b35">[36]</ref> in the following ways. First, while CNAPS has demonstrated the importance of adapting the feature extractor to a specific task, we show that adapting the classifier is actually unnecessary to obtain good performance. Second, we demonstrate that an improved choice of Bregman divergence can significantly impact accuracy. Specifically we show that regularized class-specific covariance estimation from task-specific adapted feature vectors allows the use of the Mahalanobis distance for classification, achieving a significant improvement over state of the art. A high-level diagrammatic comparison of our "Simple CNAPS" architecture to CNAPS can be found in <ref type="figure" target="#fig_1">Figure 4</ref>.</p><p>More recently, <ref type="bibr" target="#b3">[4]</ref> also explored using the Mahalanobis distance by incorporating its use in Prototypical networks <ref type="bibr" target="#b35">[36]</ref>. In particular they used a neural network to produce per-class diagonal covariance estimates, however, this approach is restrictive and limits performance. Unlike <ref type="bibr" target="#b3">[4]</ref>, Simple CNAPS generates regularized full covariance estimates from an end-to-end trained adaptation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Formal Problem Definition</head><p>We frame few-shot image classification as an amortized classification task. Assume that we have a large labelled</p><formula xml:id="formula_0">dataset D = {(x i , y i )} N i=1</formula><p>of images x i and labels y i . From this dataset we can construct a very large number of classification tasks D τ ⊆ D by repeatedly sampling without replacement from D. Let τ ∈ Z + uniquely identify a classification task. We define the support set of a task to be</p><formula xml:id="formula_1">S τ = {(x i , y i )} N τ i=1 and the query set Q τ = {(x * i , y * i )} N * τ i=1 where D τ = S τ ∪ Q τ where x i , x * i ∈ R D are vectorized images and y i , y * i ∈ {1, .</formula><p>.., K} are class labels. Our objective is to find parameters θ of a classifier f θ that maximizes</p><formula xml:id="formula_2">E τ [ Q τ p(y * i |f θ (x * i , S τ )].</formula><p>In practice, D is constructed by concatenating large image classification datasets and the set of classification tasks. {D τ } τ =1 is sampled in a more complex way than simply without replacement. In particular, constraints are placed on the relationship of the image label pairs present in the support set and those present in the query set. For instance, in few-shot learning, the constraint that the query set labels are a subset of the support set labels is imposed. With this constraint imposed, the classification task reduces to correctly assigning each query set image to one of the classes present in the support set. Also, in this constrained few-shot classification case, the support set can be interpreted as being the "training data" for implicitly training (or adapting) a task-specific classifier of query set images. Note that in conjecture with <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref> and unlike earlier work <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b39">40]</ref>, we do not impose any constraints on the support set having to be balanced and of uniform number of classes, although we do conduct experiments on this narrower setting too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Our classifier shares feature adaptation architecture with CNAPS <ref type="bibr" target="#b29">[30]</ref>, but deviates from CNAPS by replacing their adaptive classifier with a simpler classification scheme based on estimating Mahalanobis distances. To explain our classifier, namely "Simple CNAPS", we first detail CNAPS  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CNAPS</head><p>Conditional Neural Adapative Processes (CNAPS) consist of two elements: a feature extractor and a classifier, both of which are task-adapted. Adaptation is performed by trained adaptation modules that take the support set.</p><p>The feature extractor architecture used in both CNAPS and Simple CNAPS is shown in <ref type="figure">Figure 3</ref>. It consists of a ResNet18 <ref type="bibr" target="#b9">[10]</ref> network pre-trained on ImageNet <ref type="bibr" target="#b30">[31]</ref> which also has been augmented with FiLM layers <ref type="bibr" target="#b26">[27]</ref>. The parameters {γ j , β j } 4 j=1 of the FiLM layers can scale and shift the extracted features at each layer of the ResNet18, allowing the feature extractor to focus and disregard different features on a task-by-task basis. A feature adaptation module ψ f φ is trained to produce {γ j , β j } 4 j=1 based on the support examples S τ provided for the task.</p><p>The feature extractor adaptation module ψ f φ consists of two stages: support set encoding followed by film layer parameter production. The set encoder g φ (·), parameterized by a deep neural network, produces a permutation invariant task representation g φ (S τ ) based on the support images S τ . This task representation is then passed to ψ j φ which then produces the FiLM parameters {γ j , β j } for each block j in the ResNet. Once the FiLM parameters have been set, the feature extractor has been adapted to the task. We use f τ θ to denote the feature extractor adapted to task τ . The CNAPS paper <ref type="bibr" target="#b29">[30]</ref> also proposes an auto-regressive adaptation method which conditions each adaptor ψ j φ on the output of the previous adapter ψ j−1 φ . We refer to this variant as AR-CNAPS but for conciseness we omit the details of this architecture here, and instead refer the interested reader to <ref type="bibr" target="#b29">[30]</ref> or to Appendix B.1 for a brief overview.</p><p>Classification in CNAPS is performed by a task-adapted linear classifier where the class probabilities for a query image x * i are computed as softmax(Wf τ θ (x * i ) + b). The classification weights W and biases b are produced by the classifier adaptation network</p><formula xml:id="formula_3">ψ c φ forming [W, b] = [ψ c φ (µ 1 ) ψ c φ (µ 2 ) . . . ψ c φ (µ K )] T</formula><p>where for each class k in the task, the corresponding row of classification weights  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Simple CNAPS</head><p>In Simple CNAPS, we also use the same pre-trained ResNet18 for feature extraction with the same adaptation module ψ f φ , although, because of the classifier architecture we use, it becomes trained to do something different than it does in CNAPS. This choice, like for CNAPS, allows for a task-specific adaptation of the feature extractor. Unlike CNAPS, we directly compute</p><formula xml:id="formula_4">p(y * i = k|f τ θ (x * i ), S τ ) = softmax(−d k (f τ θ (x * i ), µ k )) (1) using a deterministic, fixed d k d k (x, y) = 1 2 (x − y) T (Q τ k ) −1 (x − y).<label>(2)</label></formula><p>Here Q τ k is a covariance matrix specific to the task and class. As we cannot know the value of Q τ k ahead of time, it must be estimated from the feature embeddings of the taskspecific support set. As the number of examples in any particular support set is likely to be much smaller than the dimension of the feature space, we use a regularized estimator</p><formula xml:id="formula_5">Q τ k = λ τ k Σ τ k + (1 − λ τ k )Σ τ + βI.<label>(3)</label></formula><p>formed from a convex combination of the class-within-task and all-classes-in-task covariance matrices Σ τ k and Σ τ respectively.</p><p>We estimate the class-within-task covariance matrix Σ τ k using the feature embeddings f τ θ (</p><formula xml:id="formula_6">x i ) of all x i ∈ S τ k where S τ k is the set of examples in S τ with class label k. Σ τ k = 1 |S τ k |−1 (xi,yi)∈S τ k (f τ θ (x i ) − µ k )(f τ θ (x i ) − µ k ) T .</formula><p>If the number of support instance of that class is one, i.e. |S τ k | = 1, then we define Σ τ k to be the zero matrix of the appropriate size. The all-classes-in-task covariance Σ τ is estimated in the same way as the class-within-task except that it uses all the support set examples x i ∈ S τ regardless of their class.</p><p>We choose a particular, deterministic scheme for computing the weighting of class and task specific covariance estimates, λ τ k = |S τ k |/(|S τ k |+1). This choice means that in the case of a single labeled instance for class in the support set, a single "shot," Q τ k = 0.5Σ τ k + 0.5Σ τ + βI. This can be viewed as increasing the strength of the regularization parameter β relative to the task covariance Σ τ . When |S τ k |= 2, λ τ k becomes 2/3 and Q τ k only partially favors the class-level covariance over the all-class-level covariance. In a high-shot setting, λ τ k tends to 1 and Q τ k mainly consists of the class-level covariance. The intuition behind this formula for λ τ k is that the higher the number of shots, the better the class-within-task covariance estimate gets, and the more Q τ k starts to look like Σ τ k . We considered other ratios and making λ τ k 's learnable parameters, but found that out of all the considered alternatives the simple deterministic ratio above produced the best results. The architecture of the classifier in Simple CNAPS appears in <ref type="figure" target="#fig_1">Figure 4</ref>, bottom-right, blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Theory</head><p>The class label probability calculation appearing in Equation 1 corresponds to an equally-weighted exponential family mixture model as λ → 0 <ref type="bibr" target="#b35">[36]</ref>, where the exponential family distribution is uniquely determined by a regular Bregman divergence <ref type="bibr" target="#b0">[1]</ref> </p><formula xml:id="formula_7">D F (z, z ) = F (z) − F (z ) − ∇F (z ) T (z − z )<label>(4)</label></formula><p>for a differentiable and strictly convex function F. The squared Mahalanobis distance in Equation 2 is a Bregman divergence generated by the convex function F (x) = 1 2 x T Σ −1 x and corresponds to the multivariate normal exponential family distribution. When all Q τ k ≈ Σ τ + βI, we can view the class probabilities in <ref type="figure">Equation 1</ref> as the "responsibilities" in a Gaussian mixture model</p><formula xml:id="formula_8">p(y * i = k|f τ θ (x * i ), S τ ) = π k N (µ k , Q τ k ) k π k N (µ k , Q τ k )<label>(5)</label></formula><p>with equally weighted mixing coefficient π k = 1/k. This perspective immediately highlights a problem with the squared Euclidean norm, used by a number of approaches as shown in <ref type="figure">Fig. 2</ref>. The Euclidean norm, which corresponds to the squared Mahalanobis distance with Q τ k = I, implicitly assumes each cluster is distributed according to a unit normal, as seen in <ref type="figure" target="#fig_3">Figure 5</ref>. By contrast, the squared Mahalanobis distance considers cluster covariance when computing distances to the cluster centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We evaluate Simple CNAPS on the Meta-Dataset <ref type="bibr" target="#b38">[39]</ref> family of datasets, demonstrating improvements compared to nine baseline methodologies including the current SoTA, CNAPS. Benchmark results reported come from <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>Meta-Dataset <ref type="bibr" target="#b38">[39]</ref> is a benchmark for few-shot learning encompassing 10 labeled image datasets: ILSVRC-2012 (ImageNet) <ref type="bibr" target="#b30">[31]</ref>, Omniglot <ref type="bibr" target="#b17">[18]</ref>, FGVC-Aircraft (Aircraft) <ref type="bibr" target="#b21">[22]</ref>, CUB-200-2011 (Birds) <ref type="bibr" target="#b40">[41]</ref>, Describable Textures (DTD) <ref type="bibr" target="#b1">[2]</ref>, QuickDraw <ref type="bibr" target="#b13">[14]</ref>, FGVCx Fungi (Fungi) <ref type="bibr" target="#b34">[35]</ref>, VGG Flower (Flower) <ref type="bibr" target="#b24">[25]</ref>, Traffic Signs (Signs) <ref type="bibr" target="#b11">[12]</ref> and MSCOCO <ref type="bibr" target="#b19">[20]</ref>. In keeping with prior work, we report results using the first 8 datasets for training, reserving Traffic Signs and MSCOCO for "out-of-domain" performance evaluation. Additionally, from the eight training datasets used for training, some classes are held out for testing, to evaluate "in-domain" performance. Following <ref type="bibr" target="#b29">[30]</ref>, we extend the out-of-domain evaluation with 3 more datasets: MNIST <ref type="bibr" target="#b18">[19]</ref>, CIFAR10 <ref type="bibr" target="#b16">[17]</ref> and CIFAR100 <ref type="bibr" target="#b16">[17]</ref>. We report results using standard test/train splits and benchmark baselines provided by <ref type="bibr" target="#b38">[39]</ref>, but, importantly, we have cross-validated our critical empirical claims using different test/train splits and our results are robust across folds (see Appendix C). For details on task generation, distribution of shots/ways and hyperparameter settings, see Appendix A.</p><p>Mini/tieredImageNet <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref> are two smaller but more widely used benchmarks that consist of subsets of ILSVRC-2012 (ImageNet) <ref type="bibr" target="#b30">[31]</ref> with 100 classes (60,000 images) and 608 classes (779,165 images) respectively. For comparison to more recent work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref> for which Meta-Dataset evaluations are not available, we use mini/tieredImageNet. Note that in the mini/tieredImageNet setting, all tasks are of the same pre-set number of classes and number of support examples per class, making learning comparatively easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results</head><p>Reporting format: Bold indicates best performance on each dataset while underlines indicate statistically significant improvement over baselines. Error bars represent a 95% confidence interval over tasks.</p><p>In-domain performance: The in-domain results for Simple CNAPS and Simple AR-CNAPS, which uses the autoregressive feature extraction adaptor, are shown in  Out-of-domain performance: As shown in <ref type="table" target="#tab_6">Table 2</ref>, Simple CNAPS and Simple AR-CNAPS produce substantial gains in performance on out-of-domain datasets, each exceeding the SoTA baseline. With an average out-of-domain accuracy of 69.7% and 67.6%, Simple CNAPS and Simple AR-CNAPS outperform SoTA by 8.2% and 7.8%. This means that Simple CNAPS/AR-CNAPS generalizes to outof-domain datasets better than baseline models. Also, Simple AR-CNAPS under-performs Simple CNAPS, suggesting that the auto-regressive feature adaptation approach may overfit to the domain of datasets it has been trained on.</p><p>Overall performance: Simple CNAPS achieves the best overall classification accuracy at 72.2% with Simple AR-CNAPS trailing very closely at 71.2%. Since the overall performance of the two variants are statistically indistinguishable, we recommend Simple CNAPS over Simple AR-CNAPS as it has fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to other distance metrics:</head><p>To test the significance of our choice of Mahalanobis distance, we substitute it within our architecture with other distance metrics -absolute difference (L 1 ), squared Euclidean (L 2 2 ), cosine similarity and negative dot-product. Performance comparisons are shown in <ref type="table" target="#tab_13">Table 3</ref> and 4. We observe that using the Mahalanobis distance results in the best in-domain, out-ofdomain, and overall average performance on all datasets.  With the exception of (AR-)CNAPS where the reported results are from <ref type="bibr" target="#b29">[30]</ref>, all other benchmarks are reported from <ref type="bibr" target="#b38">[39]</ref>.   Impact of the task regularizer Σ τ : We also consider a variant of Simple CNAPS where all-classes-within-task co-variance matrix Σ τ is not included in the covariance regularization (denoted with the "-TR" tag). This is equivalent to setting λ τ k to 1 in Equation 3. As shown in <ref type="table" target="#tab_13">Table 4</ref>, we observe that, while removing the task level regularizer only marginally reduces overall performance, the difference on individual datasets such as ImageNet can be large.</p><p>Sensitivity to the number of support examples per class: <ref type="figure" target="#fig_4">Figure 6</ref> shows how the overall classification accuracy varies as a function of the average number of support examples per class (shots) over all tasks. We compare Simple CNAPS, original CNAPS, and the L 2 2 variant of our method. As expected, the average number of support examples per class is highly correlated with the performance. All methods perform better with more labeled examples per support class, with Simple CNAPS performing substantially better as the number of shots increases. The surprising discovery is that Simple CNAPS is effective even when the number of labeled instances is as low as four, suggesting both that even poor estimates of the task and class specific covariance matrices are helpful and that the regularization scheme we have introduced works remarkably well.  <ref type="table" target="#tab_13">Table 3</ref>: In-domain few-shot classification accuracy of Simple CNAPS compared to ablated alternatives of the negative dot product, absolute difference (L 1 ), squared Euclidean (L 2 2 ) and removing task regularization (λ τ k = 1) denoted by "-TR".  <ref type="table" target="#tab_13">Table 4</ref>: Middle) Out-of-domain few-shot classification accuracy of Simple CNAPS compared to ablated alternatives of the negative dot product, absolute difference (L 1 ), squared Euclidean (L 2 2 ) and removing task regularization (λ τ k = 1) denoted by "-TR". Right) In-domain, out-of-domain and overall mean classification accuracies of the ablated models.  Sensitivity to the number of classes in the task: In <ref type="figure" target="#fig_5">Figure 7</ref>, we examine average accuracy as a function of the number of classes in the task. We find that, irrespective of the number of classes in the task, we maintain accuracy improvement over both CNAPS and our L 2 2 variant. Accuracy on mini/tieredImageNet: <ref type="table" target="#tab_10">Table 5</ref> shows that Simple CNAPS outperforms recent baselines on all of the standard 1-and 5-shot 5-way classification tasks. These results should be interpreted with care as both CNAPS and Simple CNAPS use a ResNet18 <ref type="bibr" target="#b9">[10]</ref> feature extractor pretrained on ImageNet. Like other models in this table, here Simple CNAPS was trained for these particular shot/way configurations. That Simple CNAPS performs well here in the 1-shot setting, improving even on CNAPS, suggests that Simple CNAPS is able to specialize to particular few-shot classification settings in addition to performing well when the number of shots and ways is unconstrained as it was in the earlier experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Few shot learning is a fundamental task in modern AI research. In this paper we have introduced a new method for amortized few shot image classification which establishes a new SoTA performance benchmark by making a simplification to the current SoTA architecture. Our specific architectural choice, that of deterministically estimating and using Mahalanobis distances for classification of task-adjusted class-specific feature vectors, seems to produce, via training, embeddings that generally allow for useful covariance estimates, even when the number of labeled instances, per task and class, is small. The effectiveness of the Mahalanobis distance in feature space for distinguishing classes suggests connections to hierarchical regularization schemes <ref type="bibr" target="#b32">[33]</ref> that could enable performance improvements even in the zero-shot setting. In the future, exploration of other Bregman divergences can be an avenue of potentially fruitful research. Additional enhancements in the form of data and task augmentation can also boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada Research Chairs (CRC) Program, the Canada CIFAR AI Chairs Program, Compute Canada, Intel, and DARPA under its D3M and LWLL programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A. Experimental Setting Section 3.2 of <ref type="bibr" target="#b38">[39]</ref> explains the sampling procedure to generate tasks from the Meta-Dataset <ref type="bibr" target="#b38">[39]</ref>, used during both training and testing. This results in tasks with varying of number of shots/ways. <ref type="figure">Figure 9a</ref> and 9b show the ways/shots frequency graphs at test time. For evaluating on Meta-Dataset and mini/tiered-ImageNet datasets, we use episodic training <ref type="bibr" target="#b35">[36]</ref> to train models to remain consistent with the prior works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. We train for 110K tasks, 16 tasks per batch, totalling 6,875 gradient steps using Adam with learning rate of 0.0005. We validate (on 8 in-domain and 1 out-of-domain datasets) every 10K tasks, saving the best model/checkpoint for testing. Please visit the Pytorch implementation of Simple CNAPS for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. (Simple) CNAPS in Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Auto-Regressive CNAPS</head><p>In <ref type="bibr" target="#b29">[30]</ref>, an additional auto-regressive variant for adapting the feature extractor is proposed, referred to as AR-CNAPS. As shown in <ref type="figure" target="#fig_6">Figure 10</ref>, AR-CNAPS extends CNAPS by introducing the block-level set encoder g ARj φ at each block j. These set encoders use the output obtained by pushing the support S τ through all previous blocks 1 : j − 1 to form the block level set representation g</p><p>ARj φ (f τj θ (S τ )). This representation is then subsequently used as input to the adaptation network ψ j φ in addition to the task representation g φ (S τ ). This way the adaptation network is not just conditioned on the task, but is also aware of the potential changes in the previous blocks as a result of the adaptation being performed by the adaptation networks before it (i.e., ) A FiLM layer.</p><p>(b) A ResNet basic block with FiLM layers. re E.9: (Left) A FiLM layer operating on convolutional feature maps indexed by channel ch. (Right) How a layer is used within a basic Residual network block <ref type="bibr" target="#b13">[14]</ref>. ResNet18 Architecture details ughout our experiments in Section 5, we use a ResNet18 <ref type="bibr" target="#b13">[14]</ref> as our feature extractor, the meters of which we denote ✓.  .</p><formula xml:id="formula_9">ψ 1 φ : ψ j−1 φ ).</formula><p>The auto-regressive nature of AR-CNAPS allows for a more dynamic adaptation procedure that boosts performance in certain domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. FiLM Layers</head><p>Proposed by <ref type="bibr" target="#b26">[27]</ref>, Feature-wise Linear Modulation (FiLM) layers were used for visual question answering, where the feature extractor could be conditioned on the question. As shown in <ref type="figure" target="#fig_11">Figure 11</ref>, these layers are inserted within residual blocks, where the feature channels are scaled and linearly shifted using the respective FiLM parameters γ i,ch and β i,ch . This can be extremely powerful in transforming the extracted feature space. In our work and <ref type="bibr" target="#b29">[30]</ref>, these FiLM parameters are conditioned on the support images in the task S τ . This way, the adapted feature extractor f τ θ is able to modify the feature space to extract the features that allow classes in the task to be distinguished most distinctly. This is in particular very powerful when the classification metric is changed to the Mahalanobis distance, as with a new objective, the feature extractor adaptation network ψ f φ is able to learn to extract better features (see difference between with and without ψ f φ in <ref type="table" target="#tab_13">Table 7</ref> on CNAPS and Simple CNAPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Network Architectures</head><p>We adapt the same architectural choices for the task encoder g φ , auto-regressive set encoders g AZ1 φ , ..., g AZ J φ and the feature extractor adaptation network ψ f φ = {ψ 1 φ , ..., ψ J φ } as <ref type="bibr" target="#b29">[30]</ref>. The neural architecture for each adaptation module inside of ψ f φ has been shown in <ref type="figure">Figure 8</ref>. The neural configurations for the task encoder g φ and the auto-regressive set encoders g AZ1 φ , ..., g AZ J φ used in AR-CNAPS are shown in <ref type="figure">Figure 12</ref>-a and <ref type="figure">Figure 12</ref>-b respectively. Note that for the auto-regressive set encoders, there is no need for convolutional layers. The input to these networks come from the output of the corresponding residual block adapted to that  <ref type="figure" target="#fig_6">Figure 10</ref>: Overview of the auto-regresive feature extractor adaptation in CNAPS: in addition to the structure shown in <ref type="figure">Figure 3</ref>, AR-CNAPS takes advantage of a series of pre-block set encoders g ARj φ to furthermore condition the output of each ψ j φ on the set representation g</p><p>ARj φ (f τj θ (S τ )). The set representation is formed by first adapting the previous blocks 1 : j − 1, then pushing the support set S through the adapted blocks to form an auto-regressive adapted set representation at block j. This way, adaptive functions later in the pipeline are more explicitly aware of the changes made by the previous adaptation networks, and can adjust better accordingly.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 ResNet18 Architecture details</head><p>Throughout our experiments in Section 5, we use a ResNet18 <ref type="bibr" target="#b13">[14]</ref> as our feature extractor, the parameters of which we denote ✓. <ref type="table" target="#tab_10">Table E.5 and Table E</ref>.6 detail the architectures of the basic block (left) and basic scaling block (right) that are the fundamental components of the ResNet that we employ. <ref type="table" target="#tab_13">Table E</ref>.7 details how these blocks are composed to generate the overall feature extractor network. We use the implementation that is provided by the PyTorch [52] 3 , though we adapt the code to enable the use of FiLM layers.  level (denoted by f τj θ for block j) which has already been processed with convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers</head><p>Unlike CNAPS, we do not use the classifier adaptation network ψ c φ . As shown in <ref type="figure">Figure 12</ref>-c, the classification weights adaptor ψ c φ consists of an MLP consisting of three fully connected (FC) layers with the intermediary nonelinearity ELU, which is the continuous approximation to ReLU as defined below:</p><formula xml:id="formula_10">ELU (x) = x x &gt; 0 e x 1 x ≤ 0<label>(6)</label></formula><p>As mentioned previously, without the need to learn the three FC layers in ψ c φ , Simple CNAPS has 788,485 fewer parameters while outperforming CNAPS by considerable margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross Validation</head><p>The Meta-Dataset <ref type="bibr" target="#b38">[39]</ref> and its 8 in-domain 2 out-ofdomain split is a setting that has defined the benchmark for the baseline results provided. The splits, between the datasets, were intended to capture an extensive set of visual domains for evaluating the models.</p><p>However, despite the fact that all past work directly rely on the provided set up, we go further by verifying that our model is not overfitting to the proposed splits and is able to consistently outperform the baseline with different permutations of the datasets. We examine this through a 4fold cross validation of Simple CNAPS and CNAPS on the following 8 datasets: ILSVRC-2012 (ImageNet) <ref type="bibr" target="#b30">[31]</ref>, Omniglot <ref type="bibr" target="#b17">[18]</ref>, FGVC-Aircraft <ref type="bibr" target="#b21">[22]</ref>, CUB-200-2011 (Birds) <ref type="bibr" target="#b40">[41]</ref>, Describable Textures (DTD) <ref type="bibr" target="#b1">[2]</ref>, QuickDraw <ref type="bibr" target="#b13">[14]</ref>, FGVCx Fungi <ref type="bibr" target="#b34">[35]</ref> and VGG Flower <ref type="bibr" target="#b24">[25]</ref>. During each fold, two of the datasets are exluded from training, and both Simple CNAPS and CNAPS are trained and evaluated in that setting.</p><p>As shown by the classification results in <ref type="table" target="#tab_13">Table 6</ref>, in all four folds of validation, Simple CNAPS is able to outperform CNAPS on 7-8 out of the 8 datasets. The in-domain, out-of-domain, and overall averages for each fold noted in <ref type="table" target="#tab_13">Table 8</ref> also show Simple CNAPS's accuracy gains over CNAPS with substantial margins. In fact, the fewer number of in-domain datasets in the cross-validation <ref type="bibr">(6 vs. 8)</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Squared Euclidean Distance (b) Squared Mahalanobis Distance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of the feature extraction and classification in CNAPS versus Simple CNAPS: Both CNAPS and Simple CNAPS share the feature extraction adaptation architecture detailed in Figure 3. CNAPS and Simple CNAPS differ in how distances between query feature vectors and class feature representations are computed for classification. CNAPS uses a trained, adapted linear classifier whereas Simple CNAPS uses a differentiable but fixed and parameter-free deterministic distance computation. Components in light blue have parameters that are trained, specifically f τ θ in both models and ψ c φ in the CNAPS adaptive classification. CNAPS classification requires 778k parameters while Simple CNAPS is fully deterministic. in Section 4.1, before presenting our model in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Problematic nature of the unit-normal assumption: The Euclidean Norm (left) assumes embedded image features f θ (x i ) are distributed around class means µ k according to a unit normal. The Mahalanobis distance (right) considers cluster variance when forming decision boundaries, indicated by the background colour. is produced by ψ c φ from the class mean µ k . The class mean µ k is obtained by mean-pooling the feature vectors of the support examples for class k extracted by the adapted feature extractor f τ θ . A visual overview of the CNAPS adapted classifier architecture is shown in Figure 4, bottom left, red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Accuracy vs. Shots: Average number of support examples (in log scale) per class v/s accuracy. TFor each class in each of the 7,800 sampled Meta-Dataset tasks (13 datasets, 600 tasks each) used at test time, the classification accuracy on the class' query examples was obtained. These class accuracies were then grouped according to the class shot, averaged and plotted to show how accuracy of CNAPS, L 2 2 and Simple-CNAPS scale with higher shots. average 73.8% accuracy on in-domain few-shot classification, a 4.2% gain over CNAPS, while Simple AR-CNAPS achieves 73.5% accuracy, a 3.8% gain over AR-CNAPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Accuracy vs. Ways: Number of ways (classes in the task) v/s accuracy. Tasks in the test set are grouped together by number of classes. The accuracies are averaged to obtain a value for each count of class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 :</head><label>10</label><figDesc>Adaptation network f . R ib j ch and R ib j ch denote a vector of regularization weights that are ed with an l2 penalty. re E.10 shows the details of the adaptation network f that generates the FiLM layer parameters ach ResNet layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Architectural overview of the feature extractor adaptation network ψ f φ : Figure has been adapted from [30] and showcases the neural architecture used for each adaptation module ψ j φ (corresponding to residual block j) in the feature extractor adaptation network ψ f φ . (a) Number of Tasks vs. Ways (b) Number of Classes vs. Shots Test-time distribution of tasks: a) Frequency of number of tasks as grouped by the number of classes in the tasks (ways). b) Frequency of the number of classes grouped by the number examples per class (shots). Both figures are for test tasks sampled when evaluating on the Meta-Dataset [39]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>A ResNet basic block with FiLM layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure E. 9 : 1 Figure E. 10 :</head><label>9110</label><figDesc>(Left) A FiLM layer operating on convolutional feature maps indexed by channel ch. (Right) How a FiLM layer is used within a basic Residual network block<ref type="bibr" target="#b13">[14]</ref>. Adaptation network f . R ib j ch and R ib j ch denote a vector of regularization weights that are learned with an l2 penalty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure E. 10</head><label>10</label><figDesc>shows the details of the adaptation network f that generates the FiLM layer parameters for each ResNet layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Overview of FiLM layers: Figure is from [30]. Left) FiLM layer operating a series of channels indexed by ch, scaling and shifting the feature channels as defined by the respective FiLM parameters γ i,ch and β i,ch . Right) Placement of these FiLM modules within a ResNet18 [10] basic block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Simple AR-CNAPS outperforms previous SoTA on 7 out of the 8 datasets while matching past SoTA on FGVCx Fungi (Fungi). Simple CNAPS outperforms baselines on 6 out of 8 datasets while matching performance on FGVCx Fungi (Fungi) and Describable Textures (DTD). Overall, indomain performance gains are considerable in the few-shot domain with 2-6% margins. Simple CNAPS achieves an</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>In-domain few-shot classification accuracy of Simple CNAPS and Simple AR-CNAPS compared to the baselines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Middle) Out-of-domain few-shot classification accuracy of Simple CNAPS and Simple AR-CNAPS compared to the baselines. Right) In-domain, out-of-domain and overall mean classification accuracy of Simple CNAPS and Simple AR-CNAPS compared to the baselines. With the exception of CNAPS and AR-CNAPS where the reported results come from<ref type="bibr" target="#b29">[30]</ref>, all other benchmarks are reported directly from<ref type="bibr" target="#b38">[39]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>5±0.9 73.7±0.8 69.0±1.0 66.3±0.6 66.5±0.9 39.7±1.1 88.6±0.5 Cosine Similarity 51.3±1.1 89.4±0.7 80.5±0.8 70.9±1.0 69.7±0.7 72.6±0.9 41.9±1.0 89.3±0.6 Absolute Distance (L1) 53.6±1.1 90.6±0.6 81.0±0.7 73.2±0.9 61.1±0.7 74.1±0.8 47.0±1.0 87.3±0.6 Squared Euclidean (L2 2 ) 53.9±1.1 90.9±0.6 81.8±0.7 73.1±0.9 64.4±0.7 74.9±0.8 45.8±1.0 88.8±0.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">In-Domain Accuracy (%)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell>ImageNet Omniglot</cell><cell>Aircraft</cell><cell>Birds</cell><cell>DTD</cell><cell>QuickDraw</cell><cell>Fungi</cell><cell>Flower</cell></row><row><cell cols="5">Negative Dot Product 83.Simple CNAPS -TR 48.0±1.1 56.7±1.1 91.1±0.7 83.0±0.7 74.6±0.9 70.2±0.8 Simple CNAPS 58.6±1.1 91.7±0.6 82.4±0.7 74.9±0.8 67.8±0.8</cell><cell>76.3±0.9 77.7±0.7</cell><cell cols="2">46.4±1.0 90.0±0.6 46.9±1.0 90.7±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Accuracy (%) compared to mini/tieredImageNet baselines. Performance measures reported for CNAPS and Simple CNAPS are averaged across 5 different runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table E</head><label>E</label><figDesc></figDesc><table /><note>.5: ResNet-18 basic block b.Layers Table E.6: ResNet-18 basic scaling block b.</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study of the Feature Extractor Adaptation Network</head><p>In addition to the choice of metric ablation study referenced in Section 6.2, we examine the behaviour of the model when the feature extractor adaptation network ψ f φ has been turned off. In such setting, the feature extractor would only consist of the pre-trained ResNet18 <ref type="bibr" target="#b9">[10]</ref> f θ . Consistent to <ref type="bibr" target="#b29">[30]</ref>, we refer to this setting as "No Adaptation" (or "No Adapt" for short). We compare the "No Adapt" variant to the feature extractor adaptive case for each of the metrics/model variants examined in Section 6.2. The in-domain, out-of-domain and overall classification accuracies are shown in <ref type="table">Table 7</ref>. As shown, without ψ f φ all models lose approximately 15, 5, and 12 percentage points across in-domain, out-of-domain and overall accuracy, while Simple CNAPS continues to hold the lead especially in out-of-domain classification accuracy. It's interesting to note that without the task specific regularization term (denoted as "-TR"), there's a considerable performance drop in the "No Adaptation" setting; while when the feature extractor adaptation network ψ f φ is present, the difference is marginal. This signifies two important observations. First, it shows the importance of of learning the feature extractor adaptation module end-to-end with the Mahalanobis distance, as it's able adapt the feature space best suited for using the squared Mahalanobis distance. Second, the adaptation function ψ f φ can reduce the importance of the task regularizer by properly de-correlating and normalizing variance within the feature vectors. However, where this is not possible, as in the "No Adaptation" case, the allclasses-task-level covariance estimate as an added regularizer in Equation 2 becomes crucial in maintaining superior performance. . Note that since this is conditioned on the channel outputs of the convolutional filter, it's not convolved any further. b) Task encoder g φ that mean-pools convolutionally filtered support examples to produce the task representation. c) architectural overview of the classifier adaptation network ψ c φ consisting of a 3 layer MLP with a residual connection. Diagrams are based on <ref type="table">Table E</ref>.8, E.9, and E.11 in <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Projection Networks</head><p>We additionally explored metric learning where in addition to changing the distance metric, we considered projecting each support feature vector f τ θ (x i ) and query vector f τ θ (x * i ) to a new decision space where then squared Mahalanobis distance was to be used for classification. Specifically, we trained a projection network u φ such that for Equations 2 and 3, µ k , Σ τ k and Σ τ were calculated based on  </p><p>Similarly, the projected query feature vector u φ (f τ θ (x * i )) was used for classifying the query example as oppose to the bare feature vector f τ θ (x * i ) used within Simple CNAPS. We define u φ in our experiments to be the following:</p><p>where ELU, a continuous approximation to ReLU as previously noted, is used as the choice of non-linearity and W 1 , W 2 and W 3 are learned parameters. We refer to this variant of our model as "Simple CNAPS +P" with the "+P" tag signifying the addition of the projection function u φ . The results for this variant of Simple CNAPS are compared to the base Simple CNAPS in <ref type="table">Table  9</ref>. As shown, the projection network generally results in lower performance, although not to statistically significant degrees in in-domain and overall accuracies. Where the addition of the projection network results in substantial loss of performance is in the out-of-domain setting with Simple CNAPS +P's average accuracy of 67.1±0.8 compared to 69.7±0.8 for the Simple CNAPS. We hypothesize the significant loss in out-of-domain performance to be due to the projection network overfitting to the in-domain datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering with bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gaussian prototypical networks for few-shot learning on omniglot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<idno>abs/1708.02735</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The mahalanobis distance for functional data with applications to classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Galeano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Lillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<idno>abs/1807.01613</idno>
		<title level="m">Conditional neural processes. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1804.09458</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01102</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comprehensive survey of deep learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Shiratuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Laga</surname></persName>
		</author>
		<idno>118:1-118:36</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The german traffic sign detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2013 international joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A survey of deep learning-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qu</surname></persName>
		</author>
		<idno>abs/1907.09408</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The quick, draw!-ai experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jongejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fox-Gieg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Edgelabeling graph neural network for few-shot learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Meta-learning with temporal convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On first-order metalearning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Rodríguez</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Metalearning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07697</idno>
		<title level="m">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno>abs/1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">One-shot learning with a hierarchical nonparametric bayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>PMLR. 8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</title>
		<editor>I. Guyon, G. Dror, V. Lemaire, G. Taylor, and D. Silver</editor>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-02" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fgvcx fungi classification challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<ptr target="https://github.com/visipedia/fgvcx_fungi_comp" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey on image classification and activity recognition using deep convolutional neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sornam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muthusubash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanitha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Advanced Computing (ICoAC)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<title level="m">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<idno>13:1-13:37</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Few-shot learning: A survey. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pinheiro. Adaptive cross-modal few-shot learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno>abs/1411.1792</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
