<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-24">24 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
						</author>
						<title level="a" type="main">Cascade R-CNN: High Quality Object Detection and Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-24">24 Jun 2019</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object Detection</term>
					<term>High Quality</term>
					<term>Cascade</term>
					<term>Bounding Box Regression</term>
					<term>Instance Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In object detection, the intersection over union (IoU) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its quality. While the commonly used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting, due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, composed of a sequence of detectors trained with increasing IoU thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-CNN without bells or whistles achieves state-of-the-art performance on the COCO dataset, and significantly improves high-quality detection on generic and specific object detection datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally, the Cascade R-CNN is generalized to instance segmentation, with nontrivial improvements over the Mask R-CNN. To facilitate future research, two implementations are made available at https://github.com/zhaoweicai/cascade-rcnn (Caffe) and https://github.com/zhaoweicai/Detectron-Cascade-RCNN (Detectron). person: 1.00 person: 1.00 person: 0.99 person: 0.99 person: 0.87 person: 0.82 person: 0.77 person: 0.70 person: 0.64 person: 0.63 person: 0.56 frisbee: 1.00 frisbee: 1.00 frisbee: 0.99 frisbee: 0.97 (a) Detection of u = 0.5 person: 1.00 person: 0.99 person: 0.96 person: 0.94 person: 0.55 frisbee: 0.99 frisbee: 0.99 frisbee: 0.99 frisbee: 0.93</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Object detection is a complex problem, requiring the solution of two tasks. First, the detector must solve the recognition problem, distinguishing foreground objects from background and assigning them the proper object class labels. Second, the detector must solve the localization problem, assigning accurate bounding boxes to different objects. An effective architecture for the solution of the two tasks, on which many of the recently proposed object detectors are based, is the two-stage R-CNN framework <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b47">[47]</ref>. This frames detection as a multi-task learning problem that combines classification, to solve the recognition problem, and bounding box regression, to solve localization.</p><p>Despite the success of this architecture, the two problems can be difficult to solve accurately. This is partly due to the fact that there are many "close" false positives, corresponding to "close but not correct" bounding boxes. An effective detector must find all true positives in an image, while suppressing these close false positives. This requirement makes detection more difficult than other classification problems, e.g. object recognition, where the difference between positives and negatives is not as fine-grained. In fact, the boundary between positives and negatives must be carefully defined. In the literature, this is done by thresholding the intersection over union (IoU) score between candidate and ground truth bounding boxes. While the threshold is typically set at the value of u = 0.5, this is a very loose requirement for positives. The resulting detectors frequently produce noisy bounding boxes, as shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. Hypotheses that most humans would consider close false positives frequently pass the IoU ≥ 0.5 test. While training examples assembled under the u = 0.5 criterion are rich and diverse, they make it difficult to train detectors that can effectively reject close false positives.</p><p>In this work, we define the quality of a detection hypothesis as its IoU with the ground truth, and the quality of a detector as the IoU threshold u used to train it. Some examples of hypotheses of increasing quality are shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>. The goal is to investigate the poorly researched problem of learning high quality object detectors. As shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, these are detectors that produce few close false positives. The starting premise is that a single detector can only be optimal for a single quality level. This is known in the cost-sensitive learning literature <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b42">[42]</ref>, where the optimization of different points of the receiver operating characteristic (ROC) requires different loss functions. The main difference is that we consider the optimization for a given IoU threshold, rather than false positive rate. Some evidence in support of this premise is given in <ref type="figure" target="#fig_1">Fig. 2</ref>, which presents the bounding box localization performance, classification loss and detection performance, respectively, of three detectors trained with IoU thresholds of u = 0.5, 0.6, 0.7. Localization and classification are evaluated as a function of the detection hypothesis IoU. Detection is evaluated as a function of the IoU threshold, as in COCO <ref type="bibr" target="#b36">[36]</ref>. <ref type="figure" target="#fig_1">Fig. 2 (a)</ref> shows that the three bounding box regressors tend to achieve the best performance for examples of IoU in the vicinity of the threshold used for detector training. <ref type="figure" target="#fig_1">Fig. 2 (c)</ref> shows a similar effect for detection, up to some overfitting for the highest thresholds. The detector trained with u = 0.5 outperforms the detector trained with u = 0.6 for low IoUs, underperforming it at higher IoUs. In general, a detector optimized for a single IoU value is not optimal for other values. This is also confirmed by the classification loss, shown in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>, whose peaks are near the thresholds used for detector training. In general, the threshold determines the classification boundary where the classifier is most discriminative, i.e. has largest margin <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>The observations above suggest that high quality detection requires a close match between the quality of the detector and that of the detection hypotheses. The detector will only achieve high quality if presented with high quality proposals. This, however, cannot be guaranteed by simply increasing the threshold u during training. On the contrary, as seen for the detector of u = 0.7 in <ref type="figure" target="#fig_1">Fig. 2</ref> (c), forcing a high value of u usually degrades detection performance. We refer to this problem, i.e. that training a detector with higher threshold leads to poorer performance, as the paradox of high-quality detection. This problem has two causes. First, object proposal mechanisms tend to produce hypotheses distributions heavily imbalanced towards low quality. In result, the use of larger IoU thresholds during training exponentially reduces the number of positive training examples. This is particularly problematic for neural networks, which are very example intensive, making the "high u" training strategy very prone to overfitting. Second, there is a mismatch between the quality of the detector and that of the hypotheses available at inference time. Since, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, high quality detectors are only optimal for high quality hypotheses, detection performance can degrade substantially for hypotheses of lower quality.</p><p>In this paper, we propose a new detector architecture, denoted as Cascade R-CNN, that addresses these problems, to enable high quality object detection. The new architecture is a multi-stage extension of the R-CNN, where detector stages deeper into the cascade are sequentially more selective against close false positives. As is usual for classifier cascades <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b54">[54]</ref>, the cascade of R-CNN stages is trained sequentially, using the output of one stage to train the next. This leverages the observation that the output IoU of a bounding box regressor is almost always better than its input IoU, as can be seen in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, where nearly all plots are above the gray line. In result, the output of a detector trained with a certain IoU threshold is a good hypothesis distribution to train the detector of the next higher IoU threshold. This has some similarity to boostrapping methods commonly used to assemble datasets for object detection <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b54">[54]</ref>. The main difference is that the resampling performed by the Cascade R-CNN does not aim to mine hard negatives. Instead, by adjusting bounding boxes, each stage aims to find a good set of close false positives for training the next stage. The main outcome of this resampling is that the quality of the detection hypotheses increases gradually, from one stage to the next. In result, the sequence of detectors addresses the two problems underlying the paradox of highquality detection. First, because the resampling operation guarantees the availability of a large number of examples for the training of all detectors in the sequence, it is possible to train detectors of high IoU without overfitting. Second, the use of the same cascade procedure at inference time produces a set of hypotheses of progressively higher quality, well matched to the increasing quality of the detector stages. This enables higher detection accuracies, as suggested by <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The Cascade R-CNN is quite simple to implement and trained end-to-end. Our results show that a vanilla implementation, without any bells and whistles, surpasses almost all previous state-of-the-art single-model detectors, on the challenging COCO detection task <ref type="bibr" target="#b36">[36]</ref>, especially under the stricter evaluation metrics. In addition, the Cascade R-CNN can be built with any two-stage object detector based on the R-CNN framework. We have observed consistent gains (of 2∼4 points, and more under stricter localization metrics), at a marginal increase in computation. This gain is independent of the strength of the baseline object detectors, for all the models we have tested. We thus believe that this simple and effective detection architecture can be of interest for many object detection research efforts.</p><p>A preliminary version of this manuscript was previously published in <ref type="bibr" target="#b2">[3]</ref>. After the original publication, the Cascade R-CNN has been successfully reproduced within many different codebases, including the popular Detectron <ref type="bibr" target="#b19">[20]</ref>, PyTorch 1 , and TensorFlow 2 , showing consistent and reliable improvements independently of implementation codebase. In this expanded version, we have extended the Cascade R-CNN to instance segmentation, by adding a mask head to the cascade, denoted as Cascade Mask R-CNN. This is shown to achieve non-trivial improvements over the popular Mask R-CNN <ref type="bibr" target="#b24">[25]</ref>. A new and more extensive evaluation is also presented, showing that the Cascade R-CNN is compatible with many complementary enhancements proposed in the detection and instance segmentation literatures, some of which were introduced after [3], e.g. GroupNorm <ref type="bibr" target="#b56">[56]</ref>. Finally, we further present the results of a larger set of experiments, performed on various popular generic/specific object detection datasets, including PASCAL VOC <ref type="bibr" target="#b12">[13]</ref>, KITTI <ref type="bibr" target="#b15">[16]</ref>, CityPerson <ref type="bibr" target="#b64">[64]</ref> and Wider-Face <ref type="bibr" target="#b61">[61]</ref>. These experiments demonstrate that the paradox of high quality object detection applies to all these tasks, and that the Cascade R-CNN enables more effective high quality detection than previously available methods. Due to these properties, as well as its generality and flexibility, the Cascade R-CNN has recently been adopted by the winning teams of the COCO 2018 instance segmentation challenge 3 , the OpenImage 2018 challenge 4 , and the Wider Challenge 2018 5 . To facilitate future research, we have released the code on two codebases, Caffe <ref type="bibr" target="#b31">[31]</ref> and Detectron <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Due to the success of the R-CNN <ref type="bibr" target="#b21">[22]</ref> detector, which combines a proposal detector and a region-wise classifier, this two-stage architecture has become predominant in the recent past. To reduce redundant CNN computations, the SPP-Net <ref type="bibr" target="#b25">[26]</ref> and Fast R-CNN <ref type="bibr" target="#b20">[21]</ref> introduced the idea of region-wise feature extraction, enabling the sharing of the bulk of feature computations by object instances. The Faster R-CNN <ref type="bibr" target="#b47">[47]</ref> then achieved further speeds-up by introducing a region proposal network (RPN), becoming the cornerstone of modern object detection. Later, some works extended this detector to address various problems of detail. For example, the R-FCN <ref type="bibr" target="#b7">[8]</ref> proposed efficient region-wise full convolutions to avoid the heavy CNN computations of the Faster R-CNN; and the Mask R-CNN <ref type="bibr" target="#b24">[25]</ref> added a network head that computes object masks to support instance segmentation. Some more recent works have focused on normalizing feature statistics <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b56">[56]</ref>, modeling relations between instances <ref type="bibr" target="#b27">[28]</ref>, non maximum suppression (NMS) <ref type="bibr" target="#b0">[1]</ref>, and other aspects <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b50">[50]</ref>.</p><p>Scale invariance, an important requisite for effective object detection, has also received substantial attention in the literature <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b52">[52]</ref>. While natural images contain objects at various scales, the fixed receptive field size of the filters implemented by the RPN <ref type="bibr" target="#b47">[47]</ref> makes it prone to scale 1. https://github.com/open-mmlab/mmdetection 2. https://github.com/tensorpack/tensorpack 3. http://cocodataset.org/#detection-leaderboard 4. https://storage.googleapis.com/openimages/web/challenge.html 5. http://wider-challenge.org/ mismatches. To overcome this, the MS-CNN <ref type="bibr" target="#b1">[2]</ref> introduced a multi-scale object proposal network, by generating outputs at multiple layers. This leverages the different receptive field sizes of the different layers to produce a set of scale-specific proposal generators, which is then combined into a strong multi-scale generator. Similarly, the FPN <ref type="bibr" target="#b37">[37]</ref> detects highrecall proposals at multiple output layers, with recourse to a scale-invariant feature representation by adding a top-down connection across feature maps of different network depths. Both the MS-CNN and FPN rely on a feature pyramid representation for multi-scale object detection. SNIP <ref type="bibr" target="#b52">[52]</ref>, on the other hand, recently revisited image pyramid in modern object detection. It normalizes the gradients from different object scales during training, such that the whole detector is scale-specific. Scale-invariant detection is achieved by using an image pyramid at inference.</p><p>One-stage object detection architectures have also become popular for their computational efficiency. YOLO <ref type="bibr" target="#b46">[46]</ref> outputs very sparse detection results and enables realtime object detection, by forwarding the input image once through an efficient backbone network. SSD <ref type="bibr" target="#b40">[40]</ref> detects objects in a way similar to the RPN <ref type="bibr" target="#b47">[47]</ref>, but uses multiple feature maps at different resolutions to cover objects at various scales. The main limitation of these detectors is that their accuracy is typically below that of two-stage detectors. The RetinaNet <ref type="bibr" target="#b38">[38]</ref> detector was proposed to address the extreme foreground-background class imbalance of dense object detection, achieving results comparable to two-stage detectors. Recently, CornerNet <ref type="bibr" target="#b33">[33]</ref> proposed to detect an object bounding box as a pair of keypoints, abandoning the widely used concept of anchors first introduced by the Faster R-CNN. This detector has achieved very good performance with the help of some training and testing enhancements. RefineDet <ref type="bibr" target="#b65">[65]</ref> added an anchor refinement module to the single-shot SSD <ref type="bibr" target="#b40">[40]</ref>, to improve localization accuracy. This is somewhat similar to the cascaded localization implemented by the proposed Cascade R-CNN, but ignores the problem of high-quality detection.</p><p>Some explorations in multi-stage object detection have also been proposed. The multi-region detector of <ref type="bibr" target="#b16">[17]</ref> introduced iterative bounding box regression, where a R-CNN is applied several times to produce successively more accurate bounding boxes. <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b60">[60]</ref> used a multi-stage procedure to generate accurate proposals, which are forwarded to an accurate model (e.g. Fast R-CNN). <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b62">[62]</ref> proposed an alternative procedure to localize objects sequentially. While this is similar in spirit to the Cascade-RCNN, these methods use the same regressor iteratively for accurate localization. On the other hand, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b44">[44]</ref> embedded the classic cascade architecture of <ref type="bibr" target="#b54">[54]</ref> in an object detection network. Finally, <ref type="bibr" target="#b6">[7]</ref> iterated between the detection and segmentation tasks, to achieve improved instance segmentation.</p><p>Upon publication of the conference version of this manuscript, several works have pursued the idea behind Cascade R-CNN <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b55">[55]</ref>. <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b55">[55]</ref> applied it to single-shot object detectors, showing nontrivial improvements for high quality single-shot detection, for general objects and pedestrians, respectively. The IoU-Net <ref type="bibr" target="#b32">[32]</ref> explored in greater detail high-quality localization, achieving some gains over the Cascade R-CNN by cascading more bounding box regression steps. <ref type="bibr" target="#b23">[24]</ref> showed it is possible to achieve state-of-the-art object detectors without ImageNet pretraining, with a help of the Cascade R-CNN. These works show that the Cascade R-CNN idea is robust and applicable to various object detection architectures. This suggests that it should continue to be useful despite future advances in object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HIGH QUALITY OBJECT DETECTION</head><p>In this section, we discuss the challenges of high quality object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Object Detection</head><p>While the ideas proposed in this work can be applied to various detector architectures, we focus on the popular twostage architecture of the Faster R-CNN <ref type="bibr" target="#b47">[47]</ref>, shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (a). The first stage is a proposal sub-network, in which the entire image is processed by a backbone network, e.g. ResNet <ref type="bibr" target="#b26">[27]</ref>, and a proposal head ("H0") is applied to produce preliminary detection hypotheses, known as object proposals. In the second stage, these hypotheses are processed by a region-of-interest detection sub-network ("H1"), denoted as a detection head. A final classification score ("C") and a bounding box ("B") are assigned per hypothesis. The entire detector is learned end-to-end, using a multi-task loss with bounding box regression and classification components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Bounding Box Regression</head><formula xml:id="formula_0">A bounding box b = (b x , b y , b w , b h )</formula><p>contains the four coordinates of an image patch x. Bounding box regression aims to regress a candidate bounding box b into a target bounding box g, using a regressor f (x, b). This is learned from a training set (g i , b i ), by minimizing the risk</p><formula xml:id="formula_1">R loc [f ] = i L loc (f (x i , b i ), g i ).</formula><p>(</p><p>As in Fast R-CNN <ref type="bibr" target="#b20">[21]</ref>,</p><formula xml:id="formula_3">L loc (a, b) = i∈{x,y,w,h} smooth L1 (a i − b i )<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">smooth L1 (x) = 0.5x 2 , |x| &lt; 1 |x| − 0.5, otherwise,<label>(3)</label></formula><p>is the smooth L 1 loss function. To encourage invariance to scale and location, smooth L1 operates on the distance vector ∆ = (δ x , δ y , δ w , δ h ) defined by</p><formula xml:id="formula_5">δ x = (g x − b x )/b w , δ y = (g y − b y )/b h δ w = log(g w /b w ), δ h = log(g h /b h ).<label>(4)</label></formula><p>Since bounding box regression usually performs minor adjustments on b, the numerical values of (4) can be very small. This usually makes the regression loss much smaller than the classification loss. To improve the effectiveness of multi-task learning, ∆ is normalized by its mean and variance, e.g. δ x is replaced by</p><formula xml:id="formula_6">δ ′ x = δ x − µ x σ x .<label>(5)</label></formula><p>This is widely used in the literature <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b47">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Classification</head><p>The classifier is a function h(x) that assigns an image patch x to one of M +1 classes, where class 0 contains background and the remaining classes the objects to detect. h(x) is a M + 1-dimensional estimate of the posterior distribution over classes, i.e. h k (x) = p(y = k|x), where y is the class label. Given a training set (x i , y i ), it is learned by minimizing the classification risk</p><formula xml:id="formula_7">R cls [h] = i L cls (h(x i ), y i ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">L cls (h(x), y) = − log h y (x)<label>(7)</label></formula><p>is the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Detection Quality</head><p>Consider a ground truth object of bounding box g associated with class label y, and a detection hypothesis x of bounding box b. Since a b usually includes an object and some amount of background, it can be difficult to determine if a detection is correct or not. This is usually addressed by the intersection over union (IoU) metric</p><formula xml:id="formula_9">IoU (b, g) = b ∩ g b ∪ g .<label>(8)</label></formula><p>If the IoU is above a threshold u, the patch is considered an example of the class of the object of bounding box g and denoted "positive". Thus, the class label of a hypothesis x is a function of u,</p><formula xml:id="formula_10">y u = y, IoU (b, g) ≥ u 0, otherwise.<label>(9)</label></formula><p>If the IoU does not exceed the threshold for any object, x is assigned to the background and denoted "negative". Although there is no need to define positive/neagtive examples for the bounding box regression task, an IoU threshold u is also required to select the set of samples</p><formula xml:id="formula_11">G = {(g i , b i )|IoU (b i , g i ) ≥ u}<label>(10)</label></formula><p>used to train the regressor. While the IoU thresholds used for the two tasks do not have to be identical, this is usual in practice. Hence, the IoU threshold u defines the quality of a detector. Large thresholds encourage detected bounding boxes to be tightly aligned with their ground truth counterparts. Small thresholds reward detectors that produce loose bounding boxes, of small overlap with the ground truth. A main challenge of object detection is that, no matter the choice of threshold, the detection setting is highly adversarial. When u is high, positives contain less background but it is difficult to assemble large positive training sets. When u is low, richer and more diverse positive training sets are possible, but the trained detector has little incentive to reject close false positives. In general, it is very difficult to guarantee that a single classifier performs uniformly well over all IoU levels. At inference, since the majority of the hypotheses produced by a proposal detector, e.g. RPN <ref type="bibr" target="#b47">[47]</ref> or selective search <ref type="bibr" target="#b53">[53]</ref>, have low quality, the detector must be more discriminant for lower quality hypotheses. A standard compromise between these conflicting requirements is to settle on u = 0.5, which is used in almost all modern object detectors. This, however, is a relatively low threshold, leading to low quality detections that most humans consider close false positives, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Challenges to High Quality Detection</head><p>Despite the significant progress in object detection of the past few years, few works attempted to address high quality detection. This is mainly due to the following reasons.</p><p>First, evaluation metrics have historically placed greater emphasis on the low quality detection regime. For performance evaluation, an IoU threshold u is used to determine whether a detection is a success (IoU (b, g) ≥ u) or failure (IoU (b, g) &lt; u). Many object detection datasets, including PASCAL VOC <ref type="bibr" target="#b12">[13]</ref>, ImageNet <ref type="bibr" target="#b48">[48]</ref>, Caltech Pedestrian <ref type="bibr" target="#b10">[11]</ref>, etc., use u = 0.5. This is partly because these datasets were established a while ago, when object detection performance was far from what it is today. However, this loose evaluation standard is adopted even by relatively recent datasets, such as WiderFace <ref type="bibr" target="#b61">[61]</ref>, or CityPersons <ref type="bibr" target="#b64">[64]</ref>. This is one of the main reasons why performance has saturated for many of these datasets. Others, such as COCO <ref type="bibr" target="#b36">[36]</ref> or KITTI <ref type="bibr" target="#b15">[16]</ref> use stricter evaluation metrics: average precision at u = 0.7 for car in KITTI, and mean average precision across u = [0.5 : 0.05 : 0.95] in COCO. While recent works have focused on these less saturated datasets, most detectors are still designed with the loose IoU threshold of u = 0.5, associated with the low-quality detection regime. In this work, we show that there is plenty of room for improvement when a stricter evaluation metric, e.g. u ≥ 0.75, is used and that it is possible to achieve significant improvements by designing detectors specifically for the high quality regime.</p><p>Second, the design of high quality object detectors is not a trivial generalization of existing approaches, due to the paradox of high quality detection. To beat the paradox, it is necessary to match the qualities of the hypotheses generator and the object detector. In the literature, there have been efforts to increase the quality of hypotheses, e.g. by iterative bounding box regression <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> or better RPN design <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b37">[37]</ref>, and some efforts to increase the quality of the object detector, e.g. by using the integral loss on a set of IoU thresholds <ref type="bibr" target="#b63">[63]</ref>. These attempts fail to guarantee high quality detection because they consider only one of the goals, missing the fact that the qualities of both tasks need to be increased simultaneously. On one hand, raising the quality of the hypotheses has little benefit if the detector remains of low quality, because the latter is not trained to discriminate high quality from low quality hypotheses. On the other, if only the detector quality is increased, there are too few high quality hypotheses for it to classify, leading to no detection improvement. In fact, because, as shown in <ref type="figure" target="#fig_3">Fig. 4 (left)</ref>, the set of positive samples decreases quickly with u, a high u detector is prone to overfitting. Hence, a high u detector can easily overfit and perform worse than a low u detector, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CASCADE R-CNN</head><p>In this section we introduce the Cascade R-CNN detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>The architecture of the Cascade R-CNN is shown in <ref type="figure" target="#fig_2">Fig.  3 (b)</ref>. It is a multi-stage extension of the Faster R-CNN architecture of <ref type="figure" target="#fig_2">Fig. 3</ref> (a). In this work, we focus on the the detection sub-network, simply adopting the RPN <ref type="bibr" target="#b47">[47]</ref> of <ref type="figure" target="#fig_2">Fig.  3</ref> (a) for proposal detection. However, the Cascade R-CNN is not limited to this proposal mechanism, other choices should be possible. As discussed in the section above, the goal is to increase the quality of hypotheses and detector simultaneously, to enable high quality object detection. This is achieved with a combination of cascaded bounding box regression and cascaded detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cascaded Bounding Box Regression</head><p>High quality hypotheses can be easily produced during training, where ground truth bounding boxes are available, e.g. by sampling around the ground truth. The difficulty is to produce high quality proposals at inference, when ground truth is unavailable. This problem is addressed with resort to cascaded bounding box regression. As shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, a single regressor cannot usually perform uniformly well over all quality levels. However, as is commonly done for pose regression <ref type="bibr" target="#b9">[10]</ref> or face alignment <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>, the regression task can be decomposed into a sequence of simpler steps. In the Cascade R-CNN detector, the idea is implemented with a cascaded regressor with the architecture of <ref type="figure" target="#fig_2">Fig. 3 (b)</ref>. This consists of a cascade of specialized regressors  where T is the total number of cascade stages. The key point is that each regressor f t is optimized for the bounding box distribution {b t } generated by the previous regressor, rather than the initial distribution {b 1 }. In this way, the hypotheses are improved progressively. This is illustrated in <ref type="figure" target="#fig_5">Fig. 5</ref>, which presents the distribution of the regression distance vector ∆ = (δ x , δ y , δ w , δ h ) at different cascade stages. Note that most hypotheses become closer to the ground truth as they progress through the cascade. There are also some hypotheses that fail to meet the stricter IoU criteria of the later cascade stages. These are declared outliers and eliminated. It should be noted that, as discussed in Section 3.1.1, ∆ needs be mean/variance normalized, as in <ref type="formula" target="#formula_6">(5)</ref>, for effective multi-task learning. The mean and variance statistics computed after this outlier removal step are used to normalize ∆ at each cascade stage. Our experiments show that this implementation of cascaded bounding box regression generates hypotheses of very high quality at both training and inference.</p><formula xml:id="formula_12">f (x, b) = f T • f T −1 • · · · • f 1 (x, b),<label>(11)</label></formula><formula xml:id="formula_13">µ x = 0.0032 µ y = −0.0021 σ x = 0.0391 σ y = 0.0376 −1 0 1 −1 0 1 δ w δ h 1st stage µ w = 0.0161 µ h = 0.0498 σ w = 0.2272 σ h = 0.2255 −1 0 1 −1 0 1 δ w δ h 2nd stage µ w = −0.0007 µ h = 0.0122 σ w = 0.1221 σ h = 0.1230 −1 0 1 −1 0 1 δ w δ h 3rd stage µ w = −0.0017 µ h = 0.0004 σ w = 0.0798 σ h = 0.0773</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cascaded Detection</head><p>As shown in the left of <ref type="figure" target="#fig_3">Fig. 4</ref>, the initial hypotheses distribution produced by the RPN is heavily tilted towards low quality. For example, only 2.9% of examples are positive for an IoU threshold u = 0.7. This makes it difficult to train a high quality detector. The Cascade R-CNN addresses the problem by using cascade regression as a resampling mechanism. This is inspired by <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, where nearly all curves are above the diagonal gray line, showing that a bounding box regressor trained for a certain u tends to produce bounding boxes of higher IoU. Hence,</p><formula xml:id="formula_14">starting from examples {(x i , b i )}, cascade regression successively resamples an example distribution {(x ′ i , b ′ i )} of higher IoU.</formula><p>This enables the sets of positive examples of the successive stages to keep a roughly constant size, even when the detector quality u is increased. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates this property, showing how the example distribution tilts more heavily towards high quality examples after each resampling step.</p><p>At each stage t, the R-CNN head includes a classifier h t and a regressor f t optimized for the corresponding IoU threshold u t , where u t &gt; u t−1 . These are learned with loss</p><formula xml:id="formula_15">L(x t , g) = L cls (h t (x t ), y t ) + λ[y t ≥ 1]L loc (f t (x t , b t ), g), (12) where b t = f t−1 (x t−1 , b t−1 )</formula><p>, g is the ground truth object for x t , λ = 1 the trade-off coefficient, y t is the label of x t under the u t criterion, according to <ref type="bibr" target="#b8">(9)</ref>, [·] is the indicator function. Note that the use of [·] implies that the IoU threshold u of bounding box regression is identical to that used for classification. This cascade learning has three important consequences for detector training. First, the potential for overfitting at large IoU thresholds u is reduced, since positive examples become plentiful at all stages (see <ref type="figure" target="#fig_3">Fig. 4</ref>). Second, detectors of deeper stages are optimal for higher IoU thresholds. Third, because some outliers are removed as the IoU threshold increases (see <ref type="figure" target="#fig_5">Fig. 5</ref>), the learning effectiveness of bounding box regression increases in the later stages. This simultaneous improvement of hypotheses and detector quality enables the Cascade R-CNN to beat the paradox of high quality detection. At inference, the same cascade is applied. The quality of the hypotheses is improved sequentially, and higher quality detectors are only required to operate on higher quality hypotheses, for which they are optimal. This enables the high quality object detection results of <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, as suggested by <ref type="figure" target="#fig_1">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Differences from Previous Works</head><p>The Cascade R-CNN has similarities to previous works using iterative bounding box regression and integral loss for detection. There are, however, important differences.</p><p>Iterative Bounding Box Regression: Some works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b26">[27]</ref> have previously argued that the use of a single bounding box regressor f is insufficient for accurate localization.</p><p>These methods apply f iteratively, as a post-processing step</p><formula xml:id="formula_16">f ′ (x, b) = f • f • · · · • f (x, b),<label>(13)</label></formula><p>that refines a bounding box b. This is called iterative bounding box regression and denoted as iterative BBox. It can be implemented with the inference architecture of <ref type="figure" target="#fig_2">Fig. 3 (c)</ref> where all heads are identical. Note that this is only for inference, as training is identical to that of a two-stage object detector, e.g. the Faster R-CNN of <ref type="figure" target="#fig_2">Fig. 3 (a)</ref> with u = 0.5. This approaches ignores two problems. First, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, a regressor f trained at u = 0.5 is suboptimal for hypotheses of higher IoUs. It actually degrades bounding box accuracy for IoUs larger than 0.85. Second, as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, the distribution of bounding boxes changes significantly after each iteration. While the regressor is optimal for the initial distribution it can be quite suboptimal after that. Due to these problems, iterative BBox requires a fair amount of human engineering, in the form of proposal accumulation, box voting, etc, and has somewhat unreliable gains <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Usually, there is no benefit beyond applying f twice. The Cascade R-CNN differs from iterative BBox in several ways. First, while iterative BBox is a post-processing procedure used to improve bounding boxes, the Cascade R-CNN uses cascade regression as a resampling mechanism that changes the distribution of hypotheses processed by the different stages. Second, because cascade regression is used at both training and inference, there is no discrepancy between training and inference distributions. Third, the multiple specialized regressors {f T , f T −1 , · · · , f 1 } are optimal for the resampled distributions of the different stages. This is unlike the single f of (13), which is only optimal for the initial distribution. Our experiments show that the Cascade R-CNN enables more precise localization than that possible with iterative BBox, and requires no human engineering.</p><p>Integral Loss: <ref type="bibr" target="#b63">[63]</ref> proposed an ensemble of classifiers with the architecture of <ref type="figure" target="#fig_2">Fig. 3 (d)</ref> and trained with the integral loss. This is a loss</p><formula xml:id="formula_17">L cls (h(x), y) = u∈U L cls (h u (x), y u )<label>(14)</label></formula><p>that targets various quality levels, defined by a set of IoU thresholds U = {0.5, 0.55, · · · , 0.75}, chosen to fit the evaluation metric of the COCO challenge. The Cascade R-CNN differs from this detector in several ways. First, <ref type="bibr" target="#b13">(14)</ref> fails to address the problem that the various loss terms operate on different numbers of positives. As shown on <ref type="figure" target="#fig_3">Fig. 4 (left)</ref>, the set of positive samples decreases quickly with u. This is particularly problematic because it makes the high quality classifiers very prone to overfitting. On the other hand, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the resampling of the Cascade R-CNN produces a nearly constant number of positive examples as the IoU threshold u increases. Second, at inference, the high quality classifiers are required to process proposals of overwhelming low quality, for which they are not optimal. This is unlike the higher quality detectors of the Cascade R-CNN, which are only required to operate on higher quality hypotheses. Third, the integral loss is designed to fit the COCO metrics and, by definition, the classifiers are ensembled at inference. The Cascade R-CNN aims to achieve high quality detection, and the high quality detector itself in the last stage can obtain the stateof-the-art detection performance. Due to all this, the integral loss detector of <ref type="figure" target="#fig_2">Fig. 3 (d)</ref> usually fails to outperform the vanilla detector of <ref type="figure" target="#fig_2">Fig. 3 (a)</ref>, for most quality levels. This is unlike the Cascade R-CNN, which can have significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INSTANCE SEGMENTATION</head><p>Instance segmentation has become popular in the recent past <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b39">[39]</ref>. It aims to predict pixel-level segmentation for each instance, in addition to determining its object class. This is more difficult than object detection, which only predicts a bounding box (plus class) per instance. In general, instance segmentation is implemented in addition to object detection, and a stronger object detector usually leads to improved instance segmentation. The most popular instance segmentation method is arguably the Mask R-CNN <ref type="bibr" target="#b24">[25]</ref>. Like the Cascade R-CNN, it is a variant on the two-stage detector. In this section, we extend the Cascade R-CNN architecture to the instance segmentation task, by adding a segmentation branch similar to that of the Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mask R-CNN</head><p>The Mask R-CNN <ref type="bibr" target="#b24">[25]</ref> extends the Faster R-CNN by adding a segmentation branch in parallel to the existing detection branch during training. It has the architecture of <ref type="figure" target="#fig_6">Fig. 6 (a)</ref>. The training instances are the positive examples also used to train the detection task. At inference, object detections are complemented with segmentation masks, for all detected objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cascade Mask R-CNN</head><p>In the Mask R-CNN, the segmentation branch is inserted in parallel to the detection branch. However, the Cascade R-CNN has multiple detection branches. This raises the questions of 1) where to add the segmentation branch and 2) how many segmentation branches to add. We consider three strategies for mask prediction in the Cascade R-CNN. The first two strategies address the first question, adding a single mask prediction head at either the first or last stage of the Cascade R-CNN, as shown in <ref type="figure" target="#fig_6">Fig. 6 (b)</ref> and (c), respectively. Since the instances used to train the segmentation branch are the positives of the detection branch, their number varies in these two strategies. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, placing the segmentation head later on the cascade leads to more examples. However, because segmentation is a pixel-wise operation, a large number of highly overlapping instances is not necessarily as helpful as for object detection, which is a patch-based operation. The third strategy addresses the second question, adding a segmentation branch to each cascade stage, as shown in <ref type="figure" target="#fig_6">Fig. 6 (d)</ref>. This maximizes the diversity of samples used to learn the mask prediction task.</p><p>At inference time, all three strategies predict the segmentation masks on the patches produced by the final object detection stage, irrespective of the cascade stage on which the segmentation mask is implemented and how many segmentation branches there are. The final mask prediction is obtained from the single segmentation branch for the architectures of <ref type="figure" target="#fig_6">Fig. 6 (b)</ref> and (c), and from the ensemble of three segmentation branches for the architecture of <ref type="figure" target="#fig_6">Fig.  6 (d)</ref>. Our experiments show that these architectures of the Cascade Mask R-CNN outperform the Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>In this section, we present an extensive evaluation of the Cascade R-CNN detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Set-up</head><p>Experiments were performed over multiple datasets and baseline network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Datasets</head><p>The bulk of the experiments was performed on MS-COCO 2017 <ref type="bibr" target="#b36">[36]</ref>, which contains ∼118k images for training, 5k for validation (val) and ∼20k for testing without provided annotations (test-dev). The COCO average precision (AP) measure averages AP across IoU thresholds from 0.5 to 0.95, with an interval of 0.05. It measures detection performance at various qualities, encouraging high quality detection results, as discussed in Section 3.3. All models were trained on the COCO training set and evaluated on the val set. Final results are also reported on the test-dev set for fair comparison with the state-of-the-art. To assess the robustness and generalization ability of the Cascade R-CNN, experiments were also performed on Pascal VOC <ref type="bibr" target="#b12">[13]</ref>, KITTI <ref type="bibr" target="#b15">[16]</ref>, CityPersons <ref type="bibr" target="#b64">[64]</ref> and WiderFace <ref type="bibr" target="#b61">[61]</ref>. Instance segmentation was also evaluated on COCO, using the same evaluation metrics as object detection. The only difference is that the IoU is computed with respect to the mask rather than a bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Implementation Details</head><p>All regressors are class agnostic for simplicity. All Cascade R-CNN detection stages have the same architecture, which is the detection head of the baseline detector. Unless otherwise noted, the Cascade R-CNN is implemented with four stages: one RPN and three detection heads with thresholds U = {0.5, 0.6, 0.7}. The sampling of the first detection stage follows <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b47">[47]</ref>. In subsequent stages, resampling is implemented by using all the regressed outputs from the previous stage, as discussed in Section 4.3. No data augmentation was used except standard horizontal image flipping. Inference was performed at a single image scale, with no further bells and whistles. All baseline detectors were reimplemented with Caffe <ref type="bibr" target="#b31">[31]</ref>, using the same codebase, for fair comparison. Some experiments with the FPN and Mask R-CNN baselines were implemented on the Detectron platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Baseline Networks</head><p>To test the versatility of the Cascade R-CNN, experiments were performed with multiple popular baselines: Faster R-CNN and MS-CNN <ref type="bibr" target="#b1">[2]</ref> with VGG-Net <ref type="bibr" target="#b51">[51]</ref> backbone, R-FCN <ref type="bibr" target="#b7">[8]</ref> and FPN <ref type="bibr" target="#b37">[37]</ref> with ResNet backbones <ref type="bibr" target="#b26">[27]</ref>, for the task of object detection, and Mask R-CNN <ref type="bibr" target="#b24">[25]</ref> with ResNet backbones for instance segmentation. These baselines have a wide range of performances. Unless noted, their default settings were used. End-to-end training was used instead of multi-step training.</p><p>Faster R-CNN: the network head has two fully connected layers. To reduce parameters, <ref type="bibr" target="#b22">[23]</ref> was used to prune less important connections. 2048 units were retained per fully connected layer and dropout layers were removed. These changes have negligible effect on detection performance. Training started with a learning rate of 0.002, which was reduced by a factor of 10 at 60k and 90k iterations, and stopped at 100k iterations, on 2 synchronized GPUs, each holding 4 images per iteration. 128 RoIs were used per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-FCN: the R-FCN adds a convolutional, a bounding box regression, and a classification layer to the ResNet. For this baseline, all Cascade R-CNN heads have this structure.</head><p>Online hard negative mining <ref type="bibr" target="#b50">[50]</ref> was not used. Training started with a learning rate of 0.003, which was decreased by a factor of 10 at 160k and 240k iterations, and stopped at 280k iterations, on 4 synchronized GPUs, each holding one image per iteration. 256 RoIs were used per image. FPN: since official source code was not publicly available for the FPN when we performed our original experiments <ref type="bibr" target="#b2">[3]</ref>, the implementation details were somewhat different from those later made available in the Detectron implementation. RoIAlign <ref type="bibr" target="#b24">[25]</ref> was used for a stronger baseline. This is denoted as FPN+ and was used in all ablation studies, with the ResNet-50 as a backbone as usual. Training used a learning rate of 0.005 for 120k iterations and 0.0005 for the next 60k iterations, on 8 synchronized GPUs, each holding one image per iteration. 256 RoIs were used per image. We have also reimplemented the Cascade R-CNN of FPN on Detectron platform, when it is publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-CNN: the MS-CNN [2]</head><p>is a popular multi-scale object detector for specific object categories, e.g. vehicle, pedestrian, face, etc. It was used as baseline detector for experiments on KITTI, CityPersons and WiderFace. For this baseline, the Cascade R-CNN adopted the same two-step training strategy of the MS-CNN: proposal sub-network trained first and then joint end-to-end training. All detection heads were only added at the second step, where the learning rate was initially 0.0005, decreased by a factor of 10 at 10k and 20k iterations and stopped at 25k iterations, on one GPU of batch size 4 images.</p><p>Mask R-CNN: the Mask R-CNN was used as baseline for instance segmentation. The default Detectron implementation was adopted, using the 1x learning schedule. Training started with a learning rate of 0.02, which was reduced by a factor of 10 at 60k and 80k iterations, and stopped at 90k iterations, on 8 synchronized GPUs, each holding 2 images per iteration. 512 RoIs were used per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Quality Mismatch</head><p>An initial set of experiments was designed to evaluate the impact of the mistmatch between proposal and detector quality on detection performance. <ref type="figure">Figure 7 (a)</ref> shows the AP curves of three individually trained detectors of increasing IoU threshold in U = {0.5, 0.6, 0.7}. The detector of u = 0.5 outperforms the detector of u = 0.6 at low IoU levels, but underperforms it at higher levels. However, the detector of u = 0.7 underperforms the other two. To understand why this happens, we changed the quality of the proposals at inference. <ref type="figure">Figure 7 (b)</ref> shows the results obtained when ground truth bounding boxes were added to the set of proposals. While all detectors improved, the detector of u = 0.7 had the largest gains, and the best performance for almost all IoU levels. These results suggest two conclusions. First, the commonly used u = 0.5 threshold is not effective for precise detection, simply more robust to low quality proposals. Second, precise detection requires hypotheses that match the detector quality.</p><p>Next, the original proposals were replaced by the Cascade R-CNN proposals of higher quality (u = 0.6 and u = 0.7 used the 2nd and 3rd stage proposals, respectively). <ref type="figure">Figure 7 (a)</ref> suggests that the performance of the two detectors is significantly improved when the quality of the test proposals matches the detector quality. Testing Cascade R-CNN detectors of different qualities at all cascade stages produced similar observations. <ref type="figure">Figure 8</ref> shows that each detector was improved by the use of more precise hypotheses, with higher quality detectors exhibiting larger gains. For example, the detector of u = 0.7 performed poorly for the low quality proposals of the 1st stage, but much better for the more precise hypotheses available at the deeper cascade stages. The jointly trained detectors of <ref type="figure">Fig.  8</ref> also outperformed the individually trained detectors of <ref type="figure">Fig. 7 (a)</ref>, even when the same proposals were used. This indicates that the detectors are better trained within the Cascade R-CNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with Iterative BBox and Integral Loss</head><p>In this section, we compare the Cascade R-CNN to the iterative BBox and integral loss detectors. Iterative BBox was implemented by applying the detection head of FPN+ baseline iteratively at inference, three times. The integral loss detector was implemented with three classification heads, using U = {0.5, 0.6, 0.7}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Localization:</head><p>The localization performances of cascade regression and iterative BBox are compared in <ref type="figure" target="#fig_8">Fig. 9 (a)</ref>. The use of a single regressor degrades localization for hypotheses of high IoU. This effect accumulates when the regressor is applied iteratively, as in iterative BBox, and performance actually drops with iteration number. Note the very poor performance of iterative BBox after 3 iterations. On the contrary, the cascade regressor has better performance at later stages, outperforming iterative BBox at almost all IoU levels. Note that, although cascade regression can slightly degrade high input IoUs, e.g. IoU&gt;0.9, this decrease is negligible   because, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the number of hypotheses with such high IoUs is extremely small.</p><p>Integral Loss: <ref type="figure" target="#fig_8">Figure 9</ref> (b) summarizes the detection performances of all classifiers of the integral loss detector, sharing a single regressor. The classifier of u = 0.6 is the best at all IoU levels, with u = 0.7 producing the worst results. The ensemble of all classifiers shows no visible gain. <ref type="table" target="#tab_1">Table 1</ref> shows that both iterative BBox and integral loss marginally improve on the baseline detector, and are not effective for high quality detection. On the other hand, the Cascade R-CNN achieves the best performance at all IoU levels. As expected, the gains are mild for low IoUs, e.g. 0.8 for AP 50 , but significant for the higher ones, e.g. 6.1 for AP 80 and 8.7 for AP 90 . Note that high quality object detection was rarely explored before this work. These experiments show that 1) it has more room for improvement than low quality detection, which focuses on AP 50 , and 2) the overall AP can be significantly improved if it is effectively addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Ablation Experiments</head><p>A few ablation experiments were run to enable a better understanding of the Cascade R-CNN. <ref type="table" target="#tab_3">Table 2</ref> summarizes stagewise performance. Note that the first stage already outperforms the baseline detector, due to the benefits of multi-stage multitask learning. Since deeper cascade stages prefer higher quality localization, they encourage the learning of features conducive to it. This benefits the earlier cascade stages, due to the feature sharing by the backbone network. The second stage improves performance substantially, and the third is equivalent to the second. This differs from the integral loss detector, where the higher IoU classifier is relatively weak. While the former (later) stage is better at low (high) IoU metrics, the ensemble of all classifiers is the best overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-wise Comparison:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IoU Thresholds:</head><p>A Cascade R-CNN was trained using IoU threshold u = 0.5 for all heads. In this case, the stages differ only in the hypotheses at their input. Each stage is trained with the corresponding hypotheses, i.e. accounting for the distribution changes of <ref type="figure" target="#fig_5">Fig. 5</ref>. The first row of <ref type="table" target="#tab_4">Table 3</ref>    that this cascade improves on the baseline detector. This supports the claim that stages should be optimized for the corresponding sample distributions. The second row shows performance that improves further when the threshold u increases across stages. As discussed in Section 4.3, the detector becomes more selective against close false positives and specialized to the more precise hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regression Statistics:</head><p>In Section 3.1.1, we saw that the distance vector ∆ is normalized by the regression statistics (mean and variance), as in <ref type="bibr" target="#b4">(5)</ref>. In the Cascade R-CNN, these statistics are updated stage by stage, as illustrated in <ref type="figure" target="#fig_5">Fig.  5</ref>. Updating the statistics of (5) in deeper stages helps the effective multi-task learning of classification and regression. Empirically, the learning is not very sensitive to the exact values of these statistics. For simplicity, we set µ = 0 for all stages, Σ = (σ x , σ y , σ w , σ h ) = (0.1, 0.1, 0.2, 0.2) for the first stage, Σ/2 for the second, and Σ/3 for the third, in all of our experiments. The third and fourth row of <ref type="table" target="#tab_4">Table 3</ref> show that this is beneficial, when compared to using the statistics of the first stage in all stages (the first and second row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage Losses:</head><p>The Cascade R-CNN has multiple detection heads, each with its own loss. We have explored two schemes to combine these losses: decay and avg. In avg, the loss of stage t receives a weight w t = 1/T , where T is the number of stages. In decay, the weight is w t = 1/2 t−1 . For both schemes, the learning rate of the head parameters of stage t is rescaled by 1/w t , to ensure that these are sufficiently trained. No rescaling is needed for the backbone network parameters, since they receive gradients from all stages. <ref type="table" target="#tab_4">Table 3</ref> shows that 1) avg has somewhat better performance for high quality metrics, but worse for low quality ones, and 2) the two methods have similar overall AP. The decay scheme is used in the remainder of the paper.  while the overall AP degrades, the four-stage cascade has the best performance at high IoU levels. The three-stage cascade achieves the best trade-off between cost and AP performance, and is used in the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Stages:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with the state-of-the-art</head><p>An implementation of the Cascade R-CNN, based on the FPN+ detector and the ResNet-101 backbone, is compared to state-of-the-art single-model detectors in <ref type="table" target="#tab_8">Table 5</ref>  <ref type="bibr" target="#b5">6</ref> . The settings are those of Section 6.1.3, but training used 280k iterations, with learning rate decreased at 160k and 240k iterations. The number of RoIs was also increased to 512. The top of the table reports to one-stage detectors, the middle to two-stage, and the bottom to multi-stage (3-stages+RPN for the Cascade R-CNN). Note that all the compared state-ofthe-art detectors are trained with u = 0.5.</p><p>An initial observation is that our FPN+ implementation is better than the original FPN <ref type="bibr" target="#b37">[37]</ref>, providing a very strong baseline. Nevertheless, the extension from FPN+ to Cascade R-CNN improved performance by ∼4 points. In fact, the vanilla Cascade R-CNN, without any bells and whistles, outperformed almost all single-model detectors under all evaluation metrics. This includes the COCO challenge 2016 winner G-RMI <ref type="bibr" target="#b28">[29]</ref>, the recent Deformable R-FCN <ref type="bibr" target="#b8">[9]</ref>, Reti-naNet <ref type="bibr" target="#b38">[38]</ref>, Mask R-CNN <ref type="bibr" target="#b24">[25]</ref>, RelationNet <ref type="bibr" target="#b27">[28]</ref>, DetNet <ref type="bibr" target="#b35">[35]</ref>, CornerNet <ref type="bibr" target="#b33">[33]</ref>, etc. Note some of these methods leverage several training or inference enhancements, e.g. multi-scale, soft NMS <ref type="bibr" target="#b0">[1]</ref>, etc, making the comparison very unfair. Finally, compared to the previously best multi-stage detector on COCO, AttractioNet <ref type="bibr" target="#b17">[18]</ref>, the vanilla Cascade R-CNN has a gain of 7.1 points.</p><p>The only detector that outperforms the Cascade R-CNN in <ref type="table" target="#tab_8">Table 5</ref> is SNIP <ref type="bibr" target="#b52">[52]</ref>, which uses multi-scale training and inference, a larger input size, a stronger backbone, Soft NMS, and some other enhancements. For a more fair comparison, we implemented the Cascade R-CNN with multiscale training/inference, a stronger backbone (ResNeXt-152 <ref type="bibr" target="#b57">[57]</ref>), mask supervision, etc. This enhanced Cascade R-CNN surpassed SNIP by 5.2 points. It also outperforms the singlemodel MegDet detector (50.6 mAP), which won the COCO challenge in 2017 and uses many other enhancements <ref type="bibr" target="#b45">[45]</ref>. The Cascade R-CNN is conceptually straightforward, simple to implement, and can be combined, in a plug and play manner, with many detector architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Generalization Capacity</head><p>To more thoroughly test this claim, a three-stage Cascade R-CNN was implemented with three baseline detectors: Faster R-CNN, R-FCN, and FPN+. All settings are as discussed above, with the variations discussed in Section 6.5 for the <ref type="bibr" target="#b5">6</ref>   FPN+ detector. <ref type="table" target="#tab_9">Table 6</ref> presents a comparison of the AP performance of the three detectors.</p><p>Detection Performance: Again, our implementations are better than the original detectors <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b47">[47]</ref>. Still, the Cascade R-CNN improves on all baselines by 2∼4 points, independently of their strength. Similar gains are observed for val and test-dev. These results show that the Cascade R-CNN is widely applicable across detector architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters and Timing:</head><p>The number of Cascade R-CNN parameters increases with the number of stages. The increase is linear and proportional to the parameter cardinality of the baseline detector head. However, because the head has much less computation than the backbone network, the Cascade R-CNN has small computational overhead, at both training and testing. This is shown in <ref type="table" target="#tab_9">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Codebase and Backbone:</head><p>The Cascade R-CNN of FPN was also reimplemented on the Detectron codebase <ref type="bibr" target="#b19">[20]</ref> with various backbone networks. Fast R-CNN: As shown in <ref type="figure" target="#fig_2">Fig. 3 (b</ref>    <ref type="bibr" target="#b45">[45]</ref>. GN, an alternative to BN that is independent of batch size, has comparable performance to large-batch synchronized BN. <ref type="table" target="#tab_10">Table 7</ref> shows that the Cascade R-CNN with GN has similar gains to those obersved for the other architectures. This suggests that the Cascade R-CNN will continue to be useful even as architectural enhancements continue to emerge in the literature.   <ref type="table" target="#tab_15">Table 8</ref> summarizes the proposal recall performance of a Cascade R-CNN implemented with the FPN detector and ResNet-50 backbone. The first Cascade R-CNN stage has proposal recall close to that of the FPN baseline. The addition of a bounding box regression stage improves recall significantly, e.g. from 59.1 to 70.7 for AP 1k and close to 20 points for AP 1k l . This shows that the additional bounding box regression is very effective at improving proposal recall performance. The addition of a third stage has a smaller but non-negligible gain. This high proposal recall performance secures the later high-quality object detection task. <ref type="table" target="#tab_16">Table 9</ref> summarizes the instance segmentation performance of the Cascade Mask R-CNN strategies of <ref type="figure" target="#fig_6">Fig. 6</ref>. These experiments, use the Mask R-CNN, implemented on Detectron with 1x schedule as baseline. All three strategies improve on baseline performance, although with smaller gains than object detection (see <ref type="table" target="#tab_3">Table 2</ref>), especially at high quality. For example, the AP 90 improvement of 8.7 points for object detection falls to 1.8 points, showing that plenty of room is left for improving high quality instance segmentation. Comparing strategies, (c) outperforms (b). This is because (b) trains the mask head in the first stage but tests after the last stage, leading to a mask prediction mismatch. This mismatch is reduced by (c). The addition of a mask branch to each stage by strategy (d) does not have noticeable benefits over (c), but requires much more computation and memory. Strategy (b) has the best trade-off between cost and AP performance, and is used in the remainder of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Proposal Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Instance Segmentation by Cascade Mask R-CNN</head><p>To evaluate the instance segmentation robustness of the Cascade Mask R-CNN, several backbone networks are compared in <ref type="table" target="#tab_1">Table 10</ref>. Since this architecture can detect objects, detection results are also shown. Note that the additional mask supervision makes these better than those of <ref type="table" target="#tab_10">Table 7</ref>. The gains of the Cascade Mask R-CNN are very consistent for all backbone networks. Even when the strongest model, ResNeXt-152 <ref type="bibr" target="#b57">[57]</ref>, is used with training data augmentation and 1.44x schedule, the Cascade Mask R-CNN has a gain of 2.9 points for detection and 1.0 point for instance segmentation. Adding inference enhancements, the gains are still 2.1 points for detection and 0.8 points for instance segmentation. This robustness explains why the Cascade R-CNN was widely used in the COCO challenge 2018, where the task is instance segmentation, not object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.9">Results on PASCAL VOC</head><p>The Cascade R-CNN was further tested on the PASCAL VOC dataset <ref type="bibr" target="#b12">[13]</ref>. Following <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b47">[47]</ref>, the models were trained on VOC2007 and VOC2012 trainval (16,551 images) and tested on VOC2007 test (4,952 images). Two detector architectures were evaluated: Faster R-CNN  (with AlexNet and VGG-Net backbones) and R-FCN (with ResNet-50 and ResNet-101). Training details were as discussed in Section 6.1.3, and both AlexNet and VGG-Net were pruned. More specifically, Faster R-CNN (R-FCN) training started with a learning rate of 0.001 (0.002), which was reduced by a factor of 10 at 30k (60k) and stopped at 45k (90k) iterations. Since the standard VOC evaluation metric (AP at IoU of 0.5) is fairly saturated, and the focus of this work is high quality detection, the COCO metrics were used for evaluation <ref type="bibr" target="#b6">7</ref> . <ref type="table" target="#tab_1">Table 11</ref> summarizes the performance of all detectors, showing that the Cascade R-CNN significantly improves the overall AP in all cases. These results are further evidence for the robustness of the Cascade R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.10">Additional Results on other Datasets</head><p>Beyond generic object detection datasets, the Cascade R-CNN was tested on some specific object detection tasks, including KITTI <ref type="bibr" target="#b15">[16]</ref>, CityPerson <ref type="bibr" target="#b64">[64]</ref> and WiderFace <ref type="bibr" target="#b61">[61]</ref>.</p><p>The MS-CNN <ref type="bibr" target="#b1">[2]</ref>, a detector of strong performance on these tasks, was used as baseline for all of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI:</head><p>One of the most popular datasets for autonomous driving, KITTI contains 7,481 training/validation images, and 7,518 for testing with held annotations. The 2D object detection task contains three categories: car, pedestrian, and cyclist. Evaluation is based on the VOC AP at IoU of 0.7, 0.5, and 0.5 for the three categories, respectively. Since the focus of this work is high quality detection, the Cascade R-CNN was only tested on the car category. As shown in <ref type="table" target="#tab_1">Table 12</ref>, it improved the baseline by 0.87 points for the Moderate, and 1.9 points for the Hard regime, on the test set. These improvements are nontrivial, given that MS-CNN is a strong detector and the KITTI car detection task is fairly saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CityPersons:</head><p>CityPersons is a recently published pedestrian detection dataset, collected across multiple European cities. It contains 2,975 training and 500 validation images, and 1,575 images for testing with held annotations. Evaluation is based on miss-rate (MR) at IoU=0.5. We also report results for MR at IoU=0.75, which is more commensurate with high quality detection. This is consistent with a recent trend to adopt the stricter COCO metric for pedestrian and face detection, see e.g. the Wider Challenge 2018. <ref type="table" target="#tab_1">Table 13</ref> compares the validation set performance of the Cascade R-CNN with that of the baseline MS-CNN (performances on validation and test sets are usually equivalent on this dataset). The Cascade R-CNN has large performance gains, especially for the stricter evaluation metric. For example, it improves the baseline performance by ∼10 points on the Reasonable set at MR 75 . <ref type="bibr" target="#b6">7</ref>. The PASCAL VOC annotations were transformed to COCO format, and the COCO toolbox used for evaluation. Results are different from the standard VOC evaluation.    WiderFace: One of the most challenging face detection datasets, mainly due to its diversity in scale, pose and occlusion, WiderFace contains 32,203 images with 393,703 annotated faces, of which 12,880 are used for training, 3,226 for validation, and the remainder for testing with held annotations. Evaluation is based on the VOC AP at IoU=0.5 on three subsets, easy, medium and hard, of different detection difficulty. Again, we have used AP at IoU=0.5 and IoU=0.75 and evaluation on the validation set. <ref type="table" target="#tab_1">Table 14</ref> shows that, while the Cascade R-CNN is close to the baseline MS-CNN for AP 50 , it significantly boosts its performance for AP 75 . The gain is smaller on the hard than on the easy and medium, because the former contains mainly very small and heavily occluded faces, for which high quality detection is difficult. This observation mirrors the COCO experiments of <ref type="table" target="#tab_9">Table 6</ref>, where improvements in AP S are smaller than for AP L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we have proposed a multi-stage object detection framework, the Cascade R-CNN, for high quality object detection, a rarely explored problem in the detection literature. This architecture was shown to overcome the high quality detection challenges of overfitting during training and quality mismatch during inference. This is achieved by training stages sequentially, using the output of one to train the next, and the same cascade is applied at inference.   The Cascade R-CNN was shown to achieve very consistent performance gains on multiple challenging datasets, including COCO, PASCAL VOC, KITTI, CityPersons, and WiderFace, for both generic and specific object detection. These gains were also observed for many object detectors, backbone networks, and techniques for detection and instance segmentation. We thus believe that the Cascade R-CNN can be useful for many future object detection and instance segmentation research efforts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(a) and (b) detections by object detectors of increasing qualities, and (c) examples of increasing quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Bounding box localization, classification loss and detection performance of object detectors of increasing IoU threshold u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The architectures of different frameworks. "I" is input image, "conv" backbone convolutions, "pool" region-wise feature extraction, "H" network head, "B" bounding box, and "C" classification. "B0" is proposals in all architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>IoU histograms of training samples of each cascade stage. The distribution of the 1st stage is the RPN output. Shown in red are the percentage of positives for the corresponding IoU threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Distribution of the distance vector ∆ of (4) (without normalization) at different cascade stages. Top: plot of (δx, δy). Bottom: plot of (δw, δ h ). Red dots are outliers for the increasing IoU thresholds of later stages, and the statistics shown are obtained after outlier removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Architectures of the Mask R-CNN (a) and three Cascade Mask R-CNN strategies for instance segmentation (b)-(d). Beyond the definitions ofFig. 3, "S" denotes a segmentation branch. Note that segmentations branches do not necessarily share heads with the detection branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>(a) detection performance of individually trained detectors, with their own proposals (solid curves) or Cascade R-CNN stage proposals (dashed curves). (b) results of adding ground truth to the proposal set. Detection performance of all Cascade R-CNN detectors at all cascade stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>(a) localization performance of iterative BBox and Cascade R-CNN regressors. (b) detection performance of the individual classifiers of the integral loss detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>AP AP 50</head><label>50</label><figDesc>AP 60 AP 70 AP 80 AP 90 Mask R-CNN baseline 33.9 55.5 49.8 41.4  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>38.9 57.8 53.4 46.9 35.8 15.8</head><label></label><figDesc>AP AP 50 AP 60 AP 70 AP 80 AP 90</figDesc><table><row><cell>FPN+ baseline</cell><cell>34.9 57.0 51.9 43.6 29.7</cell><cell>7.1</cell></row><row><cell>Iterative BBox</cell><cell>35.4 57.2 52.1 44.2 30.4</cell><cell>8.1</cell></row><row><cell>Integral Loss</cell><cell>35.4 57.3 52.5 44.4 29.9</cell><cell>6.9</cell></row><row><cell>Cascade R-CNN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Comparison of the Cascade R-CNN with iterative BBox and integral loss detectors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>shows test stage AP AP 50 AP 60 AP 70 AP 80 AP 90</figDesc><table><row><cell>1</cell><cell>35.5 57.2 52.4 44.1 30.5</cell><cell>8.1</cell></row><row><cell>2</cell><cell cols="2">38.3 57.9 53.4 46.4 35.2 14.2</cell></row><row><cell>3</cell><cell cols="2">38.3 56.6 52.2 46.3 35.7 15.9</cell></row><row><cell>1 ∼ 2</cell><cell cols="2">38.5 58.2 53.8 46.7 35.0 14.0</cell></row><row><cell>1 ∼ 3</cell><cell cols="2">38.9 57.8 53.4 46.9 35.8 15.8</cell></row><row><cell cols="2">FPN+ baseline 34.9 57.0 51.9 43.6 29.7</cell><cell>7.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 :</head><label>2</label><figDesc>Stagewise performance of the Cascade R-CNN. 1 ∼ 3 indicates an ensemble result, obtained by averaging the three classifier probabilities for 3rd stage proposals. IoU↑ update stage AP AP 50 AP 60 AP 70 AP 80 AP 90</figDesc><table><row><cell></cell><cell cols="2">statistics loss</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">decay 36.8 57.8 52.9 45.4 32.0 10.7</cell></row><row><cell>✓</cell><cell></cell><cell cols="2">decay 38.5 58.4 54.1 47.1 35.0 13.1</cell></row><row><cell></cell><cell>✓</cell><cell cols="2">decay 37.5 57.8 53.1 45.5 33.3 13.1</cell></row><row><cell>✓</cell><cell>✓</cell><cell cols="2">decay 38.9 57.8 53.4 46.9 35.8 15.8</cell></row><row><cell>✓</cell><cell>✓</cell><cell>avg</cell><cell>38.9 57.5 53.4 46.9 35.8 16.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc>Ablation experiments. "IoU↑" indicates increasing IoU thresholds, "update statistics" updating regression statistics, and "stage loss" weighting of stage losses.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>stages test stage AP AP 50 AP 60 AP 70 AP 80 AP 90</figDesc><table><row><cell>1</cell><cell>1</cell><cell>34.9 57.0 51.9 43.6 29.7</cell><cell>7.1</cell></row><row><cell>2</cell><cell>1 ∼ 2</cell><cell cols="2">38.2 58.0 53.6 46.7 34.6 13.6</cell></row><row><cell>3</cell><cell>1 ∼ 3</cell><cell cols="2">38.9 57.8 53.4 46.9 35.8 15.8</cell></row><row><cell>4</cell><cell>1 ∼ 3</cell><cell cols="2">38.9 57.4 53.2 46.8 36.0 16.0</cell></row><row><cell>4</cell><cell>1 ∼ 4</cell><cell cols="2">38.6 57.2 52.8 46.2 35.5 16.3</cell></row></table><note>summarizes the impact of the number of stages in the Cascade R-CNN performance. Adding a second stage significantly improves the baseline detector. Three detection stages still produce non-trivial improvement, but the addition of a 4 th stage (u = 0.75) has a slight performance decrease. Note, however, that#</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>The impact of the number of stages in Cascade R-CNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>. Some detectors are omitted in this comparison because their singlemodel results on COCO test-dev are not publicly available.</figDesc><table><row><cell></cell><cell>backbone</cell><cell>AP</cell><cell cols="2">AP 50 AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>YOLOv2 [46]</cell><cell>DarkNet-19</cell><cell>21.6</cell><cell>44.0</cell><cell>19.2</cell><cell>5.0</cell><cell>22.4</cell><cell>35.5</cell></row><row><cell>SSD513 [40]  *</cell><cell>ResNet-101</cell><cell>31.2</cell><cell>50.4</cell><cell>33.3</cell><cell>10.2</cell><cell>34.5</cell><cell>49.8</cell></row><row><cell>RetinaNet [38]  *</cell><cell>ResNet-101</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.8</cell><cell>42.7</cell><cell>50.2</cell></row><row><cell>CornerNet [33]  * ⋆</cell><cell>Hourglass-104</cell><cell>42.1</cell><cell>57.8</cell><cell>45.3</cell><cell>20.8</cell><cell>44.8</cell><cell>56.7</cell></row><row><cell>Faster R-CNN+++ [27]  * ⋆</cell><cell>ResNet-101</cell><cell>34.9</cell><cell>55.7</cell><cell>37.4</cell><cell>15.6</cell><cell>38.7</cell><cell>50.9</cell></row><row><cell>Faster R-CNN w FPN [37]</cell><cell>ResNet-101</cell><cell>36.2</cell><cell>59.1</cell><cell>39.0</cell><cell>18.2</cell><cell>39.0</cell><cell>48.2</cell></row><row><cell cols="2">Faster R-CNN w FPN+ (ours) ResNet-101</cell><cell>38.8</cell><cell>61.1</cell><cell>41.9</cell><cell>21.3</cell><cell>41.8</cell><cell>49.8</cell></row><row><cell>G-RMI [29]  * ⋆</cell><cell>Inception-ResNet-v2</cell><cell>41.6</cell><cell>62.3</cell><cell>45.6</cell><cell>24.0</cell><cell>43.9</cell><cell>55.2</cell></row><row><cell>Deformable R-FCN [9]  * ⋆</cell><cell cols="2">Aligned-Inception-ResNet 37.5</cell><cell>58.0</cell><cell>40.8</cell><cell>19.4</cell><cell>40.1</cell><cell>52.5</cell></row><row><cell>Mask R-CNN [25]</cell><cell>ResNet-101</cell><cell>38.2</cell><cell>60.3</cell><cell>41.7</cell><cell>20.1</cell><cell>41.1</cell><cell>50.2</cell></row><row><cell>RelationNet [28]</cell><cell>ResNet-101</cell><cell>39.0</cell><cell>58.6</cell><cell>42.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DetNet [35]</cell><cell>DetNet-59</cell><cell>40.3</cell><cell>62.1</cell><cell>43.8</cell><cell>23.6</cell><cell>42.6</cell><cell>50.0</cell></row><row><cell>SNIP [52]  * ⋆</cell><cell>DPN-98</cell><cell>45.7</cell><cell>67.3</cell><cell>51.1</cell><cell>29.3</cell><cell>48.8</cell><cell>57.1</cell></row><row><cell>AttractioNet [18] ⋆</cell><cell>VGG16+Wide ResNet</cell><cell>35.7</cell><cell>53.4</cell><cell>39.3</cell><cell>15.6</cell><cell>38.0</cell><cell>52.7</cell></row><row><cell>Cascade R-CNN</cell><cell>ResNet-101</cell><cell>42.8</cell><cell>62.1</cell><cell>46.3</cell><cell>23.7</cell><cell>45.5</cell><cell>55.2</cell></row><row><cell>Cascade R-CNN  * ⋆</cell><cell>ResNeXt-152</cell><cell>50.9</cell><cell>69.0</cell><cell>55.8</cell><cell>33.4</cell><cell>53.5</cell><cell>63.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>Performance of state-of-the-art single-model detectors on COCO test-dev. Entries denoted by * and ⋆ use enhancements at training and inference, respectively. AP AP 50 AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell cols="4">train test model speed speed size Faster R-CNN backbone cascade ✗ 0.12s 0.075s 278M 23.6 43.9 23.0 8.0 26.2 35.5 23.5 43.9 22.6 8.1 25.1 34.7 val (5k) test-dev (20k) VGG ✓ 0.14s 0.115s 704M 27.0 44.2 27.7 8.6 29.1 42.2 26.9 44.3 27.8 8.3 28.2 41.1</cell></row><row><cell>R-FCN</cell><cell>ResNet-50</cell><cell>✗ ✓</cell><cell>0.19s 0.07s 133M 27.0 48.7 26.9 9.8 30.9 40.3 27.1 49.0 26.9 10.4 29.7 39.2 0.24s 0.075s 184M 31.1 49.8 32.8 10.4 34.4 48.5 30.9 49.9 32.6 10.5 33.1 46.9</cell></row><row><cell>R-FCN</cell><cell>ResNet-101</cell><cell>✗ ✓</cell><cell>0.23s 0.075s 206M 30.3 52.2 30.8 12.0 34.7 44.3 30.5 52.9 31.2 12.0 33.9 43.8 0.29s 0.083s 256M 33.3 52.0 35.2 11.8 37.2 51.1 33.3 52.6 35.2 12.1 36.2 49.3</cell></row><row><cell>FPN+</cell><cell>ResNet-50</cell><cell>✗ ✓</cell><cell>0.30s 0.095s 165M 36.5 58.6 39.2 20.8 40.0 47.8 36.5 59.0 39.2 20.3 38.8 46.4 0.33s 0.115s 272M 40.3 59.4 43.7 22.9 43.7 54.1 40.6 59.9 44.0 22.6 42.7 52.1</cell></row><row><cell>FPN+</cell><cell>ResNet-101</cell><cell>✗ ✓</cell><cell>0.38s 0.115s 238M 38.5 60.6 41.7 22.1 41.9 51.1 38.8 61.1 41.9 21.3 41.8 49.8 0.41s 0.14s 345M 42.7 61.6 46.6 23.8 46.2 57.4 42.8 62.1 46.3 23.7 45.5 55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>Performance of Cascade R-CNN implementations with multiple detectors. All speeds are reported per image on a single Titan Xp GPU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>summarizes these</cell></row><row><cell>experiments, showing very consistent improvements (3∼4</cell></row><row><cell>points) across backbones. The Cascade R-CNN has also</cell></row><row><cell>been independently reproduced by other research groups,</cell></row><row><cell>on PyTorch and TensorFlow. These again show that the</cell></row><row><cell>Cascade R-CNN can provide reliable gains across detector</cell></row><row><cell>architectures, backbones, codebases, and implementations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>The results of backbone cascade AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>Fast ResNet-50</cell><cell>✗ ✓</cell><cell>36.4 58.4 39.3 20.3 39.8 48.1 40.5 58.7 43.9 21.5 43.6 54.9</cell></row><row><cell>ResNet-50</cell><cell>✗ ✓</cell><cell>36.7 58.4 39.6 21.1 39.8 48.1 40.9 59.0 44.6 22.5 43.6 55.3</cell></row><row><cell>ResNet-101</cell><cell>✗ ✓</cell><cell>39.4 61.2 43.4 22.6 42.9 51.4 42.8 61.4 46.8 24.1 45.8 57.4</cell></row><row><cell>ResNeXt-101</cell><cell>✗ ✓</cell><cell>41.3 63.7 44.7 25.5 45.3 52.9 44.7 63.7 48.8 26.3 48.4 58.6</cell></row><row><cell>ResNet-50-GN</cell><cell>✗ ✓</cell><cell>38.4 59.9 41.7 22.2 41.2 50.0 42.2 60.6 45.8 24.7 45.2 55.7</cell></row><row><cell>ResNet-101-GN</cell><cell>✗ ✓</cell><cell>39.9 61.3 43.3 23.6 42.8 52.3 43.8 62.2 47.6 26.2 47.2 57.7</cell></row></table><note>), the Cascade R-CNN is not limited to the standard Faster R-CNN architecture. To test this, we trained the Cascade R-CNN in the way of the Fast R-CNN, using pre-collected proposals.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 7 :</head><label>7</label><figDesc>Performance of various implementations of the Cascade R-CNN with the FPN detector on Detectron, using the 1x schedule.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7</head><label>7</label><figDesc>show that the gains of the Cascade R-CNN hold for frameworks other than the Faster R-CNN.</figDesc><table><row><cell>Group Normalization: Group normalization (GN) [56] is a</cell></row><row><cell>recent normalization technique, published after the Cascade</cell></row><row><cell>R-CNN. It addresses the problem that batch normalization</cell></row><row><cell>(BN) [30] must be frozen for object detector training, due to</cell></row><row><cell>the inaccurate statistics that can be derived from small batch</cell></row><row><cell>sizes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>stage AP 100 AP 100 35.1 61.1 82.5 70.7 55.2 77.7 88.1 3 56.5 36.1 62.4 84.1 71.4 55.5 78.1 89.8</figDesc><table><row><cell></cell><cell>s</cell><cell>AP 100 m AP 100 l</cell><cell>AP 1k AP 1k s</cell><cell>AP 1k m AP 1k l</cell></row><row><cell cols="5">FPN 47.8 32.2 54.9 65.2 59.1 48.0 66.3 68.4</cell></row><row><cell>1</cell><cell cols="4">46.8 31.0 53.8 64.8 58.7 47.6 65.9 68.2</cell></row><row><cell>2</cell><cell>55.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 8 :</head><label>8</label><figDesc>Proposal recall of Cascade R-CNN stages.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 9 :</head><label>9</label><figDesc>The instance segmentation comparison among three strategies of the Cascade Mask R-CNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L ResNet-50 ✗ 37.7 59.2 40.9 21.4 40.8 49.8 33.9 55.8 35.8 14.9 36.3 50.9 ✓ 41.3 59.4 45.3 23.2 43.8 55.8 35.4 56.4 37.7 15.9 37.7 53.6 ResNet-101 ✗ 40.0 61.8 43.7 22.5 43.4 52.7 35.9 58.3 38.0 15.9 38.9 53.2 ✓ 43.3 61.7 47.2 24.2 46.3 58.2 37.1 58.6 39.8 16.7 39.7 55.7 ResNet-50-GN ✗ 39.2 60.5 42.9 22.9 42.2 50.6 34.9 57.1 36.9 16.0 37.7 51.2 ✓ 42.9 60.7 46.6 25.1 45.9 56.7 36.6 57.7 39.2 16.8 39.3 54.5 ResNet-101-GN ✗ 41.1 62.1 45.1 23.6 44.3 53.1 36.3 58.9 38.5 16.2 39.4 53.6 ✓ 44.8 62.8 48.8 26.4 48.0 58.7 38.0 59.8 40.8 18.1 40.7 56.0 ResNeXt-101 ✗ 42.1 64.1 45.9 25.6 45.9 54.4 37.3 60.3 39.5 17.8 40.3 55.5 ✓ 45.8 64.1 50.3 27.2 49.5 60.1 38.6 60.6 41.5 18.5 41.3 57.2 ResNeXt-152 * ✗ 45.2 66.9 49.7 28.5 49.4 56.8 39.7 63.5 42.4 19.8 42.9 57.3 ✓ 48.1 66.7 52.6 29.3 52.2 62.1 40.7 63.7 43.8 19.9 44.0 59.1 ResNeXt-152 * ⋆ ✗ 48.1 68.3 52.9 32.6 51.8 61.3 41.5 65.1 44.7 22.0 44.8 59.8 ✓ 50.2 68.2 55.0 33.1 53.9 64.2 42.3 65.4 45.8 21.9 45.7 60.9</figDesc><table><row><cell>backbone</cell><cell>cascade</cell><cell>AP AP 50</cell><cell>Object Detection</cell><cell>Instance Segmentation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 10 :</head><label>10</label><figDesc>Performance of the Cascade Mask R-CNN on multiple backbone networks on COCO 2017 val. * and ⋆ denotes enhancement techniques at training and inference, respectively, as in<ref type="bibr" target="#b19">[20]</ref>. backbone cascade AP AP 50 AP 75</figDesc><table><row><cell cols="2">Faster R-CNN AlexNet</cell><cell>✗ ✓</cell><cell>29.4 63.2 23.7 38.9 66.5 40.5</cell></row><row><cell>Faster R-CNN</cell><cell>VGG</cell><cell>✗ ✓</cell><cell>42.9 76.4 44.1 51.2 79.1 56.3</cell></row><row><cell>R-FCN</cell><cell>RetNet-50</cell><cell>✗ ✓</cell><cell>44.8 77.5 46.8 51.8 78.5 57.1</cell></row><row><cell>R-FCN</cell><cell>ResNet-101</cell><cell>✗ ✓</cell><cell>49.4 79.8 53.2 54.2 79.6 59.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 11 :</head><label>11</label><figDesc>Detection results on PASCAL VOC 2007 test.</figDesc><table><row><cell></cell><cell cols="4">cascade Easy Moderate Hard</cell></row><row><cell>AP 70</cell><cell>✗ ✓</cell><cell>90.22 90.68</cell><cell>89.08 89.95</cell><cell>76.50 78.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 12 :</head><label>12</label><figDesc>MS-CNN detection results for the car class on KITTI test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 13 :</head><label>13</label><figDesc>MS-CNN detection results on CityPersons validation set.</figDesc><table><row><cell></cell><cell cols="4">cascade Easy Medium Hard</cell></row><row><cell>AP 50</cell><cell>✗ ✓</cell><cell>91.1 91.3</cell><cell>90.6 90.3</cell><cell>81.0 81.1</cell></row><row><cell>AP 75</cell><cell>✗ ✓</cell><cell>59.7 68.7</cell><cell>61.3 66.3</cell><cell>40.7 42.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE 14 :</head><label>14</label><figDesc></figDesc><table /><note>MS-CNN Detection results on WiderFace validation set.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: This work was funded by NSF Awards IIS-1546305 and IIS-1637941, and a GPU donation from NVIDIA. We would also like to thank Kaiming He for valuable discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified multiscale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2887" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1901.07518</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="973" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroCOLT</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region and semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attend refine repeat: Active box proposal generation via in-out localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Locnet: Improving localization accuracy for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<title level="m">Rethinking imagenet pretraining</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors. CoRR</title>
		<idno>abs/1611.10012</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="816" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning efficient single-stage pedestrian detectors by asymptotic localization fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="643" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Masnadi-Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="294" to="309" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">G-CNN: an iterative grid based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2369" to="2377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning chained deep features and classifiers for cascade in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1702.07054</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning optimal embedded cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection-snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Single-shot bidirectional pyramid networks for high-quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno>abs/1803.08208</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learn to combine multiple hypotheses for accurate face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">CRAFT objects from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6043" to="6051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Attentionnet: Aggregating weak directions for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2659" to="2667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A multipath network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
