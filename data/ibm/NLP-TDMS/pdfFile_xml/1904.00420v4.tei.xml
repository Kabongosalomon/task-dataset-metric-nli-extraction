<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Path One-Shot Neural Architecture Search with Uniform Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single Path One-Shot Neural Architecture Search with Uniform Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally. Comprehensive experiments verify that our approach is flexible and effective. It is easy to train and fast to search. It effortlessly supports complex search spaces (e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency). It is thus convenient to use for various needs. It achieves start-of-the-art performance on the large dataset ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning automates feature engineering and solves the weight optimization problem. Neural Architecture Search (NAS) aims to automate architecture engineering by solving one more problem, architecture design. Early NAS approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> solves the two problems in a nested manner. A large number of architectures are sampled and trained from scratch. The computation cost is unaffordable on large datasets.</p><p>Recent approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref> adopt a weight sharing strategy to reduce the computation. A supernet subsuming all architectures is trained only once. Each architecture inherits its weights from the supernet. Only fine-tuning is performed. The computation cost is greatly reduced. arXiv:1904.00420v4 [cs.CV] 8 Jul 2020</p><p>Most weight sharing approaches use a continuous relaxation to parameterize the search space <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. The architecture distribution parameters are jointly optimized during the supernet training via gradient based methods. The best architecture is sampled from the distribution after optimization. There are two issues in this formulation. First, the weights in the supernet are deeply coupled. It is unclear why inherited weights for a specific architecture are still effective. Second, joint optimization introduces further coupling between the architecture parameters and supernet weights. The greedy nature of the gradient based methods inevitably introduces bias during optimization and could easily mislead the architecture search. They adopted complex optimization techniques to alleviate the problem.</p><p>The one-shot paradigm <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> alleviates the second issue. It defines the supernet and performs weight inheritance in a similar way. However, there is no architecture relaxation. The architecture search problem is decoupled from the supernet training and addressed in a separate step. Thus, it is sequential. It combines the merits of both nested and joint optimization approaches above. The architecture search is both efficient and flexible.</p><p>The first issue is still problematic. Existing one-shot approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> still have coupled weights in the supernet. Their optimization is complicated and involves sensitive hyper parameters. They have not shown competitive results on large datasets.</p><p>This work revisits the one-shot paradigm and presents a new approach that further eases the training and enhances architecture search. Based on the observation that the accuracy of an architecture using inherited weights should be predictive for the accuracy using optimized weights, we propose that the supernet training should be stochastic. All architectures have their weights optimized simultaneously. This gives rise to a uniform sampling strategy. To reduce the weight coupling in the supernet, a simple search space that consists of single path architectures is proposed. The training is hyperparameter-free and easy to converge.</p><p>This work makes the following contributions.</p><p>1. We present a principled analysis and point out drawbacks in existing NAS approaches that use nested and joint optimization. Consequently, we hope this work will renew interest in the one-shot paradigm, which combines the merits of both via sequential optimization. 2. We present a single path one-shot approach with uniform sampling. It overcomes the drawbacks of existing one-shot approaches. Its simplicity enables a rich search space, including novel designs for channel size and bit width, all addressed in a unified manner. Architecture search is efficient and flexible. Evolutionary algorithm is used to support real world constraints easily, such as low latency.</p><p>Comprehensive ablation experiments and comparison to previous works on a large dataset (ImageNet) verify that the proposed approach is state-of-the-art in terms of accuracy, memory consumption, training time, architecture search efficiency and flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Review of NAS Approaches</head><p>Without loss of generality, the architecture search space A is represented by a directed acyclic graph (DAG). A network architecture is a subgraph a ∈ A, denoted as N (a, w) with weights w.</p><p>Neural architecture search aims to solve two related problems. The first is weight optimization,</p><formula xml:id="formula_0">w a = argmin w L train (N (a, w)) ,<label>(1)</label></formula><p>where L train (·) is the loss function on the training set. The second is architecture optimization. It finds the architecture that is trained on the training set and has the best accuracy on the validation set, as</p><formula xml:id="formula_1">a * = argmax a∈A ACC val (N (a, w a )) ,<label>(2)</label></formula><p>where ACC val (·) is the accuracy on the validation set. Early NAS approaches perform the two optimization problems in a nested manner <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1]</ref>. Numerous architectures are sampled from A and trained from scratch as in Eq. (1). Each training is expensive. Only small dataset (e.g., CIFAR 10) and small search space (e.g, a single block) are affordable.</p><p>Recent NAS approaches adopt a weight sharing strategy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref>. The architecture search space A is encoded in a supernet 1 , denoted as N (A, W ), where W is the weights in the supernet. The supernet is trained once. All architectures inherit their weights directly from W . Thus, they share the weights in their common graph nodes. Fine tuning of an architecture is performed in need, but no training from scratch is incurred. Therefore, architecture search is fast and suitable for large datasets like ImageNet.</p><p>Most weight sharing approaches convert the discrete architecture search space into a continuous one <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. Formally, space A is relaxed to A(θ), where θ denotes the continuous parameters that represent the distribution of the architectures in the space. Note that the new space subsumes the original one, A ⊆ A(θ). An architecture sampled from A(θ) could be invalid in A.</p><p>An advantage of the continuous search space is that gradient based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> is feasible. Both weights and architecture distribution parameters are jointly optimized, as</p><formula xml:id="formula_2">(θ * , W θ * ) = argmin θ,W L train (N (A(θ), W )).<label>(3)</label></formula><p>or perform a bi-level optimization, as</p><formula xml:id="formula_3">θ * = argmax θ ACC val (N (A(θ), W * θ )) s.t. W * θ = argmin W L train (N (A(θ), W ))<label>(4)</label></formula><p>After optimization, the best architecture a * is sampled from A(θ * ). Optimization of Eq. (3) is challenging. First, the weights of the graph nodes in the supernet depend on each other and become deeply coupled during optimization. For a specific architecture, it inherits certain node weights from W . While these weights are decoupled from the others, it is unclear why they are still effective.</p><p>Second, joint optimization of architecture parameter θ and weights W introduces further complexity. Solving Eq. (3) inevitably introduces bias to certain areas in θ and certain nodes in W during the progress of optimization. The bias would leave some nodes in the graph well trained and others poorly trained. With different level of maturity in the weights, different architectures are actually non-comparable. However, their prediction accuracy is used as guidance for sampling in A(θ) (e.g., used as reward in policy gradient <ref type="bibr" target="#b3">[4]</ref>). This would further mislead the architecture sampling. This problem is in analogy to the "dilemma of exploitation and exploration" problem in reinforcement learning. To alleviate such problems, existing approaches adopt complicated optimization techniques (see <ref type="table">Table 1</ref> for a summary).</p><p>Task constraints Real world tasks usually have additional requirements on a network's memory consumption, FLOPs, latency, energy consumption, etc. These requirements only depends on the architecture a, not on the weights w a . Thus, they are called architecture constraints in this work. A typical constraint is that the network's latency is no more than a preset budget, as Latency(a * ) ≤ Lat max .</p><p>Note that it is challenging to satisfy Eq. (2) and Eq. (5) simultaneously for most previous approaches. Some works augment the loss function L train in Eq. (3) with soft loss terms that consider the architecture latency <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22]</ref>. However, it is hard, if not impossible, to guarantee a hard constraint like Eq. (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Single Path One-Shot Approach</head><p>As analyzed above, the coupling between architecture parameters and weights is problematic. This is caused by joint optimization of both. To alleviate the problem, a natural solution is to decouple the supernet training and architecture search in two sequential steps. This leads to the so called one-shot approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. In general, the two steps are formulated as follows. Firstly, the supernet weight is optimized as</p><formula xml:id="formula_5">W A = argmin W L train (N (A, W )) .<label>(6)</label></formula><p>Compared to Eq. (3), the continuous parameterization of search space is absent. Only weights are optimized.    Secondly, architecture searched is performed as</p><formula xml:id="formula_6">a * = argmax a∈A ACC val (N (a, W A (a))) .<label>(7)</label></formula><p>During search, each sampled architecture a inherits its weights from W A as W A (a). The key difference of Eq. (7) from Eq. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> is that architecture weights are ready for use. Evaluation of ACC val (·) only requires inference. Thus, the search is very efficient. The search is also flexible. Any adequate search algorithm is feasible. The architecture constraint like Eq. (5) can be exactly satisfied. Search can be repeated many times on the same supernet once trained, using different constraints (e.g., 100ms latency and 200ms latency). These properties are absent in previous approaches. These make the one-shot paradigm attractive for real world tasks.</p><p>One problem in Sec. 2 still remains. The graph nodes' weights in the supernet training in Eq.( 6) are coupled. It is unclear why the inherited weights W A (a) are still good for an arbitrary architecture a.</p><p>The recent one-shot approach <ref type="bibr" target="#b1">[2]</ref> attempts to decouple the weights using a "path dropout" strategy. During an SGD step in Eq. (6), each edge in the supernet graph is randomly dropped. The random chance is controlled via a dropout rate parameter. In this way, the co-adaptation of the node weights is reduced during training. Experiments in <ref type="bibr" target="#b1">[2]</ref> indicate that the training is very sensitive to the dropout rate parameter. This makes the supernet training hard. A carefully tuned heat-up strategy is used. In our implementation of this work, we also found that the validation accuracy is very sensitive to the dropout rate parameter.</p><p>Single Path Supernet and Uniform Sampling. Let us restart to think about the fundamental principle behind the idea of weight sharing. The key to the success of architecture search in Eq. <ref type="bibr" target="#b6">(7)</ref> is that, the accuracy of any architecture a on a validation set using inherited weight W A (a) (without extra fine tuning) is highly predictive for the accuracy of a that is fully trained. Ideally, this requires that the weight W A (a) to approximate the optimal weight w a as in Eq. (1). The quality of the approximation depends on how well the training loss L train (N (a, W A (a))) is minimized. This gives rise to the principle that the supernet weights W A should be optimized in a way that all architectures in the search space are optimized simultaneously. This is expressed as</p><formula xml:id="formula_7">W A = argmin W E a∼Γ (A) [L train (N (a, W (a)))] ,<label>(8)</label></formula><p>where Γ (A) is a prior distribution of a ∈ A. Note that Eq. <ref type="formula" target="#formula_7">(8)</ref> is an implementation of Eq. <ref type="bibr" target="#b5">(6)</ref>. In each step of optimization, an architecture a is randomly sampled. Only weights W (a) are activated and updated. So the memory usage is efficient. In this sense, the supernet is no longer a valid network. It behaves as a stochastic supernet <ref type="bibr" target="#b21">[22]</ref>. This is different from <ref type="bibr" target="#b1">[2]</ref>.</p><p>To reduce the co-adaptation between node weights, we propose a supernet structure that each architecture is a single path, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref> (a). Compared to the path dropout strategy in <ref type="bibr" target="#b1">[2]</ref>, the single path strategy is hyperparameterfree. We compared the two strategies within the same search space (as in this work). Note that the original drop path in <ref type="bibr" target="#b1">[2]</ref> may drop all operations in a block, resulting in a short cut of identity connection. In our implementation, it is forced that one random path is kept in this case since our choice block does not have an identity branch. We randomly select sub network and evaluate its validation accuracy during the training stage. Results in <ref type="figure" target="#fig_1">Fig.1</ref> show that drop rate parameters matters a lot. Different drop rates make supernet achieve different validation accuracies. Our single path strategy corresponds to using drop rate 1. It works the best because our single path strategy can decouple the weights of different operations. The <ref type="figure" target="#fig_1">Fig.1</ref> verifies the benefit of weight decoupling.</p><p>The prior distribution Γ (A) is important. In this work, we empirically find that uniform sampling is good. This is not much of a surprise. A concurrent work <ref type="bibr" target="#b9">[10]</ref> also finds that purely random search based on stochastic supernet is competitive on CIFAR-10. We also experimented with a variant that samples the architectures uniformly according to their constraints, named uniform constraint sampling. Specifically, we randomly choose a range, and then sample the architecture repeatedly until the FLOPs of sampled architecture falls in the range. This is because a real task usually expects to find multiple architectures satisfying different constraints. In this work, we find the uniform constraint sampling method is slightly better. So we use it by default in this paper.</p><p>We note that sampling a path according to architecture distribution during optimization is already used in previous weight sharing approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>. The difference is that, the distribution Γ (A) is a fixed priori during our training (Eq. <ref type="formula" target="#formula_7">(8)</ref>), while it is learnable and updated (Eq. (3)) in previous approaches (e.g. RL <ref type="bibr" target="#b14">[15]</ref>, policy gradient <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref>, Gumbel Softmax <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>, APG <ref type="bibr" target="#b30">[31]</ref>). As analyzed in Sec. 2, the latter makes the supernet weights and architecture parameters highly correlated and optimization difficult. There is another concurrent work <ref type="bibr" target="#b9">[10]</ref> that also proposed to use random sampling of paths in One-Shot model, and performed random search to find the superior architecture. This paper <ref type="bibr" target="#b9">[10]</ref> achieved competitive results to several SOTA NAS approaches on CIFAR-10, but didn't verify the method on large dataset ImageNet. It didn't  prove the effectiveness of single path sampling compared to the "path dropout" strategy and analyze the correlation of the supernet performance and the final evaluation performance. These questions will be answered in our work, and our experiments also show that random search is not good enough to find superior architecture from the large search space. Comprehensive experiments in Sec. 4 show that our approach achieves better results than the SOTA methods. Note that there is no such theoretical guarantee that using a fixed prior distribution is inherently better than optimizing the distribution during training. Our better result likely indicates that the joint optimization in Eq. (3) is too difficult for the existing optimization techniques.</p><p>Supernet Architecture and Novel Choice Block Design. Choice blocks are used to build a stochastic architecture. <ref type="figure" target="#fig_4">Fig. 3 (a)</ref> illustrates an example case. A choice block consists of multiple architecture choices. For our single path supernet, each choice block only has one choice invoked at the same time. A path is obtained by sampling all the choice blocks.</p><p>The simplicity of our approach enables us to define different types of choice blocks to search various architecture variables. Specifically, we propose two novel choice blocks to support complex search spaces.</p><p>Channel Number Search. We propose a new choice block based on weight sharing, as shown in <ref type="figure" target="#fig_4">Fig. 3 (b)</ref>. The main idea is to preallocate a weight tensor with maximum number of channels, and the system randomly selects the channel number and slices out the corresponding subtensor for convolution. With the weight sharing strategy, we found that the supernet can converge quickly.</p><p>In detail, assume the dimensions of preallocated weights are (max c out, max c in, ksize). For each batch in supernet training, the number of current output channels c out is randomly sampled. Then, we slice out the weights for current batch with the form Weights[: c out, : c in, :], which is used to produce the output. The optimal number of channels is determined in the search step.</p><p>Mixed-Precision Quantization Search. In this work, We design a novel choice block to search the bit widths of the weights and feature maps, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref> (c). We also combine the channel search space discussed earlier to our mixedprecision quantization search space. During supernet training, for each choice block feature bit width and weight bit width are randomly sampled. They are determined in the evolutionary step. See Sec. 4 for details.</p><p>Evolutionary Architecture Search. For architecture search in Eq. <ref type="formula" target="#formula_6">(7)</ref>, previous one-shot works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> use random search. This is not effective for a large search space. This work uses an evolutionary algorithm. Note that evolutionary search in NAS is used in <ref type="bibr" target="#b15">[16]</ref>, but it is costly as each architecture is trained from scratch. In our search, each architecture only performs inference. This is very efficient. The algorithm is elaborated in Algorithm 1. For all experiments, population size P = 50, max iterations T = 20 and k = 10. For crossover, two randomly selected candidates are crossed to produce a new one. For mutation, a randomly selected candidate mutates its every choice block with probability 0.1 to produce a new candidate. Crossover and mutation are repeated to generate enough new candidates that meet the given architecture constraints. Before the inference of an architecture, the statistics of all the Batch Normalization (BN) <ref type="bibr" target="#b8">[9]</ref> operations are recalculated on a random subset of training data (20000 images on ImageNet). It takes a few seconds. This is because the BN statistics from the supernet are usually not applicable to the candidate nets. This is also referred in <ref type="bibr" target="#b1">[2]</ref>. <ref type="figure" target="#fig_3">Fig. 2</ref> plots the validation accuracy over generations, using both evolutionary and random search methods. It is clear that evolutionary search is more effective. Experiment details are in Sec. 4.</p><p>The evolutionary algorithm is flexible in dealing with different constraints in Eq. (5), because the mutation and crossover processes can be directly controlled to generate proper candidates to satisfy the constraints. Previous RL-based [21] <ref type="table">Table 1</ref>. Overview and comparison of SOTA weight sharing approaches. Ours is the easiest to train, occupies the smallest memory, best satisfy the architecture (latency) constraint, and easily supports the large dataset. Note that those approaches belonging to the joint optimization category (Eq. (3)) have "Supernet optimization" and "Architecture search" columns merged and gradient-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref> methods design tricky rewards or loss functions to deal with such constraints. For example, <ref type="bibr" target="#b22">[23]</ref> uses a loss function CE(a, w a ) · α log(LAT(a)) β to balance the accuracy and the latency. It is hard to tune the hyper parameter β to satisfy a hard constraint like Eq. (5).</p><p>Summary. The combination of single path supernet, uniform sampling training strategy, evolutionary architecture search, and rich search space design makes our approach simple, efficient and flexible. <ref type="table">Table 1</ref> performs a comprehensive comparison of our approach against previous weight sharing approaches on various aspects. Ours is the easiest to train, occupies the smallest memory, best satisfies the architecture (latency) constraint, and easily supports large datasets. Extensive results in Sec. 4 verify that our approach is the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head><p>Dataset. All experiments are performed on ImageNet <ref type="bibr" target="#b16">[17]</ref>. We randomly split the original training set into two parts: 50000 images are for validation (50 images for each class exactly) and the rest as the training set. The original validation set is used for testing, on which all the evaluation results are reported, following <ref type="bibr" target="#b3">[4]</ref>.</p><p>Training. We use the same settings (including data augmentation, learning rate schedule, etc.) as <ref type="bibr" target="#b13">[14]</ref> for supernet and final architecture training. Batch size is 1024. Supernet is trained for 120 epochs and the best architecture for 240 epochs (300000 iterations) by using 8 NVIDIA GTX 1080Ti GPUs.</p><p>Search Space: Building Blocks. First, we evaluate our method on the task of building block selection, i.e. to find the optimal combination of building blocks under a certain complexity constraint. Our basic building block design is inspired by a state-of-the-art manually-designed network -ShuffleNet v2 <ref type="bibr" target="#b13">[14]</ref>. <ref type="table" target="#tab_3">Table 2</ref> shows the overall architecture of the supernet. The "stride" column represents the stride of the first block in each repeated group. There are 20 choice blocks in total. Each choice block has 4 candidates, namely "choice 3", "choice 5", "choice 7" and "choice x" respectively. They differ in kernel sizes and the number of depthwise convolutions. The size of the search space is 4 20 .  We use FLOPs ≤ 330M as the complexity constraint, as the FLOPs of a plenty of previous networks lies in <ref type="bibr">[300,</ref><ref type="bibr">330]</ref>, including manually-designed networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref> and those obtained in NAS <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="table" target="#tab_4">Table 3</ref> shows the results. For comparison, we set up a series of baselines as follows: 1) select a certain block choice only (denoted by "all choice *" entries); note that different choices have different FLOPs, thus we adjust the channels to meet the constraint. 2) Randomly select some candidates from the search space. 3) Replace our evolutionary architecture optimization with random search used in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. Results show that random search equipped with our single path supernet finds an architecture only slightly better that random select (73.8 vs. 73.7). It does no mean that our single path supernet is less effective. This is because the random search is too naive to pick good candidates from the large search space. Using evolutionary search, our approach finds out an architecture that achieves superior accuracy (74.3) over all the baselines.</p><p>Search Space: Channels. Based on our novel choice block for channel number search, we first evaluate channel search on the baseline structure "all choice 3" (refer to <ref type="table" target="#tab_4">Table 3</ref>): for each building block, we search the number of "midchannels" (output channels of the first 1x1 conv in each building block) varying from 0.2x to 1.6x (with stride 0.2), where "k-x" means k times the number of default channels. Same as building block search, we set the complexity constraint FLOPs ≤ 330M . <ref type="table" target="#tab_5">Table 4</ref> (first part) shows the result. Our channel search method has higher accuracy (73.9) than the baselines.</p><p>To further boost the accuracy, we search building blocks and channels jointly. There are two alternatives: 1) running channel search on the best building block search result; or 2) searching on the combined search space directly. Our experiments show that the first pipeline is slightly better. As shown in <ref type="table" target="#tab_5">Table 4</ref>, searching in the joint space achieves the best accuracy (74.7% acc.), surpassing the previous state-of-the-art manually-designed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> and automatically-searched models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref> under complexity of ∼ 300M FLOPs. Comparison with State-of-the-arts. Results in <ref type="table" target="#tab_5">Table 4</ref> shows our method is superior. Nevertheless, the comparisons could be unfair because different search spaces and training methods are used in previous works <ref type="bibr" target="#b3">[4]</ref>. To make direct comparisons, we benchmark our approach to the same search space of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In addition, we retrain the searched models reported in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref> under the same settings to guarantee the fair comparison. The search space and supernet architecture in ProxylessNAS <ref type="bibr" target="#b3">[4]</ref> is inspired by MobileNet v2 <ref type="bibr" target="#b17">[18]</ref> and MnasNet <ref type="bibr" target="#b20">[21]</ref>. It contains 21 choice blocks; each choice block has 7 choices (6 different building blocks and one skip layer). The size of the search space is 7 <ref type="bibr" target="#b20">21</ref> . FBNet <ref type="bibr" target="#b22">[23]</ref> also uses a similar search space. <ref type="table" target="#tab_6">Table 5</ref> reports the accuracy and complexities (FLOPs and latency on our device) of 5 models searched by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>, as the baselines. Then, for each baseline, our search method runs under the constraints of same FLOPs or same latency, respectively. Results shows that for all the cases our method achieves comparable or higher accuracy than the counterpart baselines. Furthermore, it is worth noting that our architectures under different constraints in <ref type="table" target="#tab_6">Table 5</ref> are searched on the same supernet, justifying the flexibility and efficiency of our approach to deal with different complexity constraints: supernet is trained once and searched multiple times. In contrast, previous methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref> have to train multiple supernets under various constraints. According to <ref type="table" target="#tab_8">Table 7</ref>, searching is much cheaper than supernet training.</p><p>Application: Mixed-Precision Quantization. We evaluate our method on ResNet-18 and ResNet-34 as common practice in previous quantization works (e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b28">29]</ref>). Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref>, we only search and quantize the res-blocks, excluding the first convolutional layer and the last fully-connected layer. Choices of weight and feature bit widths include {(1, 2), <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b1">2)</ref>, <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>, <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref>} in the search space. As for channel search, we search the number of "bottleneck channels" (i.e. the output channels of the first convolutional layer in each residual block) in {0.5x, 1.0x, 1.5x}, where "k-x" means k times the number of original channels. The size of the search space is (3 × 6) N = 18 N , where N is the number of choice blocks (N = 8 for ResNet-18 and N = 16 for ResNet-34). Note that for each building block we use the same bit widths for the two convolutions. We use PACT <ref type="bibr" target="#b4">[5]</ref> as the quantization algorithm. <ref type="table" target="#tab_7">Table 6</ref> reports the results. The baselines are denoted as kWkA (k = 2, 3, 4), which means uniform quantization of weights and activations with k-bits. Then, our search method runs under the constraints of the corresponding BitOps. We also compare with a recent mixed-precision quantization search approach <ref type="bibr" target="#b23">[24]</ref>. Results shows that our method achieves superior accuracy in most cases. Also note that all our results for ResNet-18 and ResNet-34 are searched on the same supernet. This is very efficient.</p><p>Search Cost Analysis. The search cost is a matter of concern in NAS methods. So we analyze the search cost of our method and previous methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref> (reimplemented by us). We use the search space of our building blocks to measure the memory cost of training supernet and overall time cost. All the supernets are trained for 150000 iterations with a batch size of 256. All models are trained with 8 GPUs. The <ref type="table" target="#tab_8">Table 7</ref> shows that our approach clearly uses less memory than other two methods because of the single path supernet. And our approach is much more efficient overall although we have an extra search step that costs less than 1 GPU day. Note <ref type="table" target="#tab_8">Table 7</ref> only compares a single run. In practice, our approach is more advantageous and more convenient to use when multiple searches are needed. As summarized in <ref type="table">Table 1</ref>, it guarantees to find out the architecture satisfying constraints within one search. Repeated search is easily supported. Correlation Analysis. Recently, the effectiveness of many neural architecture search methods based on weight sharing is questioned because of lacking fair comparison on the same search space and adequate analysis on the correlation between the supernet performance and the stand-alone model performance. Some papers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19]</ref> even show that several the state-of-the-art NAS methods perform similarly to the random search. In this work, the fair comparison on the same search space has been showed in <ref type="table" target="#tab_6">Table 5</ref>, so we further provider adequate correlation analysis in this part to evaluate the effectiveness of our method. Correlation analysis requires to achieve the performances of a large number of architectures, but training lots of architectures from scratch is very timeconsuming, which also requires a large number of GPU resources, so we use the NAS-Bench-201 <ref type="bibr" target="#b6">[7]</ref> to analyze our method. NAS-Bench-201 is a cell-based search space which includes 15,625 architectures in total. It provides the performance of each architecture on CIFAR-10, CIFAR-100, and ImageNet-16-120. So the results on it will be more credible and comparable.</p><p>We apply our method on different search spaces and different datasets to verify the effectiveness adequately. The original search space of NAS-Bench-201 consists of 5 possible operations: zeroize, skip connection, 1-by-1 convolution, 3-by-3 convolution, and 3-by-3 average pooling. Based on it, we further design several reduced search spaces, named Reduce-1, Reduce-2, Reduce-3, by deleting some operations. In detail, we delete 1-by-1 convolution and 3-by-3 average pooling respectively from original search space to produce Reduce-1 and Reduce-2 search spaces, and delete both to produce Reduce-3 search space. As <ref type="table">Table.</ref>8 shows, we use Kendall Tau τ metric to show the correlation between the supernet performance and the stand-alone model performance. It is obvious that our method performs better than random search on different search spaces and different datasets, since the Kendall Tau τ metric of random search should be 0. So the performances of architectures predicted by supernet can reflect the real ranking of architectures to a certain degree. However, the results in <ref type="table">Table.</ref>8 also reveals a limitation of our method that the predicted ranking of our supernet is partially correlated, but not perfectly correlated to the real ranking. So our method can not guarantee to find the real best architecture in the search space, but is able to find some superior architectures around the best. And we think that the correlation of supernet depends on search space. The simpler search space is, the higher correlation will be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we revisit the one-shot NAS paradigm and analyze the drawbacks of weight coupling in previous weight sharing methods. To alleviate those problems, we propose a single path one-shot approach which is simple but effective.</p><p>The comprehensive experiments show that our method can achieve better results than others on several different search spaces. We also analyze the search cost and correlation of our methods. Our method is more efficient, especially when multiple searches are needed. And our method can achieve significant correlation on different search spaces derived from NAS-Bench-201, which also verify the effectiveness of our method. There is also a limitation in our method that the predicted ranking of our supernet is partially correlated, but not perfectly correlated to the real ranking. And we think that it depends on search space. The simpler search space is, the higher correlation will be achieved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Equal contribution. This work is done when Haoyuan Mu and Zechun Liu are interns at MEGVII Technology. 1 This work is supported by The National Key Research and Development Program of China (No. 2017YFA0700800) and Beijing Academy of Artificial Intelligence (BAAI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Comparison of single path strategy and drop path strategy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Evolutionary vs. random architecture search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Choice blocks for (a) our single path supernet (b) channel number search (c) mixed-precision quantization search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 : 8 ACCi− 1 :</head><label>181</label><figDesc>Evolutionary Architecture Search 1 Input: supernet weights WA, population size P, architecture constraints C, max iteration T , validation dataset D val 2 Output: the architecture with highest validation accuracy under architecture constraints 3 P0 := Initialize population(P, C); Topk := ∅; 4 n := P/2; Crossover number 5 m := P/2; Mutation number 6 prob := 0.1; Mutation probability 7 for i = 1 : T do = Inf erence(WA, D val , Pi−1); 9 Topk := U pdate T opk(Topk, Pi−1, ACCi−1); 10 Pcrossover := Crossover(Topk, n, C); 11 Pmutation := M utation(Topk, m, prob, C); 12 Pi := Pcrossover ∪ Pmutation; 13 end 14 Return the architecture with highest accuracy in Topk;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Supernet architecture. CBchoice block. GAP -global average pooling</figDesc><table><row><cell cols="2">input shape block</cell><cell cols="3">channels repeat stride</cell></row><row><cell>224 2 × 3</cell><cell cols="2">3 × 3 conv 16</cell><cell>1</cell><cell>2</cell></row><row><cell>112 2 × 16</cell><cell>CB</cell><cell>64</cell><cell>4</cell><cell>2</cell></row><row><cell>56 2 × 64</cell><cell>CB</cell><cell>160</cell><cell>4</cell><cell>2</cell></row><row><cell>28 2 × 160</cell><cell>CB</cell><cell>320</cell><cell>8</cell><cell>2</cell></row><row><cell>14 2 × 320</cell><cell>CB</cell><cell>640</cell><cell>4</cell><cell>2</cell></row><row><cell>7 2 × 640</cell><cell cols="2">1 × 1 conv 1024</cell><cell>1</cell><cell>1</cell></row><row><cell>7 2 × 1024</cell><cell>GAP</cell><cell>-</cell><cell>1</cell><cell>-</cell></row><row><cell>1024</cell><cell>fc</cell><cell>1000</cell><cell>1</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results of building block search.</figDesc><table><row><cell cols="2">SPS -single path supernet</cell><cell></cell></row><row><cell>model</cell><cell cols="2">FLOPs top-1 acc(%)</cell></row><row><cell>all choice 3</cell><cell>324M</cell><cell>73.4</cell></row><row><cell>all choice 5</cell><cell>321M</cell><cell>73.5</cell></row><row><cell>all choice 7</cell><cell>327M</cell><cell>73.6</cell></row><row><cell>all choice x</cell><cell>326M</cell><cell>73.5</cell></row><row><cell cols="3">random select (5 times) ∼320M ∼73.7</cell></row><row><cell>SPS + random search</cell><cell>323M</cell><cell>73.8</cell></row><row><cell>ours (fully-equipped)</cell><cell>319M</cell><cell>74.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Results of channel search.</figDesc><table><row><cell></cell><cell cols="2">Performances are reported in the form "x (y)",</cell></row><row><cell cols="3">where "x" means the accuracy retrained by us and "y" means accuracy reported by</cell></row><row><cell>the original paper</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">FLOPs/Params Top-1 acc(%)</cell></row><row><cell>all choice 3</cell><cell>324M/3.1M</cell><cell>73.4</cell></row><row><cell>rand sel. channels (5 times)</cell><cell>∼ 323M/3.2M</cell><cell>∼ 73.1</cell></row><row><cell>choice 3 + channel search</cell><cell>329M/3.4M</cell><cell>73.9</cell></row><row><cell>rand sel. blocks + channels</cell><cell>∼ 325M/3.2M</cell><cell>∼ 73.4</cell></row><row><cell>block search</cell><cell>319M/3.3M</cell><cell>74.3</cell></row><row><cell cols="2">block search + channel search 328M/3.4M</cell><cell>74.7</cell></row><row><cell>MobileNet V1 (0.75x) [8]</cell><cell>325M/2.6M</cell><cell>68.4</cell></row><row><cell>MobileNet V2 (1.0x) [18]</cell><cell>300M/3.4M</cell><cell>72.0</cell></row><row><cell>ShuffleNet V2 (1.5x) [14]</cell><cell>299M/3.5M</cell><cell>72.6</cell></row><row><cell>NASNET-A [36]</cell><cell>564M/5.3M</cell><cell>74.0</cell></row><row><cell>PNASNET [11]</cell><cell>588M/5.1M</cell><cell>74.2</cell></row><row><cell>MnasNet [21]</cell><cell>317M/4.2M</cell><cell>74.0</cell></row><row><cell>DARTS [12]</cell><cell>595M/4.7M</cell><cell>73.1</cell></row><row><cell>Proxyless-R (mobile)* [4]</cell><cell>320M/4.0M</cell><cell>74.2 (74.6)</cell></row><row><cell>FBNet-B* [23]</cell><cell>295M/4.5M</cell><cell>74.1 (74.1)</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Compared with state-of-the-art NAS methods<ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref> using the same search space. The latency is evaluated on a single NVIDIA Titan XP GPU, with batchsize = 32. Accuracy numbers in the brackets are reported by the original papers; others are trained by us. All our architectures are searched from the same supernet via evolutionary architecture optimization</figDesc><table><row><cell>baseline network</cell><cell>FLOPs/</cell><cell cols="3">latency top-1 acc(%) top-1 acc(%)</cell><cell>top-1 acc(%)</cell></row><row><cell></cell><cell>Params</cell><cell></cell><cell>baseline</cell><cell cols="2">(same FLOPs) (same latency)</cell></row><row><cell>FBNet-A [23]</cell><cell cols="2">249M/4.3M 13ms</cell><cell>73.0 (73.0)</cell><cell>73.2</cell><cell>73.3</cell></row><row><cell>FBNet-B [23]</cell><cell cols="2">295M/4.5M 17ms</cell><cell>74.1 (74.1)</cell><cell>74.2</cell><cell>74.8</cell></row><row><cell>FBNet-C [23]</cell><cell cols="2">375M/5.5M 19ms</cell><cell>74.9 (74.9)</cell><cell>75.0</cell><cell>75.1</cell></row><row><cell cols="3">Proxyless-R(mobile) [4] 320M/4.0M 17ms</cell><cell>74.2 (74.6)</cell><cell>74.5</cell><cell>74.8</cell></row><row><cell>Proxyless(GPU) [4]</cell><cell cols="2">465M/5.3M 22ms</cell><cell>74.7 (75.1)</cell><cell>74.8</cell><cell>75.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Results of mixed-precision quantization search. "kWkA" means k-bit quantization for all the weights and activations</figDesc><table><row><cell>Method</cell><cell>BitOPs</cell><cell cols="2">top1-acc(%) Method</cell><cell>BitoPs</cell><cell>top1-acc(%)</cell></row><row><cell cols="3">ResNet-18 float point 70.9</cell><cell cols="3">ResNet-34 float point 75.0</cell></row><row><cell>2W2A</cell><cell>6.32G</cell><cell>65.6</cell><cell>2W2A</cell><cell>13.21G</cell><cell>70.8</cell></row><row><cell>ours</cell><cell>6.21G</cell><cell>66.4</cell><cell>ours</cell><cell>13.11G</cell><cell>71.5</cell></row><row><cell>3W3A</cell><cell>14.21G</cell><cell>68.3</cell><cell>3W3A</cell><cell>29.72G</cell><cell>72.5</cell></row><row><cell cols="2">DNAS [24] 15.62G</cell><cell>68.7</cell><cell cols="2">DNAS [24] 38.64G</cell><cell>73.2</cell></row><row><cell>ours</cell><cell>13.49G</cell><cell>69.4</cell><cell>ours</cell><cell>28.78G</cell><cell>73.9</cell></row><row><cell>4W4A</cell><cell>25.27G</cell><cell>69.3</cell><cell>4W4A</cell><cell>52.83G</cell><cell>73.5</cell></row><row><cell cols="2">DNAS [24] 25.70G</cell><cell>70.6</cell><cell cols="2">DNAS [24] 57.31G</cell><cell>74.0</cell></row><row><cell>ours</cell><cell>24.31G</cell><cell>70.5</cell><cell>ours</cell><cell>51.92G</cell><cell>74.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Search Cost. Gds -GPU days</figDesc><table><row><cell>Method</cell><cell cols="3">Proxyless FBNet Ours</cell></row><row><cell cols="2">Memory cost (8 GPUs in total) 37G</cell><cell>63G</cell><cell>24G</cell></row><row><cell>Training time</cell><cell>15 Gds</cell><cell cols="2">20 Gds 12 Gds</cell></row><row><cell>Search time</cell><cell>0</cell><cell>0</cell><cell>&lt;1 Gds</cell></row><row><cell>Retrain time</cell><cell>16 Gds</cell><cell cols="2">16 Gds 16 Gds</cell></row><row><cell>Total time</cell><cell>31 Gds</cell><cell cols="2">36 Gds 29 Gds</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Correlation in Different Search Spaces</figDesc><table><row><cell>Dataset</cell><cell cols="4">Original Reduce-1 Reduce-2 Reduce-3</cell></row><row><cell>CIFAR-10</cell><cell>0.55</cell><cell>0.55</cell><cell>0.58</cell><cell>0.64</cell></row><row><cell>CIFAR-100</cell><cell>0.56</cell><cell>0.54</cell><cell>0.53</cell><cell>0.59</cell></row><row><cell cols="2">ImageNet-16-120 0.54</cell><cell>0.42</cell><cell>0.55</cell><cell>0.53</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">"Supernet" is used as a general concept in this work. It has different names and implementation in previous approaches.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<title level="m">Smash: one-shot model architecture search through hypernetworks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">I J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<title level="m">Pact: Parameterized clipping activation for quantized neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Nas-bench-102: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00326</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<title level="m">Random search and reproducibility for neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="722" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<title level="m">Efficient neural architecture search via parameter sharing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<title level="m">Evaluating the search phase of neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Single-path nas: Designing hardware-efficient convnets in less than 4 hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02877</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning time/memory-efficient deep architectures with budgeted super networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Véniat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3492" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03443</idno>
		<title level="m">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mixed precision quantization of convnets via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00090</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1284" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">Snas: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Esperança</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12522</idno>
		<title level="m">Nas evaluation is frustratingly hard</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13577</idno>
		<title level="m">Differentiable neural architecture search via proximal iterations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">You only search once: Single shot neural architecture search via direct sparse optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01567</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05584</idno>
		<title level="m">Blockqnn: Efficient block-wise neural network architecture generation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
