<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowing What, Where and When to Look: Efficient Video Action Modeling with Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Pérez-Rúa</surname></persName>
							<email>j.perez-rua@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
							<email>brais.a@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<email>xiatian.zhu@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Toisoul</surname></persName>
							<email>a.toisoul@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>tao.xiang@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowing What, Where and When to Look: Efficient Video Action Modeling with Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action Recognition</term>
					<term>Video Attention</term>
					<term>Spatio-Temporal At- tention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attentive video modeling is essential for action recognition in unconstrained videos due to their rich yet redundant information over space and time. However, introducing attention in a deep neural network for action recognition is challenging for two reasons. First, an effective attention module needs to learn what (objects and their local motion patterns), where (spatially), and when (temporally) to focus on. Second, a video attention module must be efficient because existing action recognition models already suffer from high computational cost. To address both challenges, a novel What-Where-When (W3) video attention module is proposed. Departing from existing alternatives, our W3 module models all three facets of video attention jointly. Crucially, it is extremely efficient by factorizing the high-dimensional video feature data into lowdimensional meaningful spaces (1D channel vector for 'what' and 2D spatial tensors for 'where'), followed by lightweight temporal attention reasoning. Extensive experiments show that our attention model brings significant improvements to existing action recognition models, achieving new state-of-the-art performance on a number of benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition in unconstrained videos remains an unsolved problem, particularly as the recent research interest has shifted to fine-grained action categories involving interactions between humans and objects <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b60">61]</ref>. Why is a simple action such as "placing something next to something" so hard for a computer vision system yet so trivial for humans? One explanation is that videos typically contain highly redundant information in space and time which distracts a vision model from computing discriminative representations for recognition. For instance, with a cluttered background, there could be many other objects in the scene which can also interact with humans. Removing such distracting information from video modeling poses a great challenge. Human vision systems, on the other hand, have highly effective attention mechanisms to focus on the most relevant objects and motion patterns (what) at the right place (where) and time (when) <ref type="bibr" target="#b36">[37]</ref>. Introducing attention modeling in a video action recognition model is therefore not only useful but also essential.</p><p>Although attention modeling has been universally adopted in recent natural language processing (NLP) studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref>, and many static image based computer vision problems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b55">56]</ref> in the deep learning era, it is much understudied in action recognition. This is due to a fundamental difference: there are two facets in attention modeling for NLP (what and when), as well as static image (what and where), but three for video (what, where and when). Adding one more facet into attention modeling brings about significant challenges in model architecture design, training and inference efficiency. As a result, existing attentive action recognition models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55]</ref> only focus on a subset of the three facets. Among them, only the self-attention based non-local module <ref type="bibr" target="#b52">[53]</ref> shows convincing benefits and is adopted by recent 3D CNN-based models. However, it adds significant computational cost (see <ref type="table" target="#tab_9">Table 6</ref>) and is known to be hard to train <ref type="bibr" target="#b44">[45]</ref>.</p><p>In this paper, a novel What-Where-When (W3) video attention module is proposed. As suggested by the name, it models all three facets of video attentions. Crucially, it is extremely efficient, only adding marginal computational overhead in terms of floating point operations when introduced in an existing action recognition model. Inspired by the recent spatial and channel-wise factorization adopted in static image attention <ref type="bibr" target="#b55">[56]</ref>, the key idea of W3 is to factorize channel and spatial attention into static and temporal components. More concretely, given a feature map from a video CNN block/layer denoted as a tensor of four dimensions (time, channel and two for space), two attention sub-modules are formulated. In the first sub-module, each channel (representing object categories and local movement patterns, i.e., 'what') is attended, followed by 1D CNN based temporal reasoning. It is thus designed to capture important local motions evolving over time. In the second sub-module, spatial attention evolution over time is modeled using a lightweight 3D CNN to focus on the 'where' and 'when' facets. The two sub-modules are then applied to the original feature map sequentially and trained end-to-end with the rest of the model. Even after extensive factorization, the introduction of temporal reasoning in both sub-modules makes our W3 hard to train. This is due to the vanishing gradient problem when our attention module with temporal reasoning is added to each layer of a stateof-the-art deep video CNN models such as I3D <ref type="bibr" target="#b3">[4]</ref> or TSM <ref type="bibr" target="#b28">[29]</ref>. To overcome this problem, two measures are taken. First, gradients are directed to the attention modules of each layer by an attention guided feature refinement module. Second, Mature Feature-guided Regularization (MFR) is formulated based on staged self-knowledge-distillation to prevent the model from being stuck in a bad local minimum.</p><p>The main contributions of this work are as follows. (1) We introduce a novel What-Where-When (W3) video attention module for action recognition in unconstrained videos. It differs from existing video attention modules in that all three facets of attention are modeled jointly. (2) The computational over-head of the proposed attention module is controlled to be marginal (e.g., merely 1.5% ∼ 3.2% extra cost on TSM) thanks to a new factorized network architectural design for video attention modeling. (3) The problem of effective training of a deep video attention module is addressed with a novel combination of attention guided feature refinement module and mature feature-guided (MFR) regularization. Extensive experiments are conducted on four fine-grained video action benchmarks including Something-Something V1 <ref type="bibr" target="#b16">[17]</ref> and V2 <ref type="bibr" target="#b30">[31]</ref>, EgoGesture <ref type="bibr" target="#b60">[61]</ref>, and EPIC-Kitchens <ref type="bibr" target="#b5">[6]</ref>. The results show that our module brings significant improvements to existing action recognition models, achieving new state-of-theart performance on a number of benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video action recognition Video action recognition has made significant advances in recent years, due to the availability of more powerful computing facilities (e.g., GPU and TPU), the introduction of ever larger video datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>, and the active developments of deep neural network based action models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. Early efforts on deep action recognition were focused on combining 2D CNN for image feature computing with RNN for temporal reasoning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63]</ref> or 2D CNN on optical flow <ref type="bibr" target="#b40">[41]</ref>. These have been gradually replaced by 3D convolutions (C3D) networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47]</ref> recently. The 3D kernels can be also formed via inflating 2D kernels <ref type="bibr" target="#b3">[4]</ref> which facilitates model pre-training using larger scale image datasets, e.g., ImageNet.</p><p>Two recent trends in action recognition are worth mentioning. First, the interest has shifted from coarse-grained categories such as those in UCF101 <ref type="bibr" target="#b42">[43]</ref> or Kinetics <ref type="bibr" target="#b3">[4]</ref> where background (e.g., a swimming pool for diving) plays an important role, to fine-grained categories such as those in Something-Something <ref type="bibr" target="#b16">[17]</ref> and EPIC-Kitchens <ref type="bibr" target="#b5">[6]</ref> where modeling human-object interaction is the key. Second, since 3D CNNs typically are much larger and require much more operations during inference, many recent works focus on efficient network designed based on 2D spatial + 1D temporal factorization (R2+1D) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref> or 2D+temporal shift module (TSM) <ref type="bibr" target="#b47">[48]</ref>. TSM is particularly attractive because it has the same model complexity as 2D CNN and yet can still capture temporal information in video effectively. In this work we focus on fine-grained action recognition for which attention modeling is crucial and using TSM as the main backbone instantiation even though it can be applied to any other video CNN models. Attention in action recognition Most existing video attention modules are designed for the out-of-date RNN based action recognition models. They are based on either encoder-decoder attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55]</ref>, spatial attention only <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55]</ref>, temporal attention <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b50">51]</ref>, or spatio-temporal attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b54">55]</ref>. Compared to our W3, they are much weaker on the 'what' facet -our module attends to each CNN channel representing a combination of object and its local motion pattern only when it evolves over time in a certain way. Further, as they are formulated in the context of recurrent models, they cannot be integrated to the latest video CNN-based state-of-the-art action models. Other video attention methods are designed specifically for egocentric videos <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b43">44]</ref> or skeleton videos <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b57">58]</ref>, which however assume specific domain knowledge (e.g., central objects <ref type="bibr" target="#b43">[44]</ref>), or extra supervision information (e.g., eye gaze <ref type="bibr" target="#b26">[27]</ref>), or clean input data (e.g., skeleton <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b57">58]</ref>). In contrast, our module is suited for action understanding in unconstrained videos without extra assumptions and supervision.</p><p>Note that the aforementioned video attention modules as well as our W3 are non-exhaustive, focusing on a limited subset of the input space to compute attention. Recently, inspired by the success of transformer self-attention in NLP <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref>, non-local networks have been proposed <ref type="bibr" target="#b52">[53]</ref> and adopted widely <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref>. By computing exhaustively the pairwise relationships between a given position and all others in space and time, non-local self-attention can be considered as a more generic and potentially more powerful attention mechanism than ours. However, a number of factors make it less attractive than W3. (1) Self-attention in NLP models use positioning encoding to keep the temporal information. When applied to video, the non-local operator does not process any temporal ordering information (i.e., missing 'when'), while temporal reasoning is performed explicitly in our attention module. (2) The non-local operator induces larger computational overhead (see <ref type="table" target="#tab_9">Table 6</ref>) due to exhaustive pairwise relationship modeling and is known to have convergence problems during training <ref type="bibr" target="#b44">[45]</ref>. In contrast, our W3 adds neglectable overhead (see <ref type="table" target="#tab_9">Table 6</ref>), and is easy to train thanks to our architecture and training strategy specifically designed to assist in gradient flow during training. Importantly, our model is clearly superior to non-local when applied to the same action model (see Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">What-Where-When Video Attention</head><p>Overview. Given an action recognition network based on a 3D CNN or its various lightweight variants, our W3 is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. We take a 4D feature map F ∈ R T ×C×H×W from any intermediate layer as the input of W3, where T, C, H, W denote the frame number of the input video clip, the channel number, the height and width of the frame-level feature map respectively. Note that the feature map of each channel is obtained using a 3D convolution filter or a time-aware 2D convolution in the case of TSM networks <ref type="bibr" target="#b62">[63]</ref>; it thus captures the combined information about both object category and its local movement patterns, i.e., 'what'. The objective of W3 is to compute a same-shape attention mask M ∈ R T ×C×H×W that can be used to refine the feature map in a way such that action class-discriminative cues can be sufficiently focused on, whilst the irrelevant ones are suppressed. Formally, this attention learning process is expressed as:</p><formula xml:id="formula_0">F = F ⊗ M, M = f (F) (1)</formula><p>where ⊗ specifies the element-wise multiplication operation, and f () is the W3 attention reasoning function.</p><p>To facilitate effective and efficient attention learning, we consider an attention factorization scheme by splitting the 4D attention tensor M into a channel- temporal attention sub-module M c ∈ R T ×C and a spatio-temporal attention sub-module M s ∈ R T ×H×W . This reduces the attention mask size from T CHW to T (C + HW ) and therefore the learning difficulty. As such, the above feature attending is reformulated into a two-step sequential process as:</p><formula xml:id="formula_1">F c = M c ⊗ F(T, C), M c = f c (F); F s = M s ⊗ F c (T, H, W ), M s = f s (F c )</formula><p>(2) where f c () and f s () denote the channel-temporal and spatio-temporal attention functions, respectively. The arguments of F specify the dimensions of elementwise multiplication. Next we provide the details of the two attention sub-modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Channel-temporal Attention</head><p>The channel-temporal attention focuses on the 'what-when' facets of video attention. Specifically it measures the importance of a particular object-motion pattern evolving temporally across a video sequence in a specific way. For computational efficiency, we squeeze the spatial dimensions (H × W ) of each framelevel 3D feature map to yield a compact channel descriptor d chnl ∈ R T ×C as in <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b20">21]</ref>. While average-pooling is a common choice for global spatial information aggregation, we additionally include max-pooling which would be less likely to miss small and/or occluded objects. Using both pooling operations is also found to be more effective in static image attention modeling <ref type="bibr" target="#b55">[56]</ref>. We denote the two channel descriptors as d avg-c and d max-c ∈ R C×1×1 (indicated by the purple boxes in the top of <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>To mine the inter-channel relationships for a given frame, we then forward d avg-c and d max-c into a shared MLP network θ c-frm with one hidden layer to produce two channel frame attention descriptors, respectively. We use a bottleneck design with a reduction ratio r which shrinks the hidden activation to the size of C r × 1 × 1, and combine the two frame-level channel attention descriptors by element-wise summation into a single one M c-frm . We summarize the above frame-level channel-temporal attention process as</p><formula xml:id="formula_2">M c-frm = σ f θ c-frm (d avg-c ) ⊕ f θ c-frm (d max-c ) ∈ R C×1×1 ,<label>(3)</label></formula><p>where f θ c-frm () outputs channel frame attention and σ() is the sigmoid function.</p><p>In fine-grained action recognition, temporal dynamics of semantic objects are often the distinguishing factor between classes that involve human interaction with the same object (e.g., opening/closing a book). To model the dynamics, a small channel temporal attention network θ c-vid is introduced, composed of a CNN network with two layers of 1D convolutions, to reason about the temporally evolving characteristics of each channel dimension ( <ref type="figure" target="#fig_0">Fig. 1</ref> top-right). This results in our channel-temporal attention mask M c , computed as:</p><formula xml:id="formula_3">M c = σ f θ c-vid ({M c-frm i } T i=1 ) .<label>(4)</label></formula><p>Concretely, this models the per-channel temporal relationships of successive frames in a local window specified by the kernel size K c-vid , and composed by two layers (we set K c-vid = 3 in our experiments, producing a composed temporal attention span of 6 frames with two 1D CNN layers). In summary, the parameters of our channel attention model are {θ c-frm , θ c-vid }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatio-temporal Attention</head><p>In contrast to the channel-temporal attention that attends to discriminative object local movement patterns evolving temporally in certain ways, this submodule attempts to localize them over time. Similarly, we apply average-pooling and max-pooling along the channel axis to obtain two compact 2D spatial feature maps for each video frame, denoted as d avg-s and d max-s ∈ R 1×H×W . We then concatenate the two maps and deploy a spatial attention network θ s-frm with one 2D convolutional layer for each individual frame to output the frame-level spatial attention M s-frm . The kernel size is set to 7×7 (see <ref type="figure" target="#fig_0">Fig. 1</ref> bottom-left). To incorporate the temporal dynamics to model how spatial attention evolves over time, we further perform temporal reasoning on {M s-frm i } T i=1 ∈ R T ×H×W using a lightweight sub-network θ s-vid composed of two 3D convolutional layers. We adopt the common kernel size of 3 × 3 × 3 ( <ref type="figure" target="#fig_0">Fig. 1 bottom-right)</ref>. We summarize the frame-level and video-level spatial attention learning as:</p><formula xml:id="formula_4">M s-frm = σ f θ s-frm ([d avg-s , d max-s ]) ∈ R 1×H×W ,<label>(5)</label></formula><formula xml:id="formula_5">M s = σ f θ s-vid ({M s-frm i } T i=1 ) ∈ R T ×H×W<label>(6)</label></formula><p>The parameters of spatio-temporal attention hence include {θ s-frm , θ s-vid }.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Architecture</head><p>Our W3 video attention module can be easily integrated into any existing CNN architecture. Specifically, it takes as input a 4D feature tensor and outputs an improved same-shape feature tensor with channel-spatio-temporal video attention.</p><p>In this paper, we focus on the ResNet-50 based TSM <ref type="bibr" target="#b28">[29]</ref> as the main instantiation for integration with W3. Other action models such as I3D <ref type="bibr" target="#b3">[4]</ref> and R2+1D <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">49]</ref> can be easily integrated without architectural changes (see Supplementary for more details). With ResNet-50 as an example, following the multi-block stage-wise design, we apply our attention module at each residual block of the backbone, i.e., performing the attention learning on every intermediate feature tensor of each stage. A diagram of W3-attention enhanced ResNet-50 is depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, with attention maps for every residual including a further attention refinement module to be explained in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>Learning discriminative video attention would be challenging if trained with standard gradient back propagation through multiple blocks from the top end. This is because each layer of the action model now has an attention module with temporal reasoning. For those modules, the loss supervision is indirect and gradually becomes weaker/vanishing when it reaches the bottom levels. We overcome this issue by exploiting two remedies: (1) attention guided feature refinement on architecture design and (2) mature feature-guided regularization on training strategy.</p><p>Attention guided feature refinement. In addition to the standard gradient pathway across the backbone network layers, we further create another pathway for the attention modules only. Concretely, we sequentially aggregate all the stage-wise attention masks M s,j i at the frame level, where i and j index the frame image and network stage, respectively. Suppose there are N network stages (e.g., 4 stages in ResNet-50), we obtain a multi-level attention tensor by adaptive average pooling (AAP) and channel-wise concatenation <ref type="figure" target="#fig_1">(Fig. 2)</ref>:</p><formula xml:id="formula_6">M ms i = [AAP (M s,1 i ), AAP (M s,2 i ), · · · , AAP (M s,N i )] ∈ R N ×H l ×W l<label>(7)</label></formula><p>where H l and W l refer to the spatial size of the last stage's feature map x i ∈ R C l ×H l ×W l . AAP is for aligning the spatial size of attention masks from different stages. Taking M ms i as input, we then deploy a tiny CNN network θ ref (composed of one conv layer with C l 1×1 sized kernels for channel size alignment) to produce a feature refining map, which is further element-wise added to x i . Formally, it is written as:</p><formula xml:id="formula_7">y i = x i + f θ ref (M ms i ),<label>(8)</label></formula><p>where y i is the refined feature map of frame i. This process repeats for all the frames of a video sample. Discussion. The newly introduced pathway provides dedicated, joint learning of video attention from multiple stages of the action model backbone and a shortcut for the gradient flow. This is because its output is used directly to aggregate with the final stage feature map, enabling the supervision flowing from the loss down to every single attention module via the shortcuts. This is essentially a form of deep supervision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>. Mature feature-guided regularization. Apart from attention deep supervision, we introduce mature feature guided regularization to further improve the model training. This follows a two-stage training process. In the first stage, we train a video action recognition model with the proposed attention module and attention guided feature refinement (Eq. (8)) until convergence, and treat it as a teacher model P . In the second stage, we train the target/student model Q with identical architecture by mimicking the feature maps of P at the frame level. Formally, given a frame image i we introduce a feature mimicking regression loss in the training of Q w.r.t. P as:</p><formula xml:id="formula_8">L f m = y Q i − y P i 2<label>(9)</label></formula><p>where y Q i and y P i are the feature maps obtained using Eq. (8) by the target (Q) and teacher (P ) models respectively, with the former serving as the mature feature to regularize the student's learning process via anchoring to a better local optimum than that of the teacher. For the training objective function, we use the summation of cross-entropy classification loss and attention-guided feature refinement loss (Eq. (8)) in the first stage. The feature mimicking regularization (Eq. (9)) further adds up in the second stage. During both training and testing, video-level prediction is obtained by averaging the frame-level predictions.</p><p>Discussion. Our regularization is similar to the notion of knowledge distillation (KD) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b61">62]</ref> but has key differences: (1) Unlike the conventional KD methods aiming for model compression <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38]</ref>, we use the same architecture for both teacher and student networks. (2) Compared to <ref type="bibr" target="#b37">[38]</ref>, which also distills feature map knowledge, we only limit to the last attended feature maps rather than multiple ones, and without the need of extra parameters for aligning the feature shape between student and target. (3) Although <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b61">62]</ref> also use the same network architecture for teacher and student, they differently adopt an online distillation strategy which has higher memory usage than our offline counterpart. The representation for distillation used is class prediction distribution which also differs from the feature maps utilized in our model. (4) Born-Again networks <ref type="bibr" target="#b12">[13]</ref> are more similar to ours since they also perform offline distillation using a single model architecture. However, as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b61">62]</ref> they also use the class prediction distribution in distillation. Moreover, only simpler image learning tasks are tested in <ref type="bibr" target="#b12">[13]</ref>, whilst we verify our training strategy in a more complex video analysis task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons with Existing State-of-the-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We utilized four popular fine-grained action recognition benchmarks: Something-Something V1 <ref type="bibr" target="#b16">[17]</ref>, contains 108, 499 videos from 174 finegrained action classes about hand-object interactions. Some of these classes are visually subtle and hence challenging to differentiate, such as "Pretending to turn something upside down". Something-Something V2 <ref type="bibr" target="#b30">[31]</ref> presents an extended version of V1, including 220, 847 higher-resolution videos with less noisy labels. EgoGesture <ref type="bibr" target="#b60">[61]</ref> is a large scale multi-modal dataset with 21, 161 trimmed videos, depicting 83 dynamic gesture classes recorded by a head-mounted RGB+D camera in egocentric point of view. It is challenging due to strong motion blur, heavy illumination variations, and background clutter from both indoor and outdoor scenes. We followed the classification benchmark setting. EPIC-Kitchens <ref type="bibr" target="#b5">[6]</ref> is a non-scripted first-person-view dataset recorded in 32 real kitchen environments over multiple days and cities. It has 55 hours of video, 39, 594 action segments, 454, 255 object bounding boxes, 125 verb and 331 noun classes. We evaluated the classification task on verb, noun, and action (verb+noun) on the standard test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and testing</head><p>We followed the common practice as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b52">53]</ref>. Specifically, the model was trained from ImageNet weights for all the datasets. For testing, multiple clips are sampled per video and used the full resolution images with shorter side 256. For efficient inference, we used 1 clip per video and the center crop sized at 224×224. All the competitors used the same setting for fair comparison. We reported Top-1/5 accuracy rates for performance evaluation. Results on Something-Something V1 We compared our W3 method with the state-of-the-art competitors in <ref type="table" target="#tab_1">Table 1</ref>. It is evident that our W3 with TSM <ref type="bibr" target="#b28">[29]</ref> yields the best results among all the competitors, which validates the overall performance superiority of our attention model. We summarize detailed comparisons as below: (1) 2D models (1 st block): Without temporal inference, 2D models such as TSN <ref type="bibr" target="#b51">[52]</ref> perform the worst, as expected. Whilst the performance can be improved clearly using independent temporal modeling after feature extraction with TRN <ref type="bibr" target="#b62">[63]</ref>, it remains much lower than the recent 3D models. (2) 2D+3D models (2 nd block): As shown for ECO, the introduction of 3D spatio-temporal feature extraction notably boosts the performance w.r.t. TSN and TRN. However, these methods still suffer from high computational cost for a model with competitive performance. (3) 3D models (3 rd block): The I3D model <ref type="bibr" target="#b3">[4]</ref> has been considered widely as a strong baseline and further improvements have been added in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> including self-attention based non-local network. A clear weakness of these methods is their huge computational cost, making deployment on resource-constrained devices impossible. (4) Time-shift models (4 th block): As the previous state-of-the-art model, TSM <ref type="bibr" target="#b28">[29]</ref> yields remarkable accuracy with the computational cost as low as 2D models. Importantly, our W3 attention on top of TSM further boosts the performance by a significant margin. For instance, it achieves a Top-1 gain of 3.6%/5.4% when using 8/16 frames per video in test, with only a small extra cost of 0.5G/2.1G FLOPs.</p><p>Results on Something-Something V2 The results are shown in <ref type="table" target="#tab_3">Table  2</ref>. Following <ref type="bibr" target="#b28">[29]</ref>, we used two clips per video each with 16 frames in testing. Overall, we have similar observation as on V1. For instance, TRN <ref type="bibr" target="#b62">[63]</ref> is clearly inferior to our baseline TSM <ref type="bibr" target="#b28">[29]</ref>, and our method further significantly improves the Top-1 accuracy by 3.4% when using 16×2 full resolution frames.</p><p>Results on EgoGesture   TSM and our model. It is obvious that our W3 with TSM outperforms all the competitors, often with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on EPIC-Kitchens</head><p>We compared our method with a number of state-of-the-art action models in <ref type="table" target="#tab_6">Table 4</ref>. In this experiment, we adopted the test setup of <ref type="bibr" target="#b34">[35]</ref>: using two clips per video and ten crops. On this realistic and challenging dataset, we observed consistent performance gain obtained by adding our W3 attention model to the baseline TSM across verb, noun, and action classification. This leads to the best accuracy rates among all the strong competitors evaluated in the same setting. For example, W3 improves the action top-1 accuracy by 5.3%/2.8% on seen/unseen kitchen test sets. We also report a clear margin over the current state-of-the-art model, Action Banks <ref type="bibr" target="#b56">[57]</ref> on verb classification. Note that Action Banks uses more temporal data for every action and noun prediction, and an extra object detector. This gives it an unfair advantage over our model, and explains its better performance on noun classification and subsequent action classification. The results validate the importance of spatiotemporal attention learning for action recognition in unconstrained egocentric videos, and the effectiveness of our W3 attention formulation. <ref type="table" target="#tab_1">Top-1  Top-5  Top-1  Top-5  Top-1</ref> Top-5 Model S1 S2 S1 S2 S1 S2 S1 S2 S1 S2 S1 S2  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verb Noun Action</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Setting We conducted an in-depth ablation study of our W3 attention module on Something-Something V1 <ref type="bibr" target="#b16">[17]</ref>, We used ResNet-50 based TSM <ref type="bibr" target="#b28">[29]</ref> as the baseline and adopted the same setting. We fine-tuned the model from ImageNet pre-trained weights with 16 RGB frames per video. In testing, we used 1 clip per video and the center crop of 224 × 224. We adopted the Top-1 accuracy as the performance evaluation metric.    thus far in the literature. The results in <ref type="table" target="#tab_9">Table 6</ref> show that (i) Our W3 attention yields the most significant accuracy boost over the base action model TSM <ref type="bibr" target="#b28">[29]</ref>, validating the overall performance advantages of our attention operation.</p><p>(ii) When combined with TSM and end-to-end trained, surprisingly CBAM produces a very strong performance on par with Non-Local attention, indicating that a strong video action method can be composited by simply applying image attention to top-performing action models. However, there is still a clear gap against the proposed W3 with a more principled way of learning spatio-temporal video attention. (iii) W3 achieves this by being much less compute-intensive than the Non-local alternative, adding virtually no extra computational cost on top of TSM. The above analysis suggests that CBAM and our mature feature guided regularization (MFR) both are very effective. We thus expect their combination to be a stronger competitor. The results in <ref type="table" target="#tab_10">Table 7</ref> validate this -an extra +1.4% boost in Top-1 accuracy over CBAM alone when our new training strategy is applied. Interestingly, we note that the gain by MFR is more significant (2.4% increase) when used with the proposed attention mechanism. A plausible reason is that richer spatio-temporal information is captured by our attention model in the first training stage, which further benefits the subsequent regularization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention visualization and interpretation</head><p>We visualized the attended regions of our W3 in comparison to CBAM. The attention masks of three video sequences each with five evenly sampled frames are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. We observed that, CBAM tends to attend objects and/or hands whenever they appear, regardless if some action is happening or not (i.e., static spatial attention). In contrast, our W3 attention is activated only when a specific action is ongoing (i.e., spatiotemporal attention), and importantly all the informative regions are attended including hands, active objects, and related scene context. For instance, in the second example action "Placing something next to something", the neighbouring scissor is also attended, which clearly interprets how our model can make a correct prediction. A similar phenomenon is observed in the third example action "Pushing something off something".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have presented a novel lightweight video attention module for fine-grained action recognition in unconstrained videos. Used simply as a drop-in building block, our proposed W3 module significantly improves the performance of existing action recognition methods with very small extra overhead. It yields superior performance over a number of state-of-the-art alternatives on a variety of action recognition tasks.  In the main paper, as a showcase we have introduced our proposed W3 module in the state-of-the-art TSM network <ref type="bibr" target="#b28">[29]</ref> (the baseline action model), and conducted extensive evaluations and analysis on several action benchmarks. In this supplementary, we focus on the generality analysis of W3 video attention across different baseline action models. Setting We use the Something-Something-V1 dataset <ref type="bibr" target="#b16">[17]</ref>. To examine the pure effect of W3, we exclude the proposed Mature Feature guided Regularization (MFR) in model training. Apart from TSM, we further evaluate the widely adopted I3D <ref type="bibr" target="#b3">[4]</ref> and the very recent TAM <ref type="bibr" target="#b9">[10]</ref> as the baseline models, separately. W3 is evaluated in the same training and testing setup as any baseline model for fair comparison and accurate analysis. Results The results are shown in <ref type="table" target="#tab_12">Table 8</ref>. It is clear that W3 consistently brings about accuracy performance boost to very different baseline architectures. This suggests strong generality of the proposed W3 attention in improving existing action models. It is also worth mentioning that further applying our MFR strategy to model training, we would expect more significant performance margins (cf. <ref type="table" target="#tab_8">Table 5</ref> in the main paper).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An overview of the proposed W3 attention module. Top: Detail of the channeltemporal attention sub-module (orange box). A multi-layer perceptron transforms the input feature into a per-frame attention vector. The concatenation of these vectors across the temporal dimension is further processed by a temporal CNN (1D convolutions) and a final sigmoid non-linearity. Bottom: Detail of the spatio-temporal attention sub-module (green box). After a 2D convolution on the concatenation of crosschannel max and mean pooled features, a 3D CNN is applied on the stacked singlechannel per-frame intermediate spatial attention maps. Attention maps are point-wise multiplied with the input features. For both blocks, the dark and light purple boxes are max and mean pooling operations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>W3-attention enhanced ResNet-50 architecture with the proposed attention guided feature refinement. W3-attention maps are gathered from all the ResNet stages, concatenated across the channel dimension, and fed to a 1 × 1 convolution with ReLU non-linearity. The output is then added to the final feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of spatio-temporal attention regions. Attention masks by CBAM<ref type="bibr" target="#b55">[56]</ref> and our method on Something-Something V1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="5">Backbone #Frame GFLOPs Top-1 Top-5</cell></row><row><cell>TSN [52]</cell><cell>BNI</cell><cell>8</cell><cell>16</cell><cell>19.5</cell><cell>-</cell></row><row><cell>TSN* [52]</cell><cell>R50</cell><cell>8</cell><cell>33</cell><cell>19.7</cell><cell>46.6</cell></row><row><cell>TRN-Multiscale [63]</cell><cell>BNI</cell><cell>8</cell><cell>16</cell><cell>34.4</cell><cell>-</cell></row><row><cell>TRN-Multiscale* [63]</cell><cell>R50</cell><cell>8</cell><cell>33</cell><cell>38.9</cell><cell>68.1</cell></row><row><cell>TRN RGB+Flow [63]</cell><cell>BNI</cell><cell>8+8</cell><cell>-</cell><cell>42.0</cell><cell>-</cell></row><row><cell>ECO [64]</cell><cell>BNI+3D R18</cell><cell>8</cell><cell>32</cell><cell>39.6</cell><cell>-</cell></row><row><cell>ECO [64]</cell><cell>BNI+3D R18</cell><cell>16</cell><cell>64</cell><cell>41.4</cell><cell>-</cell></row><row><cell>ECOenLight [64]</cell><cell>BNI+3D R18</cell><cell>92</cell><cell>267</cell><cell>46.4</cell><cell>-</cell></row><row><cell cols="3">ECOenLight RGB+Flow [64] BNI+3D R18 92+92</cell><cell>-</cell><cell>49.5</cell><cell>-</cell></row><row><cell>I3D  † [4]</cell><cell>3D R50</cell><cell cols="2">32×2 clip 153×2</cell><cell>41.6</cell><cell>72.2</cell></row><row><cell>I3D  † +NL [53]</cell><cell>3D R50</cell><cell cols="2">32×2 clip 168×2</cell><cell>44.4</cell><cell>76.0</cell></row><row><cell>I3D+NL+GCN [54]</cell><cell cols="3">3D R50+GCN 32×2 clip 303×2</cell><cell>46.1</cell><cell>76.8</cell></row><row><cell>SlowFast [11]</cell><cell>(2D+1)R50</cell><cell>32</cell><cell>65×2</cell><cell>47.5</cell><cell>76.0</cell></row><row><cell>TSM [29]</cell><cell>R50</cell><cell>8</cell><cell>33</cell><cell>45.6</cell><cell>74.2</cell></row><row><cell>TSM [29]</cell><cell>R50</cell><cell>16</cell><cell>65</cell><cell>47.2</cell><cell>77.1</cell></row><row><cell>TSMen [29]</cell><cell>R50</cell><cell>8+16</cell><cell>98</cell><cell>49.7</cell><cell>78.5</cell></row><row><cell>TSM+W3 (Ours)</cell><cell>R50</cell><cell>8</cell><cell>33.5</cell><cell>49.0</cell><cell>77.3</cell></row><row><cell>TSM+W3 (Ours)</cell><cell>R50</cell><cell>16</cell><cell>67.1</cell><cell>52.6</cell><cell>81.3</cell></row></table><note>Comparison with state-of-the-art on Something-Something-V1 [17]. Setting: using the center crop and 1 clip per video in test unless otherwise specified. BNI=BNInception; R18/50=ResNet-18/50;* : Results from [29].† : Results from [54].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>shows comparative results on this egocentric dataset. For fast inference, we used only 8 RGB frames per video for</figDesc><table><row><cell>Model</cell><cell cols="4">Backbone # Frames Top-1 Top-5</cell></row><row><cell>TRN-Multiscale [63]</cell><cell>BNI</cell><cell>8 × 2</cell><cell>48.4</cell><cell>77.6</cell></row><row><cell>TRN-Multiscale [63]</cell><cell>R50</cell><cell>8</cell><cell>38.9</cell><cell>68.1</cell></row><row><cell>TRN RGB+Flow [63]</cell><cell>BNI</cell><cell cols="2">(8 + 8) × 2 55.5</cell><cell>83.1</cell></row><row><cell>TSM [29]</cell><cell>R50</cell><cell>FR: 8 × 2</cell><cell>59.1</cell><cell>85.6</cell></row><row><cell>TSM [29]</cell><cell>R50</cell><cell cols="2">FR: 16 × 2 63.1</cell><cell>88.1</cell></row><row><cell>TSM+W3</cell><cell>R50</cell><cell cols="2">CC: 16 × 2 65.7</cell><cell>90.2</cell></row><row><cell>TSM+W3</cell><cell>R50</cell><cell cols="2">FR: 16 × 2 66.5</cell><cell>90.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art on Something-Something-V2<ref type="bibr" target="#b30">[31]</ref>.</figDesc><table><row><cell cols="7">FR=Full Resolution testing; CC=Center Crop testing. In testing, two clips per video</cell></row><row><cell>were used.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="6">Backbone #Frames Val Top-1 Val Top-5 Test Top-1 Test Top-5</cell></row><row><cell>VGG16+LSTM [18]</cell><cell>VGG16</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>74.7</cell><cell>-</cell></row><row><cell>C3D [47]</cell><cell>C3D</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>81.7</cell><cell>-</cell></row><row><cell>C3D [47]</cell><cell>C3D</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>86.4</cell><cell>-</cell></row><row><cell>C3D+LSTM+RSTM [3]</cell><cell>C3D</cell><cell>16</cell><cell>-</cell><cell>-</cell><cell>89.3</cell><cell>-</cell></row><row><cell>I3D [4]</cell><cell>I3D</cell><cell>32</cell><cell>-</cell><cell>-</cell><cell>90.3</cell><cell>-</cell></row><row><cell>TSM [29]</cell><cell>R50</cell><cell>8</cell><cell>79.7</cell><cell>96.9</cell><cell>80.5</cell><cell>97.8</cell></row><row><cell>TSM+W3</cell><cell>R50</cell><cell>8</cell><cell>93.9</cell><cell>98.7</cell><cell>94.3</cell><cell>99.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art on EgoGesture<ref type="bibr" target="#b60">[61]</ref>. Setting: 1 crop per video in test, using RGB frames only unless specified otherwise.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Action Banks [57] -full res 60.0 50.9 88.4 77.6 45.0 31.5 71.8 57.8 32.7 21.2 55.3 39.4</figDesc><table><row><cell>TSN  *  [52]</cell><cell>49.7 36.7 87.2 73.6 39.9 23.1 65.9 44.7 24.0 12.8 46.1 26.1</cell></row><row><cell>TRN  *  [63]</cell><cell>58.8 47.3 86.6 76.9 37.3 23.7 63.0 46.0 26.6 15.7 46.1 30.0</cell></row><row><cell>TRN-Multiscale  *  [63]</cell><cell>60.2 46.9 87.2 75.2 38.4 24.4 64.7 46.7 28.2 16.3 47.9 29.7</cell></row><row><cell>TSM  *  [29]</cell><cell>57.9 43.5 87.1 73.9 40.8 23.3 66.1 46.0 28.2 15.0 49.1 28.1</cell></row><row><cell>TSM+W3</cell><cell>64.4 50.2 88.8 78.0 44.2 26.6 68.1 49.5 33.5 17.8 53.9 32.6</cell></row><row><cell>TSM+W3 -full res</cell><cell>64.7 51.4 88.8 78.5 44.7 27.0 69.0 50.3 34.2 18.7 54.6 33.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Comparison with state-of-the-art on EPIC-Kitchens [6]. Setting: 8</cell></row><row><cell>frames per video and 10 crops in test and only RGB frames. S1: Seen Kitchens; S2:</cell></row><row><cell>Unseen Kitchens. '  *  ': Results from [35].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TSM +CBAM [56] +NL [53] +W3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top-1 47.2</cell><cell>49.1</cell><cell>49.8</cell><cell>52.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>GFLOPs 65.0</cell><cell>66.5</cell><cell>115.0</cell><cell>67.1</cell></row><row><cell></cell><cell>Model</cell><cell cols="2">component</cell><cell>analysis.</cell></row><row><cell>MFR=Mature</cell><cell>Feature</cell><cell>guided</cell><cell cols="2">regularization;</cell></row><row><cell cols="2">AFR=Attention-guided</cell><cell>Feature</cell><cell></cell><cell>Refinement;</cell></row><row><cell cols="5">SA=Spatial Attention; TA=Temporal Attention.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparing attention models. NL=Non Local;Model component analysis InTable 5, we examined the effect of three main components in our W3 attention by removing them one at a time due to the dependence nature in design. We observed that: (i) In model optimization, our mature feature guided regularization brings the majority performance gain, whilst attention guided feature refinement also helps improve the results. (ii) Among three attention facets, if we keep the channel ('what') facet and remove one of the other two, we can see that temporal attention turns out to be more important than spatial attention. This reflects the fundamental difference between video and image analysis tasks.</figDesc><table><row><cell>Comparing attention models We compared our W3 attention model with</cell></row><row><cell>two strong competitors: (1) CBAM [56] which is the state-of-the-art image at-</cell></row><row><cell>tention; (2) Non-Local operator [53] which is the best video attention module</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Impact of our Mature Feature guided Regularization (MFR). Refer to text for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>6 Supplementary: Experiments with other backbones</figDesc><table><row><cell>Model</cell><cell cols="4">Backbone #Frame Top-1 Top-5</cell></row><row><cell>I3D [4]</cell><cell>3D R18</cell><cell>32</cell><cell>34.9</cell><cell>62.6</cell></row><row><cell>I3D+W3 w/o MFR (Ours)</cell><cell>3D R18</cell><cell>32</cell><cell>35.3</cell><cell>63.5</cell></row><row><cell>I3D [4]</cell><cell>3D R18</cell><cell>64</cell><cell>36.4</cell><cell>63.7</cell></row><row><cell>I3D+W3 w/o MFR (Ours)</cell><cell>3D R18</cell><cell>64</cell><cell>36.6</cell><cell>63.7</cell></row><row><cell>TAM [10]</cell><cell>R50</cell><cell>8</cell><cell>49.6</cell><cell>78.5</cell></row><row><cell>TAM+W3 w/o MFR (Ours)</cell><cell>R50</cell><cell>8</cell><cell>50.1</cell><cell>79.3</cell></row><row><cell>TSM [29]</cell><cell>R50</cell><cell>8</cell><cell>46.2</cell><cell>74.3</cell></row><row><cell>TSM+W3 w/o MFR (Ours)</cell><cell>R50</cell><cell>8</cell><cell>47.2</cell><cell>75.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Evaluating the generality of the proposed W3 video attention on three different baseline action models on Something-Something-V1<ref type="bibr" target="#b16">[17]</ref>. Setting: using the center crop and 1 clip per video in test. R18/50=ResNet-18/50.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Human Behavior Understanding</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Egocentric gesture recognition using recurrent 3d convolutional neural networks with spatiotemporal transformer modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3763" to="3771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2109" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maria Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL. pp</title>
		<imprint>
			<biblScope unit="page" from="4171" to="4186" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent spatial-temporal attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-IP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1347" to="1360" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="34" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning region features for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions of Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7517" to="7527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gharbieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09235</idno>
		<title level="m">Fine-grained video classification and captioning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition with spatialtemporal discriminative filter banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interpretable spatio-temporal attention for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bam: Bottleneck attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An evaluation of action recognition models on epic-kitchens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00867</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3D residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all we need: Nailing down object-centric attention for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks, nonlocal diffusion and nonlocal modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09522</idno>
		<title level="m">Action classification and highlighting in videos</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action recognition by an attention-aware temporal weighted convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1979</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Hierarchical attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06416</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Longterm feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Action recognition with spatio-temporal visual attention on skeleton image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-CSVT</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2405" to="2415" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Egogesture: a new dataset and benchmark for egocentric hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1038" to="1050" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
