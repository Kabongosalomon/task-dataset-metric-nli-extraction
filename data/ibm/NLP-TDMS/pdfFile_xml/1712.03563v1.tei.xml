<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
							<email>wubo@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lang</surname></persName>
							<email>langbo@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
							<email>huanglei@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks (CNNs) can be applied to graph similarity matching, in which case they are called graph CNNs. Graph CNNs are attracting increasing attention due to their effectiveness and efficiency. However, the existing convolution approaches focus only on regular data forms and require the transfer of the graph or key node neighborhoods of the graph into the same fixed form. During this transfer process, structural information of the graph can be lost, and some redundant information can be incorporated. To overcome this problem, we propose the disordered graph convolutional neural network (DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a preprocessing layer called the disordered graph convolutional layer (DGCL). The DGCL uses a mixed Gaussian function to realize the mapping between the convolution kernel and the nodes in the neighborhood of the graph. The output of the DGCL is the input of the CNN. We further implement a backward-propagation optimization process of the convolutional layer by which we incorporate the feature-learning model of the irregular node neighborhood structure into the network. Thereafter, the optimization of the convolution kernel becomes part of the neural network learning process. The DGCNN can accept arbitrary scaled and disordered neighborhood graph structures as the receptive fields of CNNs, which reduces information loss during graph transformation. Finally, we perform experiments on multiple standard graph datasets. The results show that the proposed method outperforms the state-of-the-art methods in graph classification and retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A graph structure is a rich representational form that can describe complex structural data in the real world, such as images, biomedical data, and social networks. Many studies represent an image as an attribute graph and transform the image retrieval problem into an attribute graph search problem. The technique of chemical analysis graph searching, for example, can facilitate the study of properties of newly synthesized chemicals by referring to a database of existing chemicals with known properties. Therefore, it is important to study graph feature learning and searching.</p><p>In recent years, deep learning has been applied to many areas and has been shown to significantly outperform traditional methods. Among the available techniques, convolutional neural networks (CNNs) are widely used in image classification, semantic segmentation and object recognition. CNNs can learn the local structure and features of data. Because data such as images, video and sound have the same fixed-sized neighborhoods, convolution, pooling and other operations are well defined in the mathematical sense. For example, in an image, each pixel has eight neighboring nodes. However, traditional CNNs cannot be applied directly to graph data, whose neighborhoods are irregular.</p><p>To apply CNNs to graph-structured data, multiple methods have been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. These methods can be divided into two categories: spatial-based methods and spectral-based methods. Spatial-based approaches use the neighborhood information from the graph data space in convolution operations. The main strategy of these methods is to convert the convolution of the graph data into an inner product of the neighborhood information in the graph data space. However, it is challenging to find a convolution operation that is translation-invariant for irregular data. The spectral-based methods typically use the Laplacian to transform the graph data and then use the eigenvector as the convolution operator. The purpose of this transformation is to approximate the convolution operation of the graph data as a convolution operation of the regularized data. In recent work, researchers have attempted to design a graph-CNN architecture by employing a graph-labeling procedure for the construction of a receptive field. Mathias Niepert <ref type="bibr" target="#b7">[8]</ref> proposed a framework for learning CNNs for graphs. To a certain extent, the methods mentioned above solve the problem of applying a CNN to graph data. However, all of these methods require the graphs to be transformed into the same neighborhoods with the same ordering . This process is called graph regularization and involves the conversion of graph data into a data format that can be processed by standard CNNs.</p><p>The local receptive field of a graph is similar to the fixed-size neighborhood of an image. However, the numbers of neighboring nodes of each node in the graph are not fixed. The standard practice is to regularize the neighborhood of the node: First, a threshold value is fixed. If the neighborhood size is less than the threshold, the neighborhood will be filled with zeros, which is equivalent to adding invalid information. When the neighborhood size is greater than the threshold, we interpret the threshold size of the node as a neighborhood , which results in the loss of some of the effective neighborhood information. However, this approach cannot reflect the neighborhood information of real nodes. This type of model can support continuous labelling with graph data but requires the graph or node neighborhoods of the graph to be transformed into fixed-sized representations to meet the processing requirements of the CNN. Therefore, this method can result in the loss of important information due to the padding and interception operations. Thus, one of the challenges in improving the effect of applying CNNs to graph data is that the neural network model can perform convolution operations directly on irregular node neighborhoods and can perform parametric learning.</p><p>To address these limitations, we present a graph convolutional neural network (g-CNN) model that can perform feature learning on graph data directly. We use a continuous mapping function (which is based on a mixed Gaussian process) between the irregular local neighborhood and the convolution weight to transform the discrete parameter learning problem into a parameter sampling problem of a continuous function. Therefore, parameter sampling becomes a function of the features in the preceding layer of the network rather than being based on manually defined parameters on the graph, as in previous studies.</p><p>The main innovation of our model is that it does not need to convert the graph or its node neighborhoods into fixed structures; instead, the model learns the irregular structural data directly and can optimize the graph convolution kernel through the neural network. Thus, the model is called the disordered graph convolutional neural network (DGCNN). We conduct experiments on multiple standard graph datasets, and the experimental results show that the proposed method outperforms the existing g-CNN methods and other types of methods in graph classification and retrieval.</p><p>The remainder of this paper is organized as follows. In Section 2, we introduce the relevant work on g-CNNs and graph kernels (g-kernels). In Section 3, we introduce the model structure. In Section 4, we introduce the DGCL. In Section 5, we describe the experiment and present the results of our method and the comparison methods. Finally, in Section 6, we discuss the results and present our conclusions , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Current graph processing methods can be divided into two categories: traditional kernel approaches and g-CNNs. The g-CNN approaches often apply standard CNNs to graph data feature learning, while traditional kernel approaches typically use non-linear projection to transform sample graph data into a higher-dimensional feature space, where analysis and processing are performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">G-kernel approaches</head><p>G-kernel approaches project a graph into a feature vector space; the similarity of the two graphs is their scalar product in the space. A g-kernel often defines the similarity function for two graphs. Multiple g-kernels have been proposed, such as the random-walk (RW) kernel, the shortest-path (SP) kernel and the sub-tree kernel.</p><p>Gartnerj et al. <ref type="bibr" target="#b8">[9]</ref> proposed an RW kernel function based on computing the RW kernel functions of common steps for two graphs and proved that this function is a positive-definite function. However, the RW g-kernel function has two disadvantages. First, for both of the g-kernels, the comparison of RW paths is of enormous computational complexity. Second, an RW path often contains multiple repeated points and edges, which influences the computational efficiency of RW g-kernel functions. Weisfeiler <ref type="bibr" target="#b9">[10]</ref> proposed the WL sub-tree g-kernel, which is based on the one-dimensional WL isomorphism algorithm. This algorithm searches for the sub-tree structure that is shared by two graphs. However, the WL kernel supports only discrete features, and the memory consumed by the WL kernel is proportional to the number of training samples. The SP kernel <ref type="bibr" target="#b19">(Borgwardt and Kriegel 2006)</ref> calculates the similarity by comparing every pair of edges in SP graphs. Shervashidze <ref type="bibr" target="#b10">[11]</ref> proposed a graphlet count kernel (GK) function based on the sub-graph structure. A graphlet is a small-sized sub-graph that often contains 3 to 5 nodes. Due to the lack of an effective approach for node labelling, this GK function is not applicable to datasets that are focused on node labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph convolutional neural networks</head><p>CNNs are applied to graph data in two broad categories of research: spectral filtering methods and local filtering methods. In the field of spectral filtering methods, Henaff et al. <ref type="bibr" target="#b11">[12]</ref> used feature vectors of graph Laplacians to perform convolution and used a weighted distance to construct the similarity matrix. Defferrard et al. <ref type="bibr" target="#b1">[2]</ref> proposed a network model based on ChebNet, which is a spectrally defined method with space attributes. In this model, Chebyshev polynomials of the Laplacian are used to learn k-hop neighborhoods of graph data, thereby incorporating spatial information into neighborhoods. Kipf and Welling <ref type="bibr" target="#b12">[13]</ref> derived a semi-supervised g-CNN approach by simplifying and extending the ChebNet-based model. All of these approaches require a fixed graph data structure. In the field of local filtering methods, Atwood and Towsley developed a diffusion convolutional neural network (DCNN) that performs RWs in graph data to select the neighborhood structure in the space as the input for the CNN. However, the DCNN is of complexity O(N 2 ), which restricts the extendibility of this approach. Bruna et al. <ref type="bibr" target="#b0">[1]</ref> proposed a multi-scale cluster-based g-CNN model in which convolution defines the weight of each non-shared attribute of each cluster. Duvenaud et al. <ref type="bibr" target="#b1">[2]</ref> developed a local space filter that can be applied to any node and its neighborhood. Mathias Niepert <ref type="bibr" target="#b7">[8]</ref> proposed a method that can obtain the local receptive field of graph data and apply it to a CNN, which includes three steps: 1) select a node; 2) construct the fixed-size neighborhood of this node to form a fixed-size sub-graph; and 3) regulate the neighborhood sub-graph. It is possible to obtain a one-dimensional data unit that can be processed by a standard CNN using these three steps. However, both spectrally and spatially defined methods need to transform graph data into data structures with fixed scale, and feature information loss during the transformation process is unavoidable.</p><p>Unlike previous work, the g-CNN model we propose, namely, DGCNN, is specially designed for the disordered features of node neighborhoods and can perform convolution from irregular neighborhood structures while achieving the back-propagation of graph convolution without transforming the graph data structure into a fixed regular structure. After parameter sampling based on the Gaussian mixture model (GMM), the DGCNN can perform convolution operations on irregular and disorder neighborhood structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model structure and preprocessing</head><p>The key step for the application of CNNs to normalize grid data is to use a window of size k * k to capture the local neighborhood of the image and share the corresponding convolution kernel parameters in the window. Because of the randomness of the neighborhoods of graph nodes, the traditional window translation method is not applicable to graph data, and a g-CNN model is proposed for accommodating random node neighborhoods.</p><p>When processing an image in the framework of the standard CNN model, the local receptive field is used to implement the convolution operation on the data according to a step movement and obtain the local features of the graph, as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, where the convolution window size is fixed to W * H. Due to the normalization of the pixel position of the image, the local receptive field can be moved from left to right and from top to bottom to obtain the local information of the image. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, the neighborhoods of the different nodes correspond to various receptive fields of the convolution processes. The node neighborhoods of the graph have no fixed scale or order; thus, the convolution cannot be implemented directly on the graph using the fixed-sized and ordered convolution kernels. To overcome the above problems, we propose a disordered g-CNN model that can be applied to arbitrary graph data. The network in this model can learn the parameter mapping between the random node neighborhoods and the convolution weights of the graph. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the graph data are first transformed into a receptive field that can be processed by the CNNs. Then, a convolution operation is performed over the kernel parameter matrix and the receptive field. Finally, the g-CNN takes the output of the convolutional layer (CL) as the input data of the standard overall connection layer. Our model contains the following parts:</p><p>(1)Key Node Selection: The selection operation is implemented on the graph data to obtain a fixed number of key nodes. To ensure that the number of neighborhoods of the nodes in each graph is consistent, the same number of key nodes is sampled for each graph. (2)Neighborhood Assembly: The nodes of the k − neighborhood are the candidates for the receptive field. Note that this time, the receptive field is disordered.</p><p>(3)Parameter Sampling: The corresponding convolution kernel parameters are sampled based on the mixed Gaussian model according to the information of the nodes in each neighborhood to implement the convolution operation for each neighboring graph with its corresponding convolution kernel parameters.</p><p>(4)Feature Learning: By combining the DGCL with the standard CNN and the output layers, the g-CNNs can be built and can directly learn the neighborhood of any random node. The preprocessing procedure was implemented on each input graph data, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which includes node sampling and node neighborhood construction:</p><p>(1)Sequence sampling of key nodes: To sort all the nodes in the graph, a method proposed elsewhere <ref type="bibr" target="#b7">[8]</ref> was adopted, and a graph labeling function was introduced in which the set of nodes in the graph are mapped to an ordered node sequence according to the centripetal parameters (e.g., the node degree or centrad). From the sequence, w nodes are alternately selected according to a certain interval s to form the ultimate node sequences. The nodes in the graph are sorted first, as shown in <ref type="figure">Fig. 3</ref>, and then four nodes are alternately selected as the key node sequence according to the interval s = 2.</p><p>(2)Node neighborhood construction: As shown in <ref type="figure">Fig. 3</ref>, for each node in the node sequence that was obtained in the previous step, breadth-first searching is used to find the neighboring nodes, which form the neighborhood set of the original key nodes. The node in each node neighborhood should contain the attribute (such as the weight of the edge or the similarity) between the node and the key node and the attributes of the node (such as the node category).</p><p>After the two steps of input graph data preprocessing, the input data are transformed into a random neighborhood set with a fixed size, which is similar to the local receptive field set of the graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DGCL and its learning process</head><p>In the DGCNN, a disordered CL that can receive and process any graph data is designed. A disordered CL is a CL that can perform convolution operations on irregular and disordered node neighborhoods while achieving the back-propagation of the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Disordered graph convolutional layer</head><p>For a DGCL, the input is the node neighborhood structure that was obtained after graph preprocessing. With the GMM and an activation function, the input graph is transformed into the output of the CNNs. <ref type="figure">Fig. 4(a)</ref> shows a standard CNN with a receptive field on an image. The receptive field is of fixed size and ordered. For graph data, each node neighborhood is of variable size and disordered, as shown in <ref type="figure">Fig. 4(b)</ref>. For example, the node neighborhood in <ref type="figure">Fig. 4(b)</ref> includes 5 nodes. Among nodes N1, N2, N3, N4 and N5, N1 is the key node, and the others are its neigh boring nodes. It is necessary to obtain the convolutional kernels for these five nodes before performing the convolution operation. An existing solution is to define a fixed-sized convolutional kernel parameter, which requires a regularization process that may lead to information loss. However, we sample the convolutional parameters of each neighborhood node on the possibility distribution for the similarity of the neighborhood nodes and the key node, and the number of convolutional parameters is the same as the number of key node neighborhoods. According to the central limit theorem <ref type="bibr" target="#b13">[14]</ref>, it is reasonable to assume that the probability distribution of the parameters is defined by the GMM, which can approximate any probability distribution. Thus, for such a node neighborhood, we sample the convolutional parameters on the sheaf of a Gaussian function GMM (θ) based on the GMM for each node neighborhood.</p><formula xml:id="formula_0">F(N) ... (θ0 , X0) (θ1 , X1) (θ2 , X2) (θ3 , X3)</formula><p>(θk , Xk) <ref type="figure">Figure 5</ref>: Convolutional Unit of A Node Neighborhood The output value of the convolution operation for such a neighborhood and the sampled kernel parameter is:</p><formula xml:id="formula_1">F (N ) = (GM M (θ), X) = 5 k=1 ( n i=1 w i G(θ k , µ i , σ i ), X k )<label>(1)</label></formula><p>where N denotes the neighborhood map of a key node, X is the attribute value of a node according to this map, parameter X k is the attribute of the k − th node, θ is the correlation between the k − th node and the key node, m is the number of Gaussian components, w i is the weight of each Gaussian component, µ i and σ i are the mean value and variance of each Gaussian component, respectively, and G(·) is the Gaussian function.</p><p>After sampling the convolutional parameters for all key nodes, convolution operations are performed on the neighborhood to finish the graph data convolution in the graph CL. The convolutional processing of the graph data represents the forward propagation of the neural network, as shown in <ref type="figure">Fig. 4(b)</ref>.</p><p>In <ref type="figure">Fig. 5</ref>, the red node in the input part represents the key node, the white nodes are the neighboring nodes of the key node, the yellow node is the output node, and the green oval in the middle represents F (N ). θ 0 is a constant (in this study, θ 0 = 0), and θ 1 , θ 2 , · · · , θ k represent the attribute values of edges between neighboring nodes and the key node, and X 0 , X 1 , X 2 , ..., X k are the attribute values of the nodes.</p><p>As for the j-th receptive field, we can obtain the forward output for which the graph convolutional process is performed on its receptive field.</p><formula xml:id="formula_2">f j = I i=0 GM M (θ i )X i + b<label>(2)</label></formula><p>where I denotes the size of the neighborhood sub-graph, X i denotes the attribute value of the i-th node, GM M (θ i ) denotes mixed Gaussian values of the i-th node, and b ∈ R E denotes a vector of bias terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph convolution back-propagation and Gaussian parameter learning</head><p>In the DGCL based on the GMM, the parameters of each component of the GMM must be optimized. The difference between the output value of CNN and the real value is then used for back-propagation to adjust the parameters. The error function for back-propagation is defined in formula (3) <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_3">error(θ) = 1 2 A a=1 B b=1 (t (a) b (θ) − f (a) b (θ)) 2 = 1 2 A a=1 B b=1 (∇f a b (θ)) 2 (3) where t (a)</formula><p>i is the a-th dimension of the corresponding label of the b-th graph data, f (a) b is the similarly value of the a-th output layer unit in response to the n-th input pattern, B is the number of graph data types, and A is the number of graph data.</p><p>For the a-th graph data, we can immediately compute the gradient:</p><formula xml:id="formula_4">∇f a θ = ∂f a ∂(w 1 , µ 1 , σ 1 , ..., w m , µ m , σ m ) = ( ∂f a ∂w 1 , ∂f a ∂µ 1 , ∂f a ∂σ 1 , ..., ∂f a ∂w m , ∂f a ∂µ m , ∂f a ∂σ m )<label>(4)</label></formula><p>where f denotes the output of forward-propagation; w, µ, σ are the weight, mean value and variance of each Gaussian component, respectively; and m is the number of Gaussian components. We need to calculate the derivative and parameters of the Gaussian component:</p><formula xml:id="formula_5">∂f ∂w i = 1 √ 2πσ i e − (x−µ i ) 2 2σ 2 i (5) ∂f ∂σ i = w i √ 2πσ i e − (x−µ i ) 2 2σ 2 i [−1 + (x − µ i ) 2 σ 2 i ] (6) ∂f ∂µ i = − w i * (x − µ) √ 2πσ 3 i e − (x−µ i ) 2 2σ 2 i (7) w i+1 = λ * ∂f ∂w i (8) σ i+1 = λ * ∂f ∂σ i (9) µ i+1 = λ * ∂f ∂µ i<label>(10)</label></formula><p>where w i+1 , µ i+1 , σ i+1 are the parameters that are obtained after updating w i , µ i , σ i , respectively, and λ is a learning rate parameter. In practice, there is often a learning rate parameter λ for each Gaussian component.</p><p>The computation of the gradient of bias b is the same as that for the traditional CL and is explained here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Number of parameters and computational complexity</head><p>Each weight matrix W is obtained by sampling the mixed Gaussian model, and the number of weight matrices is equal to the number of neighboring nodes of key nodes. The parameters in our method with respect to a conventional CNN are the Gaussian component weight w, mean value µ, and variance σ for each vector. Let N be the number of nodes in the graph and M be the number of Gaussian components. The total number of parameters is 3 * N * M . Here, we ignore the bias terms, which contribute few parameters. The complexity of the DGCNN consists of the forward-and back-propagation complexities. Let k be the number of key nodes in the graph. Let E denote the average number of neighbors of each node. For the forward-propagation process, DGCNN has a worst-case complexity of O(f (k * M * E)), and for the back-propagation process, DGCNN has a worst-case complexity of O(3 * f (k * M * E)). Let T be the number of graphs. The total computational complexity is O(4 * T * f (k * M * E)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graph datasets</head><p>We conduct our experiments on the following popular real datasets to compare our approach with state-ofthe-art g-kernels and CNNs in terms of retrieval precision:</p><p>• PTC <ref type="bibr" target="#b15">[16]</ref>: PTC consists of 344 chemical compounds, where the classes indicate carcinogenicity for male and female rats.</p><p>• D&amp;D <ref type="bibr" target="#b16">[17]</ref>: D&amp;D is a data set of 1178 protein structures classified into enzymes and non-enzymes.</p><p>• AIDS <ref type="bibr" target="#b17">[18]</ref>: The dataset contain 4395 chemical compounds, of which 423 belong to class CA, 1081 to CM, and the remaining compounds to CI .</p><p>• PROTEIN <ref type="bibr" target="#b7">[8]</ref>: PROTEINS is a graph collection in which nodes are secondary structure elements and edges indicate neighborhoods in the amino-acid sequence or in 3D space. Graphs are classified as enzymes or non-enzymes.</p><p>• COLLAB <ref type="bibr" target="#b18">[19]</ref>: The dataset contains 12,000 graphs, each with an average of 400 nodes. The dataset contain users, movies, and the users' ratings of the movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental configuration</head><p>(1) We compare the DGCNN method that is proposed herein with the following methods by experiments on graph classification and graph search:</p><p>• G-kernel method: the SP kernel <ref type="bibr" target="#b19">[20]</ref>, the RW kernel <ref type="bibr" target="#b8">[9]</ref>, the GK kernel <ref type="bibr" target="#b20">[21]</ref>, and the Weisfeiler-Lehman subtree kernel (WL) <ref type="bibr" target="#b9">[10]</ref>.</p><p>• g-CNNs: PATCHY-SAN <ref type="bibr" target="#b7">[8]</ref>, LMFGCN <ref type="bibr" target="#b21">[22]</ref>, and SSGCN <ref type="bibr" target="#b12">[13]</ref>.</p><p>(2) We test the influence of the number of Gaussian components on the proposed model using 5, 10, 15, 20, 25, 30 and 35 components and find the optimal number, at which the best effect is obtained;</p><p>(3) We perform efficiency analysis. In our experiment, when calculating the Weisfeiler-Lehman fingerprint, the number of iterations is set as h = 10, the GK parameter is set as 7, and the factor of RW is set as 10 −6 , 10 −5 , . . . , 10 −1 . For PATCHY-SAN, the fixed receptive field in this paper is k = 5. The network structure consists of two CLs, which are one-dimensional (5 * 1); one dense hidden layer; and one softmax layer. The CNN uses 3 * 3 filters. The network structure of SSGCN is consistent with that of PATCHY-SAN. To obtain fair comparison results, for the graph classification experiment, the network structure in DGCNN consists of one DGCL, one standard CL, one dense hidden layer and one softmax layer. In the graph search experiment, the network structure is the same as that used for graph classification. In this study, the output of the dense hidden layer is treated as a feature vector of the graph data.</p><p>All the experiments are performed under the following configuration: an Intel Xeon X5650, a dualcore CPU running at 2.66 GHz with 8 GB memory, and a Linux system. The methods that use CNNs are implemented using the torch framework.   <ref type="figure">Figure 7</ref>: Relationship between Number of Gaussian Components and Classification precision for Different Datasets traditional g-kernel approaches, the DGCNN exhibits a significant advantage on most datasets. Existing g-CNNs use the standard CNN model and predefine only a one-dimensional convolutional kernel (e.g., 1 * 5) in the convolutional feature-learning process without making full use of the space information between the key nodes and their neighborhood information. The method proposed here transforms the discrete learning process into a process of projecting continuous functions and fully learns the relationships between the neighboring nodes and key nodes in the graph data. Therefore, on the most datasets, DGCNN outperforms the existing g-CNN methods and g-kernel methods.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Comparison of the Graph Retrieval Results</head><p>The experiment compares the DGCNN and existing methods in terms of graph similarity retrieval performance. We use the output of the dense hidden layer as the feature vector of the graph data. To ensure a fair comparison, for PATCHY-SAN, we also use the output of the dense hidden layer as the feature vector of the graph data. As indicated by the experiment results shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, on the PTC, PROTEIN, COLLAB and AIDS datasets, the DGCNN outperforms the other methods. On the D&amp;D dataset, the DGCNN method performs similarly to the optimal GK method. The DGCNN outperforms the recently proposed g-CNN methods on most datasets because the process of regularizing node neighborhoods leads to loss of information about the node neighborhoods, whereas our convolutional kernel method, which is based on the GMM, builds a dynamic graph convolutional kernel, which eliminates the local information loss during node neighborhood regularization.</p><p>(</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Influence of the Number of Gaussian Components</head><p>The purpose of this experiment is to examine the influence of the number of Gaussian components on the DGCNN. In the experiment, the number of Gaussian components is defined as m, which is set as 5, 10, 15, 20, 25, 30 or 35. For each case, we perform the same experiment on multiple graph datasets and calculate the mean result as the final result. The experimental results are shown in <ref type="figure">Fig. 7</ref>.</p><p>For dataset PTC, the classification precision increases significantly from m = 5 to m = 15 and is stable when m is larger than 15. For ADIS, the classification precision increases significantly from m = 5 to m = 20 and is relatively stable when m is larger than 20. Similarly, for the PROTRIN and D&amp;D datasets, the classification precision increases significantly from m = 5 to m = 15 and from m = 5 to m = 25, respectively, and is stable when m is larger. For the dataset with many graphs, namely COLLAB, more parameters are needed to make the model fit the data. We found that m = 25 gives the best results on this These experiments indicate that the optimal numbers of Gaussian components for different datasets are different. Increasing the number above the optimal number does not increase the precision.</p><p>In the next experiments, we use the optimal number of Gaussian components with the best classification result for each dataset.</p><p>(4) Efficiency analysis We assess the efficiency of the DGCNN by applying it to graph datasets. For a given graph, we compute the end-to-end running time. The results in <ref type="table" target="#tab_1">Table 2</ref> show that the running time of the DGCNN is less than that of WL. In addition, the average values for other graph convolutional approaches on different datasets are 11.7 s (for PTC), 18 s (for AIDS), 44 s (for PROTEIN), 126 s (for D&amp;D), and 165 s (for COLLAB). The results of the DGCNN are somewhat slower (the gap is 4.3 s for PTC, 7 s for AIDS, 13 s for PROTEIN, 26 s for D&amp;D, and 47 s for COLLAB) than the other graph convolutional approaches, probably because generating the receptive fields takes less time than sampling the kernel parameters on the GMM. The recent graph convolution approaches need to generate the receptive fields as the input of the CNN. In the CL of our approach, additional computation is required to sample the parameters on the GMM. However, the total running time of these additional computations is not significantly different and is of the same order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and conclusions</head><p>We have presented a DGCNN based on a GMM that is applicable to graph similarity matching. The main innovation of this model is that by sampling the corresponding convolutional kernel parameters from a mixed Gaussian distribution, the dynamic convolutional kernel is adapted to the size and order of the node neighborhood. Therefore, our model supports different scales of convolutional receptive fields, thereby avoiding the loss of graph information during the regularization of node neighborhoods. The key aspect of our model is the GMM-based DGCL, which performs convolutional learning on local node neighborhoods of any graph while achieving the back-propagation of graph convolution.</p><p>Graph convolutional parameters are combined into a neural network optimization process and optimized to a large degree. Finally, we perform graph classification and search experiments on standard graph datasets such as PTC, PROTEIN, COLLAB and AIDS. These experiments indicate that the DGCNN outperforms existing g-kernels and g-CNNs.</p><p>The main novelty of the DGCL is that our architecture changes the discrete parameter learning problem into a parameter sampling problem of the GMM. Therefore, the proposed approach does not rely on the format of the input data. Thus, the DGCL is also valid for feature learning on other irregular input data, such as text data and 3D shape data.  <ref type="figure">Figure 8</ref>: the common model of DGCL As shown in <ref type="figure">Fig. 8</ref>, we represent arbitrary data in matrix form. Then, the matrix serves as the input of the DGCL, and the output of the DGCL can serve as the input of a standard CL or a fully connected layer (FCL). Solid lines denote forward-propagation processes of networks and dotted lines represent back-propagation processes of networks. Therefore, DGCL can be combined with an arbitrary CNN to handle arbitrary regular and irregular data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>weights of the image ...(b) node neighborhood in the graph data Receptive fields of an image and the corresponding graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Preprocessing procedure of graph data illustration of a DGCNN based on a GMM Convolutional Process Comparison between the Standard Image and a Node Neighborhood of the Graph Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Retrieval precision on five graph datasets for DGCNN, graph kernel methods and recent graph convolution networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of Classification precision between four graph kernel and two graph CNN Methods on Multiple Graph Datasets 53±2.55 62.37±1.13 65.12±1.01 71.00±1.11 61.23±2.12 RW 57.26±1.30 58.37±2.21 68.11±2.02 68.10±0.11 65.20±3.21 GK 57.32±1.13 60.27±1.12 61.12±1.03 78.45±0.26 64.45±3.12 WL 56.97±2.01 59.97±1.01 62.23±1.03 77.95±0.70 61.25±1.72 PSCH [8] 59.43±3.14 60.10±2.72 72.10±1.72 74.58±2.85 62.32±2.45 LMFGCN [22] 61.32±1.21 59.21±2.32 67.12±2.25 73.13±1.13 66.13±2.01 SSGCN [13] 60.21±2.14 55.10±1.15 65.10±3.12 75.10±1.02 63.10± 2.12 DGCNN 65.43±3.14 65.10±1.82 75.10±2.72 77.21±0.85 68.34±3.135.3 Experimental results(1) Comparison of the Graph Classification ResultsTable 1shows the graph classification accuracy on each dataset for eight methods: SP, RW, GK, WL, PSCH, LMFGCN, and DGCNN. The DGCNN, the method presented here, demonstrated good accuracy on most datasets and attained its best results on the PTC, AIDS, COLLAB and PROTEIN datasets. Unlike</figDesc><table><row><cell>Dataset</cell><cell>PTC</cell><cell>AIDS</cell><cell>PROTEIN</cell><cell>D&amp;D</cell><cell>COLLAB</cell></row><row><cell>number of graphs</cell><cell>344</cell><cell>4395</cell><cell>600</cell><cell>1178</cell><cell>12000</cell></row><row><cell>Max</cell><cell>109</cell><cell>207</cell><cell>620</cell><cell>5748</cell><cell>4123</cell></row><row><cell>Avg</cell><cell>25.56</cell><cell>30.15</cell><cell>39.06</cell><cell>284.32</cell><cell>400</cell></row><row><cell>SP</cell><cell>58.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of running time on five graph datasets (in seconds)</figDesc><table><row><cell>Dataset</cell><cell cols="5">PTC AIDS PROTEIN D&amp;D COLLAB</cell></row><row><cell>WL [10]</cell><cell>30</cell><cell>65</cell><cell>143</cell><cell>609</cell><cell>245</cell></row><row><cell>PSCH [8]</cell><cell>6</cell><cell>10</cell><cell>31</cell><cell>154</cell><cell>235</cell></row><row><cell>LMFGCN [22]</cell><cell>14</cell><cell>23</cell><cell>41</cell><cell>72</cell><cell>62</cell></row><row><cell>SSGCN [13]</cell><cell>15</cell><cell>20</cell><cell>58</cell><cell>150</cell><cell>200</cell></row><row><cell>DGCNN</cell><cell>16</cell><cell>25</cell><cell>57</cell><cell>152</cell><cell>212</cell></row><row><cell>graph dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<title level="m">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Landim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Olla</surname></persName>
		</author>
		<title level="m">Fluctuations in Markov Processes</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularized background adaptation: a novel learning rate control scheme for gaussian mixture modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="822" to="836" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1506.01497</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dynamic filters in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<title level="m">Learning convolutional neural networks for graphs, International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gaertner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Learning Theory and Kernel Machines,Conference on Computational Learning Theory and Kernel Workshop</title>
		<meeting><address><addrLine>Washington, Dc, Usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-24" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Leman</surname></persName>
		</author>
		<imprint>
			<publisher>Nauchno-Technicheskaya Informatsia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="page" from="488" to="495" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR abs/1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Almost sure limit theorem for the maximum of stationary gaussian sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Endre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonchigdanzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics Probability Letters</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="203" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical evaluation of the predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Helma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1183" to="93" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinguishing enzymes from non-enzymes via support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machines in Bioinformatics Master Thesis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Atomnet: A deep convolutional neural network for bioactivity prediction in structure-based drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heifets</surname></persName>
		</author>
		<idno type="arXiv">CoRRabs/1510.02855.arXiv:1510.02855</idno>
		<ptr target="http://arxiv.org/abs/1510.02855" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
	<note>Shortest-path kernels on graphs</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3756" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
