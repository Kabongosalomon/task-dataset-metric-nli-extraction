<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A U-Net Based Discriminator for Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Sch√∂nfeld</surname></persName>
							<email>edgar.schoenfeld@bosch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>schiele@mpi-inf.mpg.com</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<email>anna.khoreva@bosch.com</email>
							<affiliation key="aff2">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A U-Net Based Discriminator for Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Among the major remaining challenges for generative adversarial networks (GANs) is the capacity to synthesize globally and locally coherent images with object shapes and textures indistinguishable from real images. To target this issue we propose an alternative U-Net based discriminator architecture, borrowing the insights from the segmentation literature. The proposed U-Net based architecture allows to provide detailed per-pixel feedback to the generator while maintaining the global coherence of synthesized images, by providing the global image feedback as well. Empowered by the per-pixel response of the discriminator, we further propose a per-pixel consistency regularization technique based on the CutMix data augmentation, encouraging the U-Net discriminator to focus more on semantic and structural changes between real and fake images. This improves the U-Net discriminator training, further enhancing the quality of generated samples. The novel discriminator improves over the state of the art in terms of the standard distribution and image quality metrics, enabling the generator to synthesize images with varying structure, appearance and levels of detail, maintaining global and local realism. Compared to the BigGAN baseline, we achieve an average improvement of 2.7 FID points across FFHQ, CelebA, and the newly introduced COCO-Animals dataset. The code is available at https: //github.com/boschresearch/unetgan.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The quality of synthetic images produced by generative adversarial networks (GANs) has seen tremendous improvement recently <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">20]</ref>. The progress is attributed to large-scale training <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5]</ref>, architectural modifications <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b26">27]</ref>, and improved training stability via the use of different regularization techniques <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51]</ref>. However, despite the recent advances, learning to synthesize images with global semantic coherence, long-range structure and the exactness of detail remains challenging.</p><p>One source of the problem lies potentially in the discrim- The synthetic image samples are obtained from a fixed noise vector at different training iterations. Brighter colors correspond to the discriminator confidence of pixel being real (and darker of being fake). Note that the U-Net discriminator provides very detailed and spatially coherent response to the generator, enabling it to further improve the image quality, e.g. the unnaturally large man's forehead is recognized as fake by the discriminator and is corrected by the generator throughout the training.</p><p>inator network. The discriminator aims to model the data distribution, acting as a loss function to provide the generator a learning signal to synthesize realistic image samples. The stronger the discriminator is, the better the generator has to become. In the current state-of-the-art GAN models, the discriminator being a classification network learns only a representation that allows to efficiently penalize the generator based on the most discriminative difference between real and synthetic images. Thus, it often focuses either on the global structure or local details. The problem amplifies as the discriminator has to learn in a non-stationary envi- ronment: the distribution of synthetic samples shifts as the generator constantly changes through training, and is prone to forgetting previous tasks <ref type="bibr" target="#b6">[7]</ref> (in the context of the discriminator training, learning semantics, structures, and textures can be considered different tasks). This discriminator is not incentivized to maintain a more powerful data representation, learning both global and local image differences. This often results in the generated images with discontinued and mottled local structures <ref type="bibr" target="#b26">[27]</ref> or images with incoherent geometric and structural patterns (e.g. asymmetric faces or animals with missing legs) <ref type="bibr" target="#b49">[50]</ref>.</p><p>To mitigate this problem, we propose an alternative discriminator architecture, which outputs simultaneously both global (over the whole image) and local (per-pixel) decision of the image belonging to either the real or fake class, see <ref type="figure" target="#fig_0">Figure 1</ref>. Motivated by the ideas from the segmentation literature, we re-design the discriminator to take a role of both a classifier and segmenter. We change the architecture of the discriminator network to a U-Net <ref type="bibr" target="#b38">[39]</ref>, where the encoder module performs per-image classification, as in the standard GAN setting, and the decoder module outputs perpixel class decision, providing spatially coherent feedback to the generator, see <ref type="figure" target="#fig_1">Figure 2</ref>. This architectural change leads to a stronger discriminator, which is encouraged to maintain a more powerful data representation, making the generator task of fooling the discriminator more challenging and thus improving the quality of generated samples (as also reflected in the generator and discriminator loss behavior in <ref type="figure" target="#fig_6">Figure 8</ref>). Note that we do not modify the generator in any way, and our work is orthogonal to the ongoing research on architectural changes of the generator <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b26">27]</ref>, divergence measures <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37]</ref>, and regularizations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>The proposed U-Net based discriminator allows to employ the recently introduced CutMix <ref type="bibr" target="#b46">[47]</ref> augmentation, which is shown to be effective for classification networks, for consistency regularization in the two-dimensional output space of the decoder. Inspired by <ref type="bibr" target="#b46">[47]</ref>, we cut and mix the patches from real and synthetic images together, where the ground truth label maps are spatially combined with respect to the real and fake patch class for the segmenter (U-Net decoder) and the class labels are set to fake for the classifier (U-Net encoder), as globally the CutMix image should be recognized as fake, see <ref type="figure">Figure 3</ref>. Empowered by per-pixel feedback of the U-Net discriminator, we further employ these CutMix images for consistency regularization, penalizing per-pixel inconsistent predictions of the discriminator under the CutMix transformations. This fosters the discriminator to focus more on semantic and structural changes between real and fake images and to attend less to domain-preserving perturbations. Moreover, it also helps to improve the localization ability of the decoder. Employing the proposed consistency regularization leads to a stronger generator, which pays more attention to local and global image realism. We call our model U-Net GAN.</p><p>We evaluate the proposed U-Net GAN model across several datasets using the state-of-the-art BigGAN model <ref type="bibr" target="#b4">[5]</ref> as a baseline and observe an improved quality of the generated samples in terms of the FID and IS metrics. For unconditional image synthesis on FFHQ <ref type="bibr" target="#b20">[20]</ref> at resolution 256 √ó 256, our U-Net GAN model improves 4 FID points over the BigGAN model, synthesizing high quality human faces (see <ref type="figure" target="#fig_2">Figure 4</ref>). On CelebA [29] at resolution 128√ó128 we achieve 1.6 point FID gain, yielding to the best of our knowledge the lowest known FID score of 2.95. For class-conditional image synthesis on the introduced COCO-Animals dataset <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">24]</ref> at resolution 128√ó128 we observe an improvement in FID from 16.37 to 13.73, synthesizing diverse images of different animal classes (see <ref type="figure" target="#fig_3">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Generative adversarial networks. GAN <ref type="bibr" target="#b13">[14]</ref> and its conditional variant <ref type="bibr" target="#b32">[33]</ref> have recently demonstrated impressive results on different computer vision tasks, including image synthesis <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10]</ref>. Plenty of efforts have been made to improve the training and performance of GANs, from reformulation of the objective function <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b36">37]</ref>, integration of different regularization techniques <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48]</ref> and architectural changes <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>. To enhance the quality of generated samples, <ref type="bibr" target="#b37">[38]</ref> introduced the DCGAN architecture that employs strided and transposed convolutions. In SAGAN <ref type="bibr" target="#b49">[50]</ref> the self-attention block was added to improve the network ability to model global structure. PG-GAN <ref type="bibr" target="#b19">[19]</ref> proposed to grow both the generator and discriminator networks to increase the resolution of generated images. Other lines of work focused mainly on improving the discriminator by exploiting multiple <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref> and multi-resolution <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b41">42]</ref> discriminators, using spatial feedback of the discriminator <ref type="bibr" target="#b17">[17]</ref>, an auto-encoder architecture with the reconstructionbased feedback to the generator <ref type="bibr" target="#b51">[52]</ref> or self-supervision to avoid catastrophic forgetting <ref type="bibr" target="#b6">[7]</ref>. Most recently, the attention has been switched back to the generator network. Style-GAN <ref type="bibr" target="#b20">[20]</ref> proposed to alter the generator architecture by injecting latent codes to each convolution layer, thus allowing more control over the image synthesis process. COCO-GAN <ref type="bibr" target="#b26">[27]</ref> integrated the conditional coordination mechanism into the generator, making image synthesis highly parallelizable. In this paper, we propose to alter the discriminator network to a U-Net based architecture, empowering the discriminator to capture better both global and local structures, enabled by per-pixel discriminator feedback. Local discriminator feedback is also commonly applied through PatchGAN discriminators <ref type="bibr" target="#b18">[18]</ref>. Our U-Net GAN extends this idea to dense prediction over the whole image plane, with visual information being integrated over up-and downsampling pathways and through the encoder-decoder skip connections, without trading off local over global realism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mix&amp;Cut regularizations.</head><p>Recently, a few simple yet effective regularization techniques have been proposed, which are based on augmenting the training data by creating synthetic images via mixing or/and cutting samples from different classes. In MixUp <ref type="bibr" target="#b48">[49]</ref> the input images and their target labels are interpolated using the same randomly chosen factor. <ref type="bibr" target="#b42">[43]</ref> extends <ref type="bibr" target="#b48">[49]</ref> by performing interpolation not only in the input layer but also in the intermediate layers. CutOut <ref type="bibr">[9]</ref> augments an image by masking a rectangular region to zero. Differently, CutMix <ref type="bibr" target="#b46">[47]</ref> augments training data by creating synthetic images via cutting and pasting patches from image samples of different classes, marrying the best aspects of MixUp and CutOut. Other works employ the Mix&amp;Cut approaches for consistency regularization <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b50">51]</ref>, i.e. penalizing the classification network sensitivity to samples generated via MixUp or CutOut <ref type="bibr" target="#b48">[49,</ref><ref type="bibr">9]</ref>. In our work, we propose the consistency regularization under the CutMix transformation in the pixel output space of our U-Net discriminator. This helps to improve its localization quality and induce it to attend to nondiscriminative differences between real and fake regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">U-Net GAN Model</head><p>A "vanilla" GAN consists of two networks: a generator G and a discriminator D, trained by minimizing the following competing objectives in an alternating manner:</p><formula xml:id="formula_0">L D = ‚àíE x [log D(x)] ‚àí E z [log(1 ‚àí D(G(z)))], L G = ‚àíE z [log D(G(z))] 1 .<label>(1)</label></formula><p>G aims to map a latent variable z ‚àº p(z) sampled from a prior distribution to a realistic-looking image, while D aims to distinguish between real x and generated G(z) images. Ordinarily, G and D are modeled as a decoder and an encoder convolutional network, respectively. While there are many variations of the GAN objective function and its network architectures <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b29">30]</ref>, in this paper we focus on improving the discriminator network. In Section 3.1, we propose to alter the D architecture from a standard classification network to an encoder-decoder network -U-Net <ref type="bibr" target="#b38">[39]</ref>, leaving the underlying basic architecture of D -the encoder part -untouched. The proposed discriminator allows to maintain both global and local data representation, providing more informative feedback to the generator. Empowered by local per-pixel feedback of the U-Net decoder module, in Section 3.2 we further propose a consistency regularization technique, penalizing per-pixel inconsistent predictions of the discriminator under the CutMix transformations <ref type="bibr" target="#b46">[47]</ref> of real and fake images. This helps to improve the localization quality of the U-Net discriminator and induce it to attend more to semantic and structural changes between real and fake samples. We call our model U-Net GAN. Note that our method is compatible with most GAN models as it does not modify the generator in any way and leaves the original GAN objective intact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">U-Net Based Discriminator</head><p>Encoder-decoder networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref> constitute a powerful method for dense prediction. U-Nets <ref type="bibr" target="#b38">[39]</ref> in particular have demonstrated state-of-art performance in many complex image segmentation tasks. In these methods, similarly to image classification networks, the encoder progressively downsamples the input, capturing the global image context. The decoder performs progressive upsampling, matching the output resolution to the input one and thus enabling precise localization. Skip connections route data between the matching resolutions of the two modules, improving further the ability of the network to accurately segment fine details.</p><p>Analogously, in this work, we propose to extend a discriminator to form a U-Net, by reusing building blocks of the original discriminator classification network as an encoder part and building blocks of the generator network as the decoder part. In other words, the discriminator now consists of the original downsampling network and a new upsampling network. The two modules are connected via a bottleneck, as well as skip-connections that copy and concatenate feature maps from the encoder and the decoder modules, following <ref type="bibr" target="#b38">[39]</ref>. We will refer to this discriminator as D U . While the original D(x) classifies the input image x into being real and fake, the U-Net discriminator D U (x) additionally performs this classification on a per-pixel basis, segmenting image x into real and fake regions, along with the original image classification of x from the encoder, 3 see <ref type="figure" target="#fig_1">Figure 2</ref>. This enables the discriminator to learn both global and local differences between real and fake images.</p><p>Hereafter, we refer to the original encoder module of the discriminator as D U enc and to the introduced decoder module as D U dec . The new discriminator loss is now can be computed by taking the decisions from both D U enc and D U dec :</p><formula xml:id="formula_1">L D U = L D U enc + L D U dec ,<label>(2)</label></formula><p>where similarly to Eq. 1 the loss for the encoder L D U enc is computed from the scalar output of D U enc :</p><formula xml:id="formula_2">L D U enc =‚àíE x [logD U enc (x)]‚àíE z [log(1‚àíD U enc (G(z)))],<label>(3)</label></formula><p>and the loss for the decoder L D U enc is computed as the mean decision over all pixels:</p><formula xml:id="formula_3">L D U dec = ‚àíE x i,j log[D U dec (x)] i,j ‚àí E z i,j log(1 ‚àí [D U dec (G(z))] i,j ) . (4) Here, [D U dec (x)] i,j and [D U dec (G(z))] i,j</formula><p>refer to the discriminator decision at pixel (i, j). These per-pixel outputs of D U dec are derived based on global information from highlevel features, enabled through the process of upsampling from the bottleneck, as well as more local information from low-level features, mediated by the skip connections from the intermediate layers of the encoder network.</p><p>Correspondingly, the generator objective becomes:</p><formula xml:id="formula_4">L G = ‚àíE z log D U enc (G(z)) + i,j log[D U dec (G(z))] i,j ,<label>(5)</label></formula><p>encouraging the generator to focus on both global structures and local details while synthesizing images in order to fool the more powerful discriminator D U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Consistency Regularization</head><p>Here we present the consistency regularization technique for the U-Net based discriminator introduced in the previous section. The per-pixel decision of the well-trained D U discriminator should be equivariant under any class-domainaltering transformations of images. However, this property is not explicitly guaranteed. To enable it, the discriminator should be regularized to focus more on semantic and structural changes between real and fake samples and to pay less attention to arbitrary class-domain-preserving perturbations. Therefore, we propose the consistency regularization of the D U discriminator, explicitly encouraging the decoder module D U dec to output equivariant predictions under the CutMix transformations <ref type="bibr" target="#b46">[47]</ref> of real and fake samples. The  <ref type="figure">Figure 3</ref>: Visualization of the CutMix augmentation and the predictions of the U-Net discriminator on CutMix images. 1st row: real and fake samples. 2nd&amp;3rd rows: sampled real/fake CutMix ratio r and corresponding binary masks M (color code: white for real, black for fake). 4th row: generated CutMix images from real and fake samples. 5th&amp;6th row: the corresponding real/fake segmentation maps of D U with its predicted classification scores.</p><p>CutMix augmentation creates synthetic images via cutting and pasting patches from images of different classes. We choose CutMix among other Mix&amp;Cut strategies (cf. Section 2) as it does not alter the real and fake image patches used for mixing, in contrast to <ref type="bibr" target="#b48">[49]</ref>, preserving their original class domain, and provides a large variety of possible outputs. We visualize the CutMix augmentation strategy and the D U predictions in <ref type="figure">Figure 3</ref>. Following <ref type="bibr" target="#b46">[47]</ref>, we synthesize a new training sampl·∫Ω x for the discriminator D U by mixing x and G(z) ‚àà R W √óH√óC with the mask M:</p><formula xml:id="formula_5">x = mix(x, G(z), M), mix(x, G(z), M) = M x + (1 ‚àí M) G(z),<label>(6)</label></formula><p>where M ‚àà {0, 1} W √óH is the binary mask indicating if the pixel (i, j) comes from the real (M i,j = 1) or fake (M i,j = 0) image, 1 is a binary mask filled with ones, and is an element-wise multiplication. In contrast to <ref type="bibr" target="#b46">[47]</ref>, the class label c ‚àà {0, 1} for the new CutMix imagex is set to be fake, i.e. c = 0. Globally the mixed synthetic image should be recognized as fake by the encoder D U enc , otherwise the generator can learn to introduce the CutMix augmentation into generated samples, causing undesirable artifacts. Note that for the synthetic samplex, c = 0 and M are the ground truth for the encoder and decoder modules of the discriminator D U , respectively.</p><p>Given the CutMix operation in Eq. 6, we train the discriminator to provide consistent per-pixel predictions, i.e.</p><formula xml:id="formula_6">D U dec mix(x, G(z), M) ‚âà mix D U dec (x), D U dec (G(z))</formula><p>, M , by introducing the consistency regularization loss term in the discriminator objective:</p><formula xml:id="formula_7">L cons D U dec = D U dec mix(x, G(z), M) ‚àí mix D U dec (x), D U dec (G(z)), M 2 ,<label>(7)</label></formula><p>where denotes ¬∑ the L 2 norm. This consistency loss is then taken between the per-pixel output of D U dec on the Cut-Mix image and the CutMix between outputs of the D U dec on real and fake images, penalizing the discriminator for inconsistent predictions.</p><p>We add the loss term in Eq. 7 to the discriminator objective in Eq. 2 with a weighting hyper-parameter Œª:</p><formula xml:id="formula_8">L D U = L D U enc + L D U dec + ŒªL cons D U dec ..<label>(8)</label></formula><p>The generator objective L G remains unchanged, see Eq. 5.</p><p>In addition to the proposed consistency regularization, we also use CutMix samples for training both the encoder and decoder modules of D U . Note that for the U-Net GAN we use the non-saturating GAN objective formulation <ref type="bibr" target="#b13">[14]</ref>. However, the introduced consistency regularization as well as the U-Net architecture of the discriminator can be combined with any other adversarial losses of the generator and discriminator <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b36">37</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>Here we discuss implementation details of the U-Net GAN model proposed in Section 3.1 and 3.2. U-Net based discriminator. We build upon the recent state-of-the-art BigGAN model <ref type="bibr" target="#b4">[5]</ref>, and extend its discriminator with our proposed changes. We adopt the BigGAN generator and discriminator architectures for the 256 √ó 256 (and 128 √ó 128) resolution with a channel multiplier ch = 64, as described in detail in <ref type="bibr" target="#b4">[5]</ref>. The original BigGAN discriminator downsamples the input image to a feature map of dimensions 16ch √ó 4 √ó 4, on which global sum pooling is applied to derive a 16ch dimensional feature vector that is classified into real or fake. In order to turn the discriminator into a U-Net, we copy the generator architecture and append it to the 4 √ó 4 output of the discriminator. In effect, the features are successively upsampled via ResNet blocks until the original image resolution (H √ó W ) is reached. To make the U-Net complete, the input to every decoder ResNet block is concatenated with the output features of the encoder blocks that share the same intermediate resolution. In this way, high-level and low-level information are effectively integrated on the way to the output feature map. Hereby, the decoder architecture is almost identical to the generator, with the exception of that we change the number of channels of the final output from 3 to ch, append a final block of 1√ó1 convolutions to produce the 1√óH √óW output map, and do not use class-conditional BatchNorm <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> in the decoder, nor the encoder. Similarly to <ref type="bibr" target="#b4">[5]</ref>, we provide class information to D U with projection <ref type="bibr" target="#b34">[35]</ref> to the ch-dimensional channel features of the U-Net encoder and decoder output. In contrast to <ref type="bibr" target="#b4">[5]</ref> and in alignment with <ref type="bibr" target="#b5">[6]</ref>, we find it beneficial not to use a hierarchical latent space, but to directly feed the same input vector z to BatchNorm at every layer in the generator. Lastly, we also remove the self-attention layer in both encoder and decoder, as in our experiments they did not contribute to the performance but led to memory overhead. While the original BigGAN is a class-conditional model, we additionally devise an unconditional version for our experiments. For the unconditional model, we replace class-conditional BatchNorm with selfmodulation <ref type="bibr" target="#b5">[6]</ref>, where the BatchNorm parameters are conditioned only on the latent vector z, and do not use the class projection of <ref type="bibr" target="#b34">[35]</ref> in the discriminator.</p><p>All these modifications leave us with a two-headed discriminator. While the decoder head is already sufficient to train the network, we find it beneficial to compute the GAN loss at both heads with equal weight. Analogously to Big-GAN, we keep the hinge loss <ref type="bibr" target="#b49">[50]</ref> in all basic U-Net models, while the models that also employ the consistency regularization in the decoder output space benefit from using the non-saturating loss <ref type="bibr" target="#b13">[14]</ref>. Our implementation builds on top of the original BigGAN PyTorch implementation 2 .</p><p>Consistency regularization. For each training iteration a mini-batch of CutMix images (x, c = 0, M) is created with probability p mix . This probability is increased linearly from 0 to 0.5 between the first n epochs in order to give the generator time to learn how to synthesize more real looking samples and not to give the discriminator too much power from the start. CutMix images are created from the existing real and fake images in the mini-batch using binary masks M. For sampling M, we use the original CutMix implementation <ref type="bibr" target="#b2">3</ref> : first sampling the combination ratio r between the real and generated images from the uniform distribution (0, 1) and then uniformly sample the bounding box coordinates for the cropping regions of x and G(z) to preserve the r ratio, i.e. r = |M| W * H (see <ref type="figure">Figure 3</ref>). Binary masks M also denote the target for the decoder D U dec , while we use fake, i.e c = 0, as the target for the encoder D U enc . We set Œª = 1.0 as it showed empirically to be a good choice. Note that the consistency regularization does not impose much overhead during training. Extra computational cost comes only from  feeding additional CutMix images through the discriminator while updating its parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We consider three datasets: FFHQ <ref type="bibr" target="#b20">[20]</ref>, CelebA [29] and the subset of the COCO <ref type="bibr" target="#b27">[28]</ref> and Open-Images <ref type="bibr" target="#b24">[24]</ref> images containing animal classes, which we will further on refer to as COCO-Animals. We use FFHQ and CelebA for unconditional image synthesis and COCO-Animals for class-conditional image synthesis, where the class label is used. We experiment with 256√ó256 resolution for FFHQ and 128 √ó 128 for CelebA and COCO-Animals.</p><p>CelebA is a human face dataset of 200k images, featuring ‚àº 10k different celebrities with a variety of facial poses and expressions. Similarly, FFHQ is a more recent dataset of human faces, consisting of 70k high-quality images with higher variation in terms of age, ethnicity, accessories, and viewpoints. The proposed COCO-Animals dataset consists of ‚àº 38k training images belonging to 10 animal classes, where we choose COCO and OpenImages (using the human verified subset with mask annotations) samples in the categories bird, cat, dog, horse, cow, sheep, giraffe, zebra, elephant, and monkey. With its relatively small size and imbalanced number of images per class as well as due to its variation in poses, shapes, number of objects, and backgrounds, COCO-Animals presents a challenging task for class-conditional image synthesis. We choose to create this dataset in order to perform conditional image generation in the mid-to high-resolution regime, with a reasonable computational budget and feasible training time. Other datasets in this order of size either have too few examples per class (e.g. AwA <ref type="bibr" target="#b45">[46]</ref>) or too little inter-and intra-class variability. In contrast, the intra-class variability of COCO-Animals is very high for certain classes, e.g. bird and monkey, which span many subspecies. For more details, we refer to Section C in the supplementary material. Evaluation metrics. For quantitative evaluation we use the Fr√©chet Inception distance (FID) <ref type="bibr" target="#b16">[16]</ref>  We report the best and median FID score across 5 runs and its corresponding IS, see Section 4.2 for discussion.  and additionally consider the Inception score (IS) <ref type="bibr" target="#b40">[41]</ref>. Between the two, FID is a more comprehensive metric, which has been shown to be more consistent with human evaluation in assessing the realism and variation of the generated images <ref type="bibr" target="#b16">[16]</ref>, while IS is limited by what the Inception classifier can recognise, which is directly linked to its training data <ref type="bibr" target="#b2">[3]</ref>. If one learns to generate something not present in the classifier's training data (e.g. human faces) then IS can still be low despite generating high quality images since that image does not get classified as a distinct class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFHQ COCO-Animals</head><p>In all our experiments, FID and IS are computed using 50k synthetic images, following <ref type="bibr" target="#b19">[19]</ref>. By default all reported numbers correspond to the best or median FID of five independent runs achieved with 400k training iterations for FFHQ and COCO-Animals, and 800k training iterations for CelebA. For evaluation, we employ moving averages of the generator weights following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">19]</ref>, with a decay of 0.9999. Note that we do not use any truncation tricks or rejection sampling for image generation.  Training details. We adopt the original training parameters of <ref type="bibr" target="#b4">[5]</ref>. In particular, we use a uniformly distributed noise vector z ‚àà [‚àí1, 1] 140 as input to the generator, and the Adam optimizer <ref type="bibr" target="#b22">[22]</ref> with learning rates of 1e-4 and 5e-4 for G and D U . The number of warmup epochs n for consistency regularization is chosen to be 200 for COCO-Animals and 20 for FFHQ and CelebA. In contrast to <ref type="bibr" target="#b4">[5]</ref>, we operate with considerably smaller mini-batch sizes: 20 for FFHQ, 50 for CelebA and 80 for COCO-Animals. See Section E in the supplementary material for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We first test our proposed U-Net discriminator in two settings: unconditional image synthesis on FFHQ and classconditional image synthesis on COCO-Animals, using the BigGAN model <ref type="bibr" target="#b4">[5]</ref> as a baseline for comparison. We report our key results in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_5">Figure 6</ref>.</p><p>In the unconditional case, our model achieves the FID score of 7.48, which is an improvement of 4.0 FID points over the canonical BigGAN discriminator (see <ref type="table">Table 1</ref>). In addition, the new U-Net discriminator also improves over the baseline in terms of the IS metric (3.97 vs. 4.46). The same effect is observed for the conditional image generation setting. Here, our U-Net GAN achieves an FID of 13.73, improving 2.64 points over BigGAN, as well as increases the IS score from 11.77 to 12.29. <ref type="figure" target="#fig_5">Figure 6</ref> visualizes the mean FID behaviour over the training across 5 independent runs. From <ref type="figure" target="#fig_5">Figure 6</ref> it is evident that the FID score drops for both models at the similar rate, with a constant offset for the U-Net GAN model, as well as the smaller standard deviation of FID. These results showcase the high potential of the new U-Net based discriminator. For a detailed comparison of the FID mean, median and standard deviation across 5 runs we refer to <ref type="table" target="#tab_4">Table S2</ref> in the supplementary material.</p><p>Qualitative results on FFHQ and COCO-Animals are shown in <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="figure" target="#fig_3">Figure 5</ref>. <ref type="figure" target="#fig_2">Figure 4</ref> displays human faces generated by U-Net GAN through linear interpolation in the latent space between two synthetic samples. We observe that the interpolations are semantically smooth between faces, i.e. an open mouth gradually becomes a closed mouth, hair progressively grows in length, beards or glasses smoothly fade or appear, and hair color changes seamlessly.  Furthermore, we notice that on several occasions men appear with pink beards. As FFHQ contains a fair share of people with pink hair, we suspect that our generator extrapolates hair color to beards, enabled by the global and local D U feedback during the training. <ref type="figure" target="#fig_3">Figure 5</ref> shows generated samples on COCO-Animals. We observe diverse images of high quality. We further notice that employing the classconditional projection (as used in BigGAN) in the pixel output space of the decoder does not introduce class leakage or influence the class separation in any other way. These observations confirm that our U-Net GAN is effective in both unconditional and class-conditional image generation. Ablation Study.</p><p>In <ref type="table" target="#tab_4">Table 2</ref> we next analyze the individual effect of each of the proposed components of the U-Net GAN model (see Section 3 for details) to the baseline architecture of BigGAN on the FFHQ and COCO-Animals datasets, comparing the median FID scores. Note that each of these individual components builds on each other. As shown in <ref type="table" target="#tab_4">Table 2</ref>, employing the U-Net architecture for the discriminator alone improves the median FID score from 12.42 to 10.86 for FFHQ and 16.55 to 15.86 for COCO-Animals. Adding the CutMix augmentation improves upon these scores even further, achieving FID of 10.30 for FFHQ and 14.95 for COCO-Animals. Note that we observe a similar improvement if we employ the CutMix augmentation during the BigGAN training as well. Employing the proposed consistency regularization in the segmenter D U dec output space on the CutMix images enables us to get the most out of the CutMix augmentation as well as allows to leverage better the per-pixel feedback of the U-Net discriminator, without imposing much computational or memory costs. In effect, the median FID score drops to 7.63 for FFHQ and to 13.87 for COCO-Animals. Overall, we observe that each proposed component of the U-Net GAN model leads to improved performance in terms of FID. Comparison with state of the art.  <ref type="figure">Figure 7</ref>: Visualization of the predictions of the encoder D U enc and decoder D U dec modules during training, within a batch of 50 generated samples. For visualization purposes, the D U dec score is averaged over all pixels in the output. Note that quite often decisions of D U enc and D U dec are not coherent with each other. As judged by the U-Net discriminator, samples in the upper left consist of locally plausible patterns, while not being globally coherent (example in orange), whereas samples in the lower right look globally coherent but have local inconsistencies (example in purple: giraffe with too many legs and vague background).</p><p>resentative of just one of the two well known state-of-the art GAN families, led by BigGAN and StyleGAN, and their respective further improvements <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b21">21]</ref>. While in this paper we base our framework on BigGAN, it would be interesting to also explore the application of the U-Net based discriminator for the StyleGAN family.</p><p>Discriminator response visualization. Experimentally we observe that D U enc and D U dec often assign different real/fake scores per sample. <ref type="figure">Figure 7</ref> visualizes the per-sample predictions for a complete training batch. Here, the decoder score is computed as the average per-pixel prediction. The scores correlate with each other but have a high variance. Points in the upper left quadrant correspond to samples that are assigned a high probability of being real by the decoder, but a low probability by the encoder. This implies realism on a local level, but not necessarily on a global one. Similarly, the lower right quadrant represents samples that are identified as realistic by the encoder, but contain unrealistic patches which cause a low decoder score. The fact that the encoder and decoder predictions are not tightly coupled further implies that these two components are complementary. In other words, the generator receives more pronounced feedback by the proposed U-Net discriminator than it would get from a standard GAN discriminator.</p><p>Characterizing the training dynamics. Both BigGAN and U-Net GAN experience similar stability issues, with ‚àº 60% of all runs being successful. For U-Net GAN, training collapse occurs generally much earlier (‚àº 30k iterations) than for BigGAN (&gt; 200k iterations, as also reported in <ref type="bibr" target="#b4">[5]</ref>), allowing to discard failed runs earlier. Among successful runs for both models, we observe a lower standard deviation in the achieved FID scores, compared to the Big-GAN baseline (see <ref type="table" target="#tab_4">Table S2</ref> in the supplementary material). <ref type="figure" target="#fig_6">Figure 8</ref> depicts the evolution of the generator and discriminator losses (green and blue, respectively) for U-Net GAN and BigGAN over training. For U-Net GAN, the generator and discriminator losses are additionally split into the loss components of the U-Net encoder D U enc and decoder D U dec . The U-Net GAN discriminator loss decays slowly, while the BigGAN discriminator loss approaches zero rather quickly, which prevents further learning from the generator. This explains the FID gains of U-Net GAN and shows its potential to improve with longer training. The generator and discriminator loss parts from encoder (image-level) and decoder (pixel-level) show similar trends, i.e. we observe the same decay for D U enc and D U dec losses but with different scales. This is expected as D U enc can easily classify image as belonging to the real or fake class just by looking at one distinctive trait, while to achieve the same scale D U dec needs to make a uniform real or fake decision on all image pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an alternative U-Net based architecture for the discriminator, which allows to provide both global and local feedback to the generator. In addition, we introduce a consistency regularization technique for the U-Net discriminator based on the CutMix data augmentation. We show that all the proposed changes result in a stronger discriminator, enabling the generator to synthesize images with varying levels of detail, maintaining global and local realism. We demonstrate the improvement over the state-of-the-art BigGAN model <ref type="bibr" target="#b4">[5]</ref> in terms of the FID score on three different datasets. <ref type="bibr">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material complements the presentation of U-Net GAN in the main paper with the following:</p><p>‚Ä¢ Additional quantitative results in Section A;</p><p>‚Ä¢ Exemplar synthetic images on FFHQ in Section B and on COCO-Animals in Section C.</p><p>‚Ä¢ Network architectures and hyperparameter settings in Section E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Evaluations</head><p>Here we provide more detailed evaluation of the results presented in the main paper. In <ref type="table" target="#tab_9">Table S1</ref> we report the inception metrics for images generated on FFHQ <ref type="bibr" target="#b20">[20]</ref>, COCO-Animals <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">24]</ref> and CelebA [29] at resolution 256 √ó 256, 128 √ó 128, and 128 √ó 128, respectively. In particular, we report the Fr√©chet Inception distance (FID) <ref type="bibr" target="#b16">[16]</ref> and the Inception score (IS) <ref type="bibr" target="#b40">[41]</ref> computed by both the Py-Torch 5 and TensorFlow 6 implementations. Note that the difference between two implementations lies in using either the TensorFlow or the PyTorch in-built inception network to calculate IS and FID, resulting in slightly different scores. In all experiments, FID and IS are computed using 50k synthetic images, following <ref type="bibr" target="#b19">[19]</ref>. By default all reported numbers correspond to the best FID achieved with 400k training iterations for FFHQ and COCO-Animals, and 800k iterations for CelebA, using the PyTorch implementation.</p><p>In the unconditional case, on FFHQ, our model achieves FID of 7.48 <ref type="bibr">(8.88</ref> in TensorFlow), which is an improvement of 4.0 (6.04 in TensorFlow) FID points over the BigGAN discriminator <ref type="bibr" target="#b4">[5]</ref>. The same effect is observed for the conditional image generation setting on COCO-Animals. Here, our U-Net GAN achieves FID of 13.73 (13.96 in Tensor-Flow), improving 2.64 (2.46 in TensorFlow) points over BigGAN. To compare with other state-of-the-art models we additionally evaluate U-Net GAN on CelebA for unconditional image synthesis. Our U-Net GAN achieves 2.95 FID (in TensorFlow), outperforming COCO-GAN <ref type="bibr" target="#b26">[27]</ref>, PG-GAN <ref type="bibr" target="#b19">[19]</ref>, and the BigGAN baseline <ref type="bibr" target="#b4">[5]</ref>. <ref type="table" target="#tab_4">Table S2</ref> shows that U-Net GAN does not only outperform the BigGAN baseline in terms of the best recorded FID, but also with respect to the mean, median and standard deviation computed over 5 independent runs. Note the strong drop in standard deviation from 0.24 to 0.11 on COCO-Animals and from 0.16 to 0.04 on CelebA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results on FFHQ</head><p>Here we present more qualitative results of U-Net GAN on FFHQ <ref type="bibr" target="#b20">[20]</ref>. We use FFHQ for unconditional image synthesis and generate images with a resolution of 256 √ó 256.   <ref type="table" target="#tab_4">Table S2</ref>: Best, median, mean and std of FID values across 5 runs.</p><p>Generated FFHQ samples <ref type="figure" target="#fig_0">Figure S1</ref> shows samples of human faces generated by U-Net GAN on FFHQ. We observe diverse images of high quality, maintaining local and global realism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-pixel U-Net discriminator feedback</head><p>In <ref type="figure" target="#fig_1">Figure S2</ref> we visualize synthetic images and their corresponding per-pixel feedback of the U-Net discriminator. Note that the U-Net discriminator provides a very detailed and spatially coherent response, which enables the generator to further improve the image quality.</p><p>Interpolations in the latent space <ref type="figure">Figure S3</ref> displays human faces generated by U-Net GAN through linear interpolation in the latent space between two synthetic samples. We observe that the interpolations are semantically smooth between faces, e.g. an open mouth gradually becomes a closed mouth, hair progressively grows or gets shorter in length, beards or glasses smoothly fade or appear, and hair color changes seamlessly.</p><p>Figure S1: Images generated by U-Net GAN trained on FFHQ with resolution 256 √ó 256. <ref type="figure" target="#fig_1">Figure S2</ref>: Samples generated by U-Net GAN and the corresponding real-fake predictions of the U-Net decoder. Brighter colors correspond to the discriminator confidence of pixel being real (and darker of being fake). <ref type="figure">Figure S3</ref>: Images generated with U-Net GAN on FFHQ with resolution 256 √ó 256 when interpolating in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison between BigGAN and U-Net GAN</head><p>In <ref type="figure" target="#fig_2">Figure S4</ref> we present a qualitative comparison of uncurated images generated with the unconditional BigGAN model <ref type="bibr" target="#b4">[5]</ref> and our U-Net GAN. Note that the images generated by U-Net GAN exhibit finer details and maintain better local realism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CutMix images and U-Net discriminator predictions</head><p>In <ref type="figure" target="#fig_3">Figure S5</ref> we show more examples of the CutMix images and the corresponding U-Net based discriminator D U predictions. Note that in many cases, the decoder output for fake image patches is darker than for real image ones. However, the predicted intensity for an identical local patch can change for different mixing scenarios. This indicates that the U-Net discriminator takes contextual information into account for local decisions.  <ref type="figure" target="#fig_3">Figure S5</ref>: Visualization of the CutMix augmentation and the predictions of the U-Net discriminator on CutMix images. 1st row: real and fake samples. 2nd&amp;3rd rows: sampled real/fake CutMix ratio r and corresponding binary masks M (color code: white for real, black for fake). 4th row: generated CutMix images from real and fake samples. 5th&amp;6th row: the corresponding real/fake segmentation maps of the U-Net GAN decoder D U dec with the corresponding predicted classification scores by the encoder D U enc below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results on COCO-Animals</head><p>Here we present more qualitative results of U-Net GAN on COCO-Animals <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b24">24]</ref>. We use COCO-Animals for class conditional image synthesis and generate images with the resolution of 128 √ó 128.</p><p>Generated COCO-Animals samples <ref type="figure" target="#fig_5">Figure S6</ref> shows generated samples of different classes on COCO-Animals. We observe images of good quality and high intra-class variation. We further notice that employing the class-conditional projection (as used in BigGAN) in the pixel output space of the decoder does not introduce class leakage or influence the class separation in any other way. These observations further confirm that our U-Net GAN is effective in class-conditional image generation as well.</p><p>Per-pixel U-Net discriminator feedback <ref type="figure" target="#fig_8">Figure S7</ref> shows generated examples and the corresponding per-pixel predictions of the U-Net discriminator. We observe that the resulting maps often tend to exhibit a bias towards objects. <ref type="figure" target="#fig_6">Figure S8</ref> displays images generated on COCO-Animals by U-Net GAN through linear interpolation in the latent space between two synthetic samples. We observe that the interpolations are semantically smooth between different classes of animals, e.g. background seamlessly changes between two scenes, number of instances gradually increases or decreases, shape and color of objects smoothly changes from left to right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpolations in the latent space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Details on the COCO-Animals Dataset</head><p>COCO-Animals is a medium-sized (‚àº 38k) dataset composed of 10 animal classes, and is intended for experiments that demand a high-resolution equivalent for CIFAR10. The categories are bird, cat, dog, horse, cow, sheep, giraffe, zebra, elephant, and monkey. The images are taken from COCO <ref type="bibr" target="#b27">[28]</ref> and the OpenImages <ref type="bibr" target="#b24">[24]</ref> subset that provides semantic label maps and binary mask and is also human-verified. The two datasets have a great overlap in animal classes. We take all images from COCO and the aforementioned OpenImages split in the categories horse, cow, sheep, giraffe, zebra and elephant. The monkey images are taken over directly from OpenImages, since this category contained more training samples than the next biggest COCO animal class bear. The class bear and monkey are not shared between COCO and OpenImages. Lastly, the categories bird, cat and dog contained vastly more samples than all other categories. For this reason, we took over only a subset of the total of all images in these categories. These samples were picked from OpenImages only, for their better visual quality. To ensure good quality of the picked examples, we used the provided bounding boxes to filter out images in which the animal of interest is either too small or too big (&gt; 80%, &lt; 30% of the image area for cats, &gt; 70%, &lt; 50% for birds and dogs). The thresholds were chose such that the number of appropriate images is approximately equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Architectures and Training Details</head><p>Architecture details of the BigGAN model <ref type="bibr" target="#b4">[5]</ref> and our U-Net discriminator are summarized in <ref type="table" target="#tab_6">Table S3</ref> and <ref type="table">Table  S4</ref>. From these tables it is easy to see that the encoder and decoder of the U-Net discriminator follow the original Big-GAN discriminator and generator setups, respectively. One difference is that the number of input channels in the U-Net decoder is doubled, since encoder features are concatenated to the input features. <ref type="table">Table S4</ref> presents two U-Net discriminator networks: a class-conditional discriminator for image resolution 128 √ó 128, and an unconditional discriminator for resolution 256 √ó 256. The decoder does not have 3 output channels (like the BigGAN generator that it is copied from), but ch = 64 channels, resulting in a feature map h of size 64 √ó 128 √ó 128, to which a 1 √ó 1 convolution is applied to reduce the number of channels to 1. In the classconditional architecture, a learned class-embedding is multiplied with the aforementioned 64-dimensional output h at every spatial position, and summed along the channel dimension (corresponding to the inner product). The resulting map of size 1 √ó 128 √ó 128 is added to the output, leaving us with 128 √ó 128 logits.</p><p>We follow <ref type="bibr" target="#b4">[5]</ref> for setting up the hyperparameters for training U-Net GAN, which are summarized in <ref type="table">Table S5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter</head><p>Value Optimizer</p><p>Adam (Œ≤ 1 = 0, Œ≤ 2 = 0.999) G's learning rate 1e-4 (256), 5e-5 (128) D's learning rate 5e-4 (256), 2e-4 (128) Batch size 20 (256), 80 (128) Weight Initialization Orthogonal <ref type="table">Table S5</ref>: Hyperparameters of U-Net GAN Regarding the difference between class-conditional and unconditional image generation, it is worth noting that the CutMix regularization is applied only to samples within the same class. In other words, real and generated samples are mixed only within the class (e.g. real and fake zebras, but not real zebras with fake elephants).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Images produced throughout the training by our U-Net GAN model (top row) and their corresponding perpixel feedback of the U-Net discriminator (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>U-Net GAN. The proposed U-Net discriminator classifies the input images on a global and local per-pixel level. Due to the skip-connections between the encoder and the decoder (dashed line), the channels in the output layer contain both high-and low-level information. Brighter colors in the decoder output correspond to the discriminator confidence of pixel being real (and darker of being fake).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Images generated with U-Net GAN trained on FFHQ with resolution 256 √ó 256 when interpolating in the latent space between two synthetic samples (left to right). Note the high quality of synthetic samples and very smooth interpolations, maintaining global and local realism. 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Images generated with U-Net GAN trained on COCO-Animals with resolution 128 √ó 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>FID curves over iterations of the BigGAN model (blue) and the proposed U-Net GAN (red). Depicted are the FID mean and standard deviation across 5 runs per setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of the generator and discriminator loss behavior over training for U-Net GAN and BigGAN. The generator and discriminator loss of U-Net GAN is additionally split up into its encoder-and decoder components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure S6 :</head><label>S6</label><figDesc>Images generated with U-Net GAN trained on COCO-Animals with resolution 128 √ó 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure S7 :</head><label>S7</label><figDesc>Generated samples on COCO-Animals and the corresponding U-Net decoder predictions. Brighter colors correspond to the discriminator confidence of pixel being real (and darker of being fake).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure S8 :</head><label>S8</label><figDesc>Images generated with U-Net GAN on COCO-Animals with resolution 128 √ó 128 when interpolating in the latent space between two synthetic samples (left to right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of the U-Net GAN model on FFHQ and COCO-Animals. Shown are the median FID scores.</figDesc><table><row><cell>The proposed components lead to better performance, on</cell></row><row><cell>average improving the median FID by 3.7 points over Big-</cell></row><row><cell>GAN [5]. See Section 4.2 for discussion.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art models on CelebA (128 √ó 128). See Section 4.2 for discussion.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3</head><label>3</label><figDesc>shows that U-Net GAN compares favourably with the state of the art on the CelebA dataset. The BigGAN baseline computed already outperforms COCO-GAN, the best result reported in the literature to the best of our knowledge, lowering FID from 5.74 to 4.54, whereas U-Net GAN further improves FID to 2.95 4 . It is worth noting that BigGAN is the rep-</figDesc><table><row><cell>D U dec</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S1</head><label>S1</label><figDesc></figDesc><table><row><cell cols="6">: Evaluation results on FFHQ, COCO-Animals and</cell></row><row><cell cols="6">CelebA with PyTorch and TensorFlow FID/IS scores. The</cell></row><row><cell cols="6">difference lies in the choice of framework in which the in-</cell></row><row><cell cols="6">ception network is implemented, which is used to extract</cell></row><row><cell cols="5">the inception metrics. See Section A for discussion.</cell><cell></cell></row><row><cell>Method</cell><cell>Dataset</cell><cell cols="4">FID Best Median Mean Std</cell></row><row><cell>BigGAN U-Net GAN</cell><cell>COCO-Animals</cell><cell>16.37 13.73</cell><cell>16.55 13.87</cell><cell cols="2">16.62 0.24 13.88 0.11</cell></row><row><cell>BigGAN U-Net GAN</cell><cell>FFHQ</cell><cell>11.48 7.48</cell><cell>12.42 7.63</cell><cell cols="2">12.35 0.67 7.73 0.56</cell></row><row><cell>BigGAN U-Net GAN</cell><cell>CelebA</cell><cell>3.70 2.03</cell><cell>3.89 2.07</cell><cell>3.94 2.08</cell><cell>0.16 0.04</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This formulation is originally proposed as non-saturating (NS) GAN in<ref type="bibr" target="#b13">[14]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ajbrock/BigGAN-PyTorch 3 https://github.com/clovaai/CutMix-PyTorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">FID scores for CelebA were computed with the standard TensorFlow Inception network for comparability. The PyTorch and TensorFlow FIDs for all datasets are presented inTable S1.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ReLU, Global sum pooling linear‚Üí 1   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L√©on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation. Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On self modulation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√©r√©mie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rankgan: A maximum margin ranking gan for generating faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vishnu Naresh Boddeti, and Marios Savvides</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Asian Conference on Computer Vision (ACCV)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online adaptative curriculum learning for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo√£o</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabela</forename><surname>Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrey</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative multi-adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Improved training of</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasserstein</forename><surname>Gans</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feedback adversarial learning: Spatial feedback for improving generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<title level="m">Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Marcin Michalski, and Sylvain Gelly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Luciƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04720</idno>
	</analytic>
	<monogr>
		<title level="m">The GAN landscape: Losses, architectures, regularization, and normalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab√°s P√≥czos. Mmd</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coco-gan: Generation by parts via conditional coordinating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are GANs created equal? A large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Luciƒá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04076</idno>
		<title level="m">Multi-class generative adversarial networks with the L2 loss function</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dropout-gan: Learning from a dynamic ensemble of discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gon√ßalo</forename><surname>Mordido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11346</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka. F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><forename type="middle">T</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09295</idno>
		<title level="m">Improved training with curriculum gans</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zero-shot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PA-GAN: Improving GAN training by progressive augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Energybased generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04724</idno>
		<title level="m">Improved consistency regularization for gans</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U-Net</forename><surname>Biggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<title level="m">Figure S4: Qualitative comparison of uncurated images generated with the unconditional BigGAN model (top) and our U-Net GAN (bottom) on FFHQ with resolution 256 √ó 256. Note that the images generated by U-Net GAN exhibit finer details and maintain better local realism</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
