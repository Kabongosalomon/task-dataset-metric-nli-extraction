<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combined Image-and World-Space Tracking in Traffic Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljoša</forename><surname>Ošep</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mehner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
						</author>
						<title level="a" type="main">Combined Image-and World-Space Tracking in Traffic Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking in urban street scenes plays a central role in autonomous systems such as self-driving cars. Most of the current vision-based tracking methods perform tracking in the image domain. Other approaches, e.g. based on LIDAR and radar, track purely in 3D. While some vision-based tracking methods invoke 3D information in parts of their pipeline, and some 3D-based methods utilize image-based information in components of their approach, we propose to use image-and world-space information jointly throughout our method. We present our tracking pipeline as a 3D extension of image-based tracking. From enhancing the detections with 3D measurements to the reported positions of every tracked object, we use worldspace 3D information at every stage of processing. We accomplish this by our novel coupled 2D-3D Kalman filter, combined with a conceptually clean and extendable hypothesize-andselect framework. Our approach matches the current stateof-the-art on the official KITTI benchmark, which performs evaluation in the 2D image domain only. Further experiments show significant improvements in 3D localization precision by enabling our coupled 2D-3D tracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual scene understanding in outdoor environments is a key requirement for autonomous mobile systems. The tracking and detection of traffic participants such as pedestrians, cars, and bicyclists plays an important role in safe navigation of autonomous vehicles. Through tracking, the vehicle becomes aware of the whereabouts of important objects and determines their motion.</p><p>A substantial amount of research has been done in this area, mainly driven by the goal of developing autonomous vehicles that may operate in everyday traffic. Recent advances in object detection <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b39">[40]</ref> and detection-based multi-object tracking <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b40">[41]</ref> start to approach a matured state. However, there are still several open problems in vision-based tracking approaches. The majority of existing methods only perform tracking in the image domain. Yet, in mobile robotics and autonomous driving scenarios, precise 3D localization and trajectory estimation is of fundamental importance. In order to prevent collisions, it is crucial to be aware of the extent and the orientation of objects in worldspace, especially for objects close to the camera.</p><p>In this work, we carefully combine 2D object detections and 3D stereo depth measurements in order to improve image-based tracking and, more importantly, precise 3D localization (see <ref type="figure" target="#fig_0">Fig. 1</ref>). While image-based tracking has shown to be successful even in greater distances from the camera, 3D stereo measurement precision deteriorates quickly with camera distance <ref type="bibr" target="#b28">[29]</ref>. Our system weights these sources depending on the distance from the camera.</p><p>The authors are with the Visual Computing Institute, RWTH Aachen University E-mail: {osep, mehner, mathias, leibe}@vision.rwth-aachen.de It combines 2D and 3D information when available, but is also able to cope with missing 3D measurements.</p><p>Our contributions are as follows. (i) We propose a new tracking framework 1 , which exploits both 2D and 3D measurements. To that end, we combine object detections (e.g. cars, pedestrians) and 3D object proposals obtained in a 3D point-cloud. Our method takes advantage of the strengths of both sources of information: 2D detections provide class information, while our 3D proposals assist in locating the objects in world coordinates. (ii) We introduce a novel 2D-3D Kalman filter, which is keeping both an image-and a world-space (position and size) estimate. These estimates are loosely coupled to ensure the consistency of a track. This coupling enables us to track distant objects and continue these tracks with more precise information in the close range, while smoothly transitioning between the modalities. (iii) We show competitive results on the KITTI benchmark. Additionally to these image-based evaluations, we assess the precision of our method in 3D space to quantify the advantage of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Vision-based multi-object tracking in street scenes. Most vision-based approaches to multi-object tracking (MOT) in street scenes follow the tracking-by-detection paradigm, where object detector responses are matched across multiple frames in the image domain <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Geiger et al. <ref type="bibr" target="#b12">[13]</ref> associate detections using an appearance model and bounding box overlap in the image domain. Detection bounding boxes are filtered with a Kalman filter and are associated in a two stage process: first, detections are combined to short tracks (tracklets), followed by association of these tracklets to form full trajectories. Yoon et al. <ref type="bibr" target="#b40">[41]</ref> propose a method that compensates for abrupt camera motion by employing a new data association algorithm that takes structural constraints into account. Choi <ref type="bibr" target="#b8">[9]</ref> utilizes a sparse optical flow-based descriptor as an additional affinity measure and proposes a near-online tracking formulation similar to <ref type="bibr" target="#b19">[20]</ref>. These methods are based on monocular camera systems and perform tracking in the image domain. However, for robotics applications such as path planning and obstacle avoidance it is desirable to be fully aware of the object's 3D position and spatial extent. Our method follows the same tracking-by-detection paradigm, but performs tracking jointly in 2D and 3D.</p><p>Vision-based multi-object tracking using depth sensors. Several vision-based tracking methods include depth information in the tracking process by using either stereo camera pairs or structured light based (RGB-D) sensors (e.g. Kinect, RealSense) in order to improve tracking performance. Depth information can be exploited for different purposes, e.g. to enhance the detection process <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b42">[43]</ref>, to reduce a detector's search space <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref>, or to facilitate data association <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>. While RGB-D sensors are well suited to perform pedestrian tracking in indoor scenes <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b42">[43]</ref> they are of less use in outdoor environments due to challenging lighting conditions and reflective surfaces. Therefore, multi-object tracking systems in street scenes usually rely on a stereo-camera based setup. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref> use such a setup to estimate a coarse scene geometry (ground plane) and localize detections in 3D-space by ray-casting detection footpoints and intersecting them with the ground plane <ref type="bibr" target="#b19">[20]</ref> or by performing a depth-analysis of the detection windows <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>. While such a depthanalysis of detection windows has shown to work reliably for the pedestrian category, it can fail in more challenging scenarios, e.g. for occluded cars.</p><p>Similar to our work, <ref type="bibr" target="#b32">[33]</ref> precisely estimates 3D pose and extent of the tracked objects. However, their method is limited to cars. We show the applicability of our approach for a multitude of object categories. For a detailed overview of MOT methods using RGB-D sensors we refer the interested reader to <ref type="bibr" target="#b5">[6]</ref>.</p><p>LIDAR-based multi-object tracking. When performing MOT using LIDAR sensors, the tracking pipeline is typically reversed using a tracking-before-detection paradigm <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>. LIDAR sensor outputs are more precise, and do not suffer from systematic errors compared to stereobased sensors. The acquired measurements are better suitable to delineate the shapes of objects <ref type="bibr" target="#b7">[8]</ref>. Segmentation of object candidates from LIDAR sensor data is a wellstudied problem <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>. These object candidates provide a precise object boundary, position and shape but no object category information, hence model-free tracking is performed (typically, using a Kalman filter and nearestneighbor data association). Category-agnostic trajectories can then be classified into object categories <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>. In our work, we also rely on category-agnostic object proposals, but rather than using expensive LIDAR systems our proposals are generated from stereo input images <ref type="bibr" target="#b27">[28]</ref>.  <ref type="figure">Fig. 2</ref>. Method overview. Our input data is processed in two steps. We first generate a large number of observations (observation = detection + 3D object proposal) and perform model selection to pick the most suitable ones. In the same fashion, we track the observations to generate an overcomplete set of tracking hypotheses, and again perform model selection to pick the trajectories we report as results.</p><p>In the context of LIDAR tracking, it has been shown that 3D measurements of the objects can be utilized to improve 3D tracking precision (position and velocity). <ref type="bibr" target="#b16">[17]</ref> proposes to align LIDAR object candidates in a coarse-to-fine fashion using annealed dynamic histograms in order to obtain precise position and velocity. <ref type="bibr" target="#b34">[35]</ref> uses 3D measurements to jointly estimate trajectory and shape of the tracked object by creating an object "map". We show that using 3D measurements is suitable for improving the 3D tracking precision even though our input data originates from noisy stereo-based depth data. <ref type="figure">Fig. 2</ref> shows an overview of our proposed pipeline. Our method combines information from several commonly used sources, such as object detections <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b12">[13]</ref>, stereo <ref type="bibr" target="#b14">[15]</ref>, visual odometry <ref type="bibr" target="#b15">[16]</ref>, and optionally scene flow <ref type="bibr" target="#b35">[36]</ref>. Additionally, we support the detections with stereo-based class-agnostic 3D object proposals <ref type="bibr" target="#b27">[28]</ref>, short 3D proposals. These proposals hypothesize objects and provide precise 3D measurements for them, in our case the objects' positions on the ground plane, physical size and a segmentation mask (See <ref type="figure" target="#fig_2">Fig. 3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD OVERVIEW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, bottom).</head><p>A 3D object proposal combined with a 2D detection constitutes an observation. We use a CRF model to select suitable observations out of the huge set of possible observations we generate. The CRF scores the observations according to the compatibility of their 2D and 3D information, and helps to exclude observations which would share either a detection or a 3D proposal with another selected observation.</p><p>Our tracker works using a comparable idea. We first generate an over-complete set of track hypotheses. They are then selected via a CRF model, which scores the hypotheses and prevents the selection of overlapping pairs of hypotheses.</p><p>In extension of the common paradigm of image-space tracking, we use 3D information in every part of the tracking pipeline. Hypotheses are tracked using Kalman filters with a joint 2D-3D state, which is weakly coupled by projection and back-projection operations. As a result, measurements of 2D bounding boxes can help to estimate the 3D position and vice versa. This also means that we can track opportunistically, i.e. we make use of the 3D measurements when available, but we can also perform the tracking without them. Finally, the CRF model scores the hypotheses by evaluating their consistency in image-and world-space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OBSERVATION FUSION MODEL</head><p>As inputs to our observation fusion we use 3D object proposals, obtained by performing clustering in the stereo point-cloud, and object detections. We fuse these sources of information before feeding them to the tracking process. This results in (i) extending the 2D detections by precise 3D measurements of the object position and size and (ii) selecting the relevant proposals from the huge set of available 3D proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Observation Models</head><p>For 2D object detection we use image-space detectors, e.g. <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b12">[13]</ref>. A set of detections at timestep t, t ∈ [0, . . . , T ], is defined as M t det = m t,i det , i ∈ 1, . . . , n t . Each detection measurement provides the center-point, width, and height of the 2D bounding box b 2D = x 2D , y 2D , w 2D , h 2D T (in the image domain), class information c, and score s det :</p><formula xml:id="formula_0">m t,i det = b 2D , c, s det T .<label>(1)</label></formula><p>We obtain 3D object proposals using an extension of our previous work on multi-scale 3D proposals, described in <ref type="bibr" target="#b27">[28]</ref>. That approach generates a large set of class-agnostic 3D object proposals by identifying clusters of depth measurements in stereo point-clouds. The intuition behind this approach is that relevant real-world objects usually stick out of the ground plane, surrounded by a certain amount of free space. As the class of each object is unknown, potential objects have to be searched at varying scales. Clusters that remain stable over multiple scales are selected as possible object candidates. The number of proposed objects is reduced by merging clusters if their bounding boxes overlap (intersection-over-union &gt; 0.9).</p><p>For our application, we slightly modify <ref type="bibr" target="#b27">[28]</ref>: (i) Rather than using the projected bounding boxes in the image plane for merging the clusters, we now project 3D bounding boxes to the ground plane. This results in more precise localization in 3D. (ii) Additionally to clustering with an isotropic kernel, we add two anisotropic kernels elongated along the x-and z-direction (the two dimensions of the ground plane) in order to better represent elongated objects. As a result, we obtain a rich set of 3D object proposals M t prop = m t,j prop , j ∈ 1, . . . , m t . Each proposal is defined by its position p = [x, y, z] T , velocity v = [ẋ,ẏ,ż] T (obtained from scene flow <ref type="bibr" target="#b35">[36]</ref>), size estimate s = w 3D , h 3D , l 3D T , and score:</p><formula xml:id="formula_1">m t,j prop = [p, v, s, s prop ] T .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Observation Fusion</head><p>At this step, we have a set of detections, which provide object class information, and a set of 3D proposals, which provide localization and object size estimates. However, no category information is associated to these proposals. Therefore, a group of pedestrians is a valid object, as much as each pedestrian individually is.</p><p>Only by picking a proposal at the right scale, its measurement becomes meaningful semantically. This is a first instance of image-and world-space information supporting each other. A detection has no precise 3D information, which is obtained by associating the detection to a proposal. On the other hand, a 3D measurement can only be considered precise (in a semantic sense) if the proposal is derived from a segment of the point-cloud which corresponds to an actual object. The set of detections helps selecting meaningful proposals, and rejecting the others. See <ref type="figure" target="#fig_2">Fig. 3</ref> for examples of associated detections and proposals.</p><p>We address the selection problem by performing MAP inference in a CRF model. We enumerate a large set of possible associations between detections and 3D proposals. In theory, we would need to enumerate the Cartesian product of both sets to obtain the overlapping observation</p><formula xml:id="formula_2">set o t = M t det × M t prop .</formula><p>In practice we do gating and have only a limited number of overlapping observations. These associations are interpreted as nodes in a graph and we assign binary labels s ∈ {0, 1} | o | (1: selected, 0: not selected). By minimizing the following energy, we obtain a set of consistent associations between the proposals and the detections.</p><formula xml:id="formula_3">E(s, o) = oi∈o s i φ (o i ) + oi,oj ∈o s i s j ϕ (o i , o j ) , (3)</formula><p>where the detection-to-proposal association potential is defined as:</p><formula xml:id="formula_4">φ (o i ) = −w o 1 φ size (o i ) − w o 2 φ pos (o i ) − w o 3 φ proj (o i ) + w o 4 (4)</formula><p>The first term φ size (o i ) scores the 3D proposal size, using the probability of the size given statistics (mean+variance) learned from data. The second term ensures that the detection and the proposal have a small distance on the ground plane (Mahalanobis distance given the uncertainty of the measurements). The third term matches the projected area of the proposal with the area of the detection bounding box (intersection-over-union). The weight w o 4 imposes a minimal requirement on the association, since observations with an overall positive score φ (o i ) will not be selected.</p><p>The pairwise term penalizes overlapping associations:</p><formula xml:id="formula_5">ϕ (o i , o j ) = w o 5 · |P i P j | min (|P i | , |P j |) + w o 6 · I (o i , o j ) . (5)</formula><p>The first term measures the (normalized) overlap of the two observations i and j, based on the number of shared 3D points |P i P j | of their 3D proposals. The second term is a hard-exclusion term: the indicator function I (o i , o j ) is 1 if two observation share a proposal or a detection and 0 otherwise. The purpose of the pairwise term is to penalize physical overlap of the observations and to disallow observations that are claiming the same proposal or detection. The inference problem (3) is NP-hard. We therefore obtain an approximate solution using the multi-branch method from <ref type="bibr" target="#b30">[31]</ref>. Note that the obtained solution gives us a set of valid associations between the detections and 3D proposals. As 3D proposals can mostly be obtained in the close camera range, there will be detections that are not part of any selected observation. For such detections, we augment our final observation set with partial observations, containing detections, but no 3D information. In particular, this helps to retain far-away targets which are not covered by 3D proposals.</p><p>The proposed approach of combining the information from different sources is also called early fusion, as hard decisions are made prior to invoking the tracker. This contrasts to late fusion, where the tracker performs the selection and fusion of measurements. However, late fusion would result in a combinatorial explosion of the state-space of our multihypothesis tracker. Additionally, the previously described selection of observations matches the selection process of tracking hypotheses through the tracker. The scores and interaction terms consist of the same building blocks, increasing the chance that the observations picked here will produce high-scoring hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TRACKING</head><p>Our tracking formulation follows a hypothesize-and-select framework, as initially proposed in <ref type="bibr" target="#b19">[20]</ref>, which is a current state-of-the-art tracking paradigm for vision-based tracking <ref type="bibr" target="#b8">[9]</ref>. In this paradigm an over-complete set of hypotheses is created, and then the most suitable ones are selected.</p><p>In the following, we will first describe our 2D-3D Kalman filter. It is applied to filter the observations which are associated to our trajectory hypotheses. By generating an over-complete set of hypotheses we capture a multitude of possible data associations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Coupled Filtering of 2D-3D States</head><p>We propose a formulation which keeps track of objects in both image domain and world-space. In contrast, most stateof-the-art vision-based tracking methods perform tracking just in the image domain.</p><p>We use an Extended Kalman Filter (EKF) to estimate a joint 2D-3D state, and couple the different quantities using projection and back-projection operations. The geometry of a hypothesis is estimated using detections (1) and 3D proposals (2), which we filter using the EKF. The state at time t is defined by x t = [b 2D ,ḃ 2D , p, v, s] T . For the position p on the ground plane and the 2D bounding box b 2D we use a constant-velocity model, which requires adding the rate of change of the bounding boxḃ 2D .</p><p>At each timestep, the bounding box position of each hypothesis is corrected for the ego-motion. The footpoint of the bounding box b 2D is back-projected into world-space, using the estimated distance from the camera computed using p. The translation and rotation given by the ego-motion is then applied to the 3D point. By projecting it back into the image, we obtain the corrected footpoint.</p><p>Then, the components of the position and bounding boxes are predicted using the corresponding velocities, except for the following heights and the footpoint estimate, which are weakly coupled through these projection and back-projection operations:</p><formula xml:id="formula_6">h 3D = w b dc f h 2D + w a h 3D<label>(6)</label></formula><formula xml:id="formula_7">h 2D = w a δtḣ 2D + h 2D + w b f dc h 3d .<label>(7)</label></formula><p>y 2D = w a δtẏ 2D + y 2D + w b f dc δtẏ C + y C +v 0 (8)</p><formula xml:id="formula_8">x 2D = w a δtẋ 2D + x 2D + w b f dc δtẋ C + x C +u 0 (9)</formula><p>Here δt is the time difference between two prediction steps, f is the focal length, and u 0 , v 0 are the horizontal and vertical components of the principal point. The position in camera space p C = x C , y C , z C T helps to compute the distance from camera d c = z C . The weights w a and w b determine how much the 2D and 3D states contribute to the coupling (we learn these weights on our training set, see Sec. VI).</p><p>Depending on which observations are available (see Sec. IV-B), we perform sequential updates with these different sources of information. We distinguish two cases: (i) Fused observation, where a detection is associated to a 3D proposal, resulting in observable values z t = b 2D , p, v, s T . In this case we update the corresponding quantities of the state. The coupling only happens in the prediction step. However, a measured bounding box will still influence the next 3D prediction through the coupling.</p><p>(ii) Partial observation (non-associated detections), which restricts the observable values to z t = b 2D , p bp , s mean T .</p><p>Here, we still update the 3D position using the backprojection of the footpoint of the bounding box p bp , and we update the 3D size using the mean size s mean of the category from the training data. In this case, a different measurement variance is attached to the 3D position and size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hypothesis Set Generation</head><p>We perform tracking by maintaining an over-complete set of trajectory hypotheses h = h t0:tn k , each defined on the time-span t 0 :t n (note that we will omit time indices where not needed). Each hypothesis is built from the set of observations using our 2D-3D Kalman filter. It is constrained to the ground plane and maintains a state estimate over time:</p><formula xml:id="formula_9">h k (t) = b 2D ,ḃ 2D , p, v, s, c, o t k T .<label>(10)</label></formula><p>The observation used in each frame is also attached to the hypothesis, although it may be missing for any given frame, i.e. o t k = ∅. A hypothesis h t0:tn k is furthermore associated to its inlier set I k = {o t k |t 0 ≤ t ≤ t n } of observations spread out over the temporal domain. We attach a multinomial distribution over possible object categories c = {car, pedestrian, cyclist}. It is estimated from the associated detections by performing forward Bayesian filtering (likelihood terms are learned from the data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis Extension.</head><p>We create an over-complete hypothesis set similar to the one proposed in <ref type="bibr" target="#b19">[20]</ref>: at each time step, we (i) extend existing hypotheses with a new observation and (ii) generate alternative hypotheses starting at new observations within a temporal window.</p><p>In case several detections are very close to the hypothesis (in image-space, on the ground plane, and in appearance), we branch the hypothesis, updating each branch with a different detection.</p><p>Hypothesis Persistence. We stop updating hypotheses which have left the camera view frustum, but we keep extrapolating them for a while (see <ref type="figure" target="#fig_5">Fig. 6</ref>). This keeps the hypothesis 'alive' and it continues to claim its inlier set during the optimization procedure, such that these observations are not suddenly free to support other hypotheses which would be implausible otherwise. In order to keep the size of the hypothesis set feasible, we remove duplicates and hypotheses which were not selected for some time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hypothesis Selection</head><p>In each frame we select from the set of hypotheses by performing MAP inference in a CRF model, similar to <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_10">E(m, h) = hi∈h m i ϑ (h i ) + hi,hj ∈h m i m j ψ (h i , h j ) , (11) where m ∈ {0, 1}</formula><p>|h| is a binary indicator vector, with m i = 1 meaning that a hypothesis has been picked. We search for the selection m * = argmin m E(m, h) with the lowest energy given a hypothesis set h.</p><p>Hypothesis Score. The unary term scores a hypothesis:</p><formula xml:id="formula_11">ϑ (h i ) = w h min − o t i ∈Ii S(o t i , h i ),<label>(12)</label></formula><p>where the model parameter w h min defines a minimal score for a hypothesis. The contribution of an observation</p><formula xml:id="formula_12">S o t i , h t0:tn i is: S o t i , h t0:tn i = e (−τ ·(tn−t)) · s o t i · Φ o t i , h t0:tn i .<label>(13)</label></formula><p>Using an exponential decay, observations further in the past have less influence. The score of an observation is s (o t i ) = s det . In the current formulation, we only use the score of the detection. The association affinity term:</p><formula xml:id="formula_13">Φ (o i , h i ) = I c (o i , h i ) · (w c Φ c (o i , h i ) + w m Φ m (o i , h i ) + w p Φ p (o i , h i ))<label>(14)</label></formula><p>is a linear combination of the appearance score Φ c (o i , h i ), the motion model term Φ m (o i , h i ), and the projection model term Φ p (o i , h i ), multiplied by an indicator function I c (·, ·) that prevents association between hypotheses and observations of incompatible categories. The weights are functions of the distance d (o i ) of an observation from the camera and the relative weighting of the appearance term w c :</p><formula xml:id="formula_14">w m = (1 − w c )e (−γ·d(o i )) , w p = (1 − w c − w m ) .<label>(15)</label></formula><p>For the appearance score we use intersection kernels over color histograms as in <ref type="bibr" target="#b8">[9]</ref>. The motion model term</p><formula xml:id="formula_15">Φ m (o i , h i ) ∼ N p obs | p h k pred , Σ h k pred</formula><p>scores the probability of the observation given the Kalman filter prediction (on the ground plane). The projection model</p><formula xml:id="formula_16">Φ p (o i , h i ) = IoU b 2D (o) , b 2D (h)</formula><p>is computed as the intersection-overunion between the predicted 2D bounding box of the hypothesis and the observed bounding box (in the image domain).</p><p>Hypothesis Interaction. The pairwise potential of the CRF in (11) scores the interaction of each pair of hypotheses:</p><formula xml:id="formula_17">ψ (h i , h j ) = w h ol O(h i , h j ) + w h sh |I i ∩ I j | .<label>(16)</label></formula><p>The parameter w h ol weights the physical overlap penalty:</p><formula xml:id="formula_18">O h i , h j = t IoU b 2D (h i , t) , b 2D h j , t 2 ,<label>(17)</label></formula><p>which punishes overlap in image space of the two hypotheses. Additionally, we add a penalty w h sh for each observation shared by the hypotheses.</p><p>Intuitively, after solving this inference problem we obtain a set of best-scoring trajectories that are physically plausible (i.e. do not overlap in space-time). Note the similarity of this model to the observation fusion in Sec. IV-B. The difference is that in this case we are aiming for a partition of the observations over time, whereas in Sec. IV-B we are computing associations between 3D proposals and detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL EVALUATION</head><p>All our evaluations are based on the KITTI Vision Benchmark Suite <ref type="bibr" target="#b13">[14]</ref>. It provides training sequences with a publicly available ground truth, and a separate test set which can only be evaluated using the provided evaluation server. Evaluations are based on the CLEAR MOT metrics <ref type="bibr" target="#b2">[3]</ref>. KITTI furthermore provides detections for cars, pedestrians, and bicycles from two different detectors <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b12">[13]</ref>. In our experiments we use the detections from the Regionlet detector <ref type="bibr" target="#b37">[38]</ref>. We split the KITTI training data into two disjoint sets: a training set which is used for optimizing parameters (based on MOTA score) via Hyperopt <ref type="bibr" target="#b1">[2]</ref> and a validation set for evaluating different aspects of our pipeline 2 . We interpret the reported result mostly based on the achieved MOTA score, which is considered to be the most distinctive tracking evaluation measure <ref type="bibr" target="#b4">[5]</ref>.</p><p>Observation Precision. For our detailed analysis of the fused observations, we use 2D and 3D annotations provided in the KITTI training set. We focus on single frame results here, in order to gain insights into the observation fusion without regarding the tracking. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the positioning error by distance range, split into the error made in estimating the distance to the camera, and the lateral error orthogonal to the depth error. We compare the performance of our 3D proposals to three baselines: (GP-P) Ray-casting of the footpoints of the 2D bounding boxes and intersection with the ground plane; (DA) Results obtained by depth analysis <ref type="bibr" target="#b11">[12]</ref>; (3DOP) Results from the recent 3DOP <ref type="bibr" target="#b6">[7]</ref>. While 3DOP has ultimately other goals, the estimation of 3D bounding boxes is an essential step in the pipeline.</p><p>For cars, our evaluation shows that simpler methods (GP-P and DA) are clearly outperformed by the more sophisticated ones. The lateral position of cars is best estimated by 3DOP, which is specially designed for estimating bounding boxes of specific object categories, while our method is more general. Our method is one of the two best performing methods in all cases. While DA is performing well on the pedestrian category, it lacks precision in the car category; the situation is reversed for 3DOP. <ref type="bibr" target="#b1">2</ref> We used sequences <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref>   Ablation Study. In order to evaluate the influence of the different ingredients of our approach, we switch off parts of our pipeline. MOTA and MOTP are both computed using the 2D bounding box overlap, which is the standard in KITTI. See <ref type="table" target="#tab_1">Table I</ref> for the results: (No-flow) uses no scene flow; (Det. only) does not use 3D proposals; (2D-tracker) is a pure 2D version of our tracking pipeline, disabling even the ground plane estimation and visual odometry; (DA) uses depth analysis <ref type="bibr" target="#b11">[12]</ref> rather than our 3D proposals to obtain the 3D measurements. As can be observed, in this 2D evaluation our full method performs best, and each of the components contributes to the performance. A clear benefit of our method will be seen when evaluating in world space.</p><p>3D Localization Evaluation. For each true positive association we evaluate the distance in 2D (as intersection-overunion) and in 3D (as the Euclidean distance on the ground plane). This allows to compute the MOTP-2D and MOTP-3D metrics as suggested by <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_19">MOTP-2D = i,t IoU b i gt (t), b i traj (t) t n tp (t)<label>(18)</label></formula><formula xml:id="formula_20">MOTP-3D = i,t p i gt (t) − p i traj (t) t n tp (t) ,<label>(19)</label></formula><p>with the number of true positive associations per frame n tp . We compare the results using both metrics in <ref type="figure" target="#fig_4">Fig. 5</ref>. For MOTP-2D (higher values are better), there is barely any difference between the full version and the detection-only baseline. However, for MOTP-3D (lower values are better), 3D localization precision is considerably better. This experiment shows that one can benefit from using 3D information in vision-based tracking, even without compromising the ability to accurately track objects in the image domain. Given the considered applications, the MOTP-3D results are clearly more relevant, since in practical tasks we want to obtain precise information in the real world, not in image space.</p><p>Exploiting Precise 3D Segmentations. As has been shown,   our approach is suitable for performing precise 3D localization of tracked objects. This localization can be utilized to accumulate 3D measurements over time, in order to reconstruct more of the shape of tracked objects than can be seen in one frame. The proposals come with precise segmentations of the 3D point cloud (compare <ref type="figure" target="#fig_2">Fig. 3</ref>). The precision of these segmentation masks results in a clean shape representation. The duration of tracking helps to accumulate more depth data over time. In <ref type="figure" target="#fig_5">Fig. 6</ref> we accumulate measurements by using the GCT representation and weighted ICP proposed by <ref type="bibr" target="#b24">[25]</ref>. Alternatively, one could use the segments as an input to <ref type="bibr" target="#b16">[17]</ref>. The provided result can only be obtained when performing precise segmentation in 3D. As objects and possible occluders can be well separated in world space, the shape of the tracked objects can be acquired with less noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI Evaluation</head><p>Results. <ref type="table" target="#tab_1">Table II</ref> shows the result of evaluating our full pipeline on the official KITTI test set w.r.t. the highest-ranked baselines (note, that for NOMT <ref type="bibr" target="#b8">[9]</ref>, we used the online version, NOMT-HM). Although we perform tracking jointly in 2D and 3D, the official KITTI evaluation is based on bounding box overlap in the 2D image domain. We achieve highly competitive results in both categories, cars and pedestrians. For cars, our result is on par with the best performer, NOMT-HM <ref type="bibr" target="#b8">[9]</ref>. In contrast, for pedestrians we clearly outperform NOMT-HM. Similar to us, SCEA <ref type="bibr" target="#b40">[41]</ref> has consistently good results on both categories and is the top performer for the pedestrians category, with our approach a close second. <ref type="figure" target="#fig_6">Fig. 7</ref> shows a selection of qualitative tracking results.</p><p>Runtime. Our full tracking pipeline requires 347 ms per frame, excluding external components (Intel I7 CPU, single thread, not optimized). When not using observation fusion and the corresponding 3D proposals, each frame takes 48 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We presented a novel tracking pipeline which combines 2D and 3D measurements. Our approach shows promising results, follows a clean design and is easily extendable. In future work, we plan to exploit class-agnostic tracking of objects originating from our 3D object proposals. In the context of autonomous driving cars and pedestrians are the most prominent categories, but other obstacles, from potentially unknown categories, should be identified as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example output of our method. Using stereo matching and visionbased tracking, we obtain precise 3D bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Use of depth information. The top row shows detections and associated 3D object proposals (bright green boxes are not associated). The middle row shows the corresponding stereo point-clouds, where all points originating from inside the bounding boxes are highlighted. These sets of points often include an occluder or the background. The bottom row shows the same point-clouds, but points belonging to the associated 3D proposals are highlighted. The 3D proposals separate the object from the environment much better, e.g. the dog in the left column. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Localization error by distance range in depth and lateral direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Tracking Precision MOTP by distance range in 2D (higher values are better) and 3D (lower values are better) for cars and pedestrians.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Precise 3D localization enables us to do shape integration, and increase the system's awareness of its surroundings. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative results on the KITTI tracking dataset showing 2D and 3D bounding boxes of the tracked objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>as training set and sequences 0,3,4,6,10,12,13,14,15,16,20 to perform validation.</figDesc><table><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell cols="3">Depth Error -CAR</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell cols="4">Depth Error -PEDESTRIAN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Our method DA</cell><cell></cell><cell>1.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Our method DA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GP-P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GP-P</cell></row><row><cell></cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3DOP</cell><cell></cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3DOP</cell></row><row><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Error (m)</cell><cell>0.8 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Error (m)</cell><cell>0.8 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell>55</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell>55</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Distance from camera (m)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Distance from camera (m)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Lateral Error -CAR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Lateral Error -PEDESTRIAN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Our method DA</cell><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Our method DA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GP-P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GP-P</cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3DOP</cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3DOP</cell></row><row><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Error (m)</cell><cell>0.2 0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Error (m)</cell><cell>0.2 0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell>55</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell>55</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Distance from camera (m)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Distance from camera (m)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY. 82.85 26 131 49.59 40.68 9.80 No-flow 74.17 82.74 31 141 49.50 40.20 10.29 Det. only 73.99 82.66 48 152 49.01 40.19 10.78 2D-tracker 72.29 82.40 11 72 43.13 42.64 14.22 DA 72.93 82.56 108 201 49.50 37.25 13.24</figDesc><table><row><cell>Cars Full Version</cell><cell>MOTA MOTP ID 74.38 Pedestrians Frag MT PT ML Full Version</cell><cell>MOTA MOTP ID 61.87 78.85 41</cell><cell>Frag MT 164 55.95 33.33 10.71 PT ML</cell></row><row><cell></cell><cell>No-flow</cell><cell>61.82 78.89 53</cell><cell>175 54.76 34.52 10.71</cell></row><row><cell></cell><cell>Det. only</cell><cell>61.13 78.88 51</cell><cell>172 55.95 34.52 9.52</cell></row><row><cell></cell><cell>2D-tracker</cell><cell>59.74 78.85 59</cell><cell>162 48.81 35.71 15.47</cell></row><row><cell></cell><cell>DA</cell><cell>61.69 78.97 32</cell><cell>148 55.95 33.33 10.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II RESULTS</head><label>II</label><figDesc>ON THE KITTI BENCHMARK. TRACKING ACCURACY (MOTA) AND PRECISION (MOTP), ID-SWITCHES (ID), FRAGMENTATIONS (FRAG) MOSTLY TRACKED (MT), PARTLY TRACKED (PT), MOSTLY LOST (ML). 79.25 169 675 48.93 40.09 10.98 NOMT-HM [9] 67.92 80.02 109 371 49.24 37.65 13.11 SCEA [41] 67.11 79.39 106 466 52.13 36.89 10.98 LPSSVM [32] 66.35 77.80 63 558 55.95 35.82 8.23 mbodSSP [21] 62.64 78.75 116 884 48.02 43.29 8.69 RMOT [42] 53.03 75.42 215 742 39.48 50.46 10.06 Pedestrians MOTA MOTP ID Frag MT PT ML Our method 38.37 71.44 113 912 13.40 51.55 35.05 NOMT-HM 31.43 71.14 186 870 21.31 36.77 41.92 SCEA 39.34 71.86 56 649 16.15 40.55 43.30 LPSSVM 34.97 70.48 73 814 20.27 45.36 34.71.02 156 760 19.59 39.18 41.24</figDesc><table><row><cell>Cars</cell><cell>MOTA MOTP ID</cell><cell>Frag MT</cell><cell>PT</cell><cell>ML</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our method</cell><cell cols="8">67.35 36</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>mbodSSP</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RMOT</cell><cell>36.42</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was funded by ERC Starting Grant project CV-SUPER (ERC-2012-StG-307432).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A real-time pedestrian detection system based on structure and appearance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eledath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: The CLEAR MOT metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time tracking of multiple people using continuous detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beymer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Frame Rate Workshop</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring bounding box context for multi-object tracker fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breuers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Camplani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paiement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hannuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04450</idno>
		<title level="m">Multiple human tracking in RGB-D data: A survey</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multisensor fusion system for moving object detection and tracking in urban driving environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A general framework for tracking multiple people from a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1577" to="1591" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion-based detection and tracking in 3D lidar scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Caselitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Tipaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust multi-person tracking from a mobile platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1831" to="1846" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">3D traffic scene understanding from movable platforms. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1012" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient large-scale stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">StereoScan: Dense 3D reconstruction in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intel. Vehicles Symp</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining 3D shape, color, and motion for robust anytime tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time RGB-D based people detection and tracking for mobile robots and head-worn cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mitzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative object detection and tracking in 3D range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaestner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pilat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coupled object detection and tracking from static cameras and moving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1683" to="1698" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FollowMe: Efficient online mincost flow tracking with bounded memory and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continuous energy minimization for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="72" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-person tracking with sparse detection and continuous segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mitzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time multi-person tracking with detector assisted structure propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mitzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Taking mobile multi-object tracking to the next level: People, unknown objects, and carried items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mitzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time multi-person tracking with time-constrained detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mitzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sudowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint self-localization and tracking of generic objects in 3D range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-scale object candidates for generic object tracking in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Know your limits: Accuracy of long range stereoscopic object measurements in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Amd Uwe Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perspective n-view multibody structure-and-motion through model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning optimal parameters for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shaofei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint SFM and detection cues for monocular 3D localization in road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards 3D object recognition via classification of arbitrary object tracks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teichman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Continuous-time estimation for dynamic obstacle tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ushani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlevaris-Bianco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Galceran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Piecewise rigid scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">What could move? Finding cars, pedestrians and bicyclists in 3D laser data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to track: Online multiobject tracking by decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data-driven 3D voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Online multiobject tracking via structural constraint event aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bayesian multiobject tracking using motion context from multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-time multiple human perception with color-depth cameras on a mobile robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reardon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1429" to="1441" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Global data association for multiobject tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
