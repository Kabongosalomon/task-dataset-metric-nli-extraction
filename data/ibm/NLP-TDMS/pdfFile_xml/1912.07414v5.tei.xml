<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Canonical Representations for Scene Graph to Image Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-24">24 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Amir Bar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country>NVIDIA Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Canonical Representations for Scene Graph to Image Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-24">24 Aug 2020</date>
						</imprint>
					</monogr>
					<note>† Work done while at the University of Berkeley California. 1 The project page is available at 2 Herzig et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene graphs</term>
					<term>canonical representations</term>
					<term>image generation Equal Contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>b) Baseline (c) Ours (a) Input Scene Graph (partial) <ref type="figure">Fig. 1</ref>: Generation of scenes with many objects. Our method achieves better performance on such scenes than previous methods. Left: A partial input scene graph. Middle: Generation using [17]. Right: Generation using our proposed method.</p><p>Abstract. Generating realistic images of complex visual scenes becomes challenging when one wishes to control the structure of the generated images. Previous approaches showed that scenes with few entities can be controlled using scene graphs, but this approach struggles as the complexity of the graph (the number of objects and edges) increases. In this work, we show that one limitation of current methods is their inability to capture semantic equivalence in graphs. We present a novel model that addresses these issues by learning canonical graph representations from the data, resulting in improved image generation for complex visual scenes. 1 Our model demonstrates improved empirical performance on large scene graphs, robustness to noise in the input scene graph, and generalization on semantically equivalent graphs. Finally, we show improved performance of the model on three different benchmarks: Visual Genome, COCO, and CLEVR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating realistic images is a key task in computer vision research. Recently, a series of methods were presented for creating realistic-looking images of objects and faces (e.g. <ref type="bibr">[3,</ref><ref type="bibr">20,</ref><ref type="bibr">39]</ref>). Despite this impressive progress, a key challenge remains: how can one control the content of images at multiple levels to generate images that have specific desired composition and attributes. Controlling content can be particularly challenging when generating visual scenes that contain multiple interacting objects. One natural way of describing such scenes is via the structure of a Scene Graph (SG), which contains a set of objects as nodes and their attributes and relations as edges. Indeed, several studies addressed generating images from SGs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">17,</ref><ref type="bibr">27]</ref>. Unfortunately, the quality of images generated from SGs still lags far behind that of generating single objects or faces. Here we show that one problem with current models is their failure to capture logical equivalences, and we propose an approach for overcoming this limitation.</p><p>SG-to-image typically involves two steps: first, generating a layout from the SG, and then generating pixels from the layout. In the first step, the SG does not contain bounding boxes, and is used to generate a layout that contains bounding box coordinates for all objects. The transformation relies on geometric properties specified in the SG such as "(A, right, B)". Since SGs are typically generated by humans, they usually do not contain all correct relations in the data. For example, in an SG with relation (A, right, B) it is always true that (B, left, A), yet typically only one of these relations will appear. 2 This example illustrates that multiple SGs can describe the same physical configuration, and are thus logically equivalent. Ideally, we would like all such SGs to result in the same layout and image. As we show here, this often does not hold for existing models, resulting in low-quality generated images for large graphs (see <ref type="figure">Figure 1</ref>).</p><p>Here we present an approach to overcome the above difficulty. We first formalize the problem as being invariant to certain logical equivalences (i.e., all equivalent SGs should generate the same image). Next, we propose to replace any SG with a "canonical SG" such that all logically equivalent SGs are replaced by the same canonical SG, and this canonical SG is the one used in the layout generation step. This approach, by definition, results in the same output for all logically equivalent graphs. We present a practical approach to learning such a canonicalization process that does not use any prior knowledge about the relations (e.g., it does not know that "right" is a transitive relation). We show how to integrate the resulting canonical SGs within a SG-to-image generation model, and how to learn it from data. Our method also learns more compact models than previous methods, because the canonicalization process distributes information across the graph with only few additional parameters.</p><p>In summary, our novel contributions are as follows: 1) We propose a model that uses canonical representations of SGs, thus obtaining stronger invariance properties. This in turn leads to generalization on semantically equivalent graphs and improved robustness to graph size and noise in comparison to existing methods. 2) We show how to learn the canonicalization process from data. 3) We use our canonical representations within an SG-to-image model and show that our approach results in improved generation on Visual Genome, COCO, and CLEVR, compared to the state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image generation. Earlier work on image generation used autoregressive networks <ref type="bibr">[37,</ref><ref type="bibr">38]</ref> to model pixel conditional distributions. Recently, GANs <ref type="bibr">[11]</ref> and VAEs [23] emerged as models of choice for this task. Specifically, generation techniques based on GANs were proposed for generating sharper, more diverse and better realistic images in a series of works <ref type="bibr">[5,</ref><ref type="bibr">20,</ref><ref type="bibr">28,</ref><ref type="bibr">30,</ref><ref type="bibr">34,</ref><ref type="bibr">42,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b2">55,</ref><ref type="bibr" target="#b9">62,</ref><ref type="bibr" target="#b13">66]</ref>. Conditional image synthesis. Multiple works have explored approaches for generating images with a given desired content. Conditioning inputs may include class labels <ref type="bibr">[7,</ref><ref type="bibr">32,</ref><ref type="bibr">36]</ref>, source images <ref type="bibr">[15,</ref><ref type="bibr">16,</ref><ref type="bibr">29,</ref><ref type="bibr">52,</ref><ref type="bibr" target="#b15">68,</ref><ref type="bibr" target="#b16">69]</ref>, model interventions <ref type="bibr">[2]</ref>, and text <ref type="bibr">[14,</ref><ref type="bibr">40,</ref><ref type="bibr">43,</ref><ref type="bibr">44,</ref><ref type="bibr">49,</ref><ref type="bibr" target="#b6">59,</ref><ref type="bibr" target="#b7">60,</ref><ref type="bibr" target="#b10">63]</ref>. Other studies <ref type="bibr">[9,</ref><ref type="bibr">35]</ref> focused on image manipulation using language descriptions while disentangling the semantics of both input images and text descriptions. Structured representation. Recent models <ref type="bibr">[14,</ref><ref type="bibr" target="#b14">67]</ref> incorporate intermediate structured representations, such as layouts or skeletons, to control the coarse structure of generated images. Several studies focused on generating images from such representations (e.g., semantic segmentation masks <ref type="bibr">[6,</ref><ref type="bibr">16,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b2">55]</ref>, layout <ref type="bibr" target="#b11">[64]</ref>, and SGs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">17,</ref><ref type="bibr">27]</ref>). Layout and SGs are more compact representations as compared to segmentation masks. While layout <ref type="bibr" target="#b11">[64]</ref> provides spatial information, SGs <ref type="bibr">[17]</ref> provide richer information about attributes and relations. Another advantage of SGs is that they are closely related to the semantics of the image as perceived by humans, and therefore editing an SG corresponds to clear changes in semantics. SGs and visual relations have also been used in image retrieval <ref type="bibr">[19,</ref><ref type="bibr">48]</ref>, relationship modeling <ref type="bibr">[25,</ref><ref type="bibr">41,</ref><ref type="bibr">47]</ref>, image captioning <ref type="bibr" target="#b5">[58]</ref> and action recognition <ref type="bibr">[12,</ref><ref type="bibr">31]</ref>. Several works have addressed the problem of generating SGs from text <ref type="bibr">[48,</ref><ref type="bibr">53]</ref>, standalone objects <ref type="bibr" target="#b4">[57]</ref> and images <ref type="bibr">[13]</ref>. Scene-graph-to-image generation. Sg2Im <ref type="bibr">[17]</ref> was the first to propose an end-to-end method for generating images from scene graphs. However, as we note above, the current SG-to-image models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">8,</ref><ref type="bibr">27,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b1">54]</ref> show degraded performance on complex SGs with many objects. To mitigate this, the authors in <ref type="bibr" target="#b0">[1]</ref> have utilized stronger supervision in the form of a coarse grid, where attributes of location and size are specified for each object. The focus of our work is to alleviate this difficulty by directly modeling some of the invariances in SG representation. Finally, the topic of invariance in deep architectures has also attracted considerable interest, but mostly in the context of certain permutation invariances <ref type="bibr">[13,</ref><ref type="bibr" target="#b8">61]</ref>. Our approach focuses on a more complex notion of invariance, and addresses it via canonicalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scene Graph Canonicalization</head><p>As mentioned above, the same image can be represented by multiple logicallyequivalent SGs. Next we define this formally and propose an approach to canonicalize graphs that enforces invariance to these equivalences. In Section 4 we show how to use this canonical scene graph within an SG-to-image task.</p><p>Let C be the set of objects categories and R be the set of possible relations. 3 An SG over n objects is a tuple (O, E) where O ∈ C n is the object categories and E is a set of labeled directed edges (triplets) of the form (i, r, j) where i, j ∈ {1, . . . , n} and r ∈ R. Thus an edge (i, r, j) implies that the i th object (that has category o i ) should have relation r with the j th object. Alternatively the set E can be viewed as a set of |R| directed graphs where for each r the graph E r contains only the edges for relation r.</p><p>Our key observation is that relations in SGs are often dependent, because they reflect properties of the physical world. This means that for a relation r, the presence of certain edges in E r implies that other edges have to hold. For example, assume r is a transitive relation like "left". Then if i, j ∈ E r and j, k ∈ E r , it should hold that i, k ∈ E r . There are also dependencies between different relations. For example, if r, r are converse relations (e.g., r is "left" and r "right") then i, j ∈ E r implies j, i ∈ E r . Formally, all the above dependencies are first order logic formulas. For example, r, r being converse corresponds to the formula ∀i, j : r(i, j) =⇒ r (j, i). Let F denote this set of formulas.</p><p>The fact that certain relations are implied by a graph does not mean that they are contained in its set of relations. For example, E may contain (1, left, 2) but not (2, right, 1). 4 However, we would like SGs that contain either or both of these relations to result in the same image. In other words, we would like all logically equivalent graphs to result in the same image, as formally stated next.</p><p>Given a scene graph E denote by Q(E) the set of graphs that are logically equivalent to E. 5 As mentioned above, we would like all these graphs to result in the same image. Currently, SG-to-layout architectures do not have this invariance property because they operate on E and thus sensitive to whether it has certain edges or not. A natural approach to solve this is to replace E with a canonical form C(E) such that ∀E ∈ Q(E) we have C(E ) = C(E). There are several ways of defining C(E). Perhaps the most natural one is the "relation-closure" which is the graph containing all relations implied by those in E. Definition 1. Given a set of formulas F, and relations E, the closure C(E) is the set of relations that are true in any SG that contains E and satisfies F.</p><p>We note that the above definition coincides with the standard definition for closure of relations. Our definition emphasizes the fact that C(E) are relations that are necessarily true given those in E. Additionally we allow for multiple relations, whereas closure is typically defined with respect to a single property. Next we describe how to calculate C(E) when F is known, and then explain how to learn F from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Calculating Scene Graph Canonicalization</head><p>For a general set of formulas, calculating the closure is hard as it is an instance of inference in first order logic. However, here we restrict ourselves to the following formulas for which this calculation is efficient: 6 -Transitive Relations: We assume a set of relations R trans ⊂ R where all r ∈ R trans satisfy the formula ∀x, y, z : r(x, y) ∧ r(y, z) =⇒ r(x, z). -Converse Relations: We assume a set of relations pairs R conv ⊂ R×R where all (r, r ) ∈ R conv satisfy the formula ∀x, y : r(x, y) =⇒ r (y, x).</p><p>Under the above set of formulas, the closure C(E) can be computed via the following procedure, which we call Scene Graph Canonicalization (SGC):</p><formula xml:id="formula_0">Initialization: Set C(E) = E. Converse Completion: ∀(r, r ) ∈ R conv , if (i, r, j) ∈ E, add (j, r , i) to C(E).</formula><p>Transitive Completion: For each r ∈ R trans calculate the transitive closure of C r (E) (namely the r relations in C(E)) and add it to C(E). The transitive closure can be calculated using the Floyd-Warshall algorithm <ref type="bibr">[10]</ref>.</p><p>It can be shown (see <ref type="figure">Supplementary)</ref> that the SGC procedure indeed produces the closure of C(E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Calculating Weighted Scene Graph Canonicalization</head><p>Thus far we assumed that the sets R trans and R conv were given. Generally, we don't expect this to be the case. We next explain how to construct a model that doesn't have access to these. In this formulation we will add edges with weights, to reflect our level of certainty in adding them. These weights will depend on parameters, which will be learned from data in an end-to-end manner (see Section 5). See <ref type="figure" target="#fig_0">Figure 2</ref> for a high level description of the architecture.</p><p>Since we don't know which relations are transitive or converses, we assign probabilities to reflect this uncertainty. In the transitive case, for each r ∈ R we use a parameter θ trans r ∈ R |R| to define the probability that r is transitive:</p><formula xml:id="formula_1">p trans (r) = σ(θ trans r )<label>(1)</label></formula><p>where σ is the sigmoid function. For converse relations, we let p conv (r |r) denote the probability that r is the converse of r. We add another empty relation r = φ such that p conv (φ|r) is the probability that r has no converse in R. This is parameterized via θ conv r,r ∈ R |R|×|R∪φ| which is used to define the distribution:</p><formula xml:id="formula_2">p conv (r |r) = e θ conv r,r r∈R∪φ e θ conv r,r<label>(2)</label></formula><p>Finally, since converse pairs are typically symmetric (e.g., "left" is the converse of "right" and vise-versa), for every r, r ∈ R × R we set θ conv r,r = θ conv r ,r . Our model will use these probabilities to complete edges as explained next. In Section 3.1 we described the SGC method, which takes a graph E and outputs its completion C(E). The method assumed knowledge of the converse and transitive relations. Here we extend this approach to the case where we have weights on the properties of relations, as per Equations 1 and 2. Since we have weights on possible completions we will need to work with a weighted relation graph and thus from now on consider edges (i, r, j, w). Below we describe two methods WSGC-E and WSGC-S for obtaining weighted graphs. Section 4 shows how to use these weighted graphs in an SG to image model. Exact Weighted Scene Graph Canonicalization (WSGC-E). We describe briefly a method that is a natural extension of SGC (further details are provided in the Supplementary). It begins with the user-specified graph E, with weights of one. Next two weighted completion steps are performed, corresponding to the SGC steps. Converse Completion: In SGC, this step adds all converse edges. In the weighted case it makes sense to add the converse edge with its corresponding converse weight. For example, if the graph E contains the edge (i, above, j, 1) and p conv (below|above) = 0.7, we add the edge (j, below, i, 0.7). Transitive Completion: In SGC, all transitive edges are found and added. In the weighted case, a natural alternative is to set a weight of a path to be the product of weights along this path, and set the weight of a completed edge (i, r, j) to be the maximum weight of a path between i and j times the probability p trans (r) that the relation is transitive. This can be done in poly-time, but runtime can be substantial for large graphs. We offer a faster approach next. Sampling Based Weighted Scene Graph Canonicalization (WSGC-S).</p><p>The difficulty in WSGC-E is that the transitivity step is performed on a dense graph (most weights will be non-zero). To overcome this, we propose to replace the converse completion step of WSGC-E with a sampling based approach that samples completed edges, but always gives them a weight of 1 when they are added. In this way, the transitive step is computed on a much sparser graph with weights 1. We next describe the two steps for the WSGC-S procedure.</p><p>Converse Completion: Given the original user-provided graph E, for each r and edge (i, r, j, 1) we sample a random variable Z ∈ R ∪ φ from p conv (·|r) and if Z = φ, we add the edge (j, Z, i, 1). For example, see <ref type="figure">Figure 3b</ref>. After sampling such Z for all edges, a new graph E is obtained, where all the weights are 1. 7 Transitive Completion: For the graph E and for each relation r, calculate the transitive closure of C(E r ) and add all new edges in this closure to E with weight p trans (r). See illustration in <ref type="figure">Figure 3c</ref>. Note that this can be calculated in polynomial time using the FW algorithm [10], as in the SGC case.</p><p>Finally, we note that if all assigned weights are discrete, both the WSGC-E and WSGC-S are identical to SGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Scene Graph to Image using Canonicalization</head><p>Thus far we showed how to take the original graph E and complete it into a weighted graph E , using the WSGC-S procedure. Next, we show how to use E to generate an image, by first mapping E to a scene layout (see <ref type="figure" target="#fig_0">Figure 2</ref>), and then mapping the layout to an image (see AttSPADE <ref type="figure">Figure in</ref> the Supplementary). The following two components are variants of previous SG to image models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">17,</ref><ref type="bibr">50]</ref>, and thus we describe them briefly (see Supplementary for details).</p><p>From Weighted SG to Layout: A layout is a set of bounding boxes for the nodes in the SG. A natural architecture for such graph-labeling problems is a Graph Convolutional Network (GCN) <ref type="bibr">[24]</ref>. Indeed, GCNs have recently been used for the SG to layout task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">17,</ref><ref type="bibr">27]</ref>. We also employ this approach here, but modify it to our weighted scene graph. Namely, we modify the graph convolution layer such that the aggregation step of each node is set to be a weighted average where the weights are those in the canonical SG.</p><p>From Layout to Image: We now need to transform the obtained layout in Section 4 to an actual image. Several works have proposed models for this step <ref type="bibr">[51,</ref><ref type="bibr" target="#b12">65]</ref>, where the input was a set of bounding boxes and their object categories. We follow this approach, but extend it so that attributes for each object (e.g., color, shape and material, as in the CLEVR dataset) can be specified. We achieve this via a novel generative model, AttSPADE, that supports attributes. More details are in Supplementary. <ref type="figure" target="#fig_2">Figure 4</ref> shows an example of the model trained on CLEVR and applied to several SGs. Finally, our experiments on non CLEVR datasets simply we use a pre-trained LostGAN [50] model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Losses and Training</head><p>Thus far we described a model that starts with an SG and outputs an image, using the following three steps: SG to canonical weighted SG (Section 3.2), weighted SG to layout (Section 4) and finally layout to image (Section 4). In this section we describe how the parameters of these steps are trained in an end-toend manner. We focus on training with the WSGC-S, since this is what we use in most of our experiments. See Supplementary for Training with WSGC-E.</p><p>Below we describe the loss for a single input scene graph E and its ground truth layout Y . The parameters of the model are as follows: θ g are the parameters of the GCN in Section 4, θ trans are the parameters of the transitive probability (Eq. 1), and θ conv are those of the converse probability (Eq. 2). Let θ denote the set of all parameters. Recall that in the first step Section 3.2, we sample a set of random variablesZ and use these to obtain a weighted graph WSGCZ(E; θ trans ). Denote the GCN applied to this graph by G θ g (WSGCZ(E; θ trans )).</p><p>We use the L 1 loss between the predicted and ground truth bounding boxes Y . Namely, we wish to minimize the following objective: <ref type="bibr" target="#b0">1</ref> (3) whereZ = {Z e |e ∈ E} is a set of independent random variables each sampled from p conv (r |r(e); θ conv ) (see Eq. 2 and the description of WSGC-E), and q(θ conv ) denotes this sampling distribution. The gradient of this loss with respect to all parameters except θ conv can be easily calculated. Next, we focus on the gradient with respect to θ conv . Because the sampling distribution depends on θ conv it is natural to use the REINFORCE algorithm <ref type="bibr" target="#b3">[56]</ref> in this case, as explained next. Define:</p><formula xml:id="formula_3">L(θ) = EZ q(θ conv ) Y − G θ g (WSGCZ(E; θ trans ))</formula><formula xml:id="formula_4">R(Z; θ g , θ trans ) = Y − G θ g (WSGCZ(E; θ trans )) 1 . Then Eq. 3 is: L(θ conv ) = EZ q(θ conv ) R(Z; θ g , θ trans ).</formula><p>The key idea in REINFORCE is the observation that:</p><formula xml:id="formula_5">∇ θ conv L(θ) = EZ q(θ conv ) ∇ θ conv R(Z; θ g , θ trans ) log p conv θ (Z)</formula><p>Thus, we can approximate ∇ θ conv L(θ) by samplingZ and averaging the above. 8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>To evaluate our proposed WSGC method, we test performance on two tasks. First, we evaluate on the SG-to-layout task (the task that WSGC is designed for. See Section 3.2). We then further use these layouts to generate images and demonstrate that improved layouts also yield improved generated images.</p><p>Datasets. We consider the following three datasets: COCO-stuff [4], Visual Genome (VG) [26] and CLEVR <ref type="bibr">[18]</ref>. We also created a synthetic dataset to quantify the performance of WSGC in a controlled setting. Synthetic dataset. To test the contribution of learned transitivity to layout prediction, we generate a synthetic dataset. In this dataset, every object is a square with one of two possible sizes. The set of relations includes: Above (transitive), Opposite Horizontally and XN ear (non-transitive). To generate training and evaluation data, we uniformly sample coordinates of object centers and object sizes and automatically compute relations among object pairs based on their spatial locations.  use an additional subset we call Packed VG, containing images with at least 16 objects, resulting in 6341 train images, 809 validation, and 809 test images. CLEVR <ref type="bibr">[18]</ref>. A synthetic dataset based on scene-graphs with four spatial relations: lef t, right, f ront and behind, as well as attributes shape, size, material and color. It has 70k training images and 15k for validation and test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Scene-Graph-to-layout Generation</head><p>We evaluate the SG-to-layout task using the following metrics: 1) mIOU : the mean IOU value. 2) R@0.3 and R@0.5: the average recall over predictions with IOU greater than 0.3 and 0.5 respectively. We note our WSGC model is identical to the Sg2Im baseline in the SG-to-layout module in all aspects that are not related to canonicalization. This provides a well-controlled ablation showing that canonicalization improves performance. Testing Robustness to Number of Objects. Scenes can contain a variable number of objects, and SG-to-layout models should work well across these. Here we tested how different models perform as the number of objects is changed in the synthetic dataset. We compared the following models a) A "Learned Transitivity" model that uses WSGC to learn the weights of each relation. b) A "Known Transitivity" model that is given the transitive relations in the data, and performs hard SGC completion (see Section 3.1). Comparison between "Learned Transitivity" and "Known Transitivity" is meant to evaluate how well WSGC can learn which relations are transitive. c) A baseline model Sg2Im [17] that does not use any relation completion, but otherwise has the same architecture.</p><p>We train these models with two and four GCN layers for up to 32 objects. Additionally, to evaluate generalization to a different number of objects at test time, we train models with eight GCN layers on 16 objects and test on up to 128 objects. Results are shown in <ref type="figure">Figure 6a</ref>-b. First, it can be seen that the baseline performs significantly worse than transitivity based models. Second, "Learned Transitivity" closely matches "Known Transitivity" indicating that the model successfully learned which relations are transitive (we also manually confirmed this by inspecting θ trans ). Third, the baseline model requires more layers to correctly capture scenes with more objects, whereas our model performs well with two layers. This suggests that WSGC indeed improves generalization ability by capturing invariances. <ref type="figure">Figure 6c</ref> shows that our model also generalizes well when evaluated on a much larger set of objects than what it has seen at training time, whereas the accuracy of the baseline severely degrades in this case. Layout Accuracy on Packed Scenes. Layout generation is particularly challenging in packed scenes. To quantify this, we evaluate on the Packed COCO and VG datasets. Since Sg2Im [17], PasteGAN [27], and Grid2Im <ref type="bibr" target="#b0">[1]</ref> use the same SG-to-layout module, we compare WSGC only to Sg2Im <ref type="bibr">[17]</ref>. We test Sg2Im with 5,8 and 16 GCN layers to test the effect of model capacity. The Packed setting in <ref type="table">Table 1</ref> shows that WSGC improves layout on all metrics.</p><p>We also evaluate on the "standard" COCO/VG setting, which contain relatively few objects, and we therefore do not expect WSGC to improve there. Results in <ref type="table">Table 1</ref> show comparable performance to the baselines. In addition, manual inspection revealed that the learned p conv and p trans are overall aligned with expected values (See Supplementary). Finally, the results in the standard setting also show that increasing GCN size for Sg2Im [17] results in overfitting.</p><p>Generalization on Semantically Equivalent Graphs. A key advantage of WSGC is that it produces similar layouts for semantically equivalent graphs. This is not true for methods that do not use canonicalization. To test the effectiveness of this property, we modify the test set such that input SGs are replaced with semantically equivalent variations. For example if the original SG was (A, right, B) we may change it to (B, left, A). To achieve this, we generate a semantically equivalent SG by randomly choosing to include or exclude edges which do not change the semantics of the SG. We evaluate on the Packed  <ref type="table">Table 1</ref>: Accuracy of predicted bounding boxes. We consider two different data settings: "Standard" and "Packed". (a) Standard: Training and evaluation is on VG images with 3 to 10 objects, and COCO images with 3 to 8 objects. (b) Packed: Training and evaluation is on images with 16 or more objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Semantically  COCO dataset. Results are shown in <ref type="table" target="#tab_3">Table 2</ref> and qualitative examples are shown in <ref type="figure" target="#fig_5">Figure 7</ref>. It can be seen that WSGC significantly outperforms the baselines. Testing Robustness to Input SGs.</p><p>Here we ask what happens when input SGs are modified by adding "noisy" edges. This could happen due to noise in the annotation process or even adversarial modifications. Ideally, we would like the generation model to be robust to small SG noise. We next analyze how such modifications affect the model by randomly modifying 10% of the relations in the COCO data. As can be seen in <ref type="table" target="#tab_3">Table 2</ref>, the WSGC model can better handle noisy SGs than the baseline. We further note that our model achieves good results on the VG dataset, which was manually annotated, suggesting it is robust to annotation noise. The results in <ref type="table" target="#tab_3">Table 2</ref> also show the Sg2Im generalization deteriorates when growing from 8 to 16 layers, suggesting that the effect of canonicalization cannot be achieved by just increasing model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Scene-graph-to-image Generation</head><p>To test the contribution of our proposed Scene-Graph-to-layout approach to the overall task of SG-to-image generation, we further test it in an end-to-end pipeline for generating images. For Packed COCO and Packed VG, we compare our proposed approach with Sg2Im [17] using a fixed pre-trained LostGAN [51] as the layout-to-image generator. For CLEVR, we use WSGC and our own </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Inception Human COCO VG CLEVR Sg2Im <ref type="bibr">[17]</ref> 5.4 ± 0.3 7.6 ± 1.0 3.2% WSGC (ours) 5.6 ± 0.1 8.0 ± 1.1 96.8% GT Layout 5.5 ± 0.4 8.2 ± 1.0 - <ref type="table">Table 3</ref>: Results for SG-to-image on Packed datasets (16+ objects). For VG and COCO we use the layout-to-image architecture of LostGAN [50] and test the effect of different SG-to-layout models. For CLEVR, we use our AttSPADE generator.</p><p>AttSPADE generator (see Section 4). We trained the model on images with a maximum of 10 objects and tested on larger scenes with 16+ objects.</p><p>We evaluate performance using Inception score [46] and a study where Amazon Mechanical Turk raters were asked to rank the quality of two images: one generated using our layouts, and the other using SG2Im layouts. 11 Results are provided in <ref type="table">Table 3</ref>. For COCO and VG it can be seen that WSGC improves the overall quality of generated images. In CLEVR, <ref type="table">Table 3</ref>, WSGC outperforms Sg2Im in terms of IOU. In 96.8% of the cases, our generated images were ranked higher than SG2Im. Finally, <ref type="figure" target="#fig_3">Figures 5 and 8</ref> provide qualitative examples and comparisons of images generated based on CLEVR and COCO. More generation results on COCO and VG can be seen in the Supplementary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a method for mapping SGs to images that is invariant to a set of logical equivalences. Our experiments show that the method results in improved layouts and image quality. We also observe that canonical representations allow one to handle packed scenes with fewer layers than non-canonical approaches. Intuitively, this is because the closure calculation effectively propagates information across the graph, and thus saves the need for propagation using neural architectures. The advantage is that this step is hard-coded and not learned, thus reducing the size of the model. Our results show the advantage of preprocessing an SG before layout generation. Here we studied this in the context of two types of relation properties. However, it can be extended to more complex ones. In this case, finding the closure will be computationally hard, and would amount to performing inference in Markov Logic Networks <ref type="bibr">[45]</ref>. On the other hand, it is likely that modeling such invariances will result in further robustness of the learned models, and is thus an interesting direction for future work. In this supplementary file we provide additional implementation details, empirical results, and a proof of correctness for the SGC algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Scene-Graph-to-Layout</head><p>In Section 3.2, we introduced the WSGC-E and WSGC-S methods, two different procedures proposed for mapping an input scene graph into a weighted canonicalized relation graph. As mentioned in Section 3.2, although the WSGC-E is a natural extension of the SGC procedure, it is impractical for large complex graphs whereas the WSGC-S method adds fewer edges and is thus more practical for training. In what follows, we provide additional details about WSGC-E and WSGC-S, as well as comparison and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Exact Weighted Scene Graph Canonicalization (WSGC-E)</head><p>Next, we describe in detail the WSGC-E method for obtaining a weighted relation graph that is a natural extension of SGC. The WSGC-E begins with the user-specified graph E, with weights of one. Next two weighted completion steps are performed, corresponding to the SGC steps. Converse Completion: In SGC, this step adds all converse edges. In the weighted case it makes sense to add the converse edge with its corresponding converse weight. For example, if the graph E contains the edge (i, above, j, 1) and p conv (below|above) = 0.7, we add the edge (j, below, i, 0.7). See <ref type="figure" target="#fig_8">Figure 9b</ref>. Transitive Completion: In SGC, all transitive edges are found and added. In the weighted case, a natural alternative is to set a weight of a path to be the product of weights along this path, and set the weight of a completed edge (i, r, j) to be the maximum weight of a path between i and j times the probability p trans (r) that the relation is transitive. See <ref type="figure" target="#fig_8">Figure 9c</ref>. The maximum path weight problem is equivalent to maximizing the sum of log probabilities, and since these are all negative, this can be solved in polynomial time via a shortest weight path algorithm (e.g., FW). However, when there are many nodes and relations, runtime can still be substantial, and thus we offer a faster approach next.</p><p>Training with WSGC-E. In the main text, we described the training loss and optimization for WSGC-S. Optimizing the loss for WSGC-E is similar, as we explain next. We describe the loss of the WSGC-E method for a single input scene graph E and its ground truth layout Y . The parameters of the model are as follows: θ g are the parameters of the GCN in Section 4, θ trans are the parameters of the transitive probability (Eq. 1), and θ conv are those of the converse probability (Eq. 2). Let θ denote the set of all parameters. Denote the GCN applied to this graph by G θ g . We use L 1 as the loss between predicted and ground-truth bounding boxes Y . Namely, we wish to minimize the following objective (we write WSGC instead of WSGC-E below for brevity): When calculating L(θ), most of the operations are standard and are differentiated automatically by PyTorch. The only apparent complication is with the minimum weight path. However, we next explain why there is actually not a problem and one can simply take the gradient of the PyTorch computation graph for L(θ), which includes the minimum-weight-path computation.</p><formula xml:id="formula_6">L(θ) = Y − G θ g (WSGC(E; θ trans , θ conv )) 1 (4)</formula><p>Recall that in WSGC-E we first weight each edge by the corresponding converse weights p conv θ . Let w(e; θ) denote the weight of edge e after this weighting step. Next, we perform a transitive completion as follows. Given an edge e , its new weight will be (up to the multiplicative factor of p trans which we leave out for brevity): </p><p>where P are all the paths between the two incident nodes of e . To calculate the sub-gradient of this expression with respect to θ, we note that in the exponent we have a maximum over linear functions, and thus differentiating it wrt θ corresponds to finding the maximizing P * and then differentiating e∈P * log w(e; θ). Technically, the above suggests a very simple PyTorch implementation. Implement the computation graph for Eq. 4, including the non-differentiable maximum weight path computation. And then let PyTorch take gradients for this graph. Since the maximum-weight-path cannot be differentiated through, the computation will fix the maximizing path and take the gradient there, and this is indeed the correct sub-gradient, as per our discussion above.</p><p>The downside of the WSGC-E method is that it assigns weights to all edges in the graph, and for all relations, and thus computations involve a dense graph, which makes training and inference slow. This motivates our use of WSGC-S which uses sparser graphs. <ref type="table">Table 4</ref> shows a comparison of WSGC-E and WSGC-S on the standard COCO and VG datasets, where WSGC-E runs in a reasonable time so that comparison is possible. The size of the graphs on the standard datasets is less than an average of 1000 triplets per image, while on the packed datasets it is 24, 000 triplets per image. Thus it is impossible to run the WSGC-E on packed datasets. It can be seen that the methods achieve comparable performance, suggesting that indeed WSGC-S is a scalable alternative to WSGC-E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Empirical Comparison of WSGC-E and WSGC-S</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Analysis of Learned Weights</head><p>Our approach parameterizes the weights of converse and transitive relations and learns these parameters from data. It is interesting to see whether the learned weights recover known converse and transitive relations.</p><p>Inspecting the converse weights p conv that were learned on the standard COCO dataset reveals that all weights have converged to values close to 0 and 1, and align well with the expected true converse relation. Specifically, weights corresponding to converse pairs such as ("below", "above") all converged to 1, while the rest of the pairs, such as ("left of", "inside") converged to 0. For transitive weights p trans , 5/6 of the transitive relations correctly converged to 1 and a single relation to 0. Concretely, "above", "left of", "right of", "inside" and "below" converged to 1 while "surrounding" did not. The learned values are shown in <ref type="figure" target="#fig_10">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Weighted Graph Convolutional Network</head><p>In Section 4, we presented Graph Convolutional Network (GCN) [24] as a natural architecture for the SG to layout task. We use a similar approach to recent methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">17]</ref> for this task, but modify the GCN to our weighted scene graph. This is done by revising the graph convolution layer such that the aggregation step of each node is set to be a weighted average, where the weights are those in the canonical SG. In what follows, we provide additional details about our Weighted GCN.</p><p>Each object category c ∈ C is assigned a learned embedding φ c ∈ R D and each relation r ∈ R is assigned a learned embedding ψ r ∈ R D . Given an SG with N objects, the GCN iteratively calculates a representation for each object and each relation in the graph. Let v k i ∈ R d be the representation of the i th object in the k th layer of the GCN. Similarly, for each edge e = (i, r, j, w) in the graph let u k e ∈ R d be the representation of the relation in this edge. These representations are calculated as follows. Initially we set: v 0 i = φ o(i) , u 0 e = ψ r(e) , where r(e) is the relation for edge e. Next, we use three functions (MLPs) F s , F r , F o , each from R D × R D × R D to R D to obtain an updated object representation (see Section 4.1 for implementation details). These can be thought of as processing three vectors on an edge (the subject, relation and object representations) and returning three new representations. Given these functions, the updated object representation is the weighted average of all edges incident on i:</p><formula xml:id="formula_8">12 v t+1 i = 1 c   e=(i,r,j,w) wF s (v t i , u t e , v t j ) + e=(j,r,i,w) wF o (v t j , u t e , v t i )   (6)</formula><p>where c is a normalizing constant c = e=(i,r,j,w) w + e=(j,r,i,w) w. For the edge we set: u t+1 e = F r (v t+1 i , u t e , v t+1 j ). After iterating the GCN for L updates, the layout for node i is obtained by applying an MLP with four outputs to v L i . 13 Note that F s , F r , F o and w depend on learned parameters which are optimized using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Generalization on Packed Scenes</head><p>To further test the effect of model capacity from <ref type="table">Table 1</ref> in the paper, we even trained bigger Sg2Im models with 32, 64 layers on Packed COCO, resulting in IOU of 36.93, 11.65. We also trained a Sg2Im model with 1024 hidden units and 16 layers, and IOU deteriorated to 37.01. These results suggest that increasing the capacity of Sg2Im leads to overfitting and that WSGC improvement is indeed due to canonicalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Layout-to-Image with AttSPADE</head><p>For the CLEVR dataset [18], we use a novel generator, which we refer to as AttSPADE. This generator can be used for directly controlling attributes of the generated image, and this is not supported by other generators such as LostGAN. Although the generator is not the main focus of our contribution, we believe it is of independent interest, and thus describe it in some detail below, and show images that it generates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The AttSPADE Model</head><p>The key idea in the AttSPADE model is to condition generation on the attribues, as opposed to only the object class as done in current models. In what follows we describe the model.</p><p>We consider the case where a bounding box has an associated set of attributes. For example, the object category is an attribute and the size is an attribute (with possible values "small", "medium" and "large"). Additionally, if a segmentation mask is provided as input, it can be added as a binary attribute. We encode this set of attributes via a multi-hot vector z ∈ R r that is set to one for the corresponding attributes, and apply a FC layer to it to obtain a vector v ∈ R d . Next, we construct a tensor M ∈ R d×H×W where H and W are the boxes height and width and M [:, i, j] = v. This encodes the attributes for each pixel in the bounding box. 14 Finally, we use M as input to a SPADE [39] generator to obtain the generated image. Thus, our approach simply replaces the input of the SPADE model (which is just an object mask) with the tensor M . Lastly, our model uses two discriminators: one for the image (to achieve a better quality of the entire image), and one for the boxes (in order to better capture each box). This is similar to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">17]</ref> but with a few modifications (see next section). A high level description of the architecture is shown in <ref type="figure" target="#fig_11">Figure 11</ref>.  <ref type="table">Table 5</ref>: Quantitative comparisons for SG-to-image methods using Inception Score (higher is better), FID (lower is better) and Diversity Score (higher is better). Evaluation is done on the COCO-Stuff and VG datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Loss Functions</head><p>Our AttSPADE model contains several modifications of the loss functions. First, the generator is trained with the same multi-scale discriminator and loss function used in pix2pixHD <ref type="bibr" target="#b2">[55]</ref>, except we replace the squared error loss [30] with the hinge loss <ref type="bibr">[28,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b9">62]</ref>. Second, since our layout-to-image model generates the image from a given layout of bounding boxes, we add a box term loss to guarantee that the generated objects in these boxes look real. For this purpose, we crop the bounding boxes to create object images and train the discriminator to discriminate between real object images and generated object images. The image discriminator is implemented as in SPADE [39].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Baseline Models</head><p>We report generation results that vary both the layout being used and the layoutto-image component. For the layout we consider three options: (1) Ground truth layout.</p><p>(2) Our WSGC predicted layout.</p><p>(3) The layout used in Sg2Im <ref type="bibr">[17]</ref>. For the image generation we use three options: (1) Our AttSpade generator.</p><p>(2) The LostGAN generator [50] (the most recent state-of-the-art generation model).</p><p>(3) The Grid2Im <ref type="bibr" target="#b0">[1]</ref> generator, which uses the same graph model as <ref type="bibr">[17]</ref>. The results reported in <ref type="bibr" target="#b0">[1]</ref> use a coarse version of the GT layout (i.e., the layout rounded to a 5 × 5 grid). Since this variant comes close to actually using the GT layout, we also consider an additional version of <ref type="bibr" target="#b0">[1]</ref> that does not use this information. We refer to this version as "Grid2Im No-Att" (code provided by the authors of <ref type="bibr" target="#b0">[1]</ref>). For a fair comparison, all models were tested with the same external code evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>The results in <ref type="table">Table 5</ref> suggest that the AttSPADE model improves over previous approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">50]</ref> when generating an image from a GT layout, in both resolutions. In addition, our end-to-end model, which includes the WSGC and AttSPADE model, outperforms most of the baselines on the COCO and Visual Genome datasets. <ref type="figure" target="#fig_1">Figure 13</ref> shows a direct comparison between different generators using GT layout for COCO. It can be seen that AttSPADE provides higher quality images than the other generators. <ref type="figure" target="#fig_2">Figure 14</ref> shows different generators that use both GT and generated layouts for COCO. Additional qualitative results on Visual Genome can be seen in <ref type="figure" target="#fig_3">Figure 15</ref> and <ref type="figure">Figure 16</ref>. In the generation results it can be seen that AttSPADE produces more realistic images, when compared to other generators. Furthermore when using WSGC layout the images are qualitatively similar to using GT layout, which suggests that WSGC produces high quality layouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthetic dataset</head><p>In Section 6, a synthetic dataset which was used to explore properties of the suggested WSGC model was presented. Example cases from this dataset are included in <ref type="figure" target="#fig_0">Figure 12</ref>. More specifically, this dataset was utilized to evaluate the contribution of the transitivity closure on the scene-graph-to-layout task.</p><p>Every object in this data is a square with one of two possible sizes, small or large. The set of relations includes: To generate SG-layout pairs for training and evaluation, we uniformly sample coordinates of object centers and object sizes and automatically compute relations among object pairs based on their spatial locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Packed Datasets</head><p>Here we describe the specific characteristics of the packed datasets presented in the paper. For every packed dataset, only samples with at least 16 objects per image were included. The method for constructing relations for COCO and CLEVR is as described next. For VG, since Standard VG contains a limited number of relations we supplement the dataset with relations as follows. For every two graph nodes, edges representing geometric relations such as:"left", "right", "above", "below", "inside" and "surrounding" are constructed based on  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">AttSPADE</head><p>We apply Spectral Norm [34] to all the layers in both generator and discriminator. We use the ADAM solver [22] with β 1 = 0.5 and β 2 = 0.999, and a learning rate of 0.0001 for both the generator and the discriminator. All the experiments are conducted on NVIDIA V100 GPUs. We use PyTorch synchronized BatchNorm with the following batch sizes: 32 for 128 × 128 and 16 for 256 × 256 resolutions (statistics are collected from all the GPUs). The FC layer that calculates v ∈ R d (used to construct tensor M . See Section 2), is set d to 128.</p><p>5 Proof that SGC outputs the closure C(E) (Section 3.1) Lemma 1. The SGC procedure described in Section 3.1 of the main paper outputs the closure C(E).</p><p>Proof. Let G = (O, E). DenoteĈ be the canonicalization procedure proposed. To showĈ(E) = C(E), it suffices to prove that (1) C(E) ⊆Ĉ(E) and (2) C(E) ⊆ C(E).</p><p>Proof thatĈ(E) ⊆ C(E):. Let there be e ∈Ĉ(E) s.t e = (i, r, j). We split into cases by e construction:</p><p>-Original graph edge. if e ∈ E then by C definition e ∈ C(E).</p><p>-Converse constructed edge. Therefore there exists r ∈ R such that (r, r ) ∈ R conv and (j, r , i) ∈ E. Then (j, r , i) ∈ C(E) and therefore (i, r, j) = e ∈ C(E) by definition. -Transitive constructed edge. Since e was constructed in the T ransitivity step, it must hold that r ∈ R trans and e was contained in the transitive closure of r. Therefore, after the ConverseRelations step, there existed a directed path p = (o v1 , ..., o v k ) with respect to r where v 1 = i and v k = j.</p><p>To prove e ∈ C(E), it is enough to show that for every edge in p it is also in C(E). From here, since C respects transitivity, this will follow. Namely, let there be e = (i , r, j ) ∈ {(o vm , o vm+1 )|m ∈ {1, .., k}}. If e ∈ E, then e ∈ C(E) and we are done. Otherwise, by the ConverseRelations construction step, there exists r such that (r, r ) ∈ R conv and (j , r , i ) ∈ E. Therefore, it follows that (j , r , i ) ∈ C(E) and e ∈ C(E) and we are done.</p><p>Proof that C(E) ⊆Ĉ(E): For every e = (i, r, j) ∈ C(E) we need to show that e ∈Ĉ(E). Since e ∈ C(E), e is a relation implied by E. If e ∈ E, sinceĈ does not drop edges, it holds that e ∈Ĉ(E) and we're done. Otherwise, we assume by contradiction that e / ∈Ĉ(E). let p = (o v1 , ..., o v k ) be a directed path from o i to o j in C(E). Then, there exists e = (i , r, j ) ∈ {(o vi , o vi+1 )|i ≤ k} where e / ∈Ĉ(E). Otherwise, if there is no such e , we get that there is a directed path between o i to o j and by T ransitivity step construction e ∈Ĉ(E). Therefore, there must be e conv ∈ E, such that e conv = (j, r , i) and (r, r ) ∈ R conv . However, from the ConverseRelations step construction, if there exists such edge we get that e ∈Ĉ(E), in contrary to the assumption that e / ∈Ĉ(E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Generalization on Semantically Equivalent Graphs</head><p>Results in <ref type="table" target="#tab_3">Table 2</ref> of the main paper demonstrate that the learned WSGC model is more robust to changes in the scene graph input. In this experiment, we randomly transform each test sample scene graph into a semantically equivalent one, and test models on the resulting sample. To generate such samples from a given scene graph, we start by calculating all the possible location-based relations for any pair of objects. Then, for each pair of objects we use prior knowledge to identify pairs of converse relations, and drop one of the edges in such pairs with probability p = 0.5. After this step, we compute the transitive closure with respect to each relation and randomly drop (p = 0.5) each edge that does not change the semantics of the scene graph.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Proposed Scene Graph to Layout architecture. (a) An input scene graph. (b) The graph is first canonicalized using our WSGC method in Section 3.2. Dashed edges correspond to completed relations that are assigned with weights. (c) A GCN is applied to the weighted graph, resulting in bounding box coordinates. (d) The GCN outputs are used to generate the predicted layout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Fig. 3 :</head><label>13</label><figDesc>Illustration of WSGC-S. (a) The input graph. (b) Converse edges (brown arrows) are sampled from p conv and assigned a weight 1 (here two edges were sampled). (c) Transitive edges (green arrows) are completed and assigned a weight p trans .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Demonstration of the AttSPADE generator for scene graphs with varying attributes. Top row shows SGs where each column modifies one attribute. Bottom row is the images generated by AttSPADE. For the layout-to-image component, most of our experiments use a pretrained LostGAN model. For CLEVR (Figure 4) we train our AttSPADE model which is a variant of SPADE [39] and trained similarly (see Supplementary).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Examples of image generation for CLEVR where the Sg2Im baseline and our WSGC model were trained on images with a maximum of 10 objects but tested on scenes with 16+ objects. Shown are three examples where: Top row: our WSGC generation (with boxes and without). Bottom row: Sg2Im generation (with boxes and without).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Trained on 16 objects Fig. 6: Synthetic dataset results. (a-b) The effect of the number of GCN layers on accuracy. Curves denote IOU performance as a function of the number of objects. Each point is a model trained and tested on a fixed number of objects given by the x axis. (c) Out of sample number of objects. The model is trained on 16 objects and evaluated on up to 128 objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Generalization from Semantically Equivalent Graphs. Each input SG is changed to a semantically equivalent SG at test time. The layout-to-image model is Lost-GAN [50] and different SG-to-layout models are tested. (a) Original SG (partial). (b) A modified semantically equivalent SG (partial). (c) GT image. (d-e) Sg2Im [17] and WSGC for the original SG. (f-g) Sg2Im [17] and WSGC for the modified SG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Selected Scene-graph-to-image generation results on the Packed-COCO dataset. Here, we fix the layout-to-image model to LostGAN [50], while changing different scene graph-to-layout models. (a) GT image. (b) Generation from GT layout. (c) Sg2Im [17] model with LostGAN [50]. (d) Our WSGC model with LostGAN [50].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Vision. pp. 4561-4569 (2019) 2. Bau, D., Zhu, J.Y., Strobelt, H., Zhou, B., Tenenbaum, J.B., Freeman, W.T., Torralba, A.: Gan dissection: Visualizing and understanding generative adversarial networks. In: Proceedings of the International Conference on Learning Representations (ICLR) (2019) 3. Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high fidelity natural image synthesis. In: International Conference on Learning Representations (2019) 4. Caesar, H., Uijlings, J.R.R., Ferrari, V.: Coco-stuff: Thing and stuff classes in context. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 5. Che, T., Li, Y., Jacob, A.P., Bengio, Y., Li, W.: Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136 (2016) 6. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded refinement networks. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 1511-1520 (2017) 7. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Infogan:Interpretable representation learning by information maximizing generative adversarial nets. In: Advances in neural information processing systems. pp. 2172-2180 (2016) 8. Deng, Z., Chen, J., Fu, Y., Mori, G.: Probabilistic neural programmed networks for scene generation. In: Advances in Neural Information Processing Systems. pp. 4028-4038 (2018) 9. Dong, H., Yu, S., Wu, C., Guo, Y.: Semantic image synthesis via adversarial learning. In:Proceedings of the IEEE International Conference on Computer Vision. pp. 5706-5714 (2017) 10. Floyd, R.W.: Algorithm 97: shortest path. Communications of the ACM 5(6), 345 (1962) 11. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural information processing systems. pp. 2672-2680 (2014) 12. Herzig, R., Levi, E., Xu, H., Gao, H., Brosh, E., Wang, X., Globerson, A., Darrell, T.: Spatio-temporal action graph networks. In: The IEEE International Conference on Computer Vision (ICCV) Workshops (Oct 2019) 13. Herzig, R., Raboh, M., Chechik, G., Berant, J., Globerson, A.: Mapping images to scene graphs with permutation-invariant structured prediction. In: Advances in Neural Information Processing Systems (NIPS) (2018) 14. Hong, S., Yang, D., Choi, J., Lee, H.: Inferring semantic layout for hierarchical textto-image synthesis. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7986-7994 (2018) 15. Huang, X., Liu, M.Y., Belongie, S., Kautz, J.: Multimodal unsupervised imageto-image translation. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 172-189 (2018) 16. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1125-1134 (2017) 17. Johnson, J., Gupta, A., Fei-Fei, L.: Image generation from scene graphs. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 18. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C.L., Girshick, R.: Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In: CVPR (2017) 19. Johnson, J., Krishna, R., Stark, M., Li, L.J., Shamma, D., Bernstein, M., Fei-Fei, L.: Image retrieval using scene graphs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3668-3678 (2015) 20. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4401-4410 (2019) 21. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014) 22. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Int. Conf. on Learning Representations (2015) 23. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013) 24. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016) 25. Krishna, R., Chami, I., Bernstein, M.S., Fei-Fei, L.: Referring relationships. ECCV (2018) 26. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., Bernstein, M., Fei-Fei, L.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. ArXiv e-prints (2016) 27. Li, Y., Ma, T., Bai, Y., Duan, N., Wei, S., Wang, X.: Pastegan: A semi-parametric method to generate image from scene graph. NeurIPS (2019) 28. Lim, J.H., Ye, J.C.: Geometric gan. arXiv preprint arXiv:1705.02894 (2017) 29. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation networks. In: Advances in neural information processing systems. pp. 700-708 (2017) 30. Mao, X., Li, Q., Xie, H., Lau, Y.R., Wang, Z., Smolley, S.P.: Least squares generative adversarial networks. In: Proc. Int. Conf. Comput. Vision (2017) 31. Materzynska, J., Xiao, T., Herzig, R., Xu, H., Wang, X., Darrell, T.: Somethingelse: Compositional action recognition with spatial-temporal interaction networks. In: CVPR (2020) 32. Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784 (2014) 33. Mittal, G., Agrawal, S., Agarwal, A., Mehta, S., Marwah, T.: Interactive image generation using scene graphs. arXiv preprint arXiv:1905.03743 (2019) 34. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for generative adversarial networks. In: Int. Conf. on Learning Representations (2018) 35. Nam, S., Kim, Y., Kim, S.J.: Text-adaptive generative adversarial networks: manipulating images with natural language. In: Advances in Neural Information Processing Systems. pp. 42-51 (2018) 36. Odena, A., Olah, C., Shlens, J.: Conditional image synthesis with auxiliary classifier gans. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70. pp. 2642-2651. JMLR. org (2017) 37. Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al.: Conditional image generation with pixelcnn decoders. In: Advances in neural information processing systems. pp. 4790-4798 (2016) 38. Oord, A.v.d., Kalchbrenner, N., Kavukcuoglu, K.: Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759 (2016) 39. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with spatially-adaptive normalization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2337-2346 (2019) 40. Qiao, T., Zhang, J., Xu, D., Tao, D.: Mirrorgan: Learning text-to-image generation by redescription. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1505-1514 (2019) 41. Raboh, M., Herzig, R., Chechik, G., Berant, J., Globerson, A.: Differentiable scene graphs. In: Winter Conf. on App. of Comput. Vision (2020) 42. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 (2015) 43. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396 (2016) 44. Reed, S.E., Akata, Z., Mohan, S., Tenka, S., Schiele, B., Lee, H.: Learning what and where to draw. In: Advances in Neural Information Processing Systems. pp. 217-225 (2016) 45. Richardson, M., Domingos, P.: Markov logic networks. Machine learning 62(1-2), 107-136 (2006) 46. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.: Improved techniques for training gans. In: Advances in neural information processing systems. pp. 2234-2242 (2016) 47. Schroeder, B., Tripathi, S., Tang, H.: Triplet-aware scene graph embeddings. In: The IEEE International Conference on Computer Vision (ICCV) Workshops (Oct 2019) 48. Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., Manning, C.D.: Generating semantically precise scene graphs from textual descriptions for improved image retrieval. In: Proceedings of the fourth workshop on vision and language. pp. 70-80 (2015) 49. Sharma, S., Suhubdy, D., Michalski, V., Kahou, S.E., Bengio, Y.: Chatpainter: Improving text to image generation using dialogue. arXiv preprint arXiv:1802.08216 (2018) 50. Sun, W., Wu, T.: Image synthesis from reconfigurable layout and style. In: The IEEE International Conference on Computer Vision (ICCV) (October 2019) 51. Sun, W., Wu, T.: Image synthesis from reconfigurable layout and style. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 10531-10540 (2019) 52. Taigman, Y., Polyak, A., Wolf, L.: Unsupervised cross-domain image generation. arXiv preprint arXiv:1611.02200 (2016) 53. Tan, F., Feng, S., Ordonez, V.: Text2scene: Generating compositional scenes from textual descriptions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6710-6719 (2019)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>An illustration of WSGC-E where relations are Left (L) and Right (R). (a) The input graph contains two relations with weight 1. (b) Converse edges (blue dashed arrows) are completed with the weights p conv . (c) Transitive edges (green dashed arrows) are added and assigned the weight of the corresponding path times p trans .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>w</head><label></label><figDesc>trans (e ; θ) = max P ∈P e∈P w(e; θ) = e max P ∈P e∈P log w(e;θ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Learned p conv and p trans weights for the WSGC-S model on the COCO dataset. The learned values of p conv (see a) and of p trans (see b) are presented as function of training iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Generating images with our AttSPADE model. Given a layout of boxes, our model generates an image using the layout into a series of residual blocks with upsampling layers. The layout is modeled by multiple semantic attributes per box rather than a single class descriptor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>-</head><label></label><figDesc>Above -The center of the subject is above the object. This relation is transitive. -OppositeHorizontally -The subject and the object are on opposite sides of the image with respect to the middle vertical line. This relation is not transitive. -XN ear -The subject and object are within distance equal to 10% of the image with respect to the x coordinate of each center. This relation is not transitive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 :</head><label>12</label><figDesc>Example of synthetic dataset samples. In these samples, the scene graph relations are overlaid on top of the ground truth layout. Every edge is described with a corresponding relation type and every square object is annotated with an object type: "S" for small and "L" for large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 :</head><label>13</label><figDesc>Selected GT layout-to-image generation results on COCO-Stuff dataset on 128 × 128 resultion. Here, we compare our AttSPADE model, Grid2Im [1] and Lost-GAN [50] on generation from GT layout of masks. (a) GT layout (only masks). (b) GT image. (c) Generation with LostGAN [50] model. (d) Generation with Grid2Im [1]. (e) Generation with AttSPADE model (ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 :</head><label>14</label><figDesc>Selected generation results on the COCO-Stuff dataset at 256 × 256 resolution. Here, we compare our AttSPADE model and Grid2Im [1] in two different settings: generation from GT layout of masks and generation from scene graphs. (a) GT scene graph. (b) GT layout (only masks). (c) GT image. (d) Generation with Grid2Im [1] using the GT layout. (e) Generation with Grid2Im No-att [1] from the scene graph (GT layout not used). (f) Generation with AttSPADE model (ours) using the GT layout. (g) Generation with WSGC + AttSPADE model (ours) from the scene graph (GT layout not used).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 :</head><label>15</label><figDesc>Selected scene-graph-to-image results on Visual Genome dataset on 128 × 128 resolution. Here, we compare our AttSPADE model and LostGAN [50] in two different settings: generation from GT layout of boxes and generation from scene graphs. (a) GT scene graph. (b) GT layout (only boxes). (c) GT image. (d) Generation using LostGAN [50] from the GT layout. (e) Generation with the WSGC + LostGAN [50] from the scene graph (GT layout not used). (f) Generation with the AttSPADE model (ours) from the GT Layout. (g) Generation with the WSGC + AttSPADE model (ours) from the scene graph (GT layout not used).Fig. 16: Selected scene-graph-to-image results on the Visual Genome dataset at 256 × 256 resolution. Here, we test our AttSPADE model in two different settings: generation from GT layout of boxes and generation from scene graphs. (a) GT scene graph. (b) GT layout (only boxes). (c) GT image. (d) Generation with the AttSPADE model (ours) from the GT Layout. (e) Generation with the WSGC + AttSPADE model (ours) from the scene graph (GT layout not used).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>See Supplementary file for further visual examples. COCO-Stuff 2017 [4]. Contains pixel-level annotations with 40K train and 5K validation images with bounding boxes and segmentation masks for 80 thing categories, and 91 stuff categories. We use the standard subset proposed in previous works [17], which contains ∼25K training, 1024 validation, and 2048 in test. We use an additional subset we call Packed COCO, containing images with at least 16 objects, resulting in 4, 341 train images, 238 validation, and 238 test. Visual Genome (VG) [26]. Contains 108, 077 images with SGs. We use the standard subset [17]: 62K training, 5506 validation and 5088 test images. We</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>GCN 10 41.7 16.9 62.6 24.7 37.5 9.7 35.8 25.4 56.0 36.2 25.3 15.8 Sg2Im [17] 8 GCN 10 41.5 18.3 62.9 26.2 38.1 10.6 37.2 25.8 58.6 36.9 26.4 15.9 Sg2Im [17] 16 GCN 10 40.8 16.4 61.4 23.3 36.6 7.8 37.7 27.1 60.3 39.0 26.6 17.0 WSGC 5 GCN (ours) 41.9 18.0 63.3 25.9 38.2 10.6 39.3 28.5 62.6 42.4 30.1 18.3</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Standard</cell><cell></cell><cell></cell><cell></cell><cell>Packed</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>mIOU</cell><cell></cell><cell>R@0.3</cell><cell>R@0.5</cell><cell>mIOU</cell><cell></cell><cell>R@0.3</cell><cell></cell><cell>R@0.5</cell></row><row><cell></cell><cell cols="10">COCO VG COCO VG COCO VG COCO VG COCO VG COCO VG</cell></row><row><cell>Sg2Im [17] 5 GCN 9</cell><cell>-</cell><cell>-</cell><cell cols="2">52.4 21.9 32.2 10.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Sg2Im [17] 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Evaluating the robustness of the learned canonical representation for models which were trained on Packed COCO. For each SG, a semantically equivalent SG is sampled and evaluated at test time. Additionally, models are evaluated on Noisy SGs, for which edges contain 10% randomly chosen relations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>relative (x,y) coordinates. Redundant edges are removed such that the graph is minimal. This procedure differs from the one used in [17] in two ways: first, in[17], the decision to construct such edges is based on angles between two objects and second, in [17], there can be up to a single constructed edge for every pair of objects and the decision whether to construct or not is random. Hence, the procedure proposed here results in graphs that are more complex w.r.t number of edges and are more informative.In the WSGC GCN model, we follow the implementation details proposed in[17]. We use 5 hidden layers and an embedding layer of 128 units for each object and relation. The functions F s , F r , F o which were presented in Section 4, are all implemented as a single 3 layers MLP with 512 units per layer. For optimization we use Adam [21], where for θ conv , θ trans we use LR of 1e −2 and otherwise we use 1e −4 .</figDesc><table><row><cell>4 Implementation Details</cell></row><row><cell>4.1 Scene-Graph-to-layout</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We note that human raters don't typically include all logically equivalent relations. We analyzed data and found only small fraction of these are annotated in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Objects in SGs also contain attributes but we drop these for notational simplicity. 4 This is because empirical graphs E are created by human annotators, who typically skip redundant edges that can be inferred from other edges.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Equivalence of course depends on what relations are considered, but we do not specify this directly to avoid notational clutter.6  We note that we could have added an option for symmetric relations, but we do not include these, as they not exhibited in the datasets we consider.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We could sample multiple times and average, but this is not necessary in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We sample just one instantiation ofZ per image, since this works well in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Results copied from manuscript. 10 Our implementation of[17]. This is the same as our model without WSGC.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">We used raters only for the CLEVR data, where no GT images or bounding boxes are available for 16+ objects, and thus Inception cannot be evaluated.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Note that a box can appear both as a "subject" and an "object" thus two different sums in the denominator and the normalization is needed because we want to obtain a new single object representation while the number of object occurrences is varied.13  The MLP has a sigmoid activation in the last layer so that the predicted normalized bounding box coordinates are in [0, 1].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">We note that different pixels may have different attributes in principle although we don't use this here.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080). Prof. Darrell's group was supported in part by DoD, NSF, BAIR, and BDD. This work was completed in partial fulfillment for the Ph.D degree of the first author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Specifying object attributes and relations in interactive scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer</title>
		<meeting>the IEEE International Conference on Computer</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Heuristics for image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bastidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR LLD workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tell me where i am: Object-level scene context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiaotian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rynson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 32nd meeting of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scene graph captioner: Image captioning based on structural visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="page" from="477" to="485" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantics disentangling for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2327" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8584" to="8593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<title level="m">Energy-based generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text guided person image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3663" to="3672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="465" to="476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
