<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
							<email>lei.shi@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
							<email>yfzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<email>jcheng@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeletonbased action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition methods based on skeleton data have been widely investigated and attracted considerable attention due to their strong adaptability to the dynamic circumstance and complicated background <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b34">34]</ref>. Conventional * Corresponding Author deep-learning-based methods manually structure the skeleton as a sequence of joint-coordinate vectors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref> or as a pseudo-image <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>, which is fed into RNNs or CNNs to generate the prediction. However, representing the skeleton data as a vector sequence or a 2D grid cannot fully express the dependency between correlated joints. The skeleton is naturally structured as a graph in a non-Euclidean space with the joints as vertexes and their natural connections in the human body as edges. The previous methods cannot exploit the graph structure of the skeleton data and are difficult to generalize to skeletons with arbitrary forms. Recently, graph convolutional networks (GCNs), which generalize convolution from image to graph, have been successfully adopted in many applications <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b14">15]</ref>. For the skeletonbased action recognition task, Yan et al. <ref type="bibr" target="#b32">[32]</ref> first apply GCNs to model the skeleton data. They construct a spatial graph based on the natural connections of joints in the human body and add the temporal edges between corresponding joints in consecutive frames. A distance-based sampling function is proposed for constructing the graph convolutional layer, which is then employed as a basic module to build the final spatiotemporal graph convolutional network (ST-GCN).</p><p>However, there are three disadvantages for the process of the graph construction in ST-GCN <ref type="bibr" target="#b32">[32]</ref>: <ref type="bibr" target="#b0">(1)</ref> The skeleton graph employed in ST-GCN is heuristically predefined and represents only the physical structure of the human body. Thus it is not guaranteed to be optimal for the action recognition task. For example, the relationship between the two hands is important for recognizing classes such as "clapping" and "reading." However, it is difficult for ST-GCN to capture the dependency between the two hands since they are located far away from each other in the predefined human-body-based graphs. <ref type="bibr" target="#b1">(2)</ref> The structure of GCNs is hierarchical where different layers contain multilevel semantic information. However, the topology of the graph ap-plied in ST-GCN is fixed over all the layers, which lacks the flexibility and capacity to model the multilevel semantic information contained in all of the layers; (3) One fixed graph structure may not be optimal for all the samples of different action classes. For classes such as "wiping face" and "touching head", the connection between the hands and head should be stronger, but it is not true for some other classes, such as "jumping up" and "sitting down". This fact suggests that the graph structure should be data dependent, which, however, is not supported in ST-GCN.</p><p>To solve the above problems, a novel adaptive graph convolutional network is proposed in this work. It parameterizes two types of graphs, the structure of which are trained and updated jointly with convolutional parameters of the model. One type is a global graph, which represents the common pattern for all the data. Another type is an individual graph, which represents the unique pattern for each data. Both of the two types of graphs are optimized individually for different layers, which can better fit the hierarchical structure of the model. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples.</p><p>Another notable problem in ST-GCN is that the feature vector attached to each vertex only contains 2D or 3D coordinates of the joints, which can be regarded as the first-order information of the skeleton data. However, the second-order information, which represents the feature of bones between two joints, is not exploited. Typically, the lengths and directions of bones are naturally more informative and discriminative for action recognition. In order to exploit the second-order information of the skeleton data, the lengths and directions of bones are formulated as a vector pointing from its source joint to its target joint. Similar to the firstorder information, the vector is fed into an adaptive graph convolutional network to predict the action label. Moreover, a two-stream framework is proposed to fuse the first-order and second-order information to further improve the performance.</p><p>To verify the superiority of the proposed model, namely, the two-stream adaptive graph convolutional network (2s-AGCN), extensive experiments are performed on two large-scale datasets: NTU-RGBD <ref type="bibr" target="#b27">[27]</ref> and Kinetics-Skeleton <ref type="bibr" target="#b11">[12]</ref>. Our model achieves state-of-the-art performance on both of the datasets.</p><p>The main contributions of our work lie in three folds: (1) An adaptive graph convolutional network is proposed to adaptively learn the topology of the graph for different GCN layers and skeleton samples in an end-to-end manner, which can better suit the action recognition task and the hierarchical structure of the GCNs. (2) The secondorder information of the skeleton data is explicitly formulated and combined with the first-order information using a two-stream framework, which brings notable improvement for the recognition performance. (3) On two large-scale datasets for skeleton-based action recognition, the proposed 2s-AGCN exceeds the state-of-the-art by a significant margin. The code will be released for future work and to facilitate communication 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Skeleton-based action recognition</head><p>Conventional methods for skeleton-based action recognition usually design handcrafted features to model the human body <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b7">8]</ref>. However, the performance of these handcrafted-feature-based methods is barely satisfactory since it cannot consider all factors at the same time. With the development of deep learning, data-driven methods have become the mainstream methods, where the most widely used models are RNNs and CNNs. RNN-based methods usually model the skeleton data as a sequence of the coordinate vectors each represents a human body joint <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b2">3]</ref>. CNN-based methods model the skeleton data as a pseudo-image based on the manually designed transformation rules <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>. The CNNbased methods are generally more popular than RNN-based methods because the CNNs have better parallelizability and are easier to train than RNNs.</p><p>However, both RNNs and CNNs fail to fully represent the structure of the skeleton data because the skeleton data are naturally embedded in the form of graphs rather than a vector sequence or 2D grids. Recently, Yan et al. <ref type="bibr" target="#b32">[32]</ref> propose a spatiotemporal graph convolutional network (ST-GCN) to directly model the skeleton data as the graph structure. It eliminates the need for designing handcrafted part assignment or traversal rules, thus achieves better performance than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph convolutional neural networks</head><p>There have been many works on graph convolution, and the principle of constructing GCNs mainly follows two streams: spatial perspective and spectral perspective <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b14">15]</ref>. Spatial perspective methods directly perform the convolution filters on the graph vertexes and their neighbors, which are extracted and normalized based on manually designed rules <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b14">15]</ref>. In contrast to the spatial perspective methods, spectral perspective methods utilize the eigenvalues and eigenvectors of the graph Laplace matrices. These methods perform the graph convolution in the frequency domain with the help of the graph Fourier transform <ref type="bibr" target="#b28">[28]</ref>, which does not need to extract locally connected regions from graphs at each convolutional step <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b4">5]</ref>. This work follows the spatial perspective methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph Convolutional Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph construction</head><p>The raw skeleton data in one frame are always provided as a sequence of vectors. Each vector represents the 2D or 3D coordinates of the corresponding human joint. A complete action contains multiple frames with different lengths for different samples. We employ a spatiotemporal graph to model the structured information among these joints along both the spatial and temporal dimensions. The structure of the graph follows the work of ST-GCN <ref type="bibr" target="#b32">[32]</ref>. The left sketch in <ref type="figure" target="#fig_0">Fig. 1</ref> presents an example of the constructed spatiotemporal skeleton graph, where the joints are represented as vertexes and their natural connections in the human body are represented as spatial edges (the orange lines in <ref type="figure" target="#fig_0">Fig. 1</ref>, left). For the temporal dimension, the corresponding joints between two adjacent frames are connected with temporal edges (the blue lines in <ref type="figure" target="#fig_0">Fig. 1, left)</ref>. The coordinate vector of each joint is set as the attribute of the corresponding vertex. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph convolution</head><p>Given the graph defined above, multiple layers of spatiotemporal graph convolution operations are applied on the graph to extract the high-level features. The global average pooling layer and the sof tmax classifier are then employed to predict the action categories based on the extracted features.</p><p>In the spatial dimension, the graph convolution operation on vertex v i is formulated as <ref type="bibr" target="#b32">[32]</ref>:</p><formula xml:id="formula_0">f out (v i ) = vj ∈Bi 1 Z ij f in (v j ) · w(l i (v j ))<label>(1)</label></formula><p>where f denotes the feature map and v denotes the vertex of the graph. B i denotes the sampling area of the convolution for v i , which is defined as the 1-distance neighbor vertexes (v j ) of the target vertex (v i ). w is the weighting function similar to the original convolution operation, which provides a weight vector based on the given input. Note that the number of weight vectors of convolution is fixed, while the number of vertexes in B i is varied. To map each vertex with a unique weight vector, a mapping function l i is designed specially in ST-GCN <ref type="bibr" target="#b32">[32]</ref>. The right sketch in <ref type="figure" target="#fig_0">Fig. 1</ref> shows this strategy, where × represents the center of gravity of the skeleton. B i is the area enclosed by the curve. In detail, the strategy empirically sets the kernel size as 3 and naturally divides B i into 3 subsets: S i1 is the vertex itself (the red circle in <ref type="figure" target="#fig_0">Fig. 1, right)</ref>; S i2 is the centripetal subset, which contains the neighboring vertexes that are closer to the center of gravity (the green circle); S i3 is the centrifugal subset, which contains the neighboring vertexes that are farther from the center of gravity (the blue circle). Z ij denotes the cardinality of S ik that contains v j . It aims to balance the contribution of each subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>The implementation of the graph convolution in the spatial dimension is not straightforward. Concretely, the feature map of the network is actually a C × T × N tensor, where N denotes the number of vertexes, T denotes the temporal length and C denotes the number of channels. To implement the ST-GCN, Eq. 1 is transformed into</p><formula xml:id="formula_1">f out = Kv k W k (f in A k ) M k<label>(2)</label></formula><p>where K v denotes the kernel size of the spatial dimension. With the partition strategy designed above, K v is set to 3.</p><formula xml:id="formula_2">A k = Λ − 1 2 kĀ k Λ − 1 2</formula><p>k , whereĀ k is similar to the N × N adjacency matrix, and its elementĀ ij k indicates whether the vertex v j is in the subset S ik of vertex v i . It is used to extract the connected vertexes in a particular subset from f in for the corresponding weight vector. Λ ii k = j (Ā ij k ) + α is the normalized diagonal matrix. α is set to 0.001 to avoid empty rows. W k is the C out × C in × 1 × 1 weight vector of the 1 × 1 convolution operation, which represents the weighting function w in Eq. 1. M k is an N × N attention map that indicates the importance of each vertex. denotes the dot product.</p><p>For the temporal dimension, since the number of neighbors for each vertex is fixed as 2 (corresponding joints in the two consecutive frames), it is straightforward to perform the graph convolution similar to the classical convolution operation. Concretely, we perform a K t × 1 convolution on the output feature map calculated above, where K t is the kernel size of temporal dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adaptive graph convolutional layer</head><p>The spatiotemporal graph convolution for the skeleton data described above is calculated based on a predefined graph, which may not be the best choice as explained in Sec. 1. To solve this problem, we propose an adaptive graph convolutional layer. It makes the topology of the graph optimized together with the other parameters of the network in an end-to-end learning manner. The graph is unique for different layers and samples, which greatly increases the flexibility of the model. Meanwhile, it is designed as a residual branch, which guarantees the stability of the original model.</p><p>In detail, according to Eq. 2, the topology of the graph is actually decided by the adjacency matrix and the mask, i.e., A k and M k , respectively. A k determines whether there are connections between two vertexes and M k determines the strength of the connections. To make the graph structure adaptive, we change Eq. 2 into the following form:</p><formula xml:id="formula_3">f out = Kv k W k f in (A k + B k + C k )<label>(3)</label></formula><p>The main difference lies in the adjacency matrix of the graph, which is divided into three parts: A k , B k and C k . The first part (A k ) is the same as the original normalized N × N adjacency matrix A k in Eq. 2. It represents the physical structure of the human body.</p><p>The second part (B k ) is also an N × N adjacency matrix. In contrast to A k , the elements of B k are parameterized and optimized together with the other parameters in the training process. There are no constraints on the value of B k , which means that the graph is completely learned according to the training data. With this data-driven manner, the model can learn graphs that are fully targeted to the recognition task and more individualized for different information contained in different layers. Note that the element in the matrix can be an arbitrary value. It indicates not only the existence of the connections between two joints but also the strength of the connections. It can play the same role of the attention mechanism performed by M k in Eq. 2However, the original attention matrix M k is dot multiplied to A k , which means that if one of the elements in A k is 0, it will always be 0 irrespective the value of M k . Thus, it cannot generate the new connections that not exist in the original physical graph. From this perspective, B k is more flexible than M k .</p><p>The third part (C k ) is a data-dependent graph which learn a unique graph for each sample. To determine whether there is a connection between two vertexes and how strong the connection is, we apply the normalized embedded Gaussian function to calculate the similarity of the two vertexes: <ref type="figure">Figure 2</ref>. Illustration of the adaptive graph convolutional layer. There are a total of three types of graphs in each layer, i.e., A k , B k and C k . The orange box indicates that the parameter is learnable. (1 × 1) denotes the kernel size of convolution. Kv denotes the number of subsets. ⊕ denotes the elementwise summation. ⊗ denotes the matirx multiplication. The residual box (dotted line) is only needed when Cin is not the same as Cout.</p><formula xml:id="formula_4">f (v i , v j ) = e θ(vi) T φ(vj ) N j=1 e θ(vi) T φ(vj )<label>(4)</label></formula><formula xml:id="formula_5">f f (1 × 1) (1 × 1) (1 × 1) × × × × × × × (1 × 1) = 3 ×</formula><p>where N is the total number of the vertexes. We use the dot product to measure the similarity of the two vertexes in an embedding space. In detail, given the input feature map f in whose size is C in ×T ×N , we first embed it into C e ×T ×N with two embedding functions, i.e., θ and φ. Here, through extensive experiments, we choose one 1 × 1 convolutional layer as the embedding function. The two embedded feature maps are rearranged and reshaped to an N ×C e T matrix and a C e T × N matrix. They are then multiplied to obtain an N × N similarity matrix C k , whose element C ij k represents the similarity of vertex v i and vertex v j . The value of the matrix is normalized to 0 − 1, which is used as the soft edge of the two vertexes. Since the normalized Gaussian is equipped with a sof tmax operation, we can calculate C k based on Eq.4 as follows:</p><formula xml:id="formula_6">C k = sof tmax(f in T W T θk W φk f in )<label>(5)</label></formula><p>where W θ and W φ are the parameters of the embedding functions θ and φ, respectively. Rather than directly replacing the original A k with B k or C k , we add them to it. The value of B k and the parameters of θ and φ are initialized to 0. In this way, it can strengthen the flexibility of the model without degrading the original performance.</p><p>The overall architecture of the adaptive graph convolu-tion layer is shown in <ref type="figure">Fig. 2</ref>. Except for the A k , B k and C k introduced above, the kernel size of convolution (K v ) is set the same as before, i.e., 3. w k is the weighting function introduced in Eq. 1, whose parameter is W k in Eq. 3. A residual connection, similar to <ref type="bibr" target="#b9">[10]</ref>, is added for each layer, which allows the layer to be inserted into any existing models without breaking its initial behavior. If the number of input channels is different than the number of output channels, a 1 × 1 convolution (orange box with dashed line in <ref type="figure">Fig. 2)</ref> is inserted in the residual path to transform the input to match the output in the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adaptive graph convolutional block</head><p>The convolution for the temporal dimension is the same as ST-GCN, i.e., performing the K t × 1 convolution on the C × T × N feature maps. Both the spatial GCN and temporal GCN are followed by a batch normalization (BN) layer and a ReLU layer. As shown in <ref type="figure">Fig. 3</ref>, one basic block is the combination of one spatial GCN (Convs), one temporal GCN (Convt) and an additional dropout layer with the drop rate set as 0.5. To stabilize the training, a residual connection is added for each block.  <ref type="figure">Figure 3</ref>. Illustration of the adaptive graph convolutional block. Convs represents the spatial GCN, and Convt represents the temporal GCN, both of which are followed by a BN layer and a ReLU layer. Moreover, a residual connection is added for each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptive graph convolutional network</head><p>The adaptive graph convolutional network (AGCN) is the stack of these basic blocks, as shown in <ref type="figure">Fig. 4</ref>. There are a total of 9 blocks. The numbers of output channels for each block are 64, 64, 64, 128, 128, 128, 256, 256 and 256. A data BN layer is added at the beginning to normalize the input data. A global average pooling layer is performed at the end to pool feature maps of different samples to the same size. The final output is sent to a sof tmax classifier to obtain the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Two-stream networks</head><p>As introduced in Sec. 1, the second-order information, i.e., the bone information, is also important for skeletonbased action recognition but is neglected in previous works.  <ref type="figure">Figure 4</ref>. Illustration of the AGCN. There are a total of 9 blocks (B1-B9). The three numbers of each block represent the number of input channels, the number of output channels and the stride, respectively. GAP represents the global average pooling layer.</p><p>In this paper, we propose explicitly modeling the secondorder information, namely, the bone information, with a two-stream framework to enhance the recognition.</p><p>In particular, since each bone is bound with two joints, we define that the joint close to the center of gravity of the skeleton is the source joint and the joint far away from the center of gravity is the target joint. Each bone is represented as a vector pointing to its target joint from its source joint, which contains not only the length information, but also the direction information. For example, given a bone with its source joint v 1 = (x 1 , y 1 , z 1 ) and its target joint v 2 = (x 2 , y 2 , z 2 ), the vector of the bone is calculated as e v1,v2 = (x 2 − x 1 , y 2 − y 1 , z 2 − z 1 ).</p><p>Since the graph of the skeleton data have no cycles, each bone can be assigned with a unique target joint. The number of joints is one more than the number of bones because the central joint is not assigned to any bones. To simplify the design of the network, we add an empty bone with its value as 0 to the central joint. In this way, both the graph and the network of bones can be designed the same as that of joints because each bone can be bound with a unique joint. We use J-stream and B-stream to represent the networks of joints and bones, respectively. The overall architecture (2s-AGCN) is shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. Given a sample, we first calculate the data of bones based on the data of joints. Then, the joint data and bone data are fed into the J-stream and B-stream, respectively. Finally, the sof tmax scores of the two streams are added to obtain the fused score and predict the action label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To perform a head-to-head comparison with ST-GCN, our experiments are conducted on the same two large-scale action recognition datasets: NTU-RGBD <ref type="bibr" target="#b27">[27]</ref> and Kinetics-Skeleton <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">32]</ref>. First, since the NTU-RGBD dataset is smaller than the Kinetics-Skeleton dataset, we perform exhaustive ablation studies on it to examine the contributions of the proposed model components based on the recognition performance. Then, the final model is evaluated on both of the datasets to verify the generality and is compared with the other state-of-the-art approaches. The definitions of joints and their natural connections in the two datasets are shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>NTU-RGBD: NTU-RGBD <ref type="bibr" target="#b27">[27]</ref> is currently the largest and most widely used in-door-captured action recognition dataset, which contains 56,000 action clips in 60 action classes. The clips are performed by 40 volunteers in different age groups ranging from 10 to 35. Each action is captured by 3 cameras at the same height but from different horizontal angles: −45 • , 0 • , 45 • . This dataset provides 3D joint locations of each frame detected by Kinect depth sensors. There are 25 joints for each subject in the skeleton sequences, while each video has no more than 2 subjects. The original paper <ref type="bibr" target="#b27">[27]</ref> of the dataset recommends two benchmarks: 1). Cross-subject (X-Sub): the dataset in this benchmark is divided into a training set (40,320 videos) and a validation set <ref type="bibr">(16,560 videos)</ref>, where the actors in the two subsets are different. 2).Cross-view (X-View): the training set in this benchmark contains 37,920 videos that are captured by cameras 2 and 3, and the validation set contains 18,960 videos that are captured by camera 1. We follow this convention and report the top-1 accuracy on both benchmarks.</p><p>Kinetics-Skeleton: Kinetics <ref type="bibr" target="#b11">[12]</ref> is a large-scale human action dataset that contains 300,000 videos clips in 400 classes. The video clips are sourced from YouTube videos and have a great variety. It only provides raw video clips without skeleton data. <ref type="bibr" target="#b32">[32]</ref> estimate the locations of 18 joints on every frame of the clips using the publicly available OpenPose toolbox <ref type="bibr" target="#b3">[4]</ref>. Two peoples are selected for multiperson clips based on the average joint confidence. We use their released data (Kinetics-Skeleton) to evaluate our model. The dataset is divided into a training set (240,000 clips) and a validation set (20,000 clips). Following the evaluation method in <ref type="bibr" target="#b32">[32]</ref>, we train the models on the training set and report the top-1 and top-5 accuracies on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training details</head><p>All experiments are conducted on the PyTorch deep learning framework <ref type="bibr" target="#b26">[26]</ref>. Stochastic gradient descent (SGD) with Nesterov momentum (0.9) is applied as the optimization strategy. The batch size is 64. Cross-entropy is selected as the loss function to backpropagate gradients. The weight decay is set to 0.0001.</p><p>For the NTU-RGBD dataset, there are at most two people in each sample of the dataset. If the number of bodies in the sample is less than 2, we pad the second body with 0. The max number of frames in each sample is 300. For samples with less than 300 frames, we repeat the samples until it reaches 300 frames. The learning rate is set as 0.1 and is divided by 10 at the 30 th epoch and 40 th epoch. The training process is ended at the 50 th epoch.</p><p>For the Kinetics-Skeleton dataset, the size of the input tensor of Kinetics is set the same as <ref type="bibr" target="#b32">[32]</ref>, which contains 150 frames with 2 bodies in each frame. We perform the same data-augmentation methods as in <ref type="bibr" target="#b32">[32]</ref>. In detail, we randomly choose 150 frames from the input skeleton sequence and slightly disturb the joint coordinates with randomly chosen rotations and translations. The learning rate is also set as 0.1 and is divided by 10 at the 45 th epoch and 55 th epoch. The training process is ended at the 65 th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>We examine the effectiveness of the proposed components in two-stream adaptive graph convolutional network (2s-AGCN) in this section with the X-View benchmark on the NTU-RGBD dataset. The original performance of ST-GCN on the NTU-RGBD dataset is 88.3%. By using the rearranged learning-rate scheduler and the specially designed data preprocessing methods, it is improved to 92.7%, which is used as the baseline in the experiments. The detail is introduced in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Adaptive graph convolutional block.</head><p>As introduced in Section 4.1, there are 3 types of graphs in the adaptive graph convolutional block, i.e., A, B and C. We manually delete one of the graphs and show their performance in Tab. 1. This table shows that adaptively learning the graph is beneficial for action recognition and that deleting any one of the three graphs will harm the performance. With all three graphs added together, the model obtains the best performance. We also test the importance of M used in the original ST-GCN. The result shows that given each connection, a weight parameter is important, which also proves the importance of the adaptive graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy <ref type="formula">(</ref>   <ref type="figure" target="#fig_5">Fig. 8</ref> is a visualization of the skeleton graph for different layers of one sample (from left to right is the 3 rd , 5 th and 7 th layers in <ref type="figure">Fig. 4, respectively)</ref>. The skeletons are plotted based on the physical connections of the human body. Each circle represents one joint, and its size represents the strength of the connection between the current joint and the 25 th joint in the learned adaptive graph of our model. It shows that a traditional physical connection of the human body is not the best choice for the action recognition task, and different layers need graphs with different topology structures. The skeleton graph in the 3 rd layer pays more attention to the adjacent joints in the physical graph. This result is intuitive since the lower layer only contains the low-level feature, while the global information cannot be observed. For the 5 th layer, more joints along the same arm are strongly connected. For the 7 th layer, the left hand and the right hand show a stronger connection, although they are far away from each other in the physical structure of the human body. We argue that a higher layer contains higher-level information. Hence, the graph is more relevant to the final classification task.  <ref type="figure">Fig. 9</ref> shows a similar visualization of <ref type="figure" target="#fig_5">Fig. 8</ref> but for different samples. The learned adjacency matrix is extracted from the second subset of the 5 th layer in the model <ref type="figure">(Fig. 4)</ref>. It shows that the graph structures learned by our model for different samples are also different, even for the same convolutional subset and the same layer. It verified our point of view that different samples need different topologies of the graph, and a data-driven graph structure is better than a fixed one. <ref type="figure">Figure 9</ref>. Visualization of the graphs for different samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Visualization of the learned graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Two-stream framework</head><p>Another important improvement is the utilization of secondorder information. Here, we compare the performance of using each type of input data alone, shown as Js-AGCN and Bs-AGCN in Tab. 2, and the performance when combining them as described in Section 4.4, shown as 2s-AGCN in Tab. 2. Clearly, the two-stream method outperforms the one-stream-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy (%) Js-AGCN 93.7 Bs-AGCN 93.2 2s-AGCN 95.1 <ref type="table">Table 2</ref>. Comparisons of the validation accuracy with different input modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with the state-of-the-art</head><p>We compare the final model with the state-of-the-art skeleton-based action recognition methods on both the NTU-RGBD dataset and Kinetics-Skeleton dataset. The results of these two comparisons are shown in Tab 3 and Tab 4, respectively. The methods used for comparison include the handcraft-feature-based methods <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b7">8]</ref>, RNNbased methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>, CNN-based methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref> and GCN-based methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b30">30]</ref>. Our model achieves state-of-the-art performance with a large margin on both of the datasets, which verifies the superiority of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Conclusion</head><p>In this work, we propose a novel adaptive graph convolutional neural network (2s-AGCN) for skeleton-based ac-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>X-Sub (%) X-View (%) Lie Group <ref type="bibr" target="#b31">[31]</ref> 50.1 82.8 HBRNN <ref type="bibr" target="#b5">[6]</ref> 59.1 64.0 Deep LSTM <ref type="bibr" target="#b27">[27]</ref> 60.7 67.3 ST-LSTM <ref type="bibr" target="#b22">[22]</ref> 69.2 77.7 STA-LSTM <ref type="bibr" target="#b29">[29]</ref> 73.4 81.2 VA-LSTM <ref type="bibr" target="#b33">[33]</ref> 79.2 87.7 ARRN-LSTM <ref type="bibr" target="#b19">[19]</ref> 80.7 88.8 Ind-RNN <ref type="bibr" target="#b20">[20]</ref> 81.8 88.0 Two-Stream 3DCNN <ref type="bibr" target="#b21">[21]</ref> 66.8 72.6 TCN <ref type="bibr" target="#b13">[14]</ref> 74.3 83.1 Clips+CNN+MTLN <ref type="bibr" target="#b12">[13]</ref> 79.6 84.8 Synthesized CNN <ref type="bibr" target="#b23">[23]</ref> 80.0 87.2 CNN+Motion+Trans <ref type="bibr" target="#b17">[18]</ref> 83.2 89.3 3scale ResNet152 <ref type="bibr" target="#b16">[17]</ref> 85.0 92.3 ST-GCN <ref type="bibr" target="#b32">[32]</ref> 81.5 88.3 DPRL+GCNN <ref type="bibr" target="#b30">[30]</ref> 83.5 89.8 2s-AGCN (ours) 88.5 95.1 tion recognition. It parameterizes the graph structure of the skeleton data and embeds it into the network to be jointly learned and updated with the model. This data-driven approach increases the flexibility of the graph convolutional network and is more suitable for the action recognition task. Furthermore, the traditional methods always ignore or underestimate the importance of second-order information of skeleton data, i.e., the bone information. In this work, we propose a two-stream framework to explicitly employ this type of information, which further enhances the performance. The final model is evaluated on two large-scale action recognition datasets, NTU-RGBD and Kinetics, and it achieves the state-of-the-art performance on both of them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a).Illustration of the spatiotemporal graph used in ST-GCN. (b).Illustration of the mapping strategy. Different colors denote different subsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of the overall architecture of the 2s-AGCN. The scores of two streams are added to obtain the final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>The left sketch shows the joint label of the Kinetics-Skeleton dataset and the right sketch shows the joint label of the NTU-RGBD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 Figure 7 .</head><label>77</label><figDesc>shows an example of the adjacency matrix learned by our model for the second subset. The gray scale of each element in the matrix represents the strength of the connection. The left is the original adjacency matrix for the second subset employed in ST-GCN, and the right is an example of the corresponding adaptive adjacency matrix learned by our model. It is clear that the learned structure of the graph is more flexible and not constrained to the physical connections of the human body. Example of the learned adjacency matrix. The left matrix is the original adjacency matrix for the second subset in the NTU-RGBD dataset. The right matrix is an example of the corresponding adaptive adjacency matrix learned by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of the graphs for different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of the validation accuracy when adding adaptive graph convolutional block with or without A, B and C. wo/X means deleting the X module.</figDesc><table><row><cell></cell><cell>%)</cell></row><row><cell>ST-GCN</cell><cell>92.7</cell></row><row><cell>ST-GCN wo/M</cell><cell>91.1</cell></row><row><cell>AGCN wo/A</cell><cell>93.4</cell></row><row><cell>AGCN wo/B</cell><cell>93.3</cell></row><row><cell>AGCN wo/C</cell><cell>93.4</cell></row><row><cell>AGCN</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparisons of the validation accuracy with state-of-theart methods on the NTU-RGBD dataset. Comparisons of the validation accuracy with state-of-theart methods on the Kinetics-Skeleton dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>Feature Enc. [8]</cell><cell>14.9</cell><cell>25.8</cell></row><row><cell>Deep LSTM [27]</cell><cell>16.4</cell><cell>35.3</cell></row><row><cell>TCN [14]</cell><cell>20.3</cell><cell>40.0</cell></row><row><cell>ST-GCN [32]</cell><cell>30.7</cell><cell>52.8</cell></row><row><cell>Js-AGCN (ours)</cell><cell>35.1</cell><cell>57.1</cell></row><row><cell>Bs-AGCN (ours)</cell><cell>33.3</cell><cell>55.7</cell></row><row><cell>2s-AGCN (ours)</cell><cell>36.1</cell><cell>58.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/lshiwjx/2s-AGCN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. Two-stream adaptive graph convolutional networkIn this section, we introduce the components of our proposed two-stream adaptive graph convolutional network (2s-AGCN) in detail.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61572500, 61876182 and 61872364, and in part by the State Grid Corporation Science and Technology Project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Skeleton-Based Action Recognition with Gated Convolutional Neural Networks. IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">D D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">Paul Natsev, and others. The Kinetics Human Action Video Dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ferdous Ahmed Sohel, and Farid Boussad. A New Representation of Skeleton Sequences for 3d Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senjian</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4570" to="4579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="601" to="604" />
		</imprint>
	</monogr>
	<note>Multimedia &amp; Expo Workshops</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Multimedia &amp; Expo Workshops</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
			<publisher>ICMEW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Skeleton-Based Relational Modeling for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02556</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building A longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Two-Stream 3d Convolutional Neural Network for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanhui</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08106</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatio-Temporal LSTM with Trust Gates for 3d Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A Large Scale Dataset for 3d Human Activity Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition From Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2117" to="2126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">EgoGesture: A New Dataset and Benchmark for Egocentric Hand Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
