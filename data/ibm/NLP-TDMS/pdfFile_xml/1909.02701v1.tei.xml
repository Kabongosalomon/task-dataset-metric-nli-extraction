<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Semantic Reasoning for Image-Text Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Khoury College of Computer Science</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Semantic Reasoning for Image-Text Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image-text matching has been a hot research topic bridging the vision and language areas. It remains challenging because the current representation of image usually lacks global semantic concepts as in its corresponding text caption. To address this issue, we propose a simple and interpretable reasoning model to generate visual representation that captures key objects and semantic concepts of a scene. Specifically, we first build up connections between image regions and perform reasoning with Graph Convolutional Networks to generate features with semantic relationships. Then, we propose to use the gate and memory mechanism to perform global semantic reasoning on these relationship-enhanced features, select the discriminative information and gradually generate the representation for the whole scene. Experiments validate that our method achieves a new state-of-the-art for the image-text matching on MS-COCO [28] and Flickr30K [39] datasets. It outperforms the current best method by 6.8% relatively for image retrieval and 4.8% relatively for caption retrieval on MS-COCO (Recall@1 using 1K test set). On Flickr30K, our model improves image retrieval by 12.6% relatively and caption retrieval by 5.8% relatively (Re-call@1). Our code is available at https://github. com/KunpengLi1994/VSRN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision and language are two important aspects of human intelligence to understand the real world. A large amount of research <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref> has been done to bridge these two modalities. Image-text matching is one of the fundamental topics in this field, which refers to measuring the visualsemantic similarity between a sentence and an image. It has been widely adopted to various applications such as the retrieval of text descriptions from image queries or image search for given sentences.</p><p>Although a lot of progress has been achieved in this area, it is still a challenge problem due to the huge visual semantic discrepancy. When people describe what they see in the A snowboarder in mid-air with another person watching in the background. picture using natural language, it can be observed that the descriptions will not only include the objects, salient stuff, but also will organize their interactions, relative positions and other high-level semantic concepts (such as "in mid-air" and "watching in the background" in the <ref type="figure" target="#fig_0">Figure 1)</ref>. Visual reasoning about objects and semantics is crucial for humans during this process. However, the current existing visualtext matching systems lack such kind of reasoning mechanism. Most of them <ref type="bibr" target="#b4">[5]</ref> represent concepts in an image by Convolutional Neural Network (CNN) features extracted by convolutions with a specific receptive field, which only perform local pixel-level analysis. It is hard for them to recognize the high-level semantic concepts. More recently, <ref type="bibr" target="#b22">[23]</ref> make use of region-level features from object detectors and discover alignments between image regions and words. Although grasping some local semantic concepts within regions including multiple objects, these methods still lack the global reasoning mechanism that allows information communication between regions farther away.</p><p>To address this issue, we propose Visual Semantic Reasoning Network (VSRN) to generate visual representation that captures both objects and their semantic relationships. We start from identifying salient regions in images by following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>. In this way, salient region detection at stuff/object level can be analogized to the bottom-up at-tention that is consistent with human vision system <ref type="bibr" target="#b15">[16]</ref>. Practically, the bottom-up attention module is implemented using Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>. We then build up connections between these salient regions and perform reasoning with Graph Convolutional Networks (GCN) <ref type="bibr" target="#b17">[18]</ref> to generate features with semantic relationships.</p><p>Different image regions and semantic relationships would have different contributions for inferring the imagetext similarity and some of them are even redundant. Therefore, we further take a step to attend important ones when generating the final representation for the whole image. We propose to use the gate and memory mechanism <ref type="bibr" target="#b2">[3]</ref> to perform global semantic reasoning on these relationshipenhanced features, select the discriminative information and gradually grow representation for the whole scene. This reasoning process is conducted on a graph topology and considers both local, global semantic correlations. The final image representation captures more key semantic concepts than those from existing methods that lack a reasoning mechanism, therefore, can help to achieve better image-text matching performance.</p><p>In addition to quantitative evaluation of our model on standard benchmarks, we also design an interpretation method to analyze what has been learned inside the reasoning model. Correlations between the final image representation and each region feature are visualized in an attention format. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we find the learned image representation has high response at these regions that include key semantic concepts.</p><p>To sum up, our main contributions are: (a) We propose a simple and interpretable reasoning model VSRN to generate enhanced visual representations by region relationship reasoning and global semantic reasoning. (b) We design an interpretation method to visualize and validate that the generated image representation can capture key objects and semantic concepts of a scene, so that it can be better aligned with the corresponding text caption. (c) The proposed VSRN achieves a new state-of-the-art for the image-text matching on MS-COCO <ref type="bibr" target="#b27">[28]</ref> and Flickr30K <ref type="bibr" target="#b38">[39]</ref> datasets. Our VSRN outperforms the current best method SCAN [23] by 6.8% relatively for image retrieval and 4.8% relatively for caption retrieval on MS-COCO (Recall@1 using 1K test set). On Flickr30K, our model improves image retrieval by 12.6% relatively and caption retrieval by 5.8% relatively (Recall@1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image-Text Matching. Our work is related to existing methods proposed for image-text matching, where the key issue is measuring the visual-semantic similarity between a text and an image. Learning a common space where text and image feature vectors are comparable is a typical solution for this task. Frome et al. <ref type="bibr" target="#b5">[6]</ref> propose a feature em-bedding framework that uses Skip-Gram and CNN to extract feature representations for cross-modal. Then a ranking loss is adopted to encourage the distance between the mismatched image-text pair is larger than that between the matched pair. Kiros et al. <ref type="bibr" target="#b18">[19]</ref> use a similar framework and adopt LSTM <ref type="bibr" target="#b11">[12]</ref> instead of Skip-Gram for the learning of text representations. Vendrov et al. <ref type="bibr" target="#b34">[35]</ref> design a new objective function that encourages the order structure of visual semantic can be preserved hierarchy. Faghri et al. <ref type="bibr" target="#b4">[5]</ref> focus more on hard negatives and obtain good improvement using a triplet loss. Gu et al. <ref type="bibr" target="#b7">[8]</ref> further improve the learning of cross-view feature embedding by incorporating generative objectives. Our work also belongs to this direction of learning joint space for image and sentence with an emphasis on improving image representations.</p><p>Attention Mechanism. Our work is also inspired by bottom-up attention mechanism and recent image-text matching methods based on it. Bottom-up attention <ref type="bibr" target="#b15">[16]</ref> refers to salient region detection at stuff/object level can be analogized to the spontaneous bottom-up attention that is consistent with human vision system <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. Similar observation has motivated other existing work. In <ref type="bibr" target="#b14">[15]</ref>, R-CNN [7] is adopted to detect and encode image regions at object level. Image-text similarity is then obtained by aggregating all word-region pairs similarity scores. Huang et al. <ref type="bibr" target="#b13">[14]</ref> train a multi-label CNN to classify each image region into multi-labels of objects and semantic relations, so that the improved image representation can capture semantic concepts within the local region. Lee et al. <ref type="bibr" target="#b22">[23]</ref> further propose an attention model towards attending key words and image regions for predicting the text-image similarity. Following them, we also start from bottom-up region features of an image. However, to the best of our knowledge, no study has attempted to incorporate global spatial or semantic reasoning when learning visual representations for image-text matching.</p><p>Relational Reasoning Methods. Symbolic approaches <ref type="bibr" target="#b30">[31]</ref> are the earliest form of reasoning in artificial intelligence. In these methods, relations between symbols are represented by the form of logic and mathematics, reasoning happens by abduction and deduction <ref type="bibr" target="#b10">[11]</ref> etc. However, in order to make these systems can be used practically, symbols need to be grounded in advance. More recent methods, such as path ranking algorithm <ref type="bibr" target="#b21">[22]</ref>, perform reasoning on structured knowledge bases by taking use of statistical learning to extract effective patterns. As an active research area, graph-based methods <ref type="bibr" target="#b39">[40]</ref> have been very popular in recent years and shown to be an efficient way of relation reasoning. Graph Convolution Networks (GCN) <ref type="bibr" target="#b17">[18]</ref> are proposed for semi-supervised classification. Yao et al. <ref type="bibr" target="#b37">[38]</ref> train a visual relationship detection model on Visual Genome dataset <ref type="bibr" target="#b20">[21]</ref> and use a GCN-based encoder to encode the detected relationship information into an image  captioning framework. Yang et al. <ref type="bibr" target="#b36">[37]</ref> utilize GCNs to incorporate the prior knowledge into a deep reinforcement learning framework improve semantic navigation in unseen scenes and towards novel objects. We also adopt the reasoning power of graph convolutions to obtain image region features enhanced with semantic relationship. But we do not need extra database to build the relation graph (e.g. <ref type="bibr" target="#b37">[38]</ref> needs to train the relationship detection model on Visual Genome). Beyond this, we further perform global semantic reasoning on these relationship-enhanced features, so that the final image representation can capture key objects and semantic concepts of a scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Alignments with Visual Semantic Reasoning</head><p>We describe the detail structure of the Visual Semantic Reasoning Network (VSRN) for image-text matching in this section. Our goal is to infer the similarity between a full sentence and a whole image by mapping image regions and the text descriptions into a common embedding space. For the image part, we begin with image regions and their features generated by the bottom-up attention model <ref type="bibr" target="#b0">[1]</ref> (Sec. 3.1). VSRN first builds up connections between these image regions and do reasoning using Graph Convolutional Networks (GCN) to generate features with semantic relationship information (Sec. 3.2). Then, we do global semantic reasoning on these relationship-enhanced features to select the discriminative information and filter out unimportant one to generate the final representation for the whole image (Sec. 3.3). For the text caption part, we learn a representation for the sentence using RNNs. Finally, the whole model is trained with joint optimization of image-sentence matching and sentence generation (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Representation by Bottom-Up Attention</head><p>Taking the advantage of bottom-up attention <ref type="bibr" target="#b0">[1]</ref>, each image can be represented by a set of features V = {v 1 , ..., v k }, v i ∈ R D , such that each feature v i encodes an object or a salient region in this image. Following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>, we implement the bottom-up attention with a Faster R-CNN [33] model using ResNet-101 <ref type="bibr" target="#b9">[10]</ref> as the backbone. It is pre-trained on the Visual Genomes dataset <ref type="bibr" target="#b20">[21]</ref> by <ref type="bibr" target="#b0">[1]</ref>. The model is trained to predict instance classes and attribute classes instead of the object classes, so that it can help learn feature representations with rich semantic meaning. Specifically, instance classes include objects and salient stuff which is hard to recognize. For example, attributes like "furry" and stuff like "building", "grass" and "sky". The model's final output is used and non-maximum suppression for each class is operated with an IoU threshold of 0.7. We then set a confidence threshold of 0.3 and select all image regions where any class detection probability is larger than this threshold. The top 36 ROIs with the highest class detection confidence scores are selected. All these thresholds are set as same as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>. For each selected region i, we extract features after the average pooling layer, resulting in f i with 2048 dimensions. A fully-connect layer is then applied to transform f i to a D-dimensional embedding using the following equation:</p><formula xml:id="formula_0">v i = W f f i + b f .<label>(1)</label></formula><formula xml:id="formula_1">Then V = {v 1 , ..., v k }, v i ∈ R D is constructed to rep- resent each image,</formula><p>where v i encodes an object or salient region in this image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Region Relationship Reasoning</head><p>Inspired by recent advances in deep learning based visual reasoning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref>, we build up a region relationship reasoning model to enhance the region-based representation by considering the semantic correlation between image regions. Specifically, we measure the pairwise affinity between image regions in an embedding space to construct their relationship using Eq. 2.</p><formula xml:id="formula_2">R(v i , v j ) = ϕ(v i ) T φ(v j ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">ϕ(v i ) = W ϕ v i and φ(v j ) = W φ v j are two embed- dings.</formula><p>The weight parameters W ϕ and W φ can be learned via back propagation. Then a fully-connected relationship graph G r = (V, E), where V is the set of detected regions and edge set E is described by the affinity matrix R. R is obtained by calculating the affinity edge of each pair of regions using Eq. 2. That means there will be an edge with high affinity score connecting two image regions if they have strong semantic relationships and are highly correlated.</p><p>We apply the Graph Convolutional Networks (GCN) <ref type="bibr" target="#b17">[18]</ref> to perform reasoning on this fully-connected graph. Response of each node is computed based on its neighbors defined by the graph relations. We add residual connections to the original GCN as follows:</p><formula xml:id="formula_4">V * = W r (RV W g ) + V,<label>(3)</label></formula><p>where W g is the weight matrix of the GCN layer with dimension of D × D. W r is the weight matrix of residual structure. R is the affinity matrix with shape of k × k. We follow the routine to row-wise normalize the affinity matrix</p><formula xml:id="formula_5">R. The output V * = {v * 1 , ..., v * k }, v * i ∈ R D is the relation- ship enhanced representation for image region nodes.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global Semantic Reasoning</head><p>Based on region features with relationship information, we further do global semantic reasoning to select the discriminative information and filter out unimportant one to obtain the final representation for the whole image. Specifically, we perform this reasoning by putting the sequence of region features <ref type="bibr" target="#b2">[3]</ref>. The description of the whole scene will gradually grow and update in the memory cell (hidden state) m i during this reasoning process.</p><formula xml:id="formula_6">V * = {v * 1 , ..., v * k }, v * i ∈ R D , one by one into GRUs</formula><p>At each reasoning step i, an update gate z i analyzes the current input region feature v * i and the description of the whole scene at last step m i−1 to decide how much the unit updates its memory cell. The update gate is calculated by:</p><formula xml:id="formula_7">z i = σ z (W z v * i + U z m i−1 + b z ),<label>(4)</label></formula><p>where σ z is a sigmoid activation function. W z , U z and b z are weights and bias.</p><p>The new added content helping grow the description of the whole scene is computed as follows:</p><formula xml:id="formula_8">m i = σ m (W m v * i + U z (r i • m i−1 ) + b m ),<label>(5)</label></formula><p>where σ m is a tanh activation function. W m , U m and b m are weights and bias. • is an element-wise multiplication. r i is the reset gate that decides what content to forget based on the reasoning between v * i and m i−1 . r i is computed similarly to the update gate as:</p><formula xml:id="formula_9">r i = σ r (W r v * i + U r m i−1 + b r ),<label>(6)</label></formula><p>where σ r is a sigmoid activation function. W z , U z and b z are weights and bias. Then the description of the whole scene m i at the current step is a linear interpolation using update gate z i between the previous description m i−1 and the new contentm i :</p><formula xml:id="formula_10">m i = (1 − z i ) • m i−1 + z i •m i ,<label>(7)</label></formula><p>where • is an element-wise multiplication. Since each v * i includes global relationship information, update of m i is actually based on reasoning on a graph topology, which considers both current local region and global semantic correlations. We take the memory cell m k at the end of the sequence V * as the final representation I for the whole image, where k is the length of V * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning Alignments by Joint Matching and Generation</head><p>To connect vision and language domains, we use a GRUbased text encoder <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> to map the text caption to the same D-dimensional semantic vector space C ∈ R D as the image representation I, which considers semantic context in the sentence. Then we jointly optimize matching and generation to learn the alignments between C and I.</p><p>For the matching part, we adopt a hinge-based triplet ranking loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> with emphasis on hard negatives <ref type="bibr" target="#b4">[5]</ref>, i.e., the negatives closest to each training query. We define the loss as:</p><formula xml:id="formula_11">L M =[α − S(I, C) + S(I,Ĉ)] + + [α − S(I, C) + S(Î, C)] + ,<label>(8)</label></formula><p>where α serves as a margin parameter.</p><p>[x] + ≡ max(x, 0). This hinge loss comprises two terms, one with I and one with C as queries. S(·) is the similarity function in the joint embedding space. We use the usual inner product as S(·)in our experiments.Î = arg max j =I S(j, C) and C = arg max d =C S(I, d) are the hardest negatives for a positive pair (I, T). For computational efficiency, instead of finding the hardest negatives in the entire training set, we find them within each mini-batch.</p><p>For the generation part, the learned visual representation should also has the ability to generate sentences that are close to the ground-truth captions. Specifically, we use a sequence to sequence model with attention mechanism <ref type="bibr" target="#b35">[36]</ref> to achieve this. We maximize the log-likelihood of the predicted output sentence. The loss function is defined as:</p><formula xml:id="formula_12">L G = − l t=1 log p(y t |y t−1 , V * ; θ),<label>(9)</label></formula><p>where l is the length of output word sequence Y = (y 1 , ..., y l ). θ is the parameter of the sequence to sequence model. Our final loss function is defined as follows to perform joint optimization of the two objectives.</p><formula xml:id="formula_13">L = L M + L G .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the effectiveness of the proposed Visual Semantic Reasoning Network (VSRN), we perform experiments in terms of sentence retrieval (image query) and image retrieval (sentence query) on two publicly available datasets. Ablation studies are conducted to investigate each component of our model. We also compare with recent state-of-the-art methods on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Protocols</head><p>We evaluate our method on the Microsoft COCO dataset <ref type="bibr" target="#b27">[28]</ref> and the Flickr30K dataset <ref type="bibr" target="#b38">[39]</ref>. MS-COCO includes 123,287 images, and each image is annotated with 5 text descriptions. We follow the splits of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> for MSCOCO, which contains 113,287 images for training, 5,000 images for validation and 5000 images for testing. Each image comes with 5 captions. The final results are obtained by averaging the results from 5 folds of 1K test images or testing on the full 5K test images. Flickr30K consists of 31783 images collected from the Flickr website. Each image is accompanied with 5 human annotated text descriptions. We use the standard training, validation and testing splits <ref type="bibr" target="#b14">[15]</ref>, which contain 28,000 images, 1000 images and 1000 images respectively. For evaluation matrix, as is common in information retrieval, we measure the performance by recall at K (R@K) defined as the fraction of queries for which the correct item is retrieved in the closest K points to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We set the word embedding size to 300 and the dimension of the joint embedding space D to 2048. We follow the same setting as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>  descending order of their class detection confidence scores that are generated by the bottom-up attention detector. For the training of VSRN, we use the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> to train the model with 30 epochs. We start training with learning rate 0.0002 for 15 epochs, and then lower the learning rate to 0.00002 for the rest 15 epochs. We set the margin α in Eq. 8 to 0.2 for all experiments. We use a mini-batch size of 128. For evaluation on the test set, we tackle over-fitting by choosing the snapshot of the model that performs best on the validation set. The best snapshot is selected based on the sum of the recalls on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons With the State-of-the-art</head><p>Results on MS-COCO. Quantitative results on MS-COCO 1K test set are shown in <ref type="table">Table 1</ref>, where the proposed VSRN outperforms recent methods with a large gap of R@1. Following the common protocol <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, the results are obtained by averaging over 5 folds of 1K test images. When comparing with the current best method SCAN <ref type="bibr" target="#b22">[23]</ref>, we follow the same strategy <ref type="bibr" target="#b22">[23]</ref> to combine results from two trained VSRN models by averaging their predicted similarity scores. Our VSRN improves 4.8% on caption retrieval (R@1) and 6.8% on image retrieval (R@1) relatively. In <ref type="table">Table 2</ref>, we also report results on MS-COCO 5K test set by testing on the full 5K test images and their captions. From the table, we can observe that the overall results by all the methods are lower than the first protocol. It probably results from the existence of more distracters for a given query in such a larger target set. Among all methods, the proposed VSRN still achieves the best performance, which again demonstrates its effectiveness. It improves upon the current state-of-the-art, SCAN with 5.2% on the sentence retrieval (R@1) and 4.9% on the image retrieval (R@1) relatively.</p><p>Results on Flickr30K. We show experimental results of VSRN on Flickr30K dataset and comparisons with the current state-of-the-art methods in <ref type="table" target="#tab_2">Table 3</ref>. We also list the network backbones used for visual feature extraction, such as R-CNN, VGG, ResNet, Faster R-CNN. From the results, we find the proposed VSRN outperforms all stateof-the-art methods, especially for Recall@1. When compared with SCAN [23] that uses the same feature extraction backbones with us, our VSRN improves 5.8% on caption retrieval (R@1) and 12.6% on image retrieval(R@1) relatively (following the same strategy <ref type="bibr" target="#b22">[23]</ref> of averaging predicted similarity scores of two trained models). SCAN tries to discover the full latent alignments between possible pairs of regions and words, and builds up an attention model to focus on important alignments when inferring the imagetext similarity. It mainly focuses on local pair-wise matching between regions and words. In contrast, the proposed VSRN performs reasoning on region features and generate a global scene representation that captures key objects and semantic concepts for each image. This representation can be better aligned with the corresponding text caption. The comparison shows the strength of region relationship reasoning and global semantic reasoning for image-text matching. Especially for the challenging caption retrieval task, VSRN shows strong robustness to distractors with a huge improvement (relative 12.6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Analysis each reasoning component in VSRN. We would like to incrementally validate each reasoning component in our VSRN by starting from a very basic baseline model which does not perform any reasoning. This baseline model adopts a mean-pooling operation on the region features after the fully-connected layer V = {v 1 , ..., v k }, v i ∈ R D to obtain the final representation for the whole image I ∈ R D . The other parts are kept as the same as VSRN. Results on MS-COCO 1K test set are shown in <ref type="table">Table 4</ref>.  This baseline model (noted as 'Mean-pool') achieves 64.3 of R@1 for caption retrieval and 49.2 of R@1 for image retrieval. Then we add one region relationship reasoning (RRR) layer (described in Sec. 3.3) before the meanpooling operation into this baseline model and mark it as RRR. We also replace the mean-pooling operation with the global semantic reasoning (GSR) module (described in Sec. 3.3) to get a GSR model. From <ref type="table">Table 4</ref> we can find that these two reasoning modules can both help to obtain better image representation I and improve the matching performance effectively. We then combine RRR and GSR to get our VSRN model and further try different numbers of RRR layers. Results show that adding region relationship reasoning layers before the global semantic reasoning module can gradually help to achieve better performance. This is because the RRR module can generate relationship enhanced features, which allows GSR perform reasoning on a graph topology and consider both current local region and global semantic correlations. However, we also find improvements become less when adding more RRR layers. We finally take 4RRR+GSR as the final setting of VSRN. We further report results of VSRN trained without text generation loss L G (marked as 4RRR+GSR*). Comparison shows that the joint optimization of matching and generation can help to improve around 2% relatively for R@1.</p><p>Region ordering for global semantic reasoning. Since our global semantic reasoning module (Sec. 3.3) sequentially processes region features and generates the representation of the whole image gradually, we consider several ablations about region ordering for this reasoning process in <ref type="table" target="#tab_4">Table 5</ref>. One possible setting (VSRN-Confidence) is the descending order of their class detection confidence scores that are generated by the bottom-up attention detector. We expect this to encourage the model to focus on the easy 1:On a kitchen counter top, a knife rests on an empty cutting board. 2:Dirty kitchen utensils and a stove together with fruit. 3:There are various food items on a kitchen counter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Image</head><p>Attention (d) <ref type="bibr" target="#b0">1</ref>:Two giraffes standing near trees in a grassy area. 2:Two giraffes standing next to each other on a grassy field. 3:Two giraffes rub their necks together as they stand by the trees in the sunlight .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Image Attention (b)</head><p>1:A woman and child playing frisbee in a grassy area. 2:A woman and a little girl playing with a Frisbee in the sun on a green lawn. 3:A woman throwing a frisbee as a child looks on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Image</head><p>Attention (a) 1:A person riding a donkey travels between two mountains. 2:A person in a black top riding a horse and some hills and rocks. 3:A person riding a horse down a trail with rocks around it.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Caption  regions with high confidence first and then inferring more difficult regions based on the semantic context. Another option (VSRN-BboxSize) is to sort the detection bounding boxes of these regions in descending order, as this lets the model to obtain global scene information first. We also test the model with randomly ordering of the regions (VSRN-Random). Results in <ref type="table" target="#tab_4">Table 5</ref> show that reasoning in a specific oder can help improve the performance than the random one. VSRN-Confidence and VSRN-BboxSize achieve comparable results with a reasonable ordering scheme. We take VSRN-Confidence as the setting of VSRN in our previous experiments. Besides, we also find the variance of R@1 is around 1 point for these different settings, which suggests VSRN is robust to the ordering scheme used. One possible reason could be that global information is included during the region relationship reasoning step. Based on these relationship enhanced feature, semantic reasoning can be then performed on global graph topologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization and Analysis</head><p>Attention visualization of the final image representation. Since the final goal of our visual semantic reasoning is to generate the image representation that includes key object and semantic concepts in the scene. In order to validate this, we visualize the correlation between the final representation of the whole image and these image regions included in this image in an attention form. Specifically, we calculate the inner product similarity (same as in Eq. 8) between each region feature V * = {v * 1 , ..., v * k }, v * i ∈ R D and the final whole image representation I ∈ R D . Then we rank the image regions V * in the descending order of their correlation with I and assign a score s i to each v * i according to its rank r i . The score is calculate by s i = λ(k − r attn ) 2 , where k is the total number of regions, λ is a parameter used to emphasize the high ranked regions. We set λ = 50 in our experiments. Then for the final attention map (similarity map), the attention score at each pixel location is obtained by adding up scores of all regions it belongs to. We show attention maps of each image along with the qualitative results of the image-to-text (caption) retrieval and text-to-image (image) retrieval. Qualitative results of the image-to-text retrieval. In <ref type="figure" target="#fig_4">Figure 3</ref>, we show the qualitative results of the text retrieval given image queries on MS-COCO. We show the top-3 retrieved sentences for each image query. The rank is obtained based the similarity scores predicted by VSRN. From these results, we find that our VSRN can retrieve the correct results in the top ranked sentences even for cases of cluttered and complex scenes. The model outputs some reasonable mismatches, e.g. (d)-3. There are incorrect results such as (f.4), which is possibly due to the too specific concept in the image ("US Route 1") that the model could not identify. From the attention visualization, we can find the image representation generated by VSRN well captures key objects and semantic concepts in the scene.</p><p>Qualitative results of the text-to-image retrieval. In <ref type="figure" target="#fig_5">Figure 4</ref>, we show qualitative results of image retrieval for given text queries on MS-COCO. Each text description is matched with a ground-truth image. We show the top-3 retrieved images for each text query. The true matches are outlined in green and false matches in red. We find that our model can retrieve the ground truth image in the top-3 list. Note that other results are also reasonable, which include the objects of the same category or same semantic concepts with the text descriptions. For those images with a very similar scene, our model can still distinguish them well and accurately retrieve the ground truth one at top-1 rank. This can be well explained from the attention map, e.g. for the given text query (a), the model attends on the cars on the street and the person cleaning a car in the ground-truth im-age to distinguish it with the other two images that are also about people skiing. However, for the top-2 retrieval images of query (c), the model is confused about the concept of "try field". It treats the field with less grass as a better match than the field with withered grass. This is possibly due to not enough training data for a complex concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a simple and interpretable reasoning model VSRN to generate visual representation by region relationship reasoning and global semantic reasoning. The enhanced image representation captures key objects and semantic concepts of a scene, so that it can better align with the corresponding text caption. Extensive experiments on MS-COCO and Fliker30K datasets demonstrate the resulting model consistently outperforms the-state-of-the-art methods with a large margin for the image-text matching. Compared with the complicated attention-based aggregation from pairwise similarities among regions and words, we show that the classical "image-text" similarity measure still promising given enhanced whole image representation. We will further explore the effectiveness of reasoning modules in VSRN on other vision and language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The proposed Visual Semantic Reasoning Network (VSRN) performs reasoning on the image regions to generate representation for an image. The representation captures key objects (boxes in the caption) and semantic concepts (highlight parts in the caption) of a scene as in the corresponding text caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>An overview of the proposed Visual Semantic Reasoning Network (VSRN). Based on salient image regions from bottomup attention (Sec. 3.1), VSRN first performs region relationship reasoning on these regions using GCN to generate features with semantic relationships (Sec. 3.2). Then VSRN takes use of the gate and memory mechanism to perform global semantic reasoning on the relationship enhanced features, select the discriminative information and gradually generate the representation for the whole scene (Sec. 3.3). The whole model is trained with joint optimization of matching and sentence generation (Sec. 3.4). The attention of the representation (top right) is obtained by calculating correlations between the final image representation and each region feature (Sec. 4.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 :</head><label>1</label><figDesc>A man in a helmet jumps a snowboard. 2. A snowboarder in mid-air with another person watching in the background. 3:A person on a snowboard jumping in the air.1.Traffic light hanging near power lines with trees in back. 2.Two traffic signals on a pole arm at an intersection . 3.A street sign and two traffic lights hang over US Route 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results of the image-to-text (caption) retrieval for VSRN on MS-COCO dataset. For each image query, we show the top-3 ranked text caption. Ground-truth matched sentences are with check marks, while some sentences sharing similar meanings as ground-truth ones are marked with gray underline. We also show the attention visualization of the final image representation besides its corresponding image. Our model generates interpretable image representation that captures key objects and semantic concepts in the scene. (Best viewed in color when zoomed in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Query (a): A family skiing a city street while others clean snow off their cars. Query (b): Sandwich and a lollipop in a bright orange serving tray sitting on a table. Query (d): A Frisbee team dressed in black holding their Freebees. Query (c): A bunch of cows grazing in a dry field together. Qualitative results of the text-to-image (image) retrieval for VSRN on MS-COCO dataset. We show the top-3 retrieved images for each text query, ranking from left to right. The true matches are outlined in green boxes and false matches in red boxes. We also show the attention visualization of image representation generated by VSRN under the corresponding image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to set details of visual bottomup attention model. The order of regions for GRU-based global semantic reasoning (Sec. 3.3) is determined by the</figDesc><table><row><cell>Methods</cell><cell cols="6">Caption Retrieval R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell></row><row><cell>(R-CNN, AlexNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DVSA CVPR 15 [15]</cell><cell>38.4</cell><cell>69.9</cell><cell>80.5</cell><cell>27.4</cell><cell>60.2</cell><cell>74.8</cell></row><row><cell cols="2">HMlstm ICCV 17 [32] 43.9</cell><cell>-</cell><cell>87.8</cell><cell>36.1</cell><cell>-</cell><cell>86.7</cell></row><row><cell>(VGG)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FV CVPR 15 [20]</cell><cell>39.4</cell><cell>67.9</cell><cell>80.9</cell><cell>25.1</cell><cell>59.8</cell><cell>76.6</cell></row><row><cell>OEM ICLR 16 [35]</cell><cell>46.7</cell><cell>-</cell><cell>88.9</cell><cell>37.9</cell><cell>-</cell><cell>85.9</cell></row><row><cell>VQA ECCV 16 [29]</cell><cell>50.5</cell><cell>80.1</cell><cell>89.7</cell><cell>37.0</cell><cell>70.9</cell><cell>82.9</cell></row><row><cell cols="2">SMlstm CVPR 17 [13] 53.2</cell><cell>83.1</cell><cell>91.5</cell><cell>40.7</cell><cell>75.8</cell><cell>87.4</cell></row><row><cell>2WayN CVPR 17 [4]</cell><cell>55.8</cell><cell>75.2</cell><cell>-</cell><cell>39.7</cell><cell>63.3</cell><cell>-</cell></row><row><cell>(ResNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RRF ICCV 17 [30]</cell><cell>56.4</cell><cell>85.3</cell><cell>91.5</cell><cell>43.9</cell><cell>78.1</cell><cell>88.6</cell></row><row><cell>VSE++ BMVC 18 [5]</cell><cell>64.6</cell><cell>89.1</cell><cell>95.7</cell><cell>52.0</cell><cell>83.1</cell><cell>92.0</cell></row><row><cell>GXN CVPR 18 [8]</cell><cell>68.5</cell><cell>-</cell><cell>97.9</cell><cell>56.6</cell><cell>-</cell><cell>94.5</cell></row><row><cell>SCO CVPR 18 [14]</cell><cell>69.9</cell><cell>92.9</cell><cell>97.5</cell><cell>56.7</cell><cell>87.5</cell><cell>94.8</cell></row><row><cell cols="2">(Faster R-CNN, ResNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCAN ECCV 18 [23]</cell><cell>72.7</cell><cell>94.8</cell><cell>98.4</cell><cell>58.8</cell><cell>88.4</cell><cell>94.8</cell></row><row><cell>VSRN (ours)</cell><cell>76.2</cell><cell>94.8</cell><cell>98.2</cell><cell>62.8</cell><cell>89.7</cell><cell>95.1</cell></row><row><cell cols="7">Table 1. Quantitative evaluation results of the image-to-text (cap-</cell></row><row><cell cols="7">tion) retrieval and text-to-image (image) retrieval on MS-COCO</cell></row><row><cell cols="4">1K test set in terms of Recall@K (R@K).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">Caption Retrieval R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell></row><row><cell>(R-CNN, AlexNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DVSA CVPR 15 [15] 11.8</cell><cell>32.5</cell><cell>45.4</cell><cell>8.9</cell><cell>24.9</cell><cell>36.3</cell></row><row><cell>(VGG)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FV CVPR 15 [20]</cell><cell>17.3</cell><cell>39.0</cell><cell>50.2</cell><cell>10.8</cell><cell>28.3</cell><cell>40.1</cell></row><row><cell>VQA ECCV 16 [29]</cell><cell>23.5</cell><cell>50.7</cell><cell>63.6</cell><cell>16.7</cell><cell>40.5</cell><cell>53.8</cell></row><row><cell>OEM ICLR 16 [35]</cell><cell>23.3</cell><cell>-</cell><cell>84.7</cell><cell>31.7</cell><cell>-</cell><cell>74.6</cell></row><row><cell>(ResNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">VSE++ BMVC 18 [5] 41.3</cell><cell>69.2</cell><cell>81.2</cell><cell>30.3</cell><cell>59.1</cell><cell>72.4</cell></row><row><cell>GXN CVPR 18 [8]</cell><cell>42.0</cell><cell>-</cell><cell>84.7</cell><cell>31.7</cell><cell>-</cell><cell>74.6</cell></row><row><cell>SCO CVPR 18 [14]</cell><cell>42.8</cell><cell>72.3</cell><cell>83.0</cell><cell>33.1</cell><cell>62.9</cell><cell>75.5</cell></row><row><cell cols="2">(Faster R-CNN, ResNet)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SCAN ECCV 18 [23] 50.4</cell><cell>82.2</cell><cell>90.0</cell><cell>38.6</cell><cell>69.3</cell><cell>80.4</cell></row><row><cell>VSRN (ours)</cell><cell>53.0</cell><cell>81.1</cell><cell>89.4</cell><cell>40.5</cell><cell>70.6</cell><cell>81.1</cell></row><row><cell cols="7">Table 2. Quantitative evaluation results of the image-to-text (cap-</cell></row><row><cell cols="7">tion) retrieval and text-to-image (image) retrieval on MS-COCO</cell></row><row><cell cols="4">5K test set in terms of Recall@K (R@K).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Quantitative evaluation results of the image-to-text (cap-</cell></row><row><cell>tion) retrieval and text-to-image (image) retrieval on Fliker30K</cell></row><row><cell>test set in terms of Recall@K (R@K).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies on the MS-COCO 1K test set to analyze region ordering for global semantic reasoning. Results are reported in terms of Recall@K (R@K).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is supported in part by the NSF IIS award 1651902 and U.S. Army Research Office Award W911NF-17-1-0367.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Iterative visual reasoning beyond convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vse++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interpretation as abduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">E</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Stickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention: different processes and overlapping neural systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumi</forename><surname>Katsuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Constantinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Neuroscientist</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support neighbor loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tell me where to look: Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Guided attention inference network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention bridging network for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Leveraging visual question answering for image-caption ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Erwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Physical symbol systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Hierarchical multimodal lstm for dense visual-semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Visual semantic navigation using scene priors. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal style transfer via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
