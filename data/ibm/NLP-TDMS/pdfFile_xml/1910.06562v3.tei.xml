<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compacting, Picking and Growing for Unforgetting Continual Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C Y</forename><surname>Hung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Hao</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-En</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Hung</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ming</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Science</orgName>
								<orgName type="department" key="dep2">MOST Joint Research Center for AI Technology and All Vista Healthcare</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compacting, Picking and Growing for Unforgetting Continual Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continual lifelong learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b27">28]</ref> has received much attention in recent deep learning studies. In this research track, we hope to learn a model capable of handling unknown sequential tasks while keeping the performance of the model on previously learned tasks. In continual lifelong learning, the training data of previous tasks are assumed non-available for the newly coming tasks. Although the model learned can be used as a pre-trained model, fine-tuning a model for the new task will force the model parameters to fit new data, which causes catastrophic forgetting <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref> on previous tasks.</p><p>To lessen the effect of catastrophic forgetting, techniques leveraging on regularization of gradients or weights during training have been studied <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>. In Kirkpatrick et al. <ref type="bibr" target="#b13">[14]</ref> and Zenke et al. <ref type="bibr" target="#b48">[49]</ref>, the proposed algorithms regularize the network weights and hope to search a common convergence for the current and previous tasks. Schwarz et al. <ref type="bibr" target="#b39">[40]</ref> introduce a network-distillation method for regularization, which imposes constraints on the neural weights adapted from the teacher to the student network and applies the elastic-weight-consolidation (EWC) <ref type="bibr" target="#b13">[14]</ref> for incremental training. The regularization-based approaches reduce the affection of catastrophic forgetting. However, as the training data of previous tasks are missing during learning and the network capacity is fixed (and limited), the regularization approaches often forget the learned skills gradually. Earlier tasks tend to be forgotten more catastrophically in general. Hence, they would not be a favorable choice when the number of sequential tasks is unlimited.  <ref type="figure">Figure 1</ref>: Compacting, Picking, and Growing (CPG) continual learning. Given a well-trained model, gradual pruning is applied to compact the model to release redundant weights. The compact model weights are kept to avoid forgetting. Then a learnable binary weight-picking mask is trained along with previously released space for new tasks to effectively reuse the knowledge of previous tasks. The model can be expanded for new tasks if it does not meet the performance goal. Best viewed in color.</p><p>To address the data-missing issue (i.e., lacking of the training data of old tasks), data-preserving and memory-replay techniques have been introduced. Data-preserving approaches (such as <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>) are designed to directly save important data or latent codes as an efficient form, while memory-replay approaches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b26">27]</ref> introduce additional memory models such as GANs for keeping data information or distribution in an indirect way. The memory models have the ability to replay previous data. Based on past data information, we can then train a model such that the performance can be recovered to a considerable extent for the old tasks. However, a general issue of memory-replay approaches is that they require explicit re-training using old information accumulated, which leads to either large working memory or compromise between the information memorized and forgetting.</p><p>This paper introduces an approach for learning sustainable but compact deep models, which can handle an unlimited number of sequential tasks while avoiding forgetting. As a limited architecture cannot ensure to remember the skills incrementally learned from unlimited tasks, our method allows growing the architecture to some extent. However, we also remove the model redundancy during continual learning, and thus can increasingly compact multiple tasks with very limited model expansion.</p><p>Besides, pre-training or gradually fine-tuning the models from a starting task only incorporates prior knowledge at initialization; hence, the knowledge base is getting diminished with the past tasks. As humans have the ability to continually acquire, fine-tune and transfer knowledge and skills throughout their lifespan <ref type="bibr" target="#b27">[28]</ref>, in lifelong learning, we would hope that the experience accumulated from previous tasks is helpful to learn a new task. As the model increasingly learned by using our method serves as a compact, un-forgetting base, it generally yields a better model for the subsequent tasks than training the tasks independently. Experimental results reveal that our lifelong learning method can leverage the knowledge accumulated from the past to enhance the performance of new tasks.</p><p>Motivation of Our Method Design: Our method is designed by combining the ideas of deep model compression via weights pruning (Compacting), critical weights selection (Picking), and ProgressiveNet extension (Growing). We refer it to as CPG, whose rationals are given below.</p><p>As stated above, although the regularization or memory-replay approaches lessen the effect of forgetting, they often do not guarantee to preserve the performance for previous tasks. To exactly avoid forgetting, a promising way is to keep the old-task weights already learned <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b46">47]</ref> and enlarge the network by adding nodes or weights for training new tasks. In ProgressiveNet <ref type="bibr" target="#b37">[38]</ref>, to ease the training of new tasks, the old-task weights are shared with the new ones but remain fixed, where only the new weights are adapted for the new task. As the old-task weights are kept, it ensures the performance of learned tasks. However, as the complexity of the model architecture is proportional to the number of tasks, it yields a highly redundant structure for keeping multiple models.</p><p>Motivated by ProgressiveNet, we design a method allowing the sustainability of architecture too. To avoid constructing a complex and huge structure like ProgressiveNet, we perform model compression for the current task every time so that a condensed model is established for the old tasks. According to deep-net compression <ref type="bibr" target="#b9">[10]</ref>, much redundancy is contained in a neural network and removing the redundant weights does not affect the network performance. Our approach exploits this property, which compresses the current task by deleting neglectable weights. This yields a compressing-andgrowing loop for a sequence of tasks. Following the idea of ProgressiveNet, the weights preserved for the old tasks are set as invariant to avoid forgetting in our approach. However, unlike ProgressiveNet where the architecture is always grown for a new task, as the weights deleted from the current task can be released for use for the new tasks, we do not have to grow the architecture every time but can employ the weights released previously for learning the next task. Therefore, in the growing step of our CPG approach, two possible choices are provided. The first is to use the previously released weights for the new task. If the performance goal is not fulfilled yet when all the released weights are used, we then proceed to the second choice where the architecture is expanded and both the released and expanded weights are used for the new-task training.</p><p>Another distinction of our approach is the "picking" step. The idea is motivated below. In Progres-siveNet, the old-tasks weights preserved are all co-used (yet remain fixed) for learning the new tasks. However, as the number of tasks is increased, the amount of old-task weights is getting larger too. When all of them are co-used with the weights newly added in the growing step, the old weights (that are fixed) act like inertia since only the fewer new weights are allowed to be adapted, which tends to slow down the learning process and make the solution found immature in our experience. To address this issue, we do not employ all the old-task weights but picking only some critical ones from them via a differentiable mask. In the picking step of our CPG approach, the old weights' picking-mask and the new weights added in the growing step are both adapted to learn an initial model for the new task. Then, likewise, the initial model obtained is compressed and preserved for the new task as well.</p><p>To compress the weights for a task, a main difficulty is the lacking of prior knowledge to determine the pruning ratio. To solve this problem, in the compacting step of our CPG approach, we employ the gradual pruning procedure <ref type="bibr" target="#b50">[51]</ref> that prunes a small portion of weights and retrains the remaining weights to restore the performance iteratively. The procedure stops when meeting a pre-defined accuracy goal. Note that only the newly added weights (from the released and/or expanded ones in the growing step) are allowed to be pruned, whereas the old-task weights remain unchanged.</p><p>Method Overview: Our method overview is depicted as follows. The CPG method compresses the deep model and (selectively) expands the architecture alternatively. First, a compressed model is built from pruning. Given a new task, the weights of the old-task models are fixed as well. Next, we pick and re-use some of the old-task weights critical to the new task via a differentiable mask, and use the previously released weights for learning together. If the accuracy goal is not attained yet, the architecture can be expanded by adding filters or nodes in the model and resuming the procedure. Then we repeat the gradual pruning <ref type="bibr" target="#b50">[51]</ref> (i.e., iteratively removing a portion of weights and retraining) for compacting the model of the new task. An overview of our CPG approach is given in <ref type="figure">Figure 1</ref>.</p><p>The new-task weights are formed by a combination of two parts: the first part is picked via a learnable mask on the old-task weights, and the second part is learned by gradual pruning/retraining of the extra weights. As the old-task weights are only picked but fixed, we can integrate the required function mappings in a compact model without affecting their accuracy in inference. Main characteristics of our approach are summarized as follows.</p><p>Avoid forgetting: Our approach ensures unforgetting. The function mappings previously built are maintained as exactly the same when new tasks are incrementally added.</p><p>Expand with shrinking: Our method allows expansion but keeps the compactness of the architecture, which can potentially handle unlimited sequential tasks. Experimental results reveal that multiple tasks can be condensed in a model with slight or no architecture growing.</p><p>Compact knowledge base: Experimental results show that the condensed model recorded for previous tasks serves as knowledge base with accumulated experience for weights picking in our approach, which yields performance enhancement for learning new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Continual lifelong learning <ref type="bibr" target="#b27">[28]</ref> can be divided into three main categories: network regularization, memory or data replay, and dynamic architecture. Besides, works on task-free <ref type="bibr" target="#b1">[2]</ref> or as a program synthesis <ref type="bibr" target="#b42">[43]</ref> have also been studied recently. In the following, we give a brief review of works in the main categories, and readers are suggested to refer to a recent survey paper <ref type="bibr" target="#b27">[28]</ref> for more studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network regularization:</head><p>The key idea of network regularization approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> is to restrictively update learned model weights. To keep the learned task information, some penalties are added to the change of weights. EWC <ref type="bibr" target="#b13">[14]</ref> uses Fisher's information to evaluate the importance of weights for old tasks, and updates weights according to the degree of importance. Based on similar ideas, the method in <ref type="bibr" target="#b48">[49]</ref> calculates the importance by the learning trajectory. Online EWC <ref type="bibr" target="#b39">[40]</ref> and EWC++ <ref type="bibr" target="#b4">[5]</ref> improve the efficiency issues of EWC. Learning without Memorizing(LwM) <ref type="bibr" target="#b5">[6]</ref> presents an information preserving penalty. The approach builds an attention map, and hopes that the attention region of the previous and concurrent models are consistent. These works alleviate catastrophic forgetting but cannot guarantee the previous-task accuracy exactly.</p><p>Memory replay: Memory or data replay methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27]</ref> use additional models to remember data information. Generative Replay <ref type="bibr" target="#b40">[41]</ref> introduces GANs to lifelong learning. It uses a generator to sample fake data which have similar distribution to previous data. New tasks can be trained with these generated data. Memory Replay GANs (MeRGANs) <ref type="bibr" target="#b44">[45]</ref> shows that forgetting phenomenon still exists in a generator, and the property of generated data will become worse with incoming tasks. They use replay data to enhance the generator quality. Dynamic Generative Memory (DGM) <ref type="bibr" target="#b26">[27]</ref> uses neural masking to learn connection plasticity in conditional generative models, and set a dynamic expansion mechanism in the generator for sequential tasks. Although these methods can exploit data information, they still cannot guarantee the exact performance of past tasks.</p><p>Dynamic architecture: Dynamic-architecture approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref> adapt the architecture with a sequence of tasks. ProgressiveNet <ref type="bibr" target="#b37">[38]</ref> expands the architecture for new tasks and keeps the function mappings by preserving the previous weights. LwF <ref type="bibr" target="#b19">[20]</ref> divides the model layers into two parts, shared and task-specific, where the former are co-used by tasks and the later are grown with further branches for new tasks. DAN <ref type="bibr" target="#b35">[36]</ref> extends the architecture per new task, while each layer in the new-task model is a sparse linear-combination of the original filters in the corresponding layer of a base model. Architecture expansion has also been adopted in a recent memory-replay approach <ref type="bibr" target="#b26">[27]</ref> on GANs. These methods can considerably lessen or avoid catastrophic forgetting via architecture expansion, but the model is monotonically increased and a redundant structure would be yielded.</p><p>As continually growing the architecture will retain the model redundancy, some approach performs model compression before expansion <ref type="bibr" target="#b47">[48]</ref> so that a compact model can be built. In the past, the most related method to ours is Dynamic-expansion Net (DEN) <ref type="bibr" target="#b47">[48]</ref>. DEN reduces the weights of the previous tasks via sparse-regularization. Newly added weights and old weights are both adapted for the new task with sparse constraints. However, DEN does not ensure non-forgetting. As the old-task weights are jointly trained with the new weights, part of the old-tasks weights are selected and modified. Hence, a "Split &amp; Duplication" step is introduced to further 'restore' some of the old weights modified for lessening the forgetting effect. Pack and Expand (PAE) <ref type="bibr" target="#b11">[12]</ref> is our previous approach that takes advantage of PackNet <ref type="bibr" target="#b22">[23]</ref> and ProgressiveNet <ref type="bibr" target="#b37">[38]</ref>. It can avoid forgetting, maintain model compactness, and allow dynamic model expansion. However, as it uses all weights of previous tasks for sharing, the performance becomes less favorable when learning a new task.</p><p>Our approach (CPG) is accomplished by a compacting→picking(→growing) loop, which selects critical weights from old tasks without modifying them, and thus avoids forgetting. Besides, our approach does not have to restore the old-task performance like DEN as the performance is already kept, which thus avoids a tedious "Split &amp; Duplication" process which takes extra time for model adjustment and will affect the new-task performance. Our approach is hence simple and easier to implement. In the experimental results, we show that our approach also outperforms DEN and PAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The CPG approach for Continual Lifelong Learning</head><p>Without loss of generality, our work follows a task-based sequential learning setup that is a common setting in continual learning. In the following, we present our method in the sequential-task manner.</p><p>Task 1: Given the first task (task-1) and an initial model trained via its dataset, we perform gradual pruning <ref type="bibr" target="#b50">[51]</ref> on the model to remove its redundancy with the performance kept. Instead of pruning weights one time to the pruning ratio goal, the gradual pruning removes a portion of the weights and retrains the model to restore the performance iteratively until meeting the pruning criteria. Thus, we compact the current model so that redundancy among the model weights are removed (or released). The weights in the compact model are then set unalterable and remain fixed to avoid forgetting. After gradual pruning, the model weights can be divided into two parts: one is preserved for task 1; the other is released and able to be employed by the subsequent tasks.</p><p>Task k to k+1: Assume that in task-k, a compact model that can handle tasks 1 to k has been built and available. The model weights preserved for tasks 1 to k are denoted as W P 1:k . The released (redundant) weights associated with task-k are denoted as W E k , and they are extra weights that can be used for subsequent tasks. Given the dataset of task-(k + 1), we apply a learnable mask M to pick the old weights W P 1:k , M ∈ {0, 1} D with D the dimension of W P 1:k . The weights picked are then represented as M W P 1:k , the element-wise product of the 0-1 mask M and W P 1:k . Without loss of generality, we use the piggyback approach <ref type="bibr" target="#b21">[22]</ref> that learns a real-valued maskM and applies a threshold for binarization to construct M. Hence, given a new task, we pick a set of weights (known as the critical weights) from the compact model via a learnable mask. Besides, we also use the released weights W E k for the new task. The mask M and the additional weights W E k are learned together on the training data of task-(k+1) with the loss function of task-(k+1) via back-propagation. Since the binarized mask M is not differentiable, when training the binary mask M, we update the real-valued maskM in the backward pass; then M is quantized with a threshold onM and applied to the forward pass. If the performance is unsatisfied yet, the model architecture can be grown to include more weights for training. That is, W E k can be augmented with additional weights (such as new filters in convolutional layers and nodes in fully-connected layers) and then resumes the training of both M and W E k . Note that during traning, the mask M and new weights W E k are adapted but the old weights W P 1:k are "picked" only and remain fixed. Thus, old tasks can be exactly recalled. Compaction of task k+1: After M and W E k are learned, an initial model of task-(k + 1) is obtained. Then, we fix the mask M and apply gradual pruning to compress W E k , so as to get the compact model W P k+1 and the redundant (released) weights W E k+1 for task-(k + 1). The compact model of old tasks then becomes W P 1:(k+1) = W P 1:k ∪ W P k+1 . The compacting and picking/growing loop is repeated from task to task. Details of CPG continual learning is listed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Compacting, Picking and Growing Continual Learning</head><p>Input: given task 1 and an original model trained on task 1. Set an accuracy goal for task 1; Alternatively remove small weights and re-train the remaining weights for task 1 via gradual pruning <ref type="bibr" target="#b50">[51]</ref>, whenever the accuracy goal is still hold; Let the model weights preserved for task 1 be W P 1 (referred to as task-1 weights), and those that are removed by the iterative pruning be W E 1 (referred to as the released weights); for task k = 2 · · · K (let the released weights of task k be W E k ) do Set an accuracy goal for task k; Apply a mask M to the weights W P 1:k−1 ; train both M and W E k−1 for task k, with W P 1:k−1 fixed; If the accuracy goal is not achieved, expand the number of filters (weights) in the model, reset W E k−1 and go to previous step; Gradually prune W E k−1 to obtain W E k (with W P 1:k−1 fixed) for task k, until meeting the accuracy goal;</p><formula xml:id="formula_0">W P k = W E k−1 \W E k and W P 1:k = W P 1:k−1 ∪ W P k ; end 4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>We perform three experiments to verify the effectiveness of our approach. The first experiment contains 20 tasks organized with CIFAR-100 dataset <ref type="bibr" target="#b15">[16]</ref>. In the second experiment, we follow the same settings of PackNet <ref type="bibr" target="#b22">[23]</ref> and Piggyback <ref type="bibr" target="#b21">[22]</ref> approaches, where several fine-grained datasets are chosen for classification in an incremental manner. In the third experiment, we start from face verification and compact three further facial-informatic tasks (expression, gender, and age) incrementally to examine the performance of our continual learning approach in a realistic scenario. We implement our CPG approach 1 and independent task learning (from scratch or fine-tuning) via PyTorch <ref type="bibr" target="#b29">[30]</ref> in all experiments, but implement DEN <ref type="bibr" target="#b26">[27]</ref> via Tensorflow <ref type="bibr" target="#b0">[1]</ref> with its official codes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Twenty Tasks of CIFAR-100</head><p>We divide the CIFAR-100 dataset into 20 tasks. Each task has 5 classes, 2500 training images, and 500 testing images. In the experiment, VGG16-BN model (VGG16 with batch normalization layers) is employed to train the 20 tasks sequentially. First, we compare our approach with DEN <ref type="bibr" target="#b26">[27]</ref> (as it also uses an alternating mechanism of compression and expansion) and fine-tuning. To implement fine-tuning, we train task-1 from scratch by using VGG16-BN; then, assuming the models of task 1 to task k are available, we then train the model of task-(k + 1) by fine-tuning one of the models randomly selected from tasks 1 to k. We repeat this process 5 times and get the average accuracy (referred to as Finetune Avg). To implement our CPG approach, task-1 is also trained by using VGG16-BN, and this initial model is adapted for the sequential tasks following Algorithm 1. DEN is implemented via the official codes provided by the authors and modified for VGG16-BN. <ref type="figure">Figure 2</ref> shows the classification accuracy of DEN, fine-tuning, and our CPG. <ref type="figure">Figure 2</ref>(a) is the accuracy of task-1 when all of the 20 tasks have been trained. Initially, the accuracy of DEN is higher than CPG and fine-tuning although the same model is trained from scratch. We conjecture that it is because they are implemented on different platforms (Tensorflow vs PyTorch). Nevertheless, the performance of task-1 gradually drops when the other tasks (2 to 20) are increasingly learned in DEN, as shown in <ref type="figure">Figure 2(a)</ref>, and the drops are particularly significant for tasks 15 to 20. In <ref type="figure">Figure 2(b)</ref>, the initial accuracy of DEN on task-5 becomes a little worse than that of CPG and fine-tuning. It reveals that DEN could not employ the previously leaned model (tasks 1-4) to enhance the performance of the current task (task 5). Besides, the accuracy of task-5 still drops when new tasks <ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref><ref type="bibr" target="#b17">(18)</ref><ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref> are learned. Similarly, for tasks 10 and 15 respectively shown in <ref type="figure">Figures 2(c) and (d)</ref>, DEN has a large performance gap on the initial model, with an increasing accuracy dropping either.</p><p>We attribute the phenomenon as follows. As DEN does not guarantee unforgetting, a "Split &amp; Duplication" step is enforced to recover the old-task performance. Though DEN tries to preserve the learned tasks as much as they could via optimizing weight sparsity, the tuning of hyperparameters in its loss function makes DEN non-intuitive to balance the learning of the current task and remembering the previous tasks. The performance thus drops although we have tried our best for tuning it. On the other hand, fine-tuning and our CPG have roughly the same accuracy initially on task-1 (both are trained from scratch), whereas CPG gradually outperforms fine-tuning on tasks 5, 10, and 15 in <ref type="figure">Figure 2</ref>. The results suggest that our approach can exploit the accumulated knowledge base to enhance the new task performance. After model growing for 20 tasks, the final amount of weights is increased by 1.09 times (compared to VGG16-BN) for both DEN and CPG. Hence, our approach can not only ensure maintaining the old-task performance (as the horizontal lines shown in <ref type="figure">Figure 2</ref>), but effectively accumulate the weights for knowledge picking.</p><p>Unlike ProgressiveNet that uses all of the weights kept for the old tasks when training the new task, our method only picks the old-task weights critical to the new tasks. To evaluate the effectiveness of the weights picking mechanism, we compare CPG with PAE <ref type="bibr" target="#b11">[12]</ref> and PackNet <ref type="bibr" target="#b22">[23]</ref>. In our method, if all of the old weights are always picked, it is referred to as the pack-and-expand (PAE) approach. If we further restrict PAE such that the architecture expansion is forbidden, it degenerates to an existing approach, PackNet <ref type="bibr" target="#b22">[23]</ref>. Note that both PAE and PackNet ensure unforgetting. As shown in <ref type="table" target="#tab_1">Table 1</ref>, besides the first two tasks, CPG performs more favorably than PAE and PackNet consistently. The results reveal that the critical-weights picking mechanism in CPG not only reduces the unnecessary weights but also boost the performance for new tasks. As PackNet does not allow model expansion, its weights amount remains the same (1×). However, when proceeding with more tasks, available space in PackNet gradually reduces, which limits the effectiveness of PackNet to learn new tasks. PAE uses all the previous weights during learning. As with more tasks, the weights from previous  tasks would dominate the whole network and become a burden in learning new tasks. Finally, as shown in the Expand (Exp.) field in <ref type="table" target="#tab_1">Table 1</ref>, PAE grows the model and uses 2 times of weights for the 20 tasks. Our CPG expands to 1.5× of weights (with 0.41× redundant weights that can be released to future tasks). Hence, CPG finds a more compact and sustainable model with better accuracy when the picking mechanism is enforced. <ref type="table" target="#tab_2">Table 2</ref> shows the performance of different settings of our CPG method, together with their comparison to independent task learning (including learning from scratch and fine-tuning from a pre-trained model). In this table, 'scratch' means learning each task independently from scratch via the VGG16-BN model. As depicted before, 'fine-Avg' means the average accuracy of fine-tuning from a previous model randomly selected and repeats the process 5 times. 'fine-Max.' means the maximum accuracy of these 5 random trials. In the implementation of our CPG algorithm, an accuracy goal has to be set for the gradual-pruning and model-expansion steps. In this table, the 'avg', 'max', and 'top' correspond to the settings of accuracy goals to be fine-Avg, fine-Max, and a slight increment of the maximum of both, respectively. The upper bound of model weights expansion is set as 1.5 in this experiment. As can be seen in <ref type="table" target="#tab_2">Table 2</ref>, CPG gets better accuracy than both the average and maximum of fine-tuning in general. CPG also performs more favorably than learning from scratch averagely. This reveals again that the knowledge previously learned with our CPG can help learn new tasks.</p><p>Besides, the results show that a higher accuracy goal yields better performance and more consumption of weights in general. In <ref type="table" target="#tab_2">Table 2</ref>, the accuracy achieved by 'CPG avg', 'CPG max', and 'CPG top' is getting increased. The former remains to have 0.41× redundant weights that are saved for future use, whereas the later two consume all weights. The model size includes not only the backbone model weights, but also the overhead of final layers increased with new classes, batch-normalization parameters, and the binary masks. Including all overheads, the model sizes of CPG for the three settings are 2.16×, 2.40× and 2.41× of the original VGG16-BN, as shown in <ref type="table" target="#tab_3">Table 3</ref>. Compared to independent models (learning-from-scratch or fine-tuning) that require 20× for maintaining the old-task accuracy, our approach can yield a far smaller model to achieve exact unforgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fine-grained Image Classification Tasks</head><p>In this experiment, following the same settings in the works of PackNet <ref type="bibr" target="#b22">[23]</ref> and Piggyback <ref type="bibr" target="#b21">[22]</ref>, six image classification datasets are used. The statistics are summarized in <ref type="table" target="#tab_4">Table 4</ref>, where ImageNet <ref type="bibr" target="#b16">[17]</ref> is the first task, following by fine-grained classification tasks, CUBS <ref type="bibr" target="#b43">[44]</ref>, Stanford Cars <ref type="bibr" target="#b14">[15]</ref> and Flowers <ref type="bibr" target="#b25">[26]</ref>, and finally WikiArt <ref type="bibr" target="#b38">[39]</ref> and Sketch <ref type="bibr" target="#b7">[8]</ref> that are artificial images drawing in various styles and objects. Unlike previous experiments where the first task consists of some of the five classes from CIFAR-100. In this experiment, the first-task classifier is trained on ImageNet, which is a strong base for fine-tuning. Hence, in the fine-tuning setting of this experiment, tasks 2 to 6 are all fine-tuned from the task-1, instead of selecting a previous task randomly. For all tasks, the image size is 224 × 224, and the architecture used in this experiment is ResNet50.     The performance is shown on <ref type="table" target="#tab_6">Table 6</ref>. Five methods are compared with CPG: training from scratch, fine-tuning, ProgressiveNet, PackNet, and Piggyback. For the first task (ImageNet), CPG and PackNet performs slightly worse than the others, since both methods have to compress the model (ResNet50) via pruning. Then, for tasks 2 to 6, CPG outperforms the others in almost all cases, which shows the superiority of our method on building a compact and unforgetting base for continual learning. As for the model size, ProgressiveNet increases the model per task. Learning-from-scratch and fine-tuning need 6 models to achieve unforgetting. Their model sizes are thus large. CPG yields a smaller model size comparable to piggyback, which is favorable when considering both accuracy and model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Facial-informatic Tasks</head><p>In a realistic scenario, four facial-informatic tasks, face verification, gender, expression and age classification are used with the datasets summarized in <ref type="table" target="#tab_5">Table 5</ref>. For face verification, we use VGGFace2 <ref type="bibr" target="#b3">[4]</ref> for training and LFW <ref type="bibr" target="#b17">[18]</ref> for testing. For gender classification, we combine FotW <ref type="bibr" target="#b8">[9]</ref> and IMDB-Wiki <ref type="bibr" target="#b36">[37]</ref> datasets and classify faces into three categories, male, female and other. The AffectNet dataset <ref type="bibr" target="#b24">[25]</ref> is used for expression classification that classifies faces into seven primary emotions. Finally, Adience dataset <ref type="bibr" target="#b6">[7]</ref> contains faces with labels of eight different age groups. For these datasets, faces are aligned using MTCNN <ref type="bibr" target="#b49">[50]</ref> with output size of 112 × 112. We use the 20-layer CNN in SphereFace <ref type="bibr" target="#b20">[21]</ref> and train a model for the face verification task accordingly. We compare CPG with the models fine-tuned from the face verification task. The results are reported in <ref type="table" target="#tab_7">Table 7</ref>. Compared with individual models (training-from-scratch and fine-tuning), CPG can achieve comparable or more favorable results without additional expansion. After learning four tasks, CPG still has 0.003 × of the released weights able to be used for new tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We introduce a simple but effective method, CPG, for continual learning avoiding forgetting. Compacting a model can prevent the model complexity from unaffordable when the number of tasks is increased. Picking learned weights using binary masks and train them together with newly added weights is an effective way to reuse previous knowledge. The weights for old tasks are preserved, and thus prevents them from forgetting. Growing the model for new tasks facilitates the model to learn unlimited and unknown/un-related tasks. CPG is easy to be realized and applicable to real situations. Experiments show that CPG can achieve similar or better accuracy with limited additional space. Currently, our method compacts a model by weights pruning, and we plan to include channel pruning in the future. Besides, we assume that clear boundaries exit between the tasks, and will extend our approach to handle continual learning problems without task boundaries. In the future, We also plan to provide a mechanism of "selectively forgetting" some previous tasks via the masks recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of "Compacting, Picking and Growing for Unforgetting Continual Learning," NeurIPS 2019</head><p>In this supplementary material, we report additional experimental results on CIFAR-100 dataset. Remember that we have divided this dataset into 20 tasks. Each task has 5 classes.</p><p>Random Order of Tasks: We show that our approach remains effective when the order of tasks is changed. To verify this, we reshuffle the 20 tasks and apply our continual-learning method to the tasks of new orders. The results are shown in <ref type="table" target="#tab_8">Table 8</ref>. In this table, CFP-ord.i, (i = 1 · · · 3) are the three re-ordering sequences of tasks, and CFP-ord.0 is the case of original ordering (with the same results of CPG shown in <ref type="table" target="#tab_1">Table 1</ref>). As can be seen, the average accuracy of the 20 tasks is roughly the same for the random-ordering settings, which reveals that our approach is potentially applicable to continual lifelong learning when the order of tasks is arbitrary. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>15 Figure 2 :</head><label>152</label><figDesc>The accuracy of DEN, Finetune and CPG for the sequential tasks 1, 5, 10, 15 on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The performance of PackNet, PAE and CPG on CIFAR-100 twenty tasks. We use Avg., Exp. and Red. as abbreviations for Average accuracy, Expansion weights and Redundant weights. PackNet 66.4 80.0 76.2 78.4 80.0 79.8 67.8 61.4 68.8 77.2 79.0 59.4 66.4 57.2 36.0 54.2 51.6 58.8 67.8 83.</figDesc><table><row><cell cols="2">Methods 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10 11 12 13 14 15 16 17 18 19 20 Avg.</cell><cell>Exp. (×)</cell><cell>Red. (×)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 67.5</cell><cell>1</cell><cell>0</cell></row><row><cell>PAE</cell><cell cols="10">67.2 77.0 78.6 76.0 84.4 81.2 77.6 80.0 80.4 87.8 85.4 77.8 79.4 79.6 51.2 68.4 68.6 68.6 83.2 88.8 77.1</cell><cell>2</cell><cell>0</cell></row><row><cell>CPG</cell><cell cols="12">65.2 76.6 79.8 81.4 86.6 84.8 83.4 85.0 87.2 89.2 90.8 82.4 85.6 85.2 53.2 74.4 70.0 73.4 88.8 94.8 80.9 1.5 0.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance of CPGs and individual models on CIFAR-100 twenty tasks. We use fine-Avg and fine-Max as abbreviations for Average and Max accuracy of the 5 fine-tuning models.</figDesc><table><row><cell>Methods</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10 11 12 13 14 15 16 17 18 19 20 Avg.</cell><cell>Exp. (×)</cell><cell>Red. (×)</cell></row><row><cell cols="11">Scratch 65.8 78.4 76.6 82.4 82.2 84.6 78.6 84.8 83.4 89.4 87.8 80.2 84.4 80.2 52.0 69.4 66.4 70.0 87.2 91.2 78.8 20</cell><cell>0</cell></row><row><cell cols="11">fine-Avg 65.2 76.1 76.1 77.8 85.4 82.5 79.4 82.4 82.0 87.4 87.4 81.5 84.6 80.8 52.0 72.1 68.1 71.9 88.1 91.5 78.6 20</cell><cell>0</cell></row><row><cell cols="11">fine-Max 65.8 76.8 78.6 80.0 86.2 84.8 80.4 84.0 83.8 88.4 89.4 83.8 87.2 82.8 53.6 74.6 68.8 74.4 89.2 92.2 80.2 20</cell><cell>0</cell></row><row><cell cols="12">CPG avg 65.2 76.6 79.8 81.4 86.6 84.8 83.4 85.0 87.2 89.2 90.8 82.4 85.6 85.2 53.2 74.4 70.0 73.4 88.8 94.8 80.9 1.5 0.41</cell></row><row><cell cols="12">CPG max 67.0 79.2 77.2 82.0 86.8 87.2 82.0 85.6 86.4 89.6 90.0 84.0 87.2 84.8 55.4 73.8 72.0 71.6 89.6 92.8 81.2 1.5 0</cell></row><row><cell cols="12">CPG top 66.6 77.2 78.6 83.2 88.2 85.8 82.4 85.4 87.6 90.8 91.0 84.6 89.2 83.0 56.2 75.4 71.0 73.8 90.6 93.6 81.7 1.5 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Model sizes on CIFAR-100 twenty tasks.</figDesc><table><row><cell>Methods</cell><cell>Model Size (MB)</cell></row><row><cell>VGG16-BN</cell><cell>128.25</cell></row><row><cell>Individual Models</cell><cell>2565</cell></row><row><cell>CPG avg</cell><cell>278</cell></row><row><cell>CPG max</cell><cell>308</cell></row><row><cell>CPG top</cell><cell>310</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Statistics of the finegrained datasets</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Train #Eval #Classes</cell></row><row><cell>ImageNet</cell><cell cols="3">1,281,167 50,000 1,000</cell></row><row><cell>CUBS</cell><cell>5,994</cell><cell>5,794</cell><cell>200</cell></row><row><cell>Stanford Cars</cell><cell>8,144</cell><cell>8,041</cell><cell>196</cell></row><row><cell>Flowers</cell><cell>2,040</cell><cell>6,149</cell><cell>102</cell></row><row><cell>WikiArt</cell><cell cols="2">42,129 10,628</cell><cell>195</cell></row><row><cell>Sketch</cell><cell>16,000</cell><cell>4,000</cell><cell>250</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the facialinformatic datasets</figDesc><table><row><cell>Dataset</cell><cell>#Train</cell><cell cols="2">#Eval #Classes</cell></row><row><cell cols="2">VGGFace2 3,137,807</cell><cell>0</cell><cell>8,6301</cell></row><row><cell>LFW</cell><cell>0</cell><cell>13,233</cell><cell>5,749</cell></row><row><cell>FotW</cell><cell>6,171</cell><cell>3,086</cell><cell>3</cell></row><row><cell cols="2">IMDB-Wiki 216,161</cell><cell>0</cell><cell>3</cell></row><row><cell>AffectNet</cell><cell>283,901</cell><cell>3,500</cell><cell>7</cell></row><row><cell>Adience</cell><cell>12,287</cell><cell>3,868</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Accuracy on fine-grained dataset.</figDesc><table><row><cell>Dataset</cell><cell>Train from Scratch</cell><cell>Finetune</cell><cell>Prog. Net</cell><cell cols="3">PackNet Piggyback CPG</cell></row><row><cell>ImageNet</cell><cell>76.16</cell><cell>-</cell><cell cols="2">76.16 75.71</cell><cell cols="2">76.16 75.81</cell></row><row><cell>CUBS</cell><cell>40.96</cell><cell cols="3">82.83 78.94 80.41</cell><cell cols="2">81.59 83.59</cell></row><row><cell>Stanford Cars</cell><cell>61.56</cell><cell cols="3">91.83 89.21 86.11</cell><cell cols="2">89.62 92.80</cell></row><row><cell>Flowers</cell><cell>59.73</cell><cell cols="3">96.56 93.41 93.04</cell><cell cols="2">94.77 96.62</cell></row><row><cell>Wikiart</cell><cell>56.50</cell><cell cols="3">75.60 74.94 69.40</cell><cell cols="2">71.33 77.15</cell></row><row><cell>Sketch</cell><cell>75.40</cell><cell cols="3">80.78 76.35 76.17</cell><cell cols="2">79.91 80.33</cell></row><row><cell>Model Size (MB)</cell><cell>554</cell><cell>554</cell><cell>563</cell><cell>115</cell><cell>121</cell><cell>121</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Accuracy on facial-informatic tasks.</figDesc><table><row><cell>Task</cell><cell>Train from Scratch</cell><cell>Finetune</cell><cell>CPG</cell></row><row><cell>Face</cell><cell>99, 417 ± 0.367</cell><cell>-</cell><cell>99.300 ± 0.384</cell></row><row><cell>Gender</cell><cell>83.70</cell><cell>90.80</cell><cell>89.66</cell></row><row><cell>Expression</cell><cell>57.64</cell><cell>62.54</cell><cell>63.57</cell></row><row><cell>Age</cell><cell>46.14</cell><cell>57.27</cell><cell>57.66</cell></row><row><cell>Exp. (×)</cell><cell>4</cell><cell>4</cell><cell>1</cell></row><row><cell>Red. (×)</cell><cell>0</cell><cell>0</cell><cell>0.003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The performance of CPG on CIFAR-100 twenty tasks in random order.CPG-ord.0 65.2 76.6 79.8 81.4 86.6 84.8 83.4 85.0 87.2 89.2 90.8 82.4 85.6 85.2 53.2 74.4 70.0 73.4 88.8 94.8 80.9 1.5 0.41 CPG-ord.1 81.0 83.8 78.6 79.4 82.4 52.4 67.4 89.6 86.0 74.8 85.0 85.8 87.6 92.4 87.2 80.0 72.6 71.2 86.0 89.8 80.7 1.30 0 CPG-ord.2 73.0 88.2 65.8 70.0 87.6 80.6 88.4 79.2 85.0 87.2 80.6 52.2 86.2 84.0 83.0 73.2 90.2 74.4 83.0 94.2 80.3 1.5 0 CPG-ord.3 86.0 87.8 70.0 89.0 69.6 53.6 85.2 84.8 88.6 75.6 92.4 87.2 87.0 78.0 79.6 84.0 81.8 83.2 66.4 79.2 80.5 1.4 0</figDesc><table><row><cell>Methods</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell cols="4">9 10 11 12 13 14 15 16 17 18 19 20 Avg.</cell><cell>Exp. (×)</cell><cell>Red. (×)</cell></row><row><cell>Methods</cell><cell cols="2">12 9</cell><cell>3</cell><cell>2</cell><cell cols="8">7 15 1 19 13 16 8 14 10 20 5</cell><cell>4 18 17 6 11 Avg.</cell><cell>Exp. (×)</cell><cell>Red. (×)</cell></row><row><cell>Methods</cell><cell cols="5">2 11 1 17 5</cell><cell cols="3">4 10 3</cell><cell cols="4">9 13 7 15 6 14 12 18 19 16 8 20 Avg.</cell><cell>Exp. (×)</cell><cell>Red. (×)</cell></row><row><cell>Methods</cell><cell cols="9">19 13 18 10 17 15 6 14 11 16 20 5</cell><cell>9</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>7 12 1</cell><cell>3 Avg.</cell><cell>Exp. (×)</cell><cell>Red. (×)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our codes are available at https://github.com/ivclab/CPG.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers and area chair for their constructive comments. This work is supported in part under contract MOST 108-2634-F-001-004.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Task-free continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaas</forename><surname>Kelchtermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Subset replay based continual learning for scalable improvement of autonomous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrienne</forename><surname>Pratik Prabhanjan Brahma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Othon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPRW</title>
		<meeting>the IEEE CVPRW</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE FG</title>
		<meeting>IEEE FG</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning without memorizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat Vikram</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chuan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="44" to="45" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chalearn looking at people and faces of the world: Face analysis workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><forename type="middle">Torres</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Corneou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Ali</forename><surname>Bagheri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPRW</title>
		<meeting>IEEE CVPRW</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting via model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwei</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Increasingly packing multiple facial-informatics modules in a unified deep-learning model via lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Hong</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timmy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chein-Hung</forename><surname>St Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia Retrieval (ICMR)</title>
		<meeting>International Conference on Multimedia Retrieval (ICMR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fearnet: Brain-inspired model for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in face detection and facial image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwoo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedins of ECCV</title>
		<meeting>eedins of ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall C O&amp;apos;</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reilly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Comput</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to remember: A synaptic plasticity driven framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jähnichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the role of neurogenesis in overcoming catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><forename type="middle">Ignacio</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS Workshop on Continual Learning</title>
		<meeting>NeurIPS Workshop on Continual Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comprehensive, application-oriented study of catastrophic forgetting in dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfulb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gepperth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to learn without forgetting by maximizing transfer and minimizing interference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ajemian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhai</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scalable recollections for continual lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djallel</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Franceschini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online structured laplace approximations for overcoming catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hippolyt</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Incremental learning through deep adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Large-scale classification of fine-art paintings: Learning the right metric on the right feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICDMW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Progress &amp; compress: A scalable framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A lifelong learning perspective for mobile robot control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Houdini: Lifelong learning as program synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazar</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipak</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swarat</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Memory replay gans: Learning to generate new categories without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenshen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Incremental classifier learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno>abs/1802.00853</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Error-driven incremental learning in deep convolutional neural network for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM-MM</title>
		<meeting>ACM-MM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">To prune, or not to prune: Exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
