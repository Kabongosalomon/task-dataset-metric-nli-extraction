<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformers Solve the Limited Receptive Field for Monocular Depth Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Haerbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Haerbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Haerbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformers Solve the Limited Receptive Field for Monocular Depth Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While convolutional neural networks have shown a tremendous impact on various computer vision tasks, they generally demonstrate limitations in explicitly modeling long-range dependencies due to the intrinsic locality of the convolution operation. Transformers, initially designed for natural language processing tasks, have emerged as alternative architectures with innate global self-attention mechanisms to capture long-range dependencies. In this paper, we propose TransDepth, an architecture which benefits from both convolutional neural networks and transformers. To avoid the network to loose its ability to capture local-level details due to the adoption of transformers, we propose a novel decoder which employs on attention mechanisms based on gates. Notably, this is the first paper which applies transformers into pixel-wise prediction problems involving continuous labels (i.e., monocular depth prediction and surface normal estimation). Extensive experiments demonstrate that the proposed TransDepth achieves stateof-the-art performance on three challenging datasets. The source code and trained models are available at https: //github.com/ygjwd12345/TransDepth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past decade, convolutional neural networks have become the privileged methodology to address fundamental and challenging computer vision tasks requiring dense pixel-wise prediction, such as semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, monocular depth prediction <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b16">17]</ref>, and normal surface computation <ref type="bibr" target="#b40">[41]</ref>. Since the seminal work of <ref type="bibr" target="#b25">[26]</ref>, existing depth prediction models' have been dominated by encoders implemented with architectures such as ResNet and VGG-Net. The encoder progressively reduces the spatial resolution and learns more concepts with larger receptive fields. Because context modeling is critical for pixel-level prediction, deep feature representation learning is arguably the most critical model component <ref type="bibr" target="#b4">[5]</ref>. However, it is still challenging for depth prediction networks to improve their ability in modeling global contexts. Traditionally, both stacked convolution layers and consecutive down-sampling are used in the encoders to generate sufficiently large receptive fields of deep layers. This problem is typically circumvented rather than resolved to some extent. Unfortunately, existing strategies bring several drawbacks: <ref type="bibr" target="#b0">(1)</ref> the training of very deep nets is affected by the fact that consecutive multiplications wash out low-level features; <ref type="bibr" target="#b1">(2)</ref> the local information crucial to dense prediction tasks is discarded since the spatial resolution is reduced gradually. To overcome these limitations, several methods have been recently proposed. One solution is manipulating the convolutional operation directly by using for example large kernel sizes <ref type="bibr" target="#b39">[40]</ref>, atrous convolutions <ref type="bibr" target="#b4">[5]</ref>, and image/feature pyramids <ref type="bibr" target="#b62">[63]</ref>. Another solution is to integrate attention modules into the fully convolutional network architecture. Such a module aims to model global interactions of all pixels in the feature map <ref type="bibr" target="#b53">[54]</ref>. When applied to monocular depth prediction <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b57">58]</ref> a general approach is to combine the attention module with a multi-scale fusion method. More recently, Huynh et al. <ref type="bibr" target="#b29">[30]</ref> proposed a depth-attention volume to incorporate a non-local coplanarity constraint to the network. Guizilini et al. <ref type="bibr" target="#b24">[25]</ref> rely on a fixed pre-trained semantic segmentation network to guide the global representation learning. Though these methods' performance is improved significantly, still the above mentioned issues persist.</p><p>Transformers were initially used to model sequence-tosequence predictions in NLP tasks to obtain a larger receptive field and have recently attracted a tremendous interest in the computer vision community. The first purely self-attention-based vision Transformers (ViT) for image recognition were proposed in <ref type="bibr" target="#b14">[15]</ref> attaining excellent results on ImageNet compared with the convolutional networks. Moreover, SETR <ref type="bibr" target="#b63">[64]</ref> replaces the encoders with pure Transformers, obtaining competitive results on the CityScapes dataset. Interestingly, we found that SETR-like pure Transformer-based segmentation network produces unsatisfactory performance due to the lack of spatial inductive bias in modeling the local information. Meanwhile, most previous methods based on deep feature representation learning fail to solve this problem. Nowadays, only few researchers <ref type="bibr" target="#b2">[3]</ref> are considering combining the CNNs with Transformers to create a hybrid structure to combine their advantages.</p><p>In contrast to treating pixel-level prediction tasks as a sequence-to-sequence prediction problem, we firstly propose to embed Transformers into the ResNet backbone in order to model semantic pixel dependencies. Moreover, we design a new and effective unified attention gate decoder to address the drawback that the pure linear Transformer's embedding feature lacks spatial inductive-bias in capturing the local representation. We show empirically that our method offers a new perspective in model design and achieves stateof-the-art on several challenging benchmarks.</p><p>To summarize, our contribution is threefold: • We are the first to propose the use of Transformers for both monocular depth estimation and surface normal prediction tasks. Transformers can successfully improve the ability of traditional convolutional neural networks to model long-range dependencies. • We propose a novel and effective unified attention gate structure designed to utilize and fuse multi-scale information in a parallel manner and pass information among different affinities maps in the attention gate decoders for better modeling the multi-scale affinities. • We conduct extensive experiments on two distinct pixelwise prediction tasks with three challenging datasets (e.g., NYU <ref type="bibr" target="#b43">[44]</ref>, KITTI <ref type="bibr" target="#b20">[21]</ref>, and ScanNet <ref type="bibr" target="#b10">[11]</ref>), demonstrating that our TransDepth outperforms previous methods on KITTI (0.956 on δ&lt;1.25), NYU depth (0.900 on δ&lt;1.25), and achieves new state-of-the-art results on NYU surface normal estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transformers in Computer Vision. Transformer and selfattention models have revolutionized machine translation and natural language processing <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b11">12]</ref>. Recently, there were also some explorations for the usage of Transformer structures in image recognition. Non-local networks <ref type="bibr" target="#b53">[54]</ref> append a Transformer-style attention onto the convolutional backbone. LRNet <ref type="bibr" target="#b26">[27]</ref> explored local self-attention to avoid the heavy computation brought by global self-attention. Axial-Attention <ref type="bibr" target="#b48">[49]</ref> decomposed the global spatial attention into two separate axial attentions such that the computation is vastly reduced. Apart from these pure Transformerbased models, there are also CNN-Transformer hybrid ones. For instance, DETR <ref type="bibr" target="#b2">[3]</ref> and the following deformable version utilized a Transformer for object detection where the Transformer was appended inside the detection head. LSTR <ref type="bibr" target="#b38">[39]</ref> adopted Transformers for disparity estimation and for lane shape prediction. Most recently, ViT <ref type="bibr" target="#b14">[15]</ref> was the first work to show that a pure Transformer-based image classification model can achieve the state-of-the-art. This work provides a direct inspiration to us to exploit a pure Transformer-based encoder design in a semantic segmentation model. Meanwhile, SETR <ref type="bibr" target="#b63">[64]</ref> based on ViT, lever-ages attention for image segmentation. However, there is no related work in continuous pixel prediction. The main reason is that the networks, designed for the continuous label task, extremely rely on deep representation learning and fully-convolutional networks (FCN) with a decoder architecture. In this case, the pure Transformer (without convolution and resolution reduction) regarding an image as a patch sequence is not suitable for pixel-level prediction with continual labels.</p><p>To overcome the aforementioned limitation, we propose a novel combination framework putting a linear Transformer and ResNet together enabling the previous methods based on deep representing learning, such as dilated/atrous convolutions and inserting attention modules, to be also suitable for our networks. Meanwhile, the position embedding module is removed from our linear Transformer but we take advantage of multi-scale fusion in decoder to add position information. This is essential to successfully apply Transformers to depth prediction and surface normal estimation tasks. Monocular Depth Estimation. Most recent works on monocular depth estimation are based on CNNs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, which suffer from the limited receptive field problem or from the less global representation learning. For instance, Eigen et al. <ref type="bibr" target="#b16">[17]</ref> introduced a twostream deep network to take into account both coarse global prediction and local information. Fu et al. <ref type="bibr" target="#b18">[19]</ref> proposed a discretization strategy to treat monocular depth estimation as a deep ordinal regression problem. They also employed a multi-scale network to capture relevant multi-scale information. Lee et al. <ref type="bibr" target="#b33">[34]</ref> introduced local planar guidance layers in the network decoder module to learn more effective features for depth estimation. More recently, PackNet-SfM <ref type="bibr" target="#b23">[24]</ref> used 3D convolutions with self-supervision to learn detail-preserving representations. At the same time, Guizilini et al. <ref type="bibr" target="#b24">[25]</ref> exploit semantic features into the selfsupervised depth network by using a pre-trained semantic segmentation network. The new SOTA, FAL-Net <ref type="bibr" target="#b22">[23]</ref>, focuses instead on representation learning using stereoscopic view synthesis penalizing the synthetic right-view in all image regions. Though it explicitly increases long-range modeling dependencies, more training steps are added.</p><p>Our method focuses on representation learning as well but with only one step training strategy. The Transformer mechanism is quite suitable to solve the limited receptive field issue, to guide the generation of depth features. Unlike the previous works <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b14">15]</ref> reshaping the image into a sequence of flattened 2D patches, we propose a hybrid model combining ResNet <ref type="bibr" target="#b25">[26]</ref> and linear Transformer <ref type="bibr" target="#b14">[15]</ref>. This is quite different from the previous Transformer mechanism, taking advantage of both sides. This composite structure also holds another advantage: many deep representation learning methods can be easily transferred in this network. Surface Normal Estimation. Surface normal prediction is regarded as a close task to monocular depth prediction. Extracting 3D geometry from a single image has been a longstanding problem in computer vision. Surface normal estimation is a classical task in this context requiring modeling both global and local features. Typical approaches leverage networks with high capacity to achieve accurate predictions at high resolution. For instance, FrameNet <ref type="bibr" target="#b27">[28]</ref> employed the DORN <ref type="bibr" target="#b18">[19]</ref> architecture, a modification of DeepLabv3 <ref type="bibr" target="#b4">[5]</ref> that removes multiple spatial reductions (2×2 max pool layers), to generate high resolution surface normal maps. A different strategy consists of designing appropriate loss terms. For instance, UprightNet <ref type="bibr" target="#b55">[56]</ref> considered an angular loss and showed its effectiveness for the task. More recently, Do et al. <ref type="bibr" target="#b13">[14]</ref> proposed a novel truncated angular loss and a tilted image process, keeping the atrous spatial pyramid pooling (ASPP) module to increase the receptive field. Although its performance is SOTA, two extra training phases are added due to the tilted image process.</p><p>Attention Models. Several works have considered integrating attention models within deep architectures to improve performance in several tasks, such as image categorization <ref type="bibr" target="#b56">[57]</ref>, image generation <ref type="bibr" target="#b45">[46]</ref>, speech recognition <ref type="bibr" target="#b8">[9]</ref>, and machine translation <ref type="bibr" target="#b47">[48]</ref>. Focusing on pixel-level prediction, Chen et al. <ref type="bibr" target="#b5">[6]</ref> were the first to describe an attention model to combine multi-scale features learned by a FCN for semantic segmentation. Zhang et al. <ref type="bibr" target="#b61">[62]</ref> designed EncNet, a network equipped with a channel attention mechanism to model the global context. Huang et al. <ref type="bibr" target="#b28">[29]</ref> described CCNet, a deep architecture that embeds a criss-cross attention module with the idea of modeling contextual dependencies using sparsely-connected graphs to achieve higher computational efficiency. Fu et al. <ref type="bibr" target="#b19">[20]</ref> proposed to model semantic dependencies associated with spatial and channel dimensions by using two separate attention modules. Xu et al. <ref type="bibr" target="#b57">[58]</ref> proposed the PGA-Net feature dependent conditional kernels.</p><p>Our work significantly departs from these approaches as we introduce a novel attention gate mechanism, adding spatial-and channel-level attention into the attention decoder. Notably, we also prove that our model can be successfully employed in the case of several challenging dense continual pixel-level prediction tasks, where it significantly outperforms PGA-Net <ref type="bibr" target="#b57">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed TransDepth</head><p>As previously discussed, our work aims to solve limited receptive fields by adding Transformer layers and enhancing the learned representation by an attention gate decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transformer for Depth Prediction</head><p>An overview of the network is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. Unlike the previous works <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> reshaping the image I∈R H×W ×3 into a sequence of flattened 2D patches I p ∈R N ×(p 2 ·3) , we propose a hybrid model. As shown in <ref type="figure">Figure 2</ref>: The overview of the proposed attention gate module. The symbols , ⊕, σ , * , and S denote element-wise multiplication, element-wise addition, sigmoid, convolution, and softmax operation, respectively. <ref type="figure" target="#fig_0">Figure 1</ref>, the input sequence comes from an ResNet backbone <ref type="bibr" target="#b25">[26]</ref>. Then the patch embedding is applied to patches extracted from the final feature output of a CNN. This patch embedding's kernel size should be p×p, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. In this case, we also remove position embeddings because the original physical meaning is missing. The input of the first Transformer layer is calculated as follow:</p><formula xml:id="formula_0">z 0 = [l 1 E; l 2 E; · · · ; l N E],<label>(1)</label></formula><p>where z 0 is mapped into a latent N-dimensional embedding space using a trainable linear projection layer. There are L Transformer layers which consist of multi-headed selfattention (MSA) and multi-layer perceptron (MLP) blocks. At each layer , the input of the self-attention block is a triplet of Q (query), K (key), and V (value), similar with <ref type="bibr" target="#b47">[48]</ref>, computed from z −1 ∈R L×C as:</p><formula xml:id="formula_1">Q = z −1 × W Q , K = z −1 × W K , V = z −1 × W V ,<label>(2)</label></formula><p>where W Q , W K , W V ∈R C×d are the learnable parameters of weight matrices and d is the dimension of Q, K, V . The self-attention is calculated as:</p><formula xml:id="formula_2">AH = softmax( Q × K T √ d ) · V,<label>(3)</label></formula><p>where AH is short for attention head. MSA means the attention head will be calculated m times by independent weight matrices. The final MSA(z −1 ) is defined as:</p><formula xml:id="formula_3">MSA(z −1 ) = z −1 +concat(AH 1 ; AH 2 ; · · · ; AH m )×W o ,<label>(4)</label></formula><p>where W o ∈R md×C . The output of MSA is then transformed by a MLP block with residual skip as the layer output as:</p><formula xml:id="formula_4">z = MLP(LN(z )) + z ,<label>(5)</label></formula><p>where LN(·) means the layer normalization operator and z =MSA(z −1 ). The structure of a Transformer layer is illustrated in the left part of <ref type="figure" target="#fig_0">Figure 1</ref>. After the Transformer layer, the output will be recovered to the original feature shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Gate Decoder</head><p>Given an input image I and a generic front-end CNN model with parameters W c , we consider a set of S multiscale feature maps F={f i } N i=1 . Being a generic framework, these feature maps can be the output of S intermediate CNN layers or of another representation, thus s is a virtual scale. Opposite to previous works adopting simple concatenation or weighted averaging schemes <ref type="bibr" target="#b63">[64]</ref>, we propose to combine the multi-scale feature maps by learning a set of latent kernels (I r→e , I e→r , L) with a novel structure Attention-Gated module sketched in <ref type="figure">Figure 2</ref>. We choose f N as a receive feature only, f r , while {f i } N −1 i=1 are chosen as emitting features, f e , in all tasks. The influence of the fusion of different scales is explained in the ablation part.</p><p>In detail, the whole attention gate can be divided into two parts, i.e., attention and message. We propose to bring together recent advances in pixel-wise prediction by formulating a novel attention gate mechanism for the attention part. Inspired by <ref type="bibr" target="#b19">[20]</ref>, where two spatial-and channel-wise predictions are computed, we opt to infer different spatial and channel attention variables. Our attention tensor is defined by:</p><formula xml:id="formula_5">α i e→r = softmax(A i sp ) · σ(A i ch ) · A i ,<label>(6)</label></formula><p>where i means f i is chosen as an emitting feature. Different from <ref type="bibr" target="#b19">[20]</ref>, we adapt a local conditional kernel before generating attention. The kernels I r→e , I e→r , and L are predicted from the input features using a linear transformation as follows:</p><formula xml:id="formula_6">L i,j = W L i,j concat(f i e , f j r ) + b L i,j , I i,j r→e = W I i,j r→e f i e + b I i,j r→e , I i,j e→r = W I i,j e→r f j r + b I i,j e→r .<label>(7)</label></formula><p>Then, the integrated attention is defined as follow:</p><formula xml:id="formula_7">A i = I i e→r * f r + I i r→e * f i e + f r * L * f i e .<label>(8)</label></formula><p>Compared with the attention part, the message is easy to be calculated by L i * f r . Finally, the output of our attention gate decoder is:</p><formula xml:id="formula_8">f i e = concat(L 1 * f 1 e ·α 1 e→r +f r , ..., L N −1 * f N −1 e ·α N −1 e→r +f r ).<label>(9)</label></formula><p>Once the hidden variables are updated, we use them to address several different discrete prediction tasks, including monocular depth estimation and surface normal estimation. Following previous works, the network optimization loss for depth prediction, updated from <ref type="bibr" target="#b16">[17]</ref>, is:</p><formula xml:id="formula_9">L depth = α 1 T i g 2 i − λ T 2 ( i g i ) 2 ,<label>(10)</label></formula><p>where g i = logd i − log d i with the ground truth depth d i and the predicted depthd i . We set λ and α to 0.85 and 10, same with <ref type="bibr" target="#b33">[34]</ref>. The angular loss is chosen as the surface normal loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The NYU dataset <ref type="bibr" target="#b43">[44]</ref> is used to evaluate our approach in the depth estimation task. We use 120K RGB-Depth pairs with a resolution of 480×640 pixels, acquired with a Microsoft Kinect device from 464 indoor scenes. We follow the standard train/test split as in <ref type="bibr" target="#b16">[17]</ref>, using 249 scenes for training and 215 scenes (654 images) for testing. We also use this dataset to evaluate our approach in the surface normal task, including 795 training images and 654 testing images.</p><p>The KITTI dataset <ref type="bibr" target="#b20">[21]</ref> is a large-scale outdoor dataset created for various autonomous driving tasks. We use it to evaluate the depth estimation performance of our proposed model. Following the standard training/testing split proposed by Eigen et al. <ref type="bibr" target="#b16">[17]</ref>, we specifically use 22,600 frames from 32 scenes for training and 697 frames from the rest 29 scenes for testing.</p><p>The ScanNet dataset <ref type="bibr" target="#b10">[11]</ref> is a large RGB-D dataset for 3D scene understanding. We employ it to evaluate the surface normal performance of our proposed model. ScanNet dataset is divided into 189,916 for training and 20,942 for testing with file lists provided in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Evaluation Protocol on Monocular Depth Estimation. We follow the standard evaluation protocol as in previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51]</ref> and adopt the following quantitative evaluation metrics in our experiments:</p><p>• Abs relative error (abs-rel): 1</p><formula xml:id="formula_10">K K i=1 |di−d i | d i ; • Squared Relative difference (sq-rel): 1 K K i=1 ||di−d i || 2 d i ;</formula><p>• root mean squared error (rms):</p><formula xml:id="formula_11">1 K K i=1 (d i − d i ) 2 ; • mean log10 error (log-rms): 1 K K i=1 log 10 (d i ) − log 10 (d i ) 2 ; • accuracy with threshold t: percentage (%) of d i , subject to max( d ĩ di ,d i d i )=δ&lt;t (t∈[1.25, 1.25 2 , 1.25 3 ]);</formula><p>whered i and d i is the ground-truth depth and the estimated depth at pixel i respectively; K is the total number of pixels of the test images. Evaluation Protocol on Surface Normal Estimation. We utilize five standard evaluation metrics <ref type="bibr" target="#b17">[18]</ref>. For space limitation, we pick up median angle distance between prediction and ground-truth for valid pixels and the fraction of pixels with angle difference with ground-truth less than 11.25 • listed in the main paper. The results of five standard evaluation metrics are put into supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>The proposed TransDepth is implemented in PyTorch. The experiments are conducted on four Nvidia Tesla V100 GPUs, each with 32 GB memory. The ResNet-50 architecture pretrained on ImageNet <ref type="bibr" target="#b12">[13]</ref> is considered in the experiments for initializing the backbone network of our encoder network. For parameters of Transformer, T-layers, Hidden size, and attention multi-head are set to 12, 768, and 12, respectively. As the attention gate decoder structure setting, f 5 is chosen as the receiving feature, f r , while {f i } 5 i=3 are taken up as emitting features, f e , in all tasks.</p><p>For the monocular depth estimation and surface normal prediction tasks, the learning rate is set to 10 −4 with a weight decay of 0.01. The Adam optimizer is used in all our experiments with a batch size of 16 for all tasks. The total training epochs are set to 50 for depth prediction and 20 for surface normal prediction. We train our network on a random crop of size 352×704 for KITTI dataset, 416×512 for NYU dataset for depth prediction while the input image size is uniformly set to 320×256 for surface normal prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Monocular Depth Estimation</head><p>We compare the proposed method with the leading monocular depth estimation models, i.e., <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>. Comparison results on the KITTI dataset are shown in <ref type="table" target="#tab_0">Table 1</ref>. Our  method performs favorably versus all previous fully-and self-supervised methods, achieving the best results on the majority of the metrics. Our approach employs the supervised setting using single monocular images in the training and testing phase. Specifically, BTS needs the focal length to refine the final depth output, which is not the same setting as ours and thus not directly comparable to ours. Compared with recent STOA, i.e., FAL-Net, our method is better by a large margin. Meanwhile, unlike FAL-Net using stereo split, two-step training, and post-processing, our method is end to end without extra post-processing. The more important thing is that "Ours w/ VIT" has outperformed the SOTA. It can support our standpoint that adding a linear Transformer makes networks improve their ability to capture long-range dependencies. In other words, our network becomes simpler but more powerful by adapting the linear Transformer.</p><p>To demonstrate the competitiveness of our approach in an indoor scenario, we also evaluate the proposed method on the NYU depth dataset. The results are shown in Ta-   <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b29">30]</ref>. Similar to the experiments on KITTI, it outperforms both stateof-the-art approaches and previous methods based on attention mechanism <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b29">30]</ref>. Our method successfully improves δ&lt;1.25 from 0.882 (Huynh et al. <ref type="bibr" target="#b29">[30]</ref>) to 0.900 while root mean squared error significantly drops to 0.365. Both <ref type="table" target="#tab_0">Table 1</ref> and 2 also show that our AGD can merge more low-level information and can make the network learn a more efficient deep representation. Moreover, <ref type="figure" target="#fig_1">Figure 3</ref> shows a qualitative comparison of our method with DORN <ref type="bibr" target="#b18">[19]</ref>. The red box marks the significant improvement parts. Results indicate that our method generates more precise boundaries for distant stuff like vehicles and traffic signs and near stuff like humans. <ref type="figure" target="#fig_2">Figure 4</ref> shows a similar comparison done on the NYU dataset. Owing to applying the Transformer, the corners of the room are more distinguishable. This can support our standpoint that adapting  the linear Transformer makes the CNN backbone network enhance the ability to capture long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on Surface Normal Estimation</head><p>To prove our method universality, we also conduct experiments on surface normal prediction, which is regarded as a related task to depth prediction. We compare the proposed TransDepth with several state-of-the-art methods on surface normal, including GeoNet <ref type="bibr" target="#b40">[41]</ref>, VPLNet <ref type="bibr" target="#b52">[53]</ref>, FrameNet <ref type="bibr" target="#b27">[28]</ref>, and Do et al. <ref type="bibr" target="#b13">[14]</ref>. For a fair comparison, we report our result in two different training conditions. Because of limited space, only median angle and 11.25 • are compared in <ref type="table" target="#tab_2">Table 3</ref> while a detail comparison is shown in the supplementary. Our method outperforms the state-ofthe-art on the median angle and 11.25 • . Though Do et al. reduces the median angle error much, their method needs to get extra gravity labels with two step pre-training. Our method covers these drawback. The qualitative results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Unsurprisingly, the boundaries of stuff  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>Effect of Attention Gate Decoder. We perform an ablation study on the NYU depth dataset to further demonstrate the impact of the proposed AGD. In <ref type="table" target="#tab_3">Table 4</ref>, in the f e column we indicate the emitting features while we design f 5 , the last layer's output as the only receiving feature in all the experiments. We choose the ResNet-50 with the same prediction head as our baseline. We report four different combinations with the baseline when ViT is not applied to any candidates. Interestingly, the performance does not always get better by adding more scale information. In detail, the performance increases significantly with the emitting feature increasing until the number of emitting features reaches three. Compared with the last two rows in <ref type="table" target="#tab_3">Table 4</ref>, some metrics like rel and rms go worse when the number of emitting features further expands. This could be explained by the fact that too many scale features may lead to overfitting of the receiving feature. Undoubtedly, we choose three scales of fusion in all tasks. According to <ref type="figure" target="#fig_4">Figure 6</ref>, the attention granted by different scales fusion can capture information at different range distances. This can prove that the attention gate decoder is helpful to the receive feature to capture more position information. Effect of Different Backbones. We also compare different backbones on the NYU depth dataset on <ref type="table" target="#tab_4">Table 5</ref> while the attention gate decoder is not used in this experiment. The 16/32 are no longer the input path size but the shrinkage scale of the input feature. In other words, 16 means the f 4 is the input feature of ViT-B, while 32 represents the f 5 is the input feature of ViT-B. <ref type="table" target="#tab_4">Table 5</ref> can be split into threepart: the top part belongs to the pure Transformer backbone; the middle part belongs to the pure ResNet backbone; the bottom belongs to the mixed backbone. Compared with the middle part results, the mixed backbone overpasses all of the pure ResNet backbones, leaving a significant margin for each metric. Meanwhile, according to <ref type="table" target="#tab_4">Table 5</ref>, the mixed backbone is better than the ResNet backbone, and it outperforms the pure Transformer encoder. We finally pick up the ResNet-50 with ViT-B/16 as our network's encoder for every task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a novel Transformer-based framework, i.e., TransDepth, for the pixel-wise prediction problems involving continuous labels. We are the first to propose using Transformer to solve pixel-wise prediction problems to the best of our knowledge. The proposed TransDepth leverages the inductive bias of ResNet on modeling spatial correlation and the powerful capability of Transformers on modeling global relationships. Moreover, a new and effective unified attention gate structure with independent channel-wise and spatial-wise attention is applied in the decoder. This can merge more low-level information and can make the network learn a more efficient deep representation. Extensive experiments prove that the proposed TransDepth establishes new state-of-the-art results on KITTI (0.956 on δ&lt;1.25), NYU depth (0.900 on δ&lt;1.25), and NYU surface normal (61.7 on 11.25 • ) datasets. We hope that this work can bring a new perspective on using Transformer-based architectures for computer vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The overview of the proposed TransDepth. The symbols c and ⊕ denote concatenation and addition operations, respectively. AG is short for attention gate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative examples on the KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>(a) Image(b) GT (c) DORN (d) w/ AGD (e) Ours (Full) Qualitative examples on the NYU depth dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>(a) Image (b) GT (c) Baseline (d) w/ AGD (e) Ours (Full) Qualitative examples on the NYU surface normal dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative attention examples of monocular depth prediction on the NYU dataset. First column is th original image and next three columns are different fusion attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Depth Estimation: KITTI dataset. K: KITTI. CS: CityScapes [10]. CS→K: CS pre-training. D: Depth supervision. M, Se, V, S: Monocular, segmentation, video, stereo. Sup: supervise.</figDesc><table><row><cell>Method</cell><cell>Sup</cell><cell>Data</cell><cell></cell><cell cols="3">Error (lower is better)</cell><cell cols="3">Accuracy (higher is better)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">abs rel sq rel</cell><cell>rms</cell><cell cols="4">log rms δ&lt;1.25 δ&lt;1.25 2 δ&lt;1.25 3</cell></row><row><cell cols="2">CC [42] M+Se</cell><cell>K</cell><cell>0.140</cell><cell cols="2">1.070 5.326</cell><cell>0.217</cell><cell>0.826</cell><cell>0.941</cell><cell>0.975</cell></row><row><cell cols="3">Bian et al. [2] M+V K+CS</cell><cell>0.137</cell><cell cols="2">1.089 5.439</cell><cell>0.217</cell><cell>0.830</cell><cell>0.942</cell><cell>0.975</cell></row><row><cell>DeFeat [45]</cell><cell>M</cell><cell>K</cell><cell>0.126</cell><cell cols="2">0.925 5.035</cell><cell>0.200</cell><cell>0.862</cell><cell>0.954</cell><cell>0.980</cell></row><row><cell cols="2">S 3 Net [8] M+Se</cell><cell>K</cell><cell>0.124</cell><cell cols="2">0.826 4.981</cell><cell>0.200</cell><cell>0.846</cell><cell>0.955</cell><cell>0.982</cell></row><row><cell>Monodepth2 [22]</cell><cell>M</cell><cell>K</cell><cell>0.115</cell><cell cols="2">0.903 4.863</cell><cell>0.193</cell><cell>0.877</cell><cell>0.959</cell><cell>0.981</cell></row><row><cell>pRGBD [47]</cell><cell>M</cell><cell>K</cell><cell>0.113</cell><cell cols="2">0.793 4.655</cell><cell>0.188</cell><cell>0.874</cell><cell>0.960</cell><cell>0.983</cell></row><row><cell>Johnston et al. [31]</cell><cell>M</cell><cell>K</cell><cell>0.106</cell><cell cols="2">0.861 4.699</cell><cell>0.185</cell><cell>0.889</cell><cell>0.962</cell><cell>0.982</cell></row><row><cell cols="3">SGDepth [32] M+Se K+CS</cell><cell>0.107</cell><cell cols="2">0.768 4.468</cell><cell>0.180</cell><cell>0.891</cell><cell>0.963</cell><cell>0.982</cell></row><row><cell>Shu et al. [43]</cell><cell>M</cell><cell>K</cell><cell>0.104</cell><cell cols="2">0.729 4.481</cell><cell>0.179</cell><cell>0.893</cell><cell>0.965</cell><cell>0.984</cell></row><row><cell>DORN [19]</cell><cell>D</cell><cell>K</cell><cell>0.072</cell><cell cols="2">0.307 2.727</cell><cell>0.120</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell></row><row><cell>Yin et al. [61]</cell><cell>M</cell><cell>K</cell><cell>0.072</cell><cell>-</cell><cell>3.258</cell><cell>0.117</cell><cell>0.938</cell><cell>0.990</cell><cell>0.998</cell></row><row><cell>PackNet [24]</cell><cell>V</cell><cell>K+CS</cell><cell>0.071</cell><cell cols="2">0.359 3.153</cell><cell>0.109</cell><cell>0.944</cell><cell>0.990</cell><cell>0.997</cell></row><row><cell>FAL-Net [23]</cell><cell>S</cell><cell>K+CS</cell><cell>0.068</cell><cell cols="2">0.276 2.906</cell><cell>0.106</cell><cell>0.944</cell><cell>0.991</cell><cell>0.998</cell></row><row><cell>PGA-Net [58]</cell><cell>D</cell><cell>K</cell><cell>0.063</cell><cell cols="2">0.267 2.634</cell><cell>0.101</cell><cell>0.952</cell><cell>0.992</cell><cell>0.998</cell></row><row><cell>BTS [34]</cell><cell>M</cell><cell>K</cell><cell>0.061</cell><cell cols="2">0.261 2.834</cell><cell>0.099</cell><cell>0.954</cell><cell>0.992</cell><cell>0.998</cell></row><row><cell>Baseline</cell><cell>M</cell><cell>K</cell><cell>0.106</cell><cell cols="2">0.753 3.981</cell><cell>0.104</cell><cell>0.888</cell><cell>0.967</cell><cell>0.986</cell></row><row><cell>Ours w/ AGD</cell><cell>M</cell><cell>K</cell><cell>0.065</cell><cell cols="2">0.261 2.766</cell><cell>0.101</cell><cell>0.953</cell><cell>0.993</cell><cell>0.998</cell></row><row><cell>Ours w/ VIT</cell><cell>M</cell><cell>K</cell><cell>0.064</cell><cell cols="2">0.258 2.761</cell><cell>0.099</cell><cell>0.955</cell><cell>0.993</cell><cell>0.999</cell></row><row><cell>Ours w/ AGD+ViT (Full)</cell><cell>M</cell><cell>K</cell><cell>0.064</cell><cell cols="2">0.252 2.755</cell><cell>0.098</cell><cell>0.956</cell><cell>0.994</cell><cell>0.999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Depth Estimation: NYU dataset.</figDesc><table><row><cell></cell><cell cols="3">Error (lower is better)</cell><cell cols="3">Accuracy (higher is better)</cell></row><row><cell>Method</cell><cell>rel</cell><cell>log10</cell><cell>rms</cell><cell cols="3">δ&lt;1.25 δ&lt;1.25 2 δ&lt;1.25 3</cell></row><row><cell cols="4">PAD-Net [59] 0.214 0.091 0.792</cell><cell>0.643</cell><cell>0.902</cell><cell>0.977</cell></row><row><cell cols="4">Li et al. [37] 0.152 0.064 0.611</cell><cell>0.789</cell><cell>0.955</cell><cell>0.988</cell></row><row><cell cols="4">CLIFFNet [50] 0.128 0.171 0.493</cell><cell>0.844</cell><cell>0.964</cell><cell>0.991</cell></row><row><cell cols="4">Laina et al. [33] 0.127 0.055 0.573</cell><cell>0.811</cell><cell>0.953</cell><cell>0.988</cell></row><row><cell cols="4">MS-CRF [60] 0.121 0.052 0.586</cell><cell>0.811</cell><cell>0.954</cell><cell>0.987</cell></row><row><cell cols="3">Lee et al. [35] 0.119 0.050</cell><cell>-</cell><cell>0.870</cell><cell>0.974</cell><cell>0.993</cell></row><row><cell cols="2">Xia et al. [55] 0.116</cell><cell>-</cell><cell>0.512</cell><cell>0.861</cell><cell>0.969</cell><cell>0.991</cell></row><row><cell cols="4">DORN [19] 0.115 0.051 0.509</cell><cell>0.828</cell><cell>0.965</cell><cell>0.992</cell></row><row><cell cols="4">BTS [34] 0.113 0.049 0.407</cell><cell>0.871</cell><cell>0.977</cell><cell>0.995</cell></row><row><cell cols="4">Yin et al. [61] 0.108 0.048 0.416</cell><cell>0.875</cell><cell>0.976</cell><cell>0.994</cell></row><row><cell cols="2">Huynh et al. [30] 0.108</cell><cell>-</cell><cell>0.412</cell><cell>0.882</cell><cell>0.980</cell><cell>0.996</cell></row><row><cell cols="4">Baseline 0.118 0.051 0.414</cell><cell>0.866</cell><cell>0.979</cell><cell>0.995</cell></row><row><cell cols="4">Ours w/ AGD 0.111 0.048 0.393</cell><cell>0.881</cell><cell>0.979</cell><cell>0.996</cell></row><row><cell cols="4">Ours w/ VIT 0.109 0.047 0.388</cell><cell>0.887</cell><cell>0.981</cell><cell>0.996</cell></row><row><cell cols="4">Ours w/ AGD+ ViT (Full) 0.106 0.045 0.365</cell><cell>0.900</cell><cell>0.983</cell><cell>0.996</cell></row><row><cell cols="7">ble 2, compared with the the state-of-the-art methods like</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Surface Normal Estimation: NYU dataset.</figDesc><table><row><cell>Li et al. [36]</cell><cell></cell><cell>27.8</cell><cell>19.6</cell></row><row><cell>Chen et al. [7]</cell><cell></cell><cell>15.8</cell><cell>39.2</cell></row><row><cell>Eigen et al. [16]</cell><cell></cell><cell>13.2</cell><cell>44.4</cell></row><row><cell>SURGE [52]</cell><cell>NYU</cell><cell>12.2</cell><cell>47.3</cell></row><row><cell>Bansal et al. [1]</cell><cell></cell><cell>12.0</cell><cell>47.9</cell></row><row><cell>GeoNet [41]</cell><cell>NYU</cell><cell>12.5</cell><cell>46.0</cell></row><row><cell>TransDepth (Ours)</cell><cell></cell><cell>11.8</cell><cell>48.2</cell></row><row><cell>FrameNet [28]</cell><cell></cell><cell>11.0</cell><cell>50.7</cell></row><row><cell>VPLNet [53] Do et al. [14]</cell><cell>ScanNet</cell><cell>9.8 8.1</cell><cell>54.3 59.8</cell></row><row><cell>TransDepth (Ours)</cell><cell></cell><cell>7.8</cell><cell>61.7</cell></row></table><note>Method Training Data Testing Data median ↓ 11.25 • ↑</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the NYU depth dataset: performance of TransDepth for different scales fusion.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Error (lower is better)</cell><cell cols="2">Accuracy (higher is better)</cell></row><row><cell>fe</cell><cell>fr</cell><cell>rel</cell><cell>log10</cell><cell>rms</cell><cell cols="2">δ&lt;1.25 δ&lt;1.25 2 δ&lt;1.25 3</cell></row><row><cell>-</cell><cell>-</cell><cell cols="3">0.118 0.051 0.414</cell><cell>0.866</cell><cell>0.979</cell><cell>0.995</cell></row><row><cell>f 5</cell><cell cols="4">f 5 0.120 0.071 0.407</cell><cell>0.878</cell><cell>0.982</cell><cell>0.996</cell></row><row><cell>f 4 , f 5</cell><cell cols="4">f 5 0.108 0.045 0.366</cell><cell>0.897</cell><cell>0.982</cell><cell>0.996</cell></row><row><cell>f 3 , f 4 , f 5</cell><cell cols="4">f 5 0.106 0.045 0.365</cell><cell>0.900</cell><cell>0.983</cell><cell>0.996</cell></row><row><cell cols="5">f 2 , f 3 , f 4 , f 5 f 5 0.107 0.045 0.366</cell><cell>0.899</cell><cell>0.983</cell><cell>0.996</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study about different backbone on the NYU depth dataset. R50 is short for ResNet50. B is short for base.</figDesc><table><row><cell></cell><cell cols="3">Error (lower is better)</cell><cell cols="2">Accuracy (higher is better)</cell></row><row><cell>Backbone</cell><cell>rel</cell><cell>log10</cell><cell>rms</cell><cell cols="2">δ&lt;1.25 δ&lt;1.25 2 δ&lt;1.25 3</cell></row><row><cell cols="4">ViT-B/32 [15] 0.112 0.048 0.387</cell><cell>0.849</cell><cell>0.927</cell><cell>0.940</cell></row><row><cell cols="4">ViT-B/16 [15] 0.108 0.046 0.371</cell><cell>0.885</cell><cell>0.967</cell><cell>0.979</cell></row><row><cell cols="4">ResNet50 [26] 0.118 0.051 0.414</cell><cell>0.866</cell><cell>0.979</cell><cell>0.995</cell></row><row><cell cols="4">ResNet101 [26] 0.112 0.048 0.387</cell><cell>0.848</cell><cell>0.927</cell><cell>0.939</cell></row><row><cell cols="4">ResNet152 [26] 0.111 0.047 0.381</cell><cell>0.861</cell><cell>0.941</cell><cell>0.953</cell></row><row><cell cols="4">R50+ViT-B/32 0.107 0.045 0.368</cell><cell>0.893</cell><cell>0.975</cell><cell>0.988</cell></row><row><cell cols="4">R50+ViT-B/16 (Ours) 0.106 0.045 0.365</cell><cell>0.900</cell><cell>0.983</cell><cell>0.996</cell></row></table><note>become more precise when AGD and ViT are jointly use.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised scale-consistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<idno>arXiv, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Surface normals in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gaurav Bansal, and Dinesh Bharadia. s 3 net: Semantic-aware selfsupervised depth estimation with monocular videos and synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjot</forename><surname>Singh Saggu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raunak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Surface normal estimation of tilted images via spatial rectifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khiem</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stergios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Roumeliotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Datadriven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>David F Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Forget about the lidar: Self-supervised depth estimators with med probability volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Luis</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantically-guided representation learning for self-supervised monocular depth. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rares Ambrus, and Adrien Gaidon</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Framenet: Learning local canonical frames of 3d surfaces from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Termöhlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-loss rebalancing algorithm for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
		<idno>2020. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Anton Van Den Hengel, and Mingyi He</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Endto-end lane shape prediction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Defeat-net: General monocular depth via simultaneous unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hadfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Xinggan for person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pseudo rgb-d for self-improving monocular slam and depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lokender</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saket</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Attention is all you need. arXiv, 2017. 2, 3, 4</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cliffnet for monocular depth estimation with hierarchical embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Surge: Surface regularized geometry estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Vplnet: Deep single view normal estimation with vanishing points and lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Geraghty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generating and exploiting probabilistic monocular depth estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Uprightnet: Geometry-aware camera orientation estimation from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Probabilistic graph attention network with conditional kernels for pixel-wise prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>TPAMI, 2020. 1, 3, 6</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>with transformers. arXiv, 2021. 1, 2, 3, 4</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
