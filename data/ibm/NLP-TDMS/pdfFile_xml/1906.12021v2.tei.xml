<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Densely Residual Laplacian Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Nick</forename><forename type="middle">Barnes</forename></persName>
						</author>
						<title level="a" type="main">Densely Residual Laplacian Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Super-resolution</term>
					<term>Laplacian attention</term>
					<term>Multi-scale attention</term>
					<term>Densely connected residual blocks</term>
					<term>Deep convolutional neural network !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Super-Resolution convolutional neural networks have recently demonstrated high-quality restoration for single images. However, existing algorithms often require very deep architectures and long training times. Furthermore, current convolutional neural networks for super-resolution are unable to exploit features at multiple scales and weigh them equally, limiting their learning capability. In this exposition, we present a compact and accurate super-resolution algorithm namely, Densely Residual Laplacian Network (DRLN). The proposed network employs cascading residual on the residual structure to allow the flow of low-frequency information to focus on learning high and mid-level features. In addition, deep supervision is achieved via the densely concatenated residual blocks settings, which also helps in learning from high-level complex features. Moreover, we propose Laplacian attention to model the crucial features to learn the inter and intra-level dependencies between the feature maps. Furthermore, comprehensive quantitative and qualitative evaluations on low-resolution, noisy low-resolution, and real historical image benchmark datasets illustrate that our DRLN algorithm performs favorably against the state-of-the-art methods visually and accurately.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N recent years, super-resolution (SR), a low-level vision task, became a research focus due to the high demand for betterresolution image quality. Super-resolution addresses the problem of reconstructing a high-resolution (HR) input from a lowresolution (LR) counterpart. We aim to super-resolve a single lowresolution image, a technique, commonly, known as single image super-resolution (SISR). Image SR is a challenging task to achieve as the process is ill-posed, which means that mapping between the output HR image to the input LR image is many-to-one. However, despite being a difficult problem, it is useful in many computer vision applications such as surveillance imaging <ref type="bibr" target="#b0">[1]</ref>, medical imaging <ref type="bibr" target="#b1">[2]</ref>, forensics <ref type="bibr" target="#b2">[3]</ref>, object classification <ref type="bibr" target="#b3">[4]</ref> etc.</p><p>Deep convolutional neural network (CNN) super-resolution methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> have shown improvement over traditional super-resolution methods in SISR. The performance and depth of convolutional neural super-resolution networks have evolved dramatically in recent times. As an example, SRCNN <ref type="bibr" target="#b10">[11]</ref> has three convolutional layers while RCAN <ref type="bibr" target="#b6">[7]</ref> has more than 400. However, using deep networks may be unsuitable for many applications. In this regard, it is essential to design efficient networks. The most straightforward way to reduce the size of the network is simply to reduce the depth, but this will decrease the quality. Therefore, it is essential to design an efficient network that focuses on reusability of the computed features.</p><p>An effective alternative to depth reduction is to employ recursive architectures, and such attempts are formulated in the form of DRCN <ref type="bibr" target="#b12">[13]</ref> and DRRN <ref type="bibr" target="#b11">[12]</ref>. DRCN <ref type="bibr" target="#b12">[13]</ref> avoids redundant parameters via recursive connections while DRRN <ref type="bibr" target="#b11">[12]</ref> share's parameters through residual recursive connections. The recursive nets achieved a decrease in the number of parameters, and an increase in performance compared to standard CNN's; however, these models have some limitations, which are: 1) the upsampled input, 2) increased depth and 3) increased width. Although these enable the model to reconstruct the structural features from the • Saeed Anwar is a research fellow with Data61-CSIRO. E-mail: saeed.anwar@data61.csiro.au • Nick Barnes is team lead at CSIRO and Associate Professor at ANU. low-resolution image, it is at the cost of a large number of operations and high inference time. Another approach to forming a compact model is to utilize the dense connections between convolutional layers e.g. SRDenseNet <ref type="bibr" target="#b13">[14]</ref> and RDN <ref type="bibr" target="#b14">[15]</ref>.</p><p>To optimize speed and the number of parameters CARN <ref type="bibr" target="#b7">[8]</ref> employed group convolutions. The network is primarily based on a variant of residual blocks. Although it can achieve good speed and fewer parameters, it failed to reach the PSNR standard set by RCAN <ref type="bibr" target="#b6">[7]</ref>. On the other hand, most of the CNN models <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> treat features equally or only at one scale, and therefore, lack adaptability to deal with various frequency levels, e.g. low, mid and high. Super-resolution algorithms aim to restore mid-level and high-level frequencies as the low-level frequencies can be obtained from the input low-resolution image without substantial computations. The state-of-the-art methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, models the features equally or on a limited scale, ignoring the abundant rich frequency representation at other scales; hence these lack discriminative learning capability and capacity across channels, and eventually, this limits the ability of convolutional neural networks. To address these issues, we propose the densely residual Laplacian attention Network (DRLN) to reconstruct SR images. DRLN utilizes the dense connection between the residual blocks to use the previously computed features. Similarly, we employ Laplacian pyramid attention to weight the features at multiple scales and according to their importance.</p><p>In summary, our main contributions are four-fold: <ref type="bibr">•</ref> We propose densely connected residual blocks and a Laplacian attention network for accurate image superresolution. Our network achieves much better performance through multi-shortcut connections and multi-level representation.</p><p>• Our novel design employs cascading over residual on the residual architecture, which can assist in training deep networks. Diverse connection types and cascading over residual on the residual in our DRLN help in bypassing enough low-frequency information to learn more accurate representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We introduce Laplacian attention, which has a two-fold arXiv:1906.12021v2 [eess.IV] 1 Jul 2019</p><formula xml:id="formula_0">Fig. 1. Visual Comparisons.</formula><p>Sample results on URBAN100 with Bicubic (BI) degradation for 4× on "img 074" and for 8× on "img 040". Our method recovers the structures correctly with less distortion and more faithful to the ground-truth image.</p><p>purpose: 1) To learn the features at multiple sub-band frequencies and 2) to adaptively rescale features and model feature dependencies. Laplacian attention further improves the feature capturing capability of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Through extensive experiments, we show DRLN is efficient and achieves better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>In this section of the paper, we provide chronological advancement in the deep super-resolution. Dong et al. <ref type="bibr" target="#b10">[11]</ref> proposed pioneering works in super-resolution by introducing a fully convolutional network composed of three convolutional layers followed by ReLU <ref type="bibr" target="#b18">[19]</ref> and termed it as SRCNN <ref type="bibr" target="#b10">[11]</ref>. The input to the SRCNN <ref type="bibr" target="#b10">[11]</ref> is a bicubic interpolated image which diminishes high-frequencies and requires additional computation. To reduce the burden on the network, FSRCNN <ref type="bibr" target="#b19">[20]</ref> inputs the original low-resolution image and employ deconvolution to upsample the features to the desired dimensions before the final objective function. The authors of <ref type="bibr" target="#b19">[20]</ref> also uses the shrinking and expansion of channels to make the model near real-time on a CPU. Initially, the focus was on linear networks, bearing a simple architecture with no skip-connections i.e. only one path for the signal flow with the layers stacked consecutively. SRCNN <ref type="bibr" target="#b10">[11]</ref> and FSRCNN <ref type="bibr" target="#b19">[20]</ref> are examples of linear networks. Similarly, Image Restoration CNN abbreviated as IRCNN <ref type="bibr" target="#b20">[21]</ref>, another straight model, can restore several low-level vision tasks jointly. The aim here is to employ dilation in convolutional layers to capture a larger receptive field for better learning coupled with batch normalization and non-linear activation (ReLU) to reduce the depth of the network. Furthermore, SRMD <ref type="bibr" target="#b21">[22]</ref>, an extended superresolution network, can handle different degradations. SRMD <ref type="bibr" target="#b21">[22]</ref> inputs low-resolution images and their computed degradation maps. The model structure is similar to <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>With the emergence of skip-connections in CNN networks, its usage became a prominent feature in super-resolution. In this regard, very deep super-resolution (VDSR) <ref type="bibr" target="#b8">[9]</ref> incorporated a global skip connection to enforce residual learning using gradient clipping to avoid gradient vanishing. VDSR <ref type="bibr" target="#b8">[9]</ref> improved upon the previous CNN super-resolution methods. Inspired from VDSR <ref type="bibr" target="#b8">[9]</ref>, the same authors next presented DRCN <ref type="bibr" target="#b12">[13]</ref>, which shares parameters using a deep recursive structure. This sharing technique reduces the number of parameters significantly; however, the performance is lagging behind VDSR <ref type="bibr" target="#b8">[9]</ref>. Subsequently, deep recursive residual network (DRRN) <ref type="bibr" target="#b11">[12]</ref> replicates primary skipconnections across different convolutional blocks to enforce residual learning through multi-path architecture. The aim is to reduce the memory cost and computational complexity via parameter sharing. Further, Tai et al. <ref type="bibr" target="#b22">[23]</ref> introduces a persistent memory network (MemNet), which is composed of memory blocks stacked together recursively. Each block is then connected to a gate unit densely, where each gate unit is a convolutional layer with kernel size 1×1. The performance of the networks employing recursive connections is comparable to each other.</p><p>Lim et al. <ref type="bibr" target="#b4">[5]</ref> proposed the enhanced deep super-resolution (EDSR) network, which employs residual blocks and a long skipconnection. EDSR <ref type="bibr" target="#b4">[5]</ref> rescaled the features by a factor of 0.1 to avoid gradient exploding. EDSR improved upon all previous methods by a significant margin. More recently, Ahn et al. <ref type="bibr" target="#b7">[8]</ref> proposed the cascading residual network (CARN) which also employs a variant of residual blocks i.e. having three convolutional layers as compared to the customarily-used two convolutional layers with cascading connections. CARN <ref type="bibr" target="#b7">[8]</ref> lags behind EDSR <ref type="bibr" target="#b4">[5]</ref> in terms of PSNR.</p><p>Driven by the success of the dense-connection architecture proposed in DenseNet <ref type="bibr" target="#b23">[24]</ref> by Huang et al. for image classification, super-resolution networks have focused on the dense-connections to improve performance. As an example, SR-DenseNet <ref type="bibr" target="#b13">[14]</ref> utilized dense-connections where every convolutional layer in a block operates on the output of all prior convolutional layers. To upsample the features, SRDenseNet <ref type="bibr" target="#b13">[14]</ref> orders the blocks sequentially followed by deconvolutional layers at the end of the network. Likewise, Zhang et al. <ref type="bibr" target="#b14">[15]</ref> proposed a residual dense network (RDN) to learn local features from the images via dense-connections. Furthermore, to avoid vanishing gradients and for ease of flow of information from low-level to high-level layers, RDN <ref type="bibr" target="#b14">[15]</ref> employed skip-connections. Lately, DDBPN <ref type="bibr" target="#b17">[18]</ref> aims to model a feedback mechanism with a feed-forward procedure; hence, a series of densely connected upsampling and downsampling layers are used as a single block. To predict the final super-resolved image, the outputs of the intermediate blocks are concatenated as well.</p><p>To obtain distinct features at multiple scales, multi-branch networks <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> are proposed. Ren et al. <ref type="bibr" target="#b24">[25]</ref> employ SRCNN <ref type="bibr" target="#b27">[28]</ref> at various branches with a different number of layers to learn features uniquely and lastly combine them using a sumpooling layer. Similarly, Hu et al. <ref type="bibr" target="#b25">[26]</ref> proposed cascaded multiscale cross-network composed of subnets. Each subnet has mergeand-run units consisting of two parallel branches, each having two convolutional layers. Batch normalization and Leaky-ReLU <ref type="bibr" target="#b28">[29]</ref> follows each convolutional layer in the merge-and-run unit. In contrast to multi-branch, Lai et al. <ref type="bibr" target="#b9">[10]</ref> proposed a multistage network where each sub-network progressively predicts the residual output up to an 8× factor.</p><p>To enhance the visual quality of the images, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> aim to improve the perceptual quality through super-resolution. The first exciting work in this regard is SRResNet <ref type="bibr" target="#b15">[16]</ref>, where the generator is comprised of residual blocks similar to <ref type="bibr" target="#b31">[32]</ref> with a skip-connection from the input to the output while the discriminator is fully convolutional. The SRResNet <ref type="bibr" target="#b15">[16]</ref> combined three different losses, which include perceptual, adversarial and 2 . Next, to create the textures faithful to the original image, EnhanceNet <ref type="bibr" target="#b32">[33]</ref> used an additional texture matching loss with the mentioned losses. This loss aims to match the textures of low-resolution and high-resolution patches as gram matrices computed from deep features via the 1 . Similar to <ref type="bibr" target="#b32">[33]</ref>, to generate more realistic super-resolved images, Park et al. <ref type="bibr">[34]</ref> proposed SRFeat, which utilizes an additional discriminator to help the generator. The results of SRFeat <ref type="bibr">[34]</ref> are perceptually better than <ref type="bibr" target="#b32">[33]</ref>. Inspired by <ref type="bibr" target="#b15">[16]</ref> network, ES-RGAN <ref type="bibr" target="#b33">[35]</ref> removed the batch normalization and used denseconnections between the convolutional layers in the same segment. A global skip-connection is incorporated for residual learning. Besides, changing the elements of the generator, an enhanced discriminator i.e. Relativistic GAN <ref type="bibr" target="#b34">[36]</ref> is used instead of the traditional one. The performance of the ESRGAN <ref type="bibr" target="#b33">[35]</ref> is the best among the current super-resolution GAN algorithms. Furthermore, the GAN super-resolution models have significantly improved the perceived quality compared to its CNN competitors.</p><p>Visual attention <ref type="bibr" target="#b35">[37]</ref> is primarily employed in image classification. This concept was brought to image super-resolution by RCAN <ref type="bibr" target="#b6">[7]</ref>, which uses a channel attention mechanism for modeling the inter-channel dependencies coupled with stacking of groups of residual blocks. The PSNR values of RCAN <ref type="bibr" target="#b6">[7]</ref> is the best among all the algorithms as mentioned earlier. In parallel to RCAN <ref type="bibr" target="#b6">[7]</ref>, Kim et al. <ref type="bibr" target="#b16">[17]</ref> proposed a dual attention mechanism, namely, the super-resolution residual attention module (SRRAM). The depth of the SRRAM <ref type="bibr" target="#b16">[17]</ref> is comparatively smaller than RCAN <ref type="bibr" target="#b6">[7]</ref> and lag behind RCAN <ref type="bibr" target="#b6">[7]</ref> in PSNR numbers. On the other hand, our method improves upon RCAN <ref type="bibr" target="#b6">[7]</ref> both visually and in numbers by exploiting densely connected residual blocks followed by multi-scale attention using different levels of the skip and the cascading connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>Our model is constituted of four integral components, i.e., feature extraction, cascading over residual on the residual, upsampling, and reconstruction, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Let's suppose the lowresolution input image, and the super-resolved output image is represented by x andŷ, respectively. To formally illustrate the model implementation, let f be a convolutional layer and τ be a non-linear activation function; then, we define the feature extraction component which is comprised of one convolutional layer to extract primitive features from the low-resolution input, as:</p><formula xml:id="formula_1">f 0 = H f (x),<label>(1)</label></formula><p>where H f (·) is the convolutional operator applied on the lowresolution image. Next, f 0 is passed on to the cascading residual on the residual component, termed as H crir ,</p><formula xml:id="formula_2">f r = H crir (f 0 ),<label>(2)</label></formula><p>where f r are the estimated features and H crir (·) is the main cascading residual on the residual component which is composed of dense residual Laplacian modules cascaded together. The output features of the H crir are novel to the best of our knowledge in image super-resolution. Our method's depth is not significant compared to RCAN <ref type="bibr" target="#b6">[7]</ref>; however, it provides a wide receptive field and the best results. Following this, the extracted deep f r features from the cascaded residual on the residual component are upscaled through the upsampling component, as:</p><formula xml:id="formula_3">f u = H u (f r ),<label>(3)</label></formula><p>where H u (·) and f u denote an upsampling operator and upscaled features, respectively. Although several choices are available for H u (·) such as a deconvolutional layer <ref type="bibr" target="#b19">[20]</ref>, or nearest-neighbor upsampling with convolution <ref type="bibr" target="#b36">[38]</ref>; we opt for ESPCN <ref type="bibr" target="#b37">[39]</ref> following the footsteps of <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Next, the f u features are passed through the reconstruction component which is composed of one convolutional layer to predict the super-resolved RGB color channels as an output, expressed as:</p><formula xml:id="formula_4">y = H r (f u ),<label>(4)</label></formula><p>whereŷ is the estimated super-resolved image while H r (·) denotes the reconstruction operator. To optimize our model, several choices are available for the loss function, including 2 [9], <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b0">1</ref> [5], <ref type="bibr" target="#b6">[7]</ref>, perceptual <ref type="bibr" target="#b32">[33]</ref>, total variation <ref type="bibr" target="#b38">[40]</ref> and adversarial <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b33">[35]</ref>. To be fair with the competing state-of-the-art methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, we also choose 1 loss function for our network optimization. Now, for a batch of N training pairs, i.e. {x i , y i } N i=1 , the aim is to minimize the 1 loss function as</p><formula xml:id="formula_5">L(W) = 1 N N i=1 ||DRLN(x i ) − y i || 1 ,<label>(5)</label></formula><p>where DRLN(·) is our network and W denotes the set of all the network parameters learned. The feature extraction H f and reconstruction H r are similar to previous algorithms <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>. In the next section, we focus on the H crir .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cascading Residual on the Residual</head><p>In this section, we provide more details on the cascading residual on the residual structure, which has a hierarchical architecture and composed of cascading blocks. Each cascading block has a medium skip-connection (MSC), cascading features concatenation and is made up of dense residual Laplacian modules (DRLM) each of which consists of a densely connected residual unit, compression unit and Laplacian pyramid attention unit. The lower part of <ref type="figure" target="#fig_0">Figure 2</ref> shows DRLM, which will be explained further in Section 3.3.</p><p>Recently, in image recognition <ref type="bibr" target="#b31">[32]</ref> and super-resolution <ref type="bibr" target="#b6">[7]</ref>, residual blocks are stacked together to construct a network of more than 1000 layers and 400 layers, respectively, via skipconnections, although the performance has increased; however, the computational overhead has also increased. We aim here to construct a compact and efficient model with a much lower number of the convolutional layers amidst improved performance and computation time. Therefore, we introduce cascading of the blocks employing medium and long skip connections. Let's suppose that the n-th dense residual Laplacian module (DRLM) of the m-th cascaded block B n,m is given as:</p><formula xml:id="formula_6">f n,m = f ([Z u−0;m , Z u−1;m , B u;m (w u,1;m ), b u,1;m )<label>(6)</label></formula><p>where f n,m are the features from the n-th dense residual Laplacian module (DRLM) of the m-th cascaded block. Each cascaded block is composed of k DRLMs, and hence the input to the cascaded block is summed with the output of the k DRLM as f n+k,m = f n+k,m + f n,m , i.e. medium skip-connection (MSC) as:</p><formula xml:id="formula_7">f g = f 0 + H crir (W w,b ),<label>(7)</label></formula><p>where W w,b are the weights and biases learned in the cascaded block. The addition of medium skip-connection eases the flow of information across group of DRLM while the addition of long-skip connection (LSC) helps the flow of information through cascaded blocks. The group features f g are passed to the reconstruction layer to output the same number of channels as the input to the network. Next, we provide information about the dense residual Laplacian modules and its subcomponents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dense Residual Laplacian Module</head><p>As briefly mentioned earlier, DRLM is composed of three subcomponents i.e. densely connected residual blocks unit, compression unit, and Laplacian pyramid attention.</p><p>The residual blocks we employ, have the traditional two convolutional layers and two ReLUs structure followed by an addition from the input of the residual block, as:</p><formula xml:id="formula_8">R i (w i , b i ) = τ (f (τ (f (w i,1 , b i,1 ); b i,2 ) + Z i−1 ),<label>(8)</label></formula><p>where Z i−1 is the output of the previous convolutional layer or residual block while w i,j are the weights of the convolutional layer and b i,j are the biases (j ∈ {1, 2}). Each densely connected residual unit has three residual blocks with dense connection as</p><formula xml:id="formula_9">R c = [R i−1 (w i−1 , b i−1 ); R i−2 (w i−2 , b i−2 )], f R = R i (w i , b i ) = τ (f (τ (f (R c )) + R c ),<label>(9)</label></formula><p>where R c is the concatenation of the previous residual blocks and f R is the final output of the densely connected residual unit. The f R features are then passed through a compression unit which compresses the high number of parameters resulted from dense concatenation. The compression unit is comprised of a single convolutional layer with a kernel of 1×1. The compressed features f c are then forwarded to the Laplacian attention unit which is described next.</p><p>Laplacian Attention: Image Attention has been employed in image classification <ref type="bibr" target="#b39">[41]</ref>, image captioning <ref type="bibr" target="#b40">[42]</ref> etc. to converge on essential image regions. In super-resolution, the same concept with a little variation can be applied that features should be weighted according to their relative importance. Here, we propose Laplacian attention to boost and exploit the relationship between the features that are essential for super-resolving the images. To produce attention differently at the Laplacian pyramids in the DRLM, we use a global descriptor to capture the statistics expressing the entire image. The proposed Laplacian pyramid weights the sub-band features of high importance progressively in each DRLM. The global descriptor takes the output from the compression unit i.e. f c which has size h × w with c feature maps. After processing, the global descriptor reduces the size from h × w × c to 1 × 1 × c as:</p><formula xml:id="formula_10">g d = 1 h × w h i=1 w i=1 f c (i, j),<label>(10)</label></formula><p>where f c (i, j) is the value at position (i, j) in the feature maps.</p><p>To capture the channel dependencies from the retrieved global descriptor, we utilize a gating approach. As studied in <ref type="bibr" target="#b42">[44]</ref>, the system must be able to learn the nonlinear synergies between feature maps and mutually-exclusive associations via gating. To implement the gating mechanism formally, we utilize ReLU and sigmoid functions, denoted by τ , and σ, respectively. The g d features are passed through the Laplacian pyramid to learn the critical features at different scales as:</p><formula xml:id="formula_11">r 3 = τ (D f3 (g d )), r 5 = τ (D f5 (g d )), r 7 = τ (D f7 (g d )),<label>(11)</label></formula><p>where D is the feature reduction operator while the f 3 , f 5 and f 7 are the convolutional layers with kernel dilation specified by the subscripts. The multi-level representations r 3 , r 5 and r 7 obtained from the the global descriptor g d are concatenated as:</p><formula xml:id="formula_12">g p = [r 3 ; r 5 ; r 7 ].<label>(12)</label></formula><p>Furthermore, as shown in Eq 11, the output of the Laplacian pyramid is convolved with a downsampling operator. Therefore, to upsample and differentiate between the features maps, the output is then fed into the upsampling operator U f followed by sigmoid activation as:</p><p>L p = σ(U f (g p )).</p><p>As a final step, the learned statistics are utilized by adaptively rescaling the output of sigmoid function i.e. L p by the input f c of the Laplacian attention unit as: </p><formula xml:id="formula_14">f c = L p × f c<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>In this section of the paper, we present the implementation details of our system. In each cascading residual on the residual block, we have three (k=3) DRLMs, and in each DRLM, we have three RBs densely connected, one compression unit and one Laplacian attention. The filter size in all the layers is set to 3×3 with dilation of 1×1 except in the Laplacian pyramid where it is three, five and seven. Likewise, the number of feature maps in all the convolutional layers are fixed to 64, except the last reconstruction layer where the output is either one for grayscale or three for color images. To keep the size of the feature maps the same, zeros are padded accordingly. In pyramid attention, the feature maps are reduced by a factor of four. We also use a post-upscaling procedure instead of pre-scaling for more efficient processing and to avoid the pre-scaling artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first examine the contributions of various elements of our proposed network. Then we test the model on five publicly available super-resolution dataset, namely, SET5 <ref type="bibr" target="#b44">[46]</ref>, SET14 <ref type="bibr" target="#b45">[47]</ref>, URBAN100 <ref type="bibr" target="#b43">[45]</ref>, B100 <ref type="bibr" target="#b46">[48]</ref>, and MANGA109 <ref type="bibr" target="#b41">[43]</ref>. The metrics employed for evaluation are PSNR and SSIM on the luminance channel obtained through YCbCr color space. We also give a comparison on object recognition performance against the competing super-resolution methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training settings</head><p>To make fair comparisons with the current state-of-the-art use CNN methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b33">[35]</ref>, we use the same settings which are specified in their particular papers. Similar to <ref type="bibr" target="#b33">[35]</ref>, we train our network on DIV2K and Flicker2K datasets <ref type="bibr" target="#b47">[49]</ref>. Furthermore, we diversify the training images through data augmentation, which is accomplished by random rotations using multiples of 90 • supplemented via horizontal and vertical flipping. The batch size is 16 while the size of the low-resolution input is 48 × 48. To optimize the system, ADAM <ref type="bibr" target="#b48">[50]</ref> is utilized with the default parameters of β 1 =0.9, β 2 =0.999, and = 10 −8 . The learning rate is fixed to 10 −4 originally and then decreased to half after every 2 × 10 5 iterations. The network is designed utilizing the PyTorch framework [51] on a Tesla P100 GPU.  <ref type="bibr" target="#b41">[43]</ref> for the scale of 4×. The sharpness of the edges on the objects and textures restored by our method is the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Influence of the skip connections</head><p>Skip connections are the backbone of the current state of the art network <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Here, we demonstrate the effectiveness of the skip-connections i.e. Long skip connection (LSC), Medium skip connection (MSC), and dense local connections (DLC), in our model. The connections are categorized based on their length. <ref type="table" target="#tab_0">Table 1</ref> shows the average PSNR on SET5 for 2× for the different settings of connections. The PSNR is higher when all the connections are present while the performance relatively downgrades when some of the connections are absent. In the absence of all connections, the depth of the network does not yield benefit. This experiment illustrates the significance of the different connection types for our deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Laplacian attention</head><p>The second essential component of our model is Laplacian attention. We provide a comparison of the network with and without the use of Laplacian attention in <ref type="table" target="#tab_0">Table 1</ref>. The results shown support our claim that the selection of essential features through multiple frequency bands assist enhancement of the image and improve the overall accuracy. It should be considered that super-resolution techniques <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref> have matured greatly since SRCNN <ref type="bibr" target="#b27">[28]</ref> and further improvement requires sophisticated network design and the weighting of features through appropriate selection criteria. Both of the mentioned provisions are achieved in our model through Laplacian pyramid attention, and cascading with residual on the residual architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Parameters, runtime, and depth analysis</head><p>The number of parameters is a crucial factor in determining the potential of the CNN networks. More parameters usually lead to better performance, however, computing more parameters requires deeper networks, which increases the computational load. Our aim here is to utilize previously computed features; hence, concatenating the earlier computed features maps strike a balance between performance, depth, and run time. Furthermore, our network achieves state-of-the-art performance with a mere 160 convolutional layers as compared to RCAN <ref type="bibr" target="#b6">[7]</ref> i.e. 400+. In <ref type="figure" target="#fig_2">Figure 4</ref>, we provide a comparison of the parameters and the performance. Our model has fewer parameters as compared to EDSR <ref type="bibr" target="#b4">[5]</ref> and the runtime is less as compared to RCAN <ref type="bibr" target="#b6">[7]</ref> i.e. the time taken by RCAN for an image of size 824×1168 on average is 1.14s opposed to our method 0.045s on MANGA109 <ref type="bibr" target="#b43">[45]</ref> for 4×. This efficiency is due to the fact that our method mainly uses concatenations instead of expensive addition operations. In <ref type="figure" target="#fig_3">Figure 5</ref>, the runtime comparisons are provided against stateof-the-art methods. The PSNR and efficiency are higher for our model, which essentially demonstrates that compact models can push the boundaries with non-conventional architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons</head><p>We present the comparison of our model with the state-of-theart CNN models which include SRCNN <ref type="bibr" target="#b27">[28]</ref>, FSRCNN <ref type="bibr" target="#b19">[20]</ref>, VDSR <ref type="bibr" target="#b8">[9]</ref>, SCN <ref type="bibr" target="#b50">[52]</ref>, SPMSR <ref type="bibr" target="#b51">[53]</ref>, LapSRN <ref type="bibr" target="#b9">[10]</ref>, MSLap-SRN <ref type="bibr" target="#b5">[6]</ref>, MemNet <ref type="bibr" target="#b22">[23]</ref>, EDSR <ref type="bibr" target="#b4">[5]</ref>, SRMDNF <ref type="bibr" target="#b21">[22]</ref>, D-DBPN <ref type="bibr" target="#b17">[18]</ref>, IRCNN <ref type="bibr" target="#b20">[21]</ref>, RDN <ref type="bibr" target="#b14">[15]</ref>, RCAN <ref type="bibr" target="#b6">[7]</ref> and CARN <ref type="bibr" target="#b7">[8]</ref>. Similar to <ref type="bibr" target="#b4">[5]</ref>, we employ self-ensemble to boost the performance of our model and denote it with a '+' to differentiate it from the single model. Similar to contemporary state-of-the-art models, we experiment on two types of image degradations; bicubic-downsample and blur-downsample <ref type="bibr" target="#b20">[21]</ref>. For evaluation of the models, the bicubic downsampling scales of 2×, 3×, 4×, and 8× are adopted, while blur-downsampling is achieved through a Gaussian kernel having 1.6σ 2 for a scale of 3×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Bicubic degradations</head><p>In this section, we provide the qualitative results of our model against competitive methods in <ref type="figure">Figure 6</ref> and 7. 4× Visual Comparisons: To be fair in comparison, the images furnished for qualitative comparison in <ref type="figure">Figure 6</ref> are from the same dataset images as RCAN <ref type="bibr" target="#b6">[7]</ref>. The first two images are from URBAN100 <ref type="bibr" target="#b43">[45]</ref>, and the last picture is from MANGA109 <ref type="bibr" target="#b41">[43]</ref> for upscaling of 4×. As shown in <ref type="figure">Figure 6</ref>, all competing methods, in general, fail to recover edges and introduce blurring artifacts. In "img 076", the competing CNN algorithms are unable to retrieve the rectangular shapes and blur out the edges and boundaries representing the outlines of the windows. Our method is faithful to the original image, providing results with proper rectangular structures and straight lines.</p><p>Similarly, in the second example, i.e. "img 044" in <ref type="figure">Figure 6</ref>, most of the methods distort the horizontal lines and blur out the background. Furthermore, the orientation of the lines on the cropped parts is in the opposite direction and forms a checkerboard pattern for <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Moreover, a close inspection reveals that RCAN <ref type="bibr" target="#b6">[7]</ref> and CARN <ref type="bibr" target="#b7">[8]</ref> fused the background with the horizontal lines, removing the sharpness from the images shown while in our case, the lines are clearly visible and separated from the background, recovering sharp details.</p><p>The last image in <ref type="figure">Figure 6</ref> is from MANGA109 <ref type="bibr" target="#b41">[43]</ref> for 4× scale. The textures in the cropped regions are blemished and mixed by together most of the state-of-the-art methods and are unable to recover the shape of the green colored strokes except for RCAN <ref type="bibr" target="#b6">[7]</ref>. However, RCAN <ref type="bibr" target="#b6">[7]</ref> is also unequipped to superresolve the lines correctly as it can be witnessed that the lines are blended in the right side of the cropped image. Furthermore, the super-resolved green lines are blurry in the case of RCAN <ref type="bibr" target="#b6">[7]</ref>. Our network can super-resolve most of the details and textures without producing visible artifacts from the shown low-resolution image. The green lines are sharp and closer in structure to the original. 8× Visual Comparisons: To show the powerful reconstruction ability of our network, we present extreme examples of 8× super-resolution in <ref type="figure">Figure 7</ref> from URBAN100 <ref type="bibr" target="#b43">[45]</ref> and MANGA109 <ref type="bibr" target="#b41">[43]</ref>. Because of the significant scaling factor in image "img 045", the models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref> using bicubic upsampled input creates artificial checkerboard artifacts and structures in the images due to the incorrect initial input. While, on the other hand, the recent state-of-the-art method <ref type="bibr" target="#b6">[7]</ref> which takes Quantitative evaluation of competing methods. We report the performance of state-of-the-art algorithms on widely used publicly available datasets, in terms of PSNR (in dB) and SSIM. The best results are highlighted with red color while the blue color represents the second best SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Scale SET5 <ref type="bibr" target="#b44">[46]</ref> SET14 <ref type="bibr" target="#b45">[47]</ref> BSD100 <ref type="bibr" target="#b46">[48]</ref> URBAN100 <ref type="bibr" target="#b43">[45]</ref> MANGA109 <ref type="bibr">[</ref> the low-resolution photographs as input, is unable to recover the high frequencies reliably due to the notable upscaling and also produces new structures instead of recovering the original lines. MSLapSRN <ref type="bibr" target="#b5">[6]</ref> is able to super-resolve the lines correctly in the lower half of the image, and this may be due to the progressive reconstruction i.e. employing loss after every 2× resolution to achieve 8× output. In our case, the lines are super-resolved correctly without employing multiple losses. This shows the superresolution capability of our CNN model.</p><p>The second low-resolution image, in <ref type="figure">Figure 7</ref> for 8× is from MANGA109 <ref type="bibr" target="#b41">[43]</ref>, titled, "TaiyouNiSmash", contains minimal high-frequencies and hence is challenging to super-resolve to the desired outcome. Nevertheless, our method still can reproduce better results and avoids producing blurring and artificial structures as compared to competitive techniques. The algorithms of VDSR <ref type="bibr" target="#b8">[9]</ref>, MSLapSRN <ref type="bibr" target="#b5">[6]</ref>, etc. creates blurry outputs and unsharp images. Similarly, RCAN <ref type="bibr" target="#b6">[7]</ref> can produce slightly sharp edges than its predecessor; however, it connected the gaps in different letters. Our model is more faithful to the original image and better captures the small gaps present in the letters.</p><p>Quantitative Comparisons:  The improvement of our method on SET5 <ref type="bibr" target="#b44">[46]</ref> and SET14 <ref type="bibr" target="#b45">[47]</ref> is marginal due to a small number of images (the number with the dataset shows the images present i.e. SET5 has only five photographs, and SET14 has only 14 images) in these mentioned datasets; however, a clear trend emerges when the number of images increases. For example, on MANGA109 <ref type="bibr" target="#b41">[43]</ref>, the average PSNR increment across all scales for our model is 0.34dB and 3.98dB compared to second leading method i.e. RCAN <ref type="bibr" target="#b6">[7]</ref> and the pioneering SRCNN <ref type="bibr" target="#b27">[28]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Blur-Downscale (BD) degradations</head><p>More recently, blur-down (BD) degraded images <ref type="bibr" target="#b20">[21]</ref> are superresolved to showcase the potential of super-resolution architectures. We also utilize blur-downsampled photographs to compare against the state-of-the-art. 3× Visual Comparisons: We first present three examples; two from URBAN100 <ref type="bibr" target="#b43">[45]</ref> and one from MANGA109 <ref type="bibr" target="#b41">[43]</ref> in <ref type="figure">Fig. 8</ref>.</p><p>The images from URBAN100 <ref type="bibr" target="#b43">[45]</ref> super-resolved by the competing methods contain blur effects near the upper end of the buildings as can be seen in the cropped versions. The only methods which perform comparatively better are RDN <ref type="bibr" target="#b14">[15]</ref> and RCAN <ref type="bibr" target="#b6">[7]</ref>; however, our approach is not only able to remove the blur but also restore the high-frequency details. This may be due to the Laplacian-attention used at the end of each DRLM, which captures the important discriminative features that are useful for highfrequency restoration.</p><p>Next, we evaluate our algorithm on an image from the classical SET14 <ref type="bibr" target="#b45">[47]</ref> shown in <ref type="figure">Figure 8</ref>. In the crop sections, our method produces relatively sharp edges and crisper text than state-ofthe-art algorithms which mostly exhibit blurry and distorted text. IRCNN <ref type="bibr" target="#b20">[21]</ref>, SRMD <ref type="bibr" target="#b21">[22]</ref>, and RCAN <ref type="bibr" target="#b6">[7]</ref> are specifically designed to handle blur-downscale super-resolution; however, our method produces best qualitative results having more than 1dB PSNR for this particular image.</p><p>Quantitative Comparisons: Next, <ref type="table" target="#tab_5">Table 3</ref> provides the comparison against nine competitive methods. Here, again RDN <ref type="bibr" target="#b14">[15]</ref> and RCAN <ref type="bibr" target="#b6">[7]</ref> shows good results compared to <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr">[</ref>   performance gain over all methods in general, and RDN <ref type="bibr" target="#b14">[15]</ref> and RCAN <ref type="bibr" target="#b6">[7]</ref> particular. The average PSNR gain over both the mentioned methods for 3× super-resolution of blur-down degraded images is 0.55dB and 0.33dB for all the datasets. Our architecture better generalizes the task at hand, and our Laplacian attention can select the relevant features more reliably in contrast to RCAN's <ref type="bibr" target="#b6">[7]</ref> channel attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Noisy downscale (ND) degradations 2× Visual Comparisons:</head><p>We present two images for superresolving the noisy images. <ref type="figure">Figure 9</ref> shows the comparison of our method with CNN methods on the Birds image from BSD100 <ref type="bibr" target="#b46">[48]</ref> for a low-level noise (σ = 10). It can be observed that our method significantly recovered more texture and removed the noise close to the ground truth image. IRCNN <ref type="bibr" target="#b20">[21]</ref> and RCAN <ref type="bibr" target="#b6">[7]</ref> fail to remove the noise, rather they amplify it. The other noisy image, "Llama", shown in <ref type="figure" target="#fig_4">Figure 10</ref> is taken from Singh et al. <ref type="bibr" target="#b53">[55]</ref> for a fair comparison against traditional algorithms. The difference in texture details on the fur and the background can be observed in our case and the conventional methods. Our method can superresolve the fur more appropriately as compared to the other methods.</p><p>Quantitative Comparisons: To compare quantitatively, we follow the footsteps of Singh et al. <ref type="bibr" target="#b53">[55]</ref> which uses the first 50 images from the BSD100 <ref type="bibr" target="#b46">[48]</ref>. We compare against super-resolving noisy image algorithms, which include (SRNI) <ref type="bibr" target="#b53">[55]</ref>, BM3D-SR <ref type="bibr" target="#b52">[54]</ref> and NLM-SR <ref type="bibr" target="#b54">[56]</ref> as well as the image restoration algorithm IRCNN <ref type="bibr" target="#b20">[21]</ref>. Moreover, BM3D-SR or NLM-SR indicates applying the traditional denoising approach first i.e. (BM3D or NLM), followed by image super-resolution (SR). Currently, RCAN <ref type="bibr" target="#b6">[7]</ref> is state-of-the-art in single image superresolution; however, we show that it is unable to handle noisy images. In <ref type="figure" target="#fig_5">Figure 11</ref>, we present the quantitative results with the competing algorithms for four noise levels i.e. σ = 10, 15, 20 and 25. Our algorithm constantly outperforms CNN-based algorithms and specifically designed methods at all noise levels. Furthermore, the CNN methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b20">[21]</ref> performance is relative to the traditional algorithms when the noise levels are low i.e. σ = 10 and 15; however, it degrades significantly as σ increases while, on the other hand, the performance of our algorithm is better at high-noise levels as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Real-World historic super-resolution</head><p>In this section, we illustrate the application of our algorithm on real-world historic <ref type="bibr" target="#b9">[10]</ref> images suffering from JPEG compression artifacts. The information about the downsampling operators and ground-truth images are unavailable. The results of our reconstruction against state-of-the-art algorithms are shown in <ref type="figure" target="#fig_0">Figure 12</ref>.</p><p>The output of our algorithm is clear and sharper as shown in both Figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance on Object Recognition</head><p>As discussed earlier, image restoration tasks assist in high-level computer vision tasks; therefore, we demonstrate and analyze the effectiveness of our method on object recognition current stateof-the-art super-resolution techniques. We use the same settings as <ref type="bibr" target="#b6">[7]</ref>, which evaluates the performance on the initial 1k images from ImageNet <ref type="bibr" target="#b56">[58]</ref>. The image of size 224×224 is downscaled by 4× to achieve the image size of 56×56. The downscaled versions are then upscaled to the original size images via the superresolution algorithms and subsequently fed through ResNet50 <ref type="bibr" target="#b31">[32]</ref> for classifications. The accuracy of the classification network is used to determine the potential of the super-resolution algorithms. We compare with six methods i.e. Bicubic, DRCN <ref type="bibr" target="#b12">[13]</ref>, FSRCNN <ref type="bibr" target="#b19">[20]</ref>, PSyCo <ref type="bibr" target="#b55">[57]</ref>, ENet-E <ref type="bibr" target="#b32">[33]</ref> and RCAN <ref type="bibr" target="#b6">[7]</ref>. The top-1 and top-5 errors for object recognition are recorded in <ref type="table">Table 4</ref>. Our method is more accurate as it provides the lowest error; hence, this illustrates the ability of our network to reconstruct appropriate frequencies more reliably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Bicubic SRCNN <ref type="bibr" target="#b10">[11]</ref> FSRCNN <ref type="bibr" target="#b19">[20]</ref> SelfExSR <ref type="bibr" target="#b57">[59]</ref> DRCN <ref type="bibr" target="#b12">[13]</ref> Ground-truth VDSR <ref type="bibr" target="#b8">[9]</ref> LapSRN <ref type="bibr" target="#b9">[10]</ref> Ours <ref type="figure" target="#fig_1">Fig. 13</ref>. Limitation. A failure case for super-resolution of 8×. Our algorithm is not able to create finer details if the input low-resolution images lack sufficient high-frequency details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Limitations</head><p>Our model has shown the ability to render sharp and clean images for all upsampling scales; however, it struggles to "hallucinate" finer details. For example, an image with 8× is shown in <ref type="figure" target="#fig_1">Figure 13</ref>, the top of the building is very challenging as due to the large downsampling operator i.e. 8×. All the algorithms fail to recover the fine details at this level. The traditional methods, e.g. SelfExSR <ref type="bibr" target="#b57">[59]</ref>, which exploit the self-similarity and 3D scene geometry, also fail to recover fine details. Similarly, MS-LapSRN <ref type="bibr" target="#b5">[6]</ref> progressively upsamples to produce the 8× results as opposed to ours where the 8× upsampling is achieved directly; however, <ref type="bibr" target="#b5">[6]</ref> is unable to give the desired outcome. Furthermore, this limitation is common to all the super-resolution methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this exposition, we propose a modular convolution neural network for highly accurate image super-resolution. We also employ various components to boost the performance of super-resolution. We thoroughly analyze and present a comprehensive evaluation of the choice of our network design. We employ cascading residual on the residual structure to design a large depth network using long skip connection, short skip connection, and local connections. The cascading residual on the residual architecture helps in the flow of low-frequency information to make network learn high and mid-level frequency information. We use densely connected residual blocks, which reuse the previously computed features. This type of setting has multiple advantages such as implicit "deep supervision" and learning from high-level complex features. We also introduce Laplacian attention, which models the essential features on multiple scales and learns the inter and intra-level dependencies between the feature maps.</p><p>Furthermore, we perform an extensive evaluation of superresolution datasets, low-resolution noisy images and real-world images (unknown blur downsampling). We also have shown the results on Bicubic and blur-down kernels to demonstrate the effectiveness of our proposed methods. Further, we present the performance of object recognition on the super-resolved images by different methods. We have illustrated the potential of our network for image super-resolution; however, our network is general and can be applied to other low-level vision tasks such as image restoration, synthesis, and transformation problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The detailed network architecture of the proposed Network. The top figure shows the overall architecture of our proposed network with cascading residual on the residual architecture i.e. a long skip connection, short skip connections, and cascading structures. The bottom figure presents the backbone of our network i.e. Dense Residual Laplacian Module (DRLM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Laplacian attention. Our model consists of pyramid-level attention to model the features non-linearly. The Laplacian attention weights the residual features at different sub-frequency-bands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Parameters vs. performance. Comparisons are presented on the MANGA109 [43] for 4× super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Performance vs. Time. Comparisons are presented on the URBAN100<ref type="bibr" target="#b43">[45]</ref> for 4× super-resolution. Our proposed method strides a balance between performance and computation time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 .</head><label>10</label><figDesc>Noisy visual comparison on Llama. Textures on the fur, and on rocks in the background are much better reconstructed in our result as compared to the conventional BM3D-SR and BM3D-SRNI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .</head><label>11</label><figDesc>Noisy super-resolution. The plots show average PSNR as functions of noise sigma. Our method consistently improves over specific noisy super-resolution methods and CNN for all σ levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Contribution of different components.</head><label>1</label><figDesc>Investigation of the performance due to different components of our network.</figDesc><table><row><cell>Dense Connections (DC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Medium Skip Connections (MSC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Long Skip Connection (LSC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Laplacian Attention (LA)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR (in dB)</cell><cell>31.92</cell><cell>32.30</cell><cell>32.06</cell><cell>32.06</cell><cell>32.07</cell><cell>31.85</cell><cell>32.12</cell><cell>31.97</cell><cell>32.10</cell><cell>32.37</cell></row><row><cell></cell><cell>Original</cell><cell></cell><cell>Bicubic</cell><cell></cell><cell cols="2">SRCNN [11]</cell><cell></cell><cell cols="2">VDSR [9]</cell><cell>MSLapSRN [6]</cell></row><row><cell></cell><cell>PSNR/SSIM</cell><cell cols="3">21.58/0.6290</cell><cell cols="2">22.03/0.6786</cell><cell></cell><cell cols="2">22.15/0.6925</cell><cell>22.31/0.7030</cell></row><row><cell>Urban100 (4×)</cell><cell>DRRN [12]</cell><cell></cell><cell cols="2">EDSR [5]</cell><cell cols="2">RCAN [7]</cell><cell></cell><cell cols="2">CARN [8]</cell><cell>Ours</cell></row><row><cell>img 076</cell><cell>21.93/0.6903</cell><cell cols="3">23.07/0.7367</cell><cell cols="2">24.31/0.7897</cell><cell></cell><cell cols="2">22.57/0.7175</cell><cell>24.62/0.8032</cell></row><row><cell></cell><cell>Original</cell><cell></cell><cell>Bicubic</cell><cell></cell><cell cols="2">SRCNN [11]</cell><cell></cell><cell cols="2">VDSR [9]</cell><cell>MSLapSRN [6]</cell></row><row><cell></cell><cell>PSNR/SSIM</cell><cell cols="3">26.92/0.7254</cell><cell cols="2">29.70/0.8102</cell><cell></cell><cell cols="2">29.69/0.8312</cell><cell>30.03/0.8430</cell></row><row><cell>Urban100 (4×)</cell><cell>DRRN [12]</cell><cell></cell><cell cols="2">EDSR [5]</cell><cell cols="2">RCAN [7]</cell><cell></cell><cell cols="2">CARN [8]</cell><cell>Ours</cell></row><row><cell>img 044</cell><cell>29.30/0.8373</cell><cell cols="3">33.36/0.9054</cell><cell cols="2">31.45/0.7955</cell><cell></cell><cell cols="2">31.34/0.8648</cell><cell>34.77/0.9188</cell></row><row><cell></cell><cell>Original</cell><cell></cell><cell>Bicubic</cell><cell></cell><cell cols="2">SRCNN [11]</cell><cell cols="3">FSRCNN [20]</cell><cell>LapSRN [10]</cell></row><row><cell></cell><cell>PSNR/SSIM</cell><cell cols="3">24.69/0.7873</cell><cell cols="2">26.26/0.8487</cell><cell></cell><cell cols="2">26.38/0.8500</cell><cell>26.92/0.8752</cell></row><row><cell>MANGA109 (4×)</cell><cell>VDSR [9]</cell><cell></cell><cell cols="2">DRRN [12]</cell><cell cols="2">RCAN [7]</cell><cell></cell><cell cols="2">CARN [8]</cell><cell>Ours</cell></row><row><cell>YumeiroCooking</cell><cell>26.92/0.8731</cell><cell cols="3">27.20/0.8822</cell><cell cols="2">29.85/0.9368</cell><cell></cell><cell cols="2">27.58/0.8953</cell><cell>30.33/0.9422</cell></row></table><note>Fig. 6. Visual comparison for 4×. Super-resolution comparison on sample images with sharp edges and texture, taken from URBAN100 [45] and MANGA109</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>shows the quantitative results for all the competing methods. These results are borrowed from Comparison on sample images with sharp edges and texture, taken from URBAN100 and SET14 datasets for the scale of 3×. The sharpness of the edges on the objects and textures restored by our method is the best.</figDesc><table><row><cell></cell><cell>Original</cell><cell>Bicubic</cell><cell>SRCNN [11]</cell><cell>FSRCNN [20]</cell><cell>VDSR [9]</cell></row><row><cell></cell><cell>PSNR/SSIM</cell><cell>26.10/0.7032</cell><cell>27.91/0.7874</cell><cell>24.34/0.6711</cell><cell>28.34/0.8166</cell></row><row><cell>URBAN100 (3×)</cell><cell>IRCNN [21]</cell><cell>SRMDNF [22]</cell><cell>RDN [15]</cell><cell>RCAN [7]</cell><cell>Ours</cell></row><row><cell>img 078</cell><cell>28.57/0.8184</cell><cell>29.08/0.8342</cell><cell>29.94/0.8513</cell><cell>30.65/0.8624</cell><cell>31.13/0.8685</cell></row><row><cell></cell><cell>Original</cell><cell>Bicubic</cell><cell>FSRCNN [20]</cell><cell>VDSR [9]</cell><cell>IRCNN [21]</cell></row><row><cell></cell><cell>PSNR/SSIM</cell><cell>22.58/0.84597</cell><cell>21.09/0.8254</cell><cell>19.30/0.6960</cell><cell>24.60/0.9092</cell></row><row><cell>URBAN100 (3×)</cell><cell>EDSR [5]</cell><cell>SRMDNF [22]</cell><cell>RCAN [7]</cell><cell>CARN [8]</cell><cell>Ours</cell></row><row><cell>img 062</cell><cell>24.48/0.9105</cell><cell>28.63/0.9695</cell><cell>29.41/0.9775</cell><cell>24.45/0.9096</cell><cell>30.78/0.9830</cell></row></table><note>Fig. 8. Blur-Downscale (BD) degradation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 Quantitative results with blur-down degradation.</head><label>3</label><figDesc>The best results are highlighted with red color while the blue color represents the second best.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SET5 [46]</cell><cell cols="2">SET14 [47]</cell><cell cols="2">BSD100 [48]</cell><cell cols="2">URBAN100 [45]</cell><cell cols="2">MANGA109 [43]</cell></row><row><cell>Method</cell><cell>Scale</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>Bicubic</cell><cell></cell><cell>28.78</cell><cell>0.8308</cell><cell>26.38</cell><cell>0.7271</cell><cell>26.33</cell><cell>0.6918</cell><cell>23.52</cell><cell>0.6862</cell><cell>25.46</cell><cell>0.8149</cell></row><row><cell>SPMSR</cell><cell></cell><cell>32.21</cell><cell>0.9001</cell><cell>28.89</cell><cell>0.8105</cell><cell>28.13</cell><cell>0.7740</cell><cell>25.84</cell><cell>0.7856</cell><cell>29.64</cell><cell>0.9003</cell></row><row><cell>SRCNN [11]</cell><cell></cell><cell>32.05</cell><cell>0.8944</cell><cell>28.80</cell><cell>0.8074</cell><cell>28.13</cell><cell>0.7736</cell><cell>25.70</cell><cell>0.7770</cell><cell>29.47</cell><cell>0.8924</cell></row><row><cell>FSRCNN [20]</cell><cell></cell><cell>26.23</cell><cell>0.8124</cell><cell>24.44</cell><cell>0.7106</cell><cell>24.86</cell><cell>0.6832</cell><cell>22.04</cell><cell>0.6745</cell><cell>23.04</cell><cell>0.7927</cell></row><row><cell>VDSR [9]</cell><cell></cell><cell>33.25</cell><cell>0.9150</cell><cell>29.46</cell><cell>0.8244</cell><cell>28.57</cell><cell>0.7893</cell><cell>26.61</cell><cell>0.8136</cell><cell>31.06</cell><cell>0.9234</cell></row><row><cell>IRCNN [21]</cell><cell>3×</cell><cell>33.38</cell><cell>0.9182</cell><cell>29.63</cell><cell>0.8281</cell><cell>28.65</cell><cell>0.7922</cell><cell>26.77</cell><cell>0.8154</cell><cell>31.15</cell><cell>0.9245</cell></row><row><cell>SRMDNF [22]</cell><cell></cell><cell>34.01</cell><cell>0.9242</cell><cell>30.11</cell><cell>0.8364</cell><cell>28.98</cell><cell>0.8009</cell><cell>27.50</cell><cell>0.8370</cell><cell>32.97</cell><cell>0.9391</cell></row><row><cell>RDN [15]</cell><cell></cell><cell>34.58</cell><cell>0.9280</cell><cell>30.53</cell><cell>0.8447</cell><cell>29.23</cell><cell>0.8079</cell><cell>28.46</cell><cell>0.8582</cell><cell>33.97</cell><cell>0.9465</cell></row><row><cell>RCAN [7]</cell><cell></cell><cell>34.70</cell><cell>0.9288</cell><cell>30.63</cell><cell>0.8462</cell><cell>29.32</cell><cell>0.8093</cell><cell>28.81</cell><cell>0.8647</cell><cell>34.38</cell><cell>0.9483</cell></row><row><cell>DRLN (ours)</cell><cell></cell><cell>34.81</cell><cell>0.9297</cell><cell>30.81</cell><cell>0.8487</cell><cell>29.40</cell><cell>0.8121</cell><cell>29.11</cell><cell>0.8697</cell><cell>34.84</cell><cell>0.9506</cell></row><row><cell>DRLN+ (ours)</cell><cell></cell><cell>34.87</cell><cell>0.9301</cell><cell>30.86</cell><cell>0.8495</cell><cell>29.44</cell><cell>0.8128</cell><cell>29.26</cell><cell>0.8718</cell><cell>35.07</cell><cell>0.9516</cell></row><row><cell cols="6">their corresponding published papers. Our scheme outperforms all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">other approaches on all datasets for all scales which complements</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">the visual sequences presented earlier in Figures 6 and 7. Our</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">model quantitative results outperform even without employing the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>self-ensemble technique.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Textures on the birds are much better reconstructed, and the noise removed by our method as compared to the IRCNN<ref type="bibr" target="#b20">[21]</ref> and RCAN<ref type="bibr" target="#b6">[7]</ref> for σ = 10.</figDesc><table><row><cell cols="2">Original(PSNR/SSIM)</cell><cell cols="2">IRCNN [21] (28.73/0.6762)</cell><cell>RCAN [7] (28.44/0.6607)</cell><cell>Ours (32.46/0.8760)</cell></row><row><cell>Noisy</cell><cell>GT</cell><cell></cell><cell>BM3D-SR [54]</cell><cell>BM3D-SRNI [55]</cell><cell>Ours</cell></row><row><cell>σ = 20</cell><cell cols="2">PSNR/SSIM</cell><cell>25.05/0.5868</cell><cell>25.31/0.6206</cell><cell>27.03/0.7330</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>21], [28];</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">however, our single and self-ensemble models achieve a notable</cell></row></table><note>Fig. 9. Noisy SR visual Comparison on BSD100.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 Objection recognition .</head><label>4recognition</label><figDesc>We report the performance of ResNet<ref type="bibr" target="#b3">[4]</ref> using different SR algorithms as a pre-processing step. In these cases, neither the downsampling blur kernels nor the ground-truth images are available. In the top photograph, our methods reconstruct the letters "W" and "I" correctly while the competing methods combine the letters. Similarly, in the bottom picture, the rail is accurately super-resolved without any artifacts while others fail to super-resolve the horizontal lines correctly.</figDesc><table><row><cell>Evaluation</cell><cell cols="6">Bicubic DRCN [13] FSRCNN [20] PSyCo [57] ENet-E [33] RCAN [7]</cell><cell>Ours</cell><cell>Baseline</cell></row><row><cell>Top-1 error</cell><cell>0.506</cell><cell>0.477</cell><cell>0.437</cell><cell>0.454</cell><cell>0.449</cell><cell>0.393</cell><cell>0.345</cell><cell>0.260</cell></row><row><cell>Top-5 error</cell><cell>0.266</cell><cell>0.242</cell><cell>0.196</cell><cell>0.224</cell><cell>0.214</cell><cell>0.167</cell><cell>0.121</cell><cell>0.072</cell></row><row><cell></cell><cell></cell><cell>Bicubic</cell><cell>SRCNN [11]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR input</cell><cell></cell><cell>DRCN [13]</cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">FSRCNN [20]</cell><cell>VDSR [9]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR input</cell><cell cols="2">MSLapSRN [6]</cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Fig. 12. Comparison of real-world images.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Low resolution face recognition across variations in pose and illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Mudunuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution in medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CJ</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Digital image forensics via intrinsic fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIFS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast, accurate, and, lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ram: Residual attention module for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning a single convolutional superresolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Image super resolution based on fusing multiple convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Single image superresolution via cascaded multi-scale cross network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast and accurate single image superresolution via information distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Generative adversarial nets,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<editor>34] S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Formresnet: formatted residual learning for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Manga109 dataset and creation of metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>MANPU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A statistical prediction model based on sparse representations for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Super-resolving noisy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Psyco: Manifold span reduction for super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pérez-Pellitero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
