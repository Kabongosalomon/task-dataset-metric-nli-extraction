<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Instance-Based Video Anomaly Detection using Deep Temporal Encoding-Decoding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Ammar</forename><surname>Mansoor Kamoona</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Khodadadian Gostar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Alireza</forename><surname>Bab-Hadiashar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Reza</forename><surname>Hoseinnezhad</surname></persName>
						</author>
						<title level="a" type="main">Multiple Instance-Based Video Anomaly Detection using Deep Temporal Encoding-Decoding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Anomaly detection</term>
					<term>surveillance videos</term>
					<term>weakly supervised multiple instance learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a weakly supervised deep temporal encoding-decoding solution for anomaly detection in surveillance videos using multiple instance learning. The proposed approach uses both abnormal and normal video clips during the training phase which is developed in the multiple instance framework where we treat the video as a bag and video clips as instances in the bag. Our main contribution lies in the proposed novel approach to consider temporal relations between video instances. We deal with video instances (clips) as sequential visual data rather than a set of independent instances. We employ a deep temporal encoding-decoding network that is designed to capture spatio-temporal evolution of video instances over time. We also propose a new loss function that maximizes the mean distance between normal and abnormal instance predictions. The new loss function ensures a low false alarm rate which is very crucial in practical surveillance application.The proposed temporal encoding-decoding approach with modified loss is benchmarked against the state of the art in simulation studies. The results show that the proposed method performs similar to or better than the state-of-the-art solutions for anomaly detection in video surveillance applications and achieve state of the art false alarm rate on UCF-crime dataset. , where he has worked, since 2010, and is currently a Professor, and a Research Development Lead, as well as the Discipline Leader (Manufacturing and Mechatronics) with the School of Engineering. His main research interests include statistical information fusion, random Finite sets, multi-object tracking, deep learning, and robust multi-structure data ftting in computer vision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO anomaly detection is defined as the process of detecting the occurrence of "abnormal" events in video clips that differ from previously defined "normal" clips. Automatic detection of anomalies in video has gained a significant attention in the past few years <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. This is mainly due to the difficulty of manual processing (requires extensive manpower) of the abundant visual information generated by surveillance cameras. From security perspective, the detection of events such as stealing, fighting and shoplifting is of particular interest. Examples of such activities are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Unsupervised anomaly detection is commonly formulated for detection of rare abnormal events in which only frequently occurring behaviour of normal samples is used in the training. The most common approach is to treat abnormal events as outliers to a model that is trained using normal videos <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b11">[12]</ref>. Unsupervised anomaly detection is usually performed either by using handcrafted features followed by feature learning or via development of an end-to-end deep network. The earlier approaches for anomaly detection commonly involved extraction of trajectory features to make use of its ability to describe the dynamics of moving objects <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In addition, different spatio-temporal handcrafted features such as color, texture and optical flow have been used for anomaly detection <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>. However, due to illumination changes, scale and deformation, these features do not generalize well for large scale video analysis. Therefore, unsupervised deep learning has been used for feature extraction and model learning <ref type="bibr" target="#b0">[1]</ref>.</p><p>The aforementioned methods are based on normality deviation. However, Chandola et al. <ref type="bibr" target="#b17">[18]</ref> showed that it is ambiguous to define a boundary between normal and abnormal, mostly due to the definition of normal events that can not take into account all possible normal patterns or behaviors. As a result, a any new occurrence of normal event may also deviate from the trained model and cause a false alarm <ref type="bibr" target="#b18">[19]</ref>. Recently, a weakly supervised learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> has gained a popularity which leverages the aforementioned problem by using of both normal and abnormal videos. A video is labeled as normal if all the video frames are normal and abnormal when video frames has normal and abnormal frames. The main reason for formalizing the problem within weak supervision is due to the lack of temporal labeling of abnormal videos.</p><p>Sultani et al. <ref type="bibr" target="#b5">[6]</ref> proposed to tackle the weakly supervised problem within the multiple instance learning framework where the bag (video) label is available and a model is trained to infer the instance label. They employed multiple instance hinge loss function and designed a network that processes video clips (independently from each other). Another weakly supervised learning approach is by training classifier under noisy labels <ref type="bibr" target="#b19">[20]</ref>. The noisy labels refer to normal segments in the anomalous video.</p><p>In this paper, we propose a new solution that hierarchically captures low, intermediate and high level temporal and spatial information. The contribution of this work is two-fold: (i) we propose a novel solution for anomaly detection in videos that uses a temporal encoding network to capture the temporal and spatial information of video instances, and (ii) the formulation of this solution is implemented within the weakly supervised multiple instance framework where we propose a loss function that has a smoother instances to bag mapping than its counterpart and penalize the false alarm. In addition, it achieves a competitive results compared to the state-of-the-art. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section, a brief review of the most recent works on video-based anomaly detection methods is presented. Generally, the common approach for visual anomaly detection is based on extracting handcrafted or deep representation features, and model learning. In this approach, anomaly detection is usually formulated as an outlier detection problem.</p><p>Tracking-based anomaly detection methods were the earlier approaches used for dynamic feature extraction to model the normal pattern of movements of objects of the interest <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, anomaly detection based on semantic scene trajectory clustering was proposed. In this method, first blob detection is performed to detect the object of interest. Then feature extraction is performed using spatial and velocity information. These features are then clustered based on their similarities (in term of the size of the objects and their velocities) and the result is used to form trajectories. Hu et al. <ref type="bibr" target="#b22">[23]</ref> proposed another anomaly detection technique using trajectory clustering of motion patterns. First, foreground pixels are detected using background subtraction method, then the features are clustered using the Fuzzy K-mean clustering algorithm. Anomaly detection is then carried out by comparing the learned motion pattern probability distribution obtained from the trajectories. In general, tracking-based methods are not robust enough for complex video scene analysis since they involve different complex steps such as object detection, data association and tracking, and any failure in these steps causes a failure in the anomaly detection system.</p><p>Due to the limitations of tracking-based methods, handcrafted spatio-temporal features have been employed to model the motion pattern for anomaly detection. The most straightforward approach is to extract low-level appearance features and motion cues such as color, texture and optical flow and use them to model motion activity patterns <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Mehran et al. <ref type="bibr" target="#b3">[4]</ref> proposed to use social force model combined with optical flow features to learn normal pattern for global motion and any deviation from this model (with low probability) is considered an anomaly. Zhao et al. <ref type="bibr" target="#b24">[25]</ref> used the Spatial Temporal Interest Point (STIP) detector to detect the region of interest and then Histogram of Gradient (HOG) as an appearance feature descriptor and Histogram of Optical Flow (HOF) as motion feature descriptor were used to detect abnormal activity in videos. Mahadevan et al. <ref type="bibr" target="#b25">[26]</ref> tackled the problem of spatial and temporal anomaly detection in crowded scenes by jointly modeling a mixture of appearance and dynamics. Spatial anomalies are detected using discriminant saliency, while temporal anomalies are detected as an event with low probability.</p><p>Recently, unsupervised deep learning using autoencoder network has been widely used for latent features representation and anomaly detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Hasan et al. <ref type="bibr" target="#b18">[19]</ref> used the reconstruction error of the fully connected convolution autoencoder as an anomaly score. Xu et al. <ref type="bibr" target="#b26">[27]</ref> proposed a rich and descriptive motion and appearance feature representation using a stacked autoencoder. Anomaly detection is performed based on anomaly score calculated using multiple one class SVM on the learned feature representation. Nguyen and Meunier <ref type="bibr" target="#b27">[28]</ref> proposed unsupervised anomaly detection by combining a convolutional autoencoder with a U-Net network. The resulting network tries to learn the normal appearance spatial structure through autoecoder and their related motion pattern from optical flow through the second stream which is performed by U-Net. A modified version of inception module has been integrated to the network leading to a patch-based scheme for estimating frame level anomaly. The network has been trained end-to-end using three loss functions: distance loss function and optical flow loss and adversarial loss.</p><p>Abate et al. <ref type="bibr" target="#b0">[1]</ref> tackled the anomaly detection problem by utilizing the ability of the network on remembering the normal events and evaluating the degree of network surprisal. The remembering aspect of the network is modeled by the reconstruction error of the autoencoder. Also, the suprisal aspect of the network is modeled by calculating the density of latent features using the autoregressive network. During the training phase, the joint loss function that combines negative log of the reconstruction error and probability density of latent features is used. The novelty of this work lies in modelling the probability density of the latent features using autoregressive model. Most of the autoencoder-based networks are based on element-wise measures such as the squared error. However, the problem of the element-wise metric is its poor performance in modeling the properties of human visual perception. For instance, a small image translation might cause large pixelwise error <ref type="bibr" target="#b28">[29]</ref>.</p><p>The majority of solutions developed for anomaly detection in video are based on unsupervised learning where only normal videos are used for learning and anomaly detection is detected as an outlier detection problem (low probability, anomaly score and reconstruction error). Most video anomaly datasets used for the training and testing are short scenes and cannot generalize to all possible normal patterns. As a result, it is very hard to build a boundary between normal and abnormal events due to the lack of videos that model all possible normal patterns <ref type="bibr" target="#b17">[18]</ref>. Sultani et al. <ref type="bibr" target="#b5">[6]</ref> introduced a new approach based on weak supervision where both normal and abnormal videos are used for anomaly detection. In their solution, the anomaly detection problem is formulated within the Multiple Instance Learning (MIL) framework where only bag label is available. In this work, the video is divided into a fixed number of instances (clips) and a multi layer feedfoward perceptron network is trained to predict instance labels based on the deep ranking approach <ref type="bibr" target="#b5">[6]</ref>. Recently, Zhong et al. <ref type="bibr" target="#b19">[20]</ref> proposed to address the problem of weak supervision as a supervised learning task under noisy label in which they propose graph convolutional label noise clearner (GCN). This network uses video characteristics such as feature similarity and temporal consistency of video snippets to clean the noise (normal segments of anomalous video). In contrast to its superior performance and since the method is trained using the a whole video at each iteration, the method prone to data correlation <ref type="bibr" target="#b20">[21]</ref>. To overcome the data correlation problem. Zaheer et al. <ref type="bibr" target="#b20">[21]</ref> proposed to train the network using batch approach where each batch consist of temporally consecutive segments of a video. In addition, they propose normalcy suppression meachanism to suppress normal features.</p><p>The network proposed by Sultani et al. <ref type="bibr" target="#b5">[6]</ref> deals with instances (clips) independently and does not capture low, intermediate and long temporal information which are very important for video data analysis. Sequence modeling has been used in different fields such as language modeling <ref type="bibr" target="#b29">[30]</ref>, video summarization <ref type="bibr" target="#b30">[31]</ref> and action segmentation <ref type="bibr" target="#b31">[32]</ref> to capture the temporal information.</p><p>Inspired by the success of temporal convolution in the sequence modelling, we propose a temporal encoding network for anomaly detection in surveillance videos. The proposed network aims to capture the temporal information between video instances. In addition, the problem is also tackled within the MIL framework (weakly supervised) and we formulate a loss function that uses mean mapping which is smoother than max operation. Also, the loss function penalise the false alarm possibilities which is ultimate desire for real surveillance applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The proposed solution is shown in <ref type="figure">Figure 2</ref>. First, we extract the video spatio-temporal features using C3D network <ref type="bibr" target="#b32">[33]</ref>. Then, these feature are divided into a fixed number of nonoverlapped clips. These clips form sequential instances in the bag. The proposed temporal encoding-decoding network finds how normal/abnormal feature instances evolve over time.</p><p>During the training phase, we use both normal and abnormal videos using our a deep ranking loss function to update the network weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal encoding-decoding network</head><p>Before introducing the network structure, we define the sequence modeling task. Assume that a sequence of features, (x 0 , . . . , x T ), is given where x i ∈ X ⊂ R d and d is the dimension of the feature space X . At each time step, we want to predict the corresponding output as another sequence  </p><formula xml:id="formula_0">x n x n−1 x 3 x 2 x 1 C3D network abnormal video frames Fig. 2.</formula><p>Proposed approach of temporal encoding-decoding network. Normal and abnormal videos are fed into a C3D <ref type="bibr" target="#b32">[33]</ref> network to extract spatio-temporal features which are then divided into 32 clips to form instances. These instances are treated as sequential visual information processed by a temporal encodingdecoding network that captures how the features evolve over time and predicts an anomaly score using based on a mapping from instance to label.</p><p>(y 0 , . . . , y T ) of entities in an output space Y. Formally, we can define the sequence modeling network as a mapping function f : X T +1 → Y T +1 as follows <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_1">(ŷ 0 , . . . ,ŷ T ) = f (x 0 , . . . , x T ).<label>(1)</label></formula><p>Note that the number of time steps T is not fixed and depends on the number of sequences of the problem. If the above formulation satisfies the causal constrain, this mean y t depends on only previous features (x 0 , . . . , x t−1 ) and not on any future inputs. The goal of learning in sequence modeling is to build a network f that minimizes a loss function between actual output and the predicted ones, L(y 0 , . . . , y T , f (x 0 , . . . , x)).</p><p>The sequences and the outputs are drawn according to some distribution. In our proposed solution, we deal with video instances as a sequence and the entire learning process as sequence to sequence learning within a weak supervision. In this setting, causal constrain is very important to ensure no information leakage during the training. The most commonly used network architecture for sequence modeling is the Recurrent Neural Network (RNN) <ref type="bibr" target="#b34">[35]</ref>. Recent works have shown that 1-D convolution can also be employed for different tasks that involve sequence modeling such as audio synthesis <ref type="bibr" target="#b29">[30]</ref>, machine translation <ref type="bibr" target="#b35">[36]</ref> and action segmentation <ref type="bibr" target="#b36">[37]</ref> where no future information is used to predict each output. We have also employed 1D temporal convolution for our network.</p><p>Temporal Convolutional Network (TCN) is a network which inherits the properties of convolutional neural networks and uses them to learn a sequence model. TCN is a causal network where there is no information leakage from the future to the past <ref type="bibr" target="#b33">[34]</ref>. Commonly, TCN consists of a 1D fully convolutional network (FCN) and causal convolution in which zero padding of length (kernel size-1) is added to keep the length of the hidden layer the same as the input layer <ref type="bibr" target="#b33">[34]</ref>.</p><p>Inspired by <ref type="bibr" target="#b36">[37]</ref>, we propose a modified TCN network, shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The new TCN network consists of a temporal encoder/decoder that consists of two steps. Each step has a 1D temporal convolutional layer, a temporal pooling/up-sampling layer, and a channel-wise normalization layer. In particular, the layer l in the encoder/decoder network contains a set of 1D temporal filters, parameterized by tensor</p><formula xml:id="formula_2">W l ∈ R F l ×C d ×F l−1 where l ∈ N ∩ [1, L]</formula><p>is the layer index, C d is the temporal convolution duration length and F l is the number of convolution filters in layer l. These filters are designed to capture the spatio-temporal features and their evolution over the time from one clip to another. The activation functionÊ</p><formula xml:id="formula_3">(l) i,t for the i-th component (i ∈ N∩[1, F l ])</formula><p>of the l-th layer at time step t is defined as:</p><formula xml:id="formula_4">E (l) i,t = f b l i + C d t =1 W l i,t , E (l−1) 0,t+C d −t ,<label>(2)</label></formula><p>where E (l−1) is the normalized activation from the previous layer, f (·) is the Leaky Rectified Liner Unit, b l i is the bias vector for the i-the component in layer l, and ·, · is the regular inner product operation. The channel-wise normalization is done as follows <ref type="bibr" target="#b36">[37]</ref>: whereÊ (l) = max iÊ (i,l) is the highest response at time step t and is a very small number (usually set as = 1 × 10 −5 . Max-pooling with width equal to T l = 1 2 T l−1 is performed across the temporal domain for each encoder layer.</p><formula xml:id="formula_5">E (l) = 1/(m + )Ê (l) ,<label>(3)</label></formula><p>The decoder architecture is similar to the encoder with the exception of the max-pooling layer which is replayed by the up-sampling layer. The last layer of the decoder is a Sigmoid layer that calculates the anomaly score with the temporal domain. The training is performed within a weakly supervised framework using normal and abnormal videos. Note that the temporal annotation of abnormal videos is not provided. As a result, the loss function is formulated within the multiple instance learning framework as explained in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiple instance deep ranking</head><p>In a multiple instance learning (MIL) context, the task is to learn a classifier based on a set of bags where each bag contains multiple instances. In this setting, the label of the bag is available during training. However, the labels of the instances are not provided. Existing MIL methods can be classified into bag paradigm and instance paradigm <ref type="bibr" target="#b37">[38]</ref>. In the bag paradigm, the aim is to predict the label of the bag, where as, in the instance paradigm, the aim is to predict the instance label <ref type="bibr" target="#b38">[39]</ref>. Commonly, the main assumption of the MIL is that the bag is positive if at least one instance is positive (for example it is anomaly), otherwise the bag is negative. This assumption is used to map the label from the instance level to the bag level. However, in most applications such as image segmentation or fine-grained sentiment classification, it is crucial to find the instance label with only bag labels given during training (weakly supervised).</p><p>Recently, there has been an increased interest from the computer vision community in studying weakly supervised solutions specially within the MIL framework, in applications such as object detection and localization <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, image classification <ref type="bibr" target="#b41">[42]</ref>, and video-based anomaly detection <ref type="bibr" target="#b5">[6]</ref>. This is due to the fact that MIL relaxes the need for instance labels (the temporal annotation in our case) and only bag label (video label) is needed. In the following sections, we review MIL deep ranking and related works.</p><p>1) Mathematical formulation of MIL deep ranking: Let X ∈ X be the instance-level input random variable and Y X ⊂ Y X is the instance output, where the space of X is X ⊂ R d , and d is the dimension of the feature vector, also, Y X = {0, 1}. Assume B ∈ B is the bag-level input random variable and Y B ∈ Y B is the bag-level output. For instance-based binary classification problem where the training samples are i.i,d, the binary classifier can be defined as follows:</p><formula xml:id="formula_6">f (x) = sign(g(x)) ∈ {+1, −1} ,<label>(4)</label></formula><p>where g(·) is a mapping function g : R d → R. For the support vector machine (SVM), the optimization problem is reduced to a quadratic programming problem <ref type="bibr" target="#b42">[43]</ref>:</p><formula xml:id="formula_7">min α∈R d ,β∈R 1 2 ||α|| 2 + C N i=1 max 0, 1 − y i (α φ(x i ) + β) ,<label>(5)</label></formula><p>where C &gt; 0 is a penalty parameter, β ∈ R is a bias parameter, α ∈ R m is an m-dimensional classifier weight to be learned, x i ∈ R d is the d-dimensional instance feature vector, φ : R d → R m and y i is the label of the instance. The first term of (5) is the L 2 regularization and the second term is the hinge loss term, defined as g(x) = max(0, 1 − x). It is important to mention that the hinge loss function is not differentiable <ref type="bibr" target="#b43">[44]</ref>. Therefore, different algorithms have been proposed as a solution, such as using the numerical approximation of the hinge loss <ref type="bibr" target="#b44">[45]</ref> or using the generalized hinge loss <ref type="bibr" target="#b45">[46]</ref>. The maximum margin classifier SVM formulation has been extended to the MIL. In this case, the goal of SVM is to infer the bag label from the instances using the maximum score of the instances in the bag <ref type="bibr" target="#b46">[47]</ref>:</p><formula xml:id="formula_8">min α∈R d ,β∈R 1 2 ||α|| 2 + C N B j=1 max x∈B j 0, 1 − Y Bj (max(α φ(x) + β)) ,<label>(6)</label></formula><p>where N B is the total number of the bags. Sultani et al. <ref type="bibr" target="#b5">[6]</ref> reformulate the MIL ranking problem into a rank regression problem. The main assumption is that the anomalous bags should always have a higher anomaly score than the normal bags:</p><formula xml:id="formula_9">max x∈Ba f (x) &gt; max x∈Bn f (x)</formula><p>, for any B a ∈ B a , and any B n ∈ B n <ref type="bibr" target="#b6">(7)</ref> where B a and B n are the given ensembles of abnormal and normal video bags, respectively, and f (·) is the predicted anomaly score for an instance in a bag. The first term of Eq. <ref type="formula">(7)</ref> represents the instance (segment) that has the highest anomaly score in a given abnormal bag (video), and is highly likely to be an anomaly. However, the second term of Eq. <ref type="formula">(7)</ref> represents the video segment with the highest anomaly score in a given normal video, which is likely to be a normal instance. The loss equation proposed in <ref type="bibr" target="#b5">[6]</ref>, L(·), includes temporal smoothness of the abnormal video segments as well as sparsity term:</p><formula xml:id="formula_10">L(B a , B n ) = max 0, 1 − max x∈Ba f (x) + max x∈Bn f (x) +λ 1 x∈Ba ∆f (x) 2 + λ 2 x∈Ba f (x),<label>(8)</label></formula><p>where λ 1 and λ 2 are hyper-parameters that control the amount of trade-off, and the ∆f (·) : R d → [−1, 1] is the discrete gradient function, defined as</p><formula xml:id="formula_11">∆f (x i ) f (x i ) − f (x i+1 )</formula><p>assuming that B a = (x 1 , · · · , x n ) where n is the number of instances in the video bag (in our proposed solution examined in the experiments, n = 32). The second term in equation <ref type="formula" target="#formula_10">(8)</ref> is an L 2 regularization term to ensure that the anomaly score of abnormal video varies smoothly from each video segment to the next. The last term in equation <ref type="formula" target="#formula_10">(8)</ref> is the L 1 regularization sparse that reflects the fact that anomaly in abnormal videos occurs for a short time only. It is evident that the first term of the loss function penalizes the positive bags with low scores. The problem of the aforementioned loss function is based on the assumption that the bag label is inferred from the maximum score of the instances in the bag, Y B = max(f (x i )). The problem arises from the fact that the max function is not smooth <ref type="bibr" target="#b47">[48]</ref> and the optimization suffers from the vanishing gradients <ref type="bibr" target="#b48">[49]</ref>. To alleviate this problem, we propose to use the average difference between normal and abnormal bags in our loss function instead of the max operation:</p><formula xml:id="formula_12">L(B a , B n ) = 1 − max (0, d)) + λ x∈Ba f (x) (9) where d =f (B a ) −f (B n ), f (B) = x∈B f (x) |B|,</formula><p>and | · | means "cardinality of" or "the number of elements in". Indeed, |B| is the bag size for both normal and abnormal video bags that was already denoted by n which is 32 in our experiments.</p><p>This formulation will take into account not only the one instance with the maximum score but also the scores of all instances in the bag. In addition, the defined loss function maximizes the distance between the average of normal instance score and abnormal instance score, see <ref type="figure">Figure 4</ref>. The proposed loss function has only one max operation involved which makes it smoother than that of the one proposed in <ref type="bibr" target="#b7">(8)</ref>.</p><p>To penalize the loss function of the abnormal bag, similar to (8), we added L 1 regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we test our proposed temporal encodingdecoding network with the proposed loss function using two public datasets which are UCF-cirme dataset <ref type="bibr" target="#b5">[6]</ref> and Shang-haiTech <ref type="bibr" target="#b8">[9]</ref>. We also compare the performance of the proposed solution with the state of the art. In addition, both qualitative and quantitative analysis are carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>In this paper, we have conducted experiment on two public datasets which are the UCF-crime dataset <ref type="bibr" target="#b5">[6]</ref> and the Shang-haiTech <ref type="bibr" target="#b8">[9]</ref>.</p><p>1) UCF-Crime: is a large scale dataset of long videos with different scenes that represent real-life situations. The dataset consists of 1900 videos divided into training sets and testing sets. The training sets consist of 800 normal videos and 810 abnormal videos and the test sets include 150 normal and 140 abnormal videos (290 videos in total). The abnormal videos in both training and testing cover 13 real-world anomalies with following descriptions: Abuse, Arrest, Arson, Assault, Accident, Burglary, Explosion, Fighting, Robbery, Shooting, Stealing, Shoplifting and Vandalism, see <ref type="figure" target="#fig_0">Figure 1</ref>. The total dataset duration is 128 hours. In this dataset, no temporal (frame-level) annotation is available except for the testing videos. The UCF-crime dataset is the biggest video anomaly dataset and the only one that has multiple scenes with real surveillance videos. Refer to <ref type="bibr" target="#b5">[6]</ref> for more details. All possible false alarms <ref type="figure">Fig. 4</ref>. The proposed loss function use the mean distance d between abnormal bag instances score and normal bag instances score. To ensure the abnormal bag always has a higher mean score, we penalize d when it is negative.</p><formula xml:id="formula_13">d =f (B a ) −f (B n ) f (B a ) Mean of the predictions f (x n ) f (x n−1 ) f (x 3 ) f (x 2 ) f (x 1 ) Instance level predictions of abnormal bags B af (B n ) Mean of the predictions f (x n ) f (x n−1 ) f (x 3 ) f (x 1 ) f (x 1 )</formula><p>2) ShanghaiTech: is a medium-scale dataset that contains 437 different videos captured at a university campus. It has 13 different scenes of total 31739 frames of resolution 489× 856 pixels with different lighting conditions and camera angles. ShanghaiTech dataset is commonly used for unsupervised anomaly detection, thus, there is no abnormal videos for training. To accommodate this dataset for weakly supervised problem, a new split has been created by Zhong et al. <ref type="bibr" target="#b19">[20]</ref> which in this split the training set has normal and abnormal videos. The new split has 175 normal and 63 abnormal training videos and the test set has 155 normal and 44 abnormal videos. For fair comparison, we have used the same split in our experiment.</p><p>B. Implementation details 1) Feature extraction and bag generation: : First, preprocessing operations is performed before feeding it into the C3D network. Each video frame is resized into 240 × 320 with frame rate fixed to 30 fps. We followed the same technical procedure in <ref type="bibr" target="#b5">[6]</ref> to extract the C3D features. We extracted the spatio-temporal features from the fully connected layer (FC6) of the C3D network <ref type="bibr" target="#b32">[33]</ref>. The C3D network computes the C3D features for every 16 frames and then followed by l 2 normalization. We divide each video to 32 non-overlapping clips. The video is treated as a bag, and each clip is treated as an instance in the bag. Since we deal with a fixed number of instances per bag, the video instance feature (clip) is generated by taking the average for all 16-frame clip features within that video clip. During the training, a random selection of 30 normal videos and 30 abnormal videos is carried out, then we feed it as mini-batch to the proposed network as shown in <ref type="figure">Figure 2</ref>.</p><p>2) Network implementation: : The proposed network is implemented using Keras <ref type="bibr" target="#b49">[50]</ref> backend with TensorFlow (https: //www.tensorflow.org/) and python. We set the temporal convolution kernel length to 4 and different kernel lengths are tested and reported. The number of convolution filters used in our network is set to {F 1 = 512, F 2 = 128}. The last layer of our network is a temporal fully connected layer with a sigmoid activation. We use l 2 regularizer for kernel weight parameters for each layer. We used dropout to prevent overfitting. The adaptive subgradient optimizer <ref type="bibr" target="#b50">[51]</ref> is used to update network parameters with learning rate set to 0.01. The λ hyper-parameter in our loss function is set to 8 × 10 −5 similar to <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Metrics</head><p>Similar to <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, we used the frame level-based receiver characteristic (ROC) and area under the curve (AUC) metrics to evaluate the proposed method. These metrics are calculated using frame-level ground truth annotation of the test videos. Please note that we do not use equal error rate (EER) since it does not measure anomaly correctly as reported in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. UCF experimental results</head><p>We compared our proposed method with the baseline method <ref type="bibr" target="#b5">[6]</ref> as well as the methods discussed in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>. The proposed method by Lu et al. <ref type="bibr" target="#b16">[17]</ref> used a dictionarybased approach to learn the normal pattern from the normal videos and used the reconstruction error as the anomaly score. Hassan et al. <ref type="bibr" target="#b18">[19]</ref> used deep autoencoder using normal videos to learn normal feature representations and used reconstruction error as an anomaly score.</p><p>The quantitative comparisons in terms of ROC and the AUC are reported in <ref type="figure" target="#fig_4">Figure 5</ref> and <ref type="table" target="#tab_1">Table I</ref>. From <ref type="figure" target="#fig_4">Figure 5</ref> it is observed that using normal and abnormal videos in the training phase increases the true positive rate as shown in both Sultani et al. <ref type="bibr" target="#b5">[6]</ref> (red plot) and our proposed method (green plot). In addition, it is evident that our proposed approach has a higher true positive rate compared to base-line method. <ref type="table" target="#tab_1">Table I</ref> shows that our network with the modified loss function achieves the third place compared to the state of the art results for video anomaly detection. We also show that training our network with Sultani et al. loss achieves higher results compared to their network because our network exploits the temporal relation between video instances via the spatiotemporal autoencoder.</p><p>Qualitative results on eight different videos that show success and failure cases are reported in <ref type="figure">Figure 6</ref>. The first row shows that our method produces a high anomaly score for abnormal videos (explosion and fighting) in a timely manner and generates near-zero anomaly score for normal videos which means that our method generates a low false alarm. We believe that the reason for generating an early anomaly score (first and second columns) is due to the temporal convolution over instances, which proves that our method produces an early detection. The second row shows the cases that our method fails in producing correct anomaly score for different abnormal videos and normal video. Also, we provide the evolution of frame-level anomaly scores prediction over several training iterations as shown in <ref type="figure">Figure 7</ref>. It is clear as the number of iteration increases, the proposed method starts predict the correct anomaly scores of both normal and anomalous video segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. False alarm rate</head><p>Similar to <ref type="bibr" target="#b5">[6]</ref>, the false alarm rate on normal testing videos is analysed. The reason for this study is that most of the surveillance videos are normal and generating a high false alarm rate is not practical. Therefore, a robust anomaly detection method should report a low false alarm rate on normal videos. We evaluated the performance of our proposed method  <ref type="bibr" target="#b16">[17]</ref> 50.60 Hassan et al. <ref type="bibr" target="#b18">[19]</ref> 65.51 Sultani et al. <ref type="bibr" target="#b5">[6]</ref> 75.41 Zhong et al. <ref type="bibr" target="#b19">[20]</ref> 81.08 Zaheer et al. <ref type="bibr" target="#b20">[21]</ref> 83.03</p><p>Our network+ Sultani et al. <ref type="bibr" target="#b5">[6]</ref> loss 76.41 Proposed (our network (2 layers)+ our loss) 79.49 on normal testing videos. The false alarm rate is reported at 50% threshold for different methods as shown in <ref type="table" target="#tab_1">Table II</ref>. It is clear that our proposed method has generated a very low false alarm rate in comparison to the based-line method and other methods. The reason why the proposed approach generates a low false alarm arises due to nature of our loss function. The false alarm occurs when the normal video segment generate high anomaly score. Formally, this happens when d in eq. (9) is negative, ((f B n ) &gt;f (B b )), see Figure (4). Therefore; our loss function penalise this by setting d = 0 during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. ShanghaiTech experimental results</head><p>We trained our proposed model on the ShanghaiTech using the train and test split provided by by Zhong et al. <ref type="bibr" target="#b19">[20]</ref>. Since this a recent split, there is no much work reported on this split. We compared our model accuracy with only <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b52">[53]</ref> and <ref type="bibr" target="#b20">[21]</ref>. We followed the same protocol in extracting the C3D features and model parameters. We outperform Zhong et al. <ref type="bibr" target="#b19">[20]</ref> by significant 11.23% margin and Zhaeer et al. <ref type="bibr" target="#b52">[53]</ref> by 3% margin. However, Zaheer et al. <ref type="bibr" target="#b20">[21]</ref> outperform our model by 2.36% margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATIVE EXPERIMENTS</head><p>In this section, we show how our network architecture is different from the ED-TCN network <ref type="bibr" target="#b36">[37]</ref>. First, we note that the ED-TCN network has been used for action segmentation and trained in a supervised manner <ref type="bibr" target="#b36">[37]</ref> while our network is trained via weak supervision. To accommodate the ED-TCN network to our problem, we changed the last layer, the network regularization and the number of convolution fillers. In addition, we used our loss function and similar parameter </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure cases</head><p>Anomaly score <ref type="figure">Fig. 6</ref>. Qualitative results of our method on testing videos. The first row shows an example of success cases and the second row shows an example of failure cases where our method can not produce correct anomaly score. The red frame represents at the ground truth and the blue ones is at the predicted time.</p><p>Training iteration 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame number</head><p>Training iteration 40</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame number</head><p>Training iteration 1.4k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame number</head><p>Training iteration 1k</p><p>Frame number Anomaly score <ref type="figure">Fig. 7</ref>. Frame-level anomaly scores evaluation over training iterations generated by our proposed method. Despite of weakly supervised method, our model learns to localize the normal and anomalous regions in the videos by predicting low anomaly score for normal segments and high anomaly score for anomalous segments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method AUC%</head><p>Zhong et al. <ref type="bibr" target="#b19">[20]</ref> 76.44 Zaheer et al. <ref type="bibr" target="#b52">[53]</ref> 84. <ref type="bibr" target="#b15">16</ref> Zaheer et al. <ref type="bibr" target="#b20">[21]</ref> 89.67</p><p>Proposed (our network+ our loss) 87.42 settings as mentioned earlier. For the sake of conducting a fair comparison, we set the convolution kernel-size to four for ED-TCN network. <ref type="table" target="#tab_1">Table IV</ref> shows that the accuracy (AUC) of ED-TCN network trained by using the proposed loss has lower AUC compared to our network. In addition, we show how our loss function is different compared to Sultani et al. <ref type="bibr" target="#b5">[6]</ref> by replacing the max operation by mean operation and report the results which demonstrate the effectiveness of our loss </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>function.</head><p>A. Effect of temporal convolution kernel size <ref type="figure" target="#fig_7">Figure 8</ref> shows the effect of different convolution kernel sizes on the performance of the proposed approach (in terms of AUC metric). The results demonstrate that temporal convolution with kernel size set to 4 has the highest AUC performance compared to other setups.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We propose a deep temporal encoding-decoding network for anomaly detection in video surveillance applications. Our proposed solution is based on the deep ranking multiple instance learning where we use normal and abnormal videos during training to localize the anomaly event in real surveillance videos. We deal with video instances (clips) as sequential visual data and build a temporal encoding network that exploits the low, intermediate, and high-level spatio-temporal evolution between the feature instances. Due to the lack of temporal annotation of visual video instances, we use the average sum of the instance predication to pool from the instance-level to bag-level predication. Therefore, our loss function is smoother than using max-pooling in previous work. In addition, the loss function ensure a low false alarm during the training. The results of experiments using normal and abnormal videos in the UCF-crime dataset and ShanghaiTec demonstrate that effectiveness of our proposed solution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Different abnormal examples for different surveillance videos taken from UCF-crime dataset [6]. (A): Vandalism; (B): Stealing; (C): Shoplifting; (D): Shooting; (E): Robbery; (F): Road Accident; (G): Robbery; (H): Explosion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The proposed network, temporal encoding-decoding network. F l is the number of convolution filters at layer l used in our network. The script T represents the number of instances in temporal domain. The temporal convolution duration length C d is set to 4 in our network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>ROC comparative results for different methods on UCF-crime dataset, binary classifier (blue), Lu et al. [17] (cyan), Hassan et al. [19] (black), Sultani et al. [6] (red), Zaheer et al.<ref type="bibr" target="#b20">[21]</ref> (green) and our proposed method (magenta).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fighting003</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>The AUC of the proposed network with different convolution kernel sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I AUC</head><label>I</label><figDesc>COMPARISON RESULTS OF DIFFERENT METHODS ON UCF-CRIME DATASET USING C3D FEATURES.</figDesc><table><row><cell>Method</cell><cell>AUC%</cell></row><row><cell>Binary classifier</cell><cell>50.00</cell></row><row><cell>Lu et al.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="2">FALSE ALARM RATE COMPARISON ON NORMAL TEST VIDEOS USING C3D</cell></row><row><cell cols="2">FEATURES.</cell></row><row><cell>Method</cell><cell>False alarm rate</cell></row><row><cell>Li et al. [52]</cell><cell>27.2</cell></row><row><cell>Hasan et al. [19]</cell><cell>3.1</cell></row><row><cell>Sultani et al. [6]</cell><cell>1.9</cell></row><row><cell>Zhong et al. [20]</cell><cell>2.8</cell></row><row><cell>Zaheer et al. [21]</cell><cell>-</cell></row><row><cell>Proposed Method</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III AUC</head><label>III</label><figDesc>COMPARISON RESULTS OF DIFFERENT METHODS ON SHANGHAITECH USING C3D FEATURES.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV AUC</head><label>IV</label><figDesc>COMPARISON RESULTS FOR DIFFERENT NETWORK SETTINGS AND DIFFERENT TEMPORAL NETWORKS ON UCF CIRME DATASET. Method AUC% Our loss+ Lea et al. [37] network settings 76.56 Sultani et al. [6] loss+ Lea et al. [37] network settings 78.89 Our loss+ Bi-LSTM [54] network 50.12 Our network+Sultani et al. loss (with average mapping) 74.53</figDesc><table><row><cell>Proposed method(our network+ our loss)</cell><cell>79.49</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object-centric auto-encoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>2017 IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning discriminative reconstructions for unsupervised outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1511" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3619" to="3627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep-cascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep appearance features for abnormal behavior detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trajectory-based anomalous behaviour detection for intelligent traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET intelligent transport systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="810" to="816" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Trajectory-based anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for video Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1544" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1446" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1237" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Astrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="358" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning semantic scene models by trajectory analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision (ECCV)</title>
		<meeting>European conference on computer vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="110" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A system for learning statistical motion patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1450" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="555" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abnormal activity detection using spatio-temporal feature and laplacian sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing</title>
		<meeting>International Conference on Neural Information Processing</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="410" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1273" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International conference on machine learning</title>
		<meeting>International conference on machine learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video summarization using fully convolutional sequence networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="347" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiple instance learning: A survey of problem characteristics and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gagnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="329" to="353" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Address instance-level label prediction in multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12226</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1081" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="189" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Matrix completion for weakly-supervised multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="121" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
	<note>Neural processing letters</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for image classification with large vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scene discovery by matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Loeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Uncovering shared structures in multiclass classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiple-instance ranking: Learning to rank images for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">keras</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A self-reasoning framework for anomaly detection using video-level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1705" to="1709" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
