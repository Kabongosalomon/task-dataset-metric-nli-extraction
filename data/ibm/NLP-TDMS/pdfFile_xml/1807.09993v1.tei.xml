<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Babu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Neeraj</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajjan</forename><forename type="middle">R</forename><surname>Venkatesh Babu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<postCode>560012</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Mukundhan Srinivasan NVIDIA</orgName>
								<address>
									<postCode>560045</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Divide and Grow: Capturing Huge Diversity in Crowd Images with Incrementally Growing CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated counting of people in crowd images is a challenging task. The major difficulty stems from the large diversity in the way people appear in crowds. In fact, features available for crowd discrimination largely depend on the crowd density to the extent that people are only seen as blobs in a highly dense scene. We tackle this problem with a growing CNN which can progressively increase its capacity to account for the wide variability seen in crowd scenes. Our model starts from a base CNN density regressor, which is trained in equivalence on all types of crowd images. In order to adapt with the huge diversity, we create two child regressors which are exact copies of the base CNN. A differential training procedure divides the dataset into two clusters and fine-tunes the child networks on their respective specialties. Consequently, without any hand-crafted criteria for forming specialties, the child regressors become experts on certain types of crowds. The child networks are again split recursively, creating two experts at every division. This hierarchical training leads to a CNN tree, where the child regressors are more fine experts than any of their parents. The leaf nodes are taken as the final experts and a classifier network is then trained to predict the correct specialty for a given test image patch. The proposed model achieves higher count accuracy on major crowd datasets. Further, we analyse the characteristics of specialties mined automatically by our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Crowds are common in public places; be it the daily city traffic or some special gatherings, analysing crowds is becoming increasingly important both in terms of security as well as planning. Counting people in crowds, especially dense gatherings, is a difficult task even for humans. This is primarily because of the large diversity in the way people appear in crowded scenes. In highly dense crowds, people are only seen as blobs, while in less dense gatherings facial features may be visible. Hence, the visibility of features for crowd discrimination is related to the density of the crowd. Severe occlusion, pose changes and view-point variations further compound the problem. Head or body detection based methods fails to adapt to such huge diversity. As a result, most modern approaches solve counting by regression. With advent of deep learning, often Convolutional Neural Networks (CNN) are trained to predict crowd density map. Density maps represent count per unit area and provide spatial information. These models learn to map crowd features (head or shoulder patterns as they appear in crowds) to its density as opposed to detecting individuals with facial or body features. Typical CNN density regressors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">26]</ref> are optimized over an entire dataset containing images of all densities. And in many datasets, the density of crowd is not uniform. For example, in PartA of Shanghaitech dataset <ref type="bibr" target="#b26">[26]</ref>, the distribution is skewed with less number of dense crowd images than sparse ones. Consequently, performance of models varies widely across different categories of crowds. This usually results in over-estimating count for sparse images and under predicting for dense images as shown in <ref type="bibr" target="#b16">[16]</ref>.</p><p>One obvious solution to address this wide variability is having multiple regressors, each specialized for a particular type of crowd. This is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> with predictions made by regressors fine-tuned for sparse and dense crowds. The experts perform well in their own specialties but worse in others. The major difficulty in such approaches is determining a criteria for creating experts. For <ref type="figure" target="#fig_0">Figure 1</ref>, we choose division based on density, but it can be based on other characteristics too. Even if the criteria is chosen, what would be the basis of division (how many people make a crowd dense or sparse)? These metrics are dataset as well as model dependent and need to be manually specified. In crowd counting, till now no principled method has been proposed to do this. Moreover, improper divisions can lead to suboptimal solutions. Learning experts automatically with classical mixture of expert <ref type="bibr" target="#b4">[5]</ref> models do not work well in this scenario as shown by some works like <ref type="bibr" target="#b5">[6]</ref>.</p><p>The aim of this work is to introduce a principled methodology for creating experts, without any handcrafted dataset dependent criteria for specialization. Hence, we propose an Incrementally Growing CNN (IG-CNN) for crowd counting. IG-CNN starts from a base CNN density regressor which is trained on the entire dataset. Then we replicate the base CNN into two child networks by copying the weights of the parent. To make these child regressors specialized, we do differential training <ref type="bibr" target="#b13">[13]</ref>, where clustering of the dataset is done jointly with fine-tuning of the child networks (Section 3.4). In the next growing step, we replicate each of the child regressors again into two networks and perform differential training. This procedure is done recursively forming a hierarchical CNN tree where each node has two child nodes which are more specialized than their parent. At the end of the growing, a set of experts are created at the leaf nodes of the CNN tree. At test time, a classifier routes the input image patches to the appropriate expert regressors.</p><p>In a nutshell, this paper introduces the following:</p><p>• A hierarchical clustering method that jointly creates image clusters and a set of expert neural networks specialized on their respective clusters.</p><p>• A crowd counting system that can adapt and grow based on the complexity of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>Crowd Counting: There are numerous crowd counting methods in the literature which detect individual humans by identifying heads or other body structures <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b20">20]</ref>. Such models primarily rely on hand-crafted features to train detectors. Stewart et al. <ref type="bibr" target="#b17">[17]</ref> use a recurrent framework to sequentially detect people. All these detection methods fail in highly dense crowds as the discriminative features on which they can be trained are absent. Hence, regression based methods have gained traction. For example in <ref type="bibr" target="#b3">[4]</ref>, head detections along with features from interest points and Fourier analysis are used to regress crowd count.</p><p>With deep learning, CNN based regressors become popular and give better performance than classical models. The counting CNN introduced in <ref type="bibr" target="#b25">[25]</ref>, is trained by alternatively optimizing both crowd density loss as well as crowd count loss. The deep CNN employed by <ref type="bibr" target="#b19">[19]</ref> directly regresses the crowd count instead of a density map. But such approaches prevent the CNN from acquiring good feature detectors and lead to lower performance than training for density map prediction. Walach et al. <ref type="bibr" target="#b18">[18]</ref> train multiple CNNs where each network predicts corrections on the density map generated by the previous. In contrast, top-down feedback is leveraged by <ref type="bibr" target="#b12">[12]</ref> to rectify initial detections of the bottom-up CNN regressor. Further works like <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> supplement the main density regressors with high-level, low-level or both prior information. These priors are in the form of confidences predicted by a network trained to classify images based on density (sparse, dense etc.). This helps regressors to select specialized pathways based on the prior for generating density maps. But training of the classifiers require manual division based on density and is dataset dependent.</p><p>Approach by <ref type="bibr" target="#b9">[10]</ref> makes use of multi-scale input and CNNs are trained for a particular scale. The last layer features of the networks are fused through learned fully connected layers. Though feeding multi-scale images can account for some scale variability, multi-columns CNNs are shown to be better. Boominathan et al. <ref type="bibr" target="#b1">[2]</ref> propose a VGG based deep CNN along with a shallow CNN. The shallow CNN is designed to capture dense crowds while deep network is for sparse crowds. Multi-column network from <ref type="bibr" target="#b26">[26]</ref> has three CNN columns, each having different receptive fields and hence can capture crowds at multiple scales. Features of the columns are fused through a learned 1 × 1 filter to generate the final density map. Since in these multicolumn approaches the entire model is trained together, specialization gained among the columns need not be drastic. Sam et al. <ref type="bibr" target="#b13">[13]</ref> address this issue by performing a differential training procedure to accentuate the specialization between CNN columns of varied architecture. But the model is limited by the availability of regressors with different architectures. In contrast, our method requires only one base regressor. More specific standard mixture of experts <ref type="bibr" target="#b4">[5]</ref> based model is used in <ref type="bibr" target="#b5">[6]</ref> for direct count regression, but performs worse than the hard switching mechanism of <ref type="bibr" target="#b13">[13]</ref>.</p><p>Growing Networks: The concept of a neural network that incrementally enlarges its capacity while learning has been there for a while. Several such Growing Neural Network models have been proposed in the literature <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b2">3]</ref> for supervised as well as unsupervised learning. Few works like <ref type="bibr" target="#b8">[9]</ref> grow a CNN progressively by adding new neurons in a data-dependent fashion. In the domain of transfer learning, <ref type="bibr" target="#b21">[21]</ref> analyse different approaches for developmental networks which can increase its model capacity as and when new tasks are given. They explore adding new layers along the depth or width of the neural network.</p><p>Specialization based Methods: Expert specialization approaches like <ref type="bibr" target="#b24">[24]</ref>, increase classification performance by imposing coarse and fine class hierarchy onto a deep CNN. But this method requires coarse labels which is not required by method introduced in <ref type="bibr" target="#b0">[1]</ref>. In a generalist-specialist setting, Ahmed et al. <ref type="bibr" target="#b0">[1]</ref> jointly train specialty branches along with a generalist CNN which can classify the specialties. Specialty groups are formed such that they can be easily discriminated by the classifier. The algorithm proposed in <ref type="bibr" target="#b22">[22]</ref> can learn a CNN tree where the nodes down the tree capture progressively fine-grained features. Our model also hierarchically grows a CNN tree, but it is employed only as a method to create a set of experts without any manually specified specialization criteria. In contrast to works like <ref type="bibr" target="#b22">[22]</ref>, the hierarchy is discarded in IG-CNN after training and only the networks at the leaf nodes of the CNN tree are kept. These leaf networks are finer experts and are selectively used at test time to evaluate inputs corresponding to their specialties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Creating Experts with Hierarchical Differential Training</head><p>As motivated in Section 1, counting models have to handle severe diversity in the way people appear in crowds. We try to mitigate this issue with a set of expert regressors, each of which are specialized on one particular subset of the training data. Many previous works leveraging such specialization methods require the specialty information to be given either in the form of priors <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> or indirectly as regressors with different architectures <ref type="bibr" target="#b13">[13]</ref>. In this scenario, naive mixture of experts based methods are shown to perform subpar <ref type="bibr" target="#b5">[6]</ref>. Hence, we design a model which does not require any specialty criteria to be manually specified for training experts and yet achieves better count estimates.</p><p>Our incrementally growing CNN or IG-CNN architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. IG-CNN training begins with a base CNN regressor, which is trained on the full dataset to estimate crowd density. To create specialties, a hierarchical training procedure is employed. Let R 0 represent the base CNN and D 0 = {X N i=0 } denotes the dataset of N images on which the base regressor R 0 is trained. Initially, the base CNN is replicated into two networks R 00 and R 01 . Now with differential training, R 00 and R 01 need to be made experts in separate specialties. The differential training procedure divides the dataset into two and fine-tunes the two regressors on their respective clusters. For a given image patch, only the regressor with the best count accuracy is trained. This way the oracle loss <ref type="bibr" target="#b6">[7]</ref> of the set of regressors is minimized. Oracle loss is the minimum loss achievable if the correct regressor is chosen for all samples (see Section 3.4). We use a modified version of the differential training introduced in <ref type="bibr" target="#b13">[13]</ref>. Our algorithm does not require regressors with different architectures as in <ref type="bibr" target="#b13">[13]</ref> and also uses count loss function for fine-tuning. After the first level of training, we have two expert regressors R 00 and R 01 along with their corresponding specialty subsets D 00 and D 01 . Each of the networks R 00 and R 01 , is replicated again to form respective child networks in the second iteration of growing. Regressors R 000 and R 001 will have same weights as of R 00 while R 01 is copied to R 010 and R 011 . Differential training is performed on R 000 and R 001 with dataset D 00 only. Similarly, D 01 is used for fine-tuning R 010 and R 011 . This makes sure that specialization acquired by the parent is propagated to its child networks. The two way replicating and specialization process is recursively continued forming a CNN tree. A child node in the tree is more specialized than any of their parent network. More deeper the tree, more finer the specialties with leaf nodes being the finest experts. Section 3.4 elucidates training algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Growing CNN Architecture</head><p>The hierarchical differential training procedure results in the creation of a set of regressors at the leaf nodes of the CNN tree. All the regressors have the same architecture but give better performance on their specialties. Additionally, a classifier is trained for selecting the right expert for a given scene patch. <ref type="figure" target="#fig_2">Figure 3</ref> shows the test time architecture of IG-CNN.</p><p>A neural network with five convolutional layers is used as the base CNN. Because of two pooling layers, the density prediction is at 1 4 th scale of the input image. All convolutional layers use ReLU activation function. This simple regressor is introduced in <ref type="bibr">[</ref>  mance. But it is to be noted that our training methodology is generic and is not limited by any particular base CNN.</p><p>For expert classifier, we use a modified VGG-16 <ref type="bibr" target="#b14">[14]</ref> network. Features of the last convolution layer of VGG-16 are reduced via global average pooling followed by two fully connected layers and softmax prediction at the end. The number of units in the last fully connected layer depends on how many expert regressors are generated with the hierarchical training. During testing, we take overlapping patches from the image. Each patch extracted is of the size P W × P H . A region of interest (RoI) of size R W ×R H is defined for the patch on which the CNN regressor predicts the crowd density. Area outside the RoI acts as context and aids in better regression. The RoI is slided over the entire image with an overlap. The predictions of the overlapping areas are averaged to get the final density. Characteristics of crowd in the RoI is assumed to be constant. For IG-CNN hierarchical differential training, patches are extracted at random locations from images and the loss is computed only on the RoI. The expert classifier also uses the RoI part of a patch to select the suitable regressor. Typical patch size is 224 × 224 with an RoI size of 80 × 80.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pretraining of Base CNN</head><p>The base CNN is trained on the entire dataset to regress crowd density map. The network is trained by backpropagating l 2 loss between the predicted and the ground truth density maps. There are numerous methods in the literature to generate ground truth density maps from head annotations available with the datasets <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>. The most widely used method is to place a Gaussian at every head location. This helps CNN to train better as otherwise it would have to predict exactly at the annotation point. The spread of the Gaussian is an hyper-parameter and depends on the dataset. Since Gaussian blurring of each annotation sums to one, the crowd count can be obtained by summing the density map.</p><p>Let M Xi (x; Θ) denote the density map predicted by the CNN regressor and M GT Xi (x) be the ground truth for image X i . Then, the l 2 loss function is defined as</p><formula xml:id="formula_0">L l2 (Θ) = 1 2N N i=1 M Xi (x; Θ) − M GT Xi (x) 2 2 ,<label>(1)</label></formula><p>where Θ refers to the trainable parameters of the CNN and N is the total number of training samples. The parameters Θ are found by optimizing L l2 using standard stochastic gradient descent (SGD) with momentum. Even though our objective is to minimize the count error, l 2 loss acts as proxy for the count loss. Reduction in l 2 distance implicitly lowers the count error between the prediction and ground truth. For pretraining, we crop patches at different locations from every image and apply flip augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Algorithm for IG-CNN</head><p>The overall training procedure of IG-CNN is depicted in Algorithm 1. The first step is the pretraining of the base CNN R 0 . For any regressor, the final metric of performance is the count error which needs to be minimized. Count predicted by a regressor R for an image X i is computed as</p><formula xml:id="formula_1">C R Xi = x M Xi (x; Θ R ), where its ground truth count is C GT Xi = x M GT Xi (x).</formula><p>The count error for regressor R on image X i is the absolute difference of predicted and actual count or mathematically, E Xi (R) = |C R Xi − C GT Xi |. After pretraining of the base CNN, a CNN tree is progressively built where each node represents a regressor finetuned on a subset of the dataset. This is done by replicating each regressor at the tree leaves into two and specializing the child networks with differential training. At any node m, let R m0 and R m1 be the child regressors and D m be the subset of dataset available for the node. Now R m0 and R m1 need to be made experts in the specialty sets D m0 and D m1 respectively. But we have neither the specialty sets nor the expert regressors. Differential training allows to jointly obtain the specialties and the expert regressors by minimizing the oracle count error. The oracle count error for patch X i is</p><formula xml:id="formula_2">E oracle Xi = min R∈[Rm0,Rm1]</formula><p>|C R Xi − C GT Xi |, the minimum of the count errors obtained by the two regressors. The basic idea is to evaluate both the regressors on a particular image patch and fine-tune only the one giving lesser count error. Choose the best regressor by r best Xi = argmin</p><formula xml:id="formula_3">R∈[Rm0,Rm1] |C R Xi − C GT Xi |.</formula><p>Note that when count predictions by both networks are same, which mostly happens at the start of the training, the first regressor is chosen to break the tie. This makes sure that the differentiation between the networks builds up progressively. By selectively fine-tuning R m0 and R m1 based on its performance on the training patches, the regressors become more and more specialized in two groups D m0 and D m1 . These specialty subsets might be skewed and completely depends on the dataset as well as the regressors. The fine-tuning is done with lower learning rate (10 −6 ) and continue till validation accuracy stops improving. Unlike differential training in <ref type="bibr" target="#b13">[13]</ref>, count loss is used instead of l 2 loss for fine-tuning regressors. We define the count loss as,</p><formula xml:id="formula_4">L C (Θ) = λ 2N N i=1 (C Xi − C GT Xi ) 2 .<label>(2)</label></formula><p>Here constant λ is used to check the magnitude of the loss.</p><p>For all experiments, λ is set as 10 −2 . Since the CNN is pretrained with l 2 loss, it has good initial features and finetuning with count loss provides complementary information. This is found to give better clustering and more accurate count estimation. Differential training minimizes oracle error over the training set. This count error is achievable only if there is an oracle to classify a test patch to the correct regressor. But the ability of a classifier to achieve high results in determining the specialty depends on the quality of the specialization. If the expert specialties do not have any generalizable features, the performance might decay on the test set.</p><p>The leaf regressors (R leaf ) at a particular level of growth are experts on specific specialties. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, at test time, a classifier selects the right expert regressor for the image patch. The classifier is trained on the labels obtained from the expert regressors. For a given image patch X i , the corresponding label is attributed to the regressor with minimum count error, R best Xi = argmin</p><formula xml:id="formula_5">R∈R leaf |C R Xi − C GT Xi |.</formula><p>As the samples per expert specialty need not be uniform, class balancing is done before training the classifier.</p><p>At every increment of the growing process, regressors at the leaf nodes of the CNN tree are split and new expert regressors are created. We monitor the Oracle MAE and Actual MAE for the leaf regressors over a validation set. Mean Absolute Error or MAE is the performance metric used for counting models (see Section 4.1). While Oracle MAE indicates the count error incurred when right expert is always chosen for regression, Actual MAE is obtained with the expert classifier. Note that the validation set is randomly sampled from the training images and is fixed across entire training procedure (irrespective of tree level). The hierarchical tree splitting is stopped when the Actual MAE on validation set is not improving (see <ref type="table">Table 4</ref>).</p><p>input : </p><formula xml:id="formula_6">Dataset D 0 = {X i , M GT Xi } N i=1 (image &amp; map) output: Parameters {Θ r } of experts and classifier Θ c Random initialize Θ 0 for base CNN R 0 ; Pretrain R 0 ; R leaf = {R 0 }; D leaf [R 0 ] = D 0 ; /* Hierarchical Differential Training */ for l = 0 to max tree depth do /* Replicate R twice */ for R in R leaf do R child [R] = {R, R}; end /* Differential Training */ /* R predicts count C R i while C GT i is the actual */ for i = 1 to max iterations do for R l in R leaf do for X, M in D leaf [R l ] do r = argmin R∈R child [R l ] |C R X − C GT X |; Fine-tune R r with X to update Θ r ; end end Break if validation Oracle MAE stagnates; end /* Dataset division for leaf regressors */ D leaf = []; for X, M in D 0 do for R l in R leaf do r = argmin R∈R child [R l ] |C R X − C GT X |; Add (X, M ) to D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Scheme</head><p>We benchmark our IG-CNN model on three crowd counting datasets. For a given test image, patches are extracted and evaluated by the expert classifier to route them to the regressors specialized for the specific crowd types.  Two metrics are commonly used to evaluate crowd counting models. MAE or Mean Absolute Error is the most important metric and indicates the count accuracy of the model. If the count predicted by the model is C Xi for an actual count of C GT Xi , then MAE is defined as</p><formula xml:id="formula_7">MAE = 1 N N i=1 |C Xi − C GT Xi |,<label>(3)</label></formula><p>where the test set has N images. The second metric is the Mean squared error or MSE to measure the variance of count estimation. MSE is given by,</p><formula xml:id="formula_8">MSE = 1 N N i=1 (C Xi − C GT Xi ) 2 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Shanghaitech dataset</head><p>The Shanghaitech dataset <ref type="bibr" target="#b26">[26]</ref> is partitioned into two sets, namely Part A and Part B. The sets are quite distinctive with Part A images sourced from the Internet and have crowd counts ranging from 33 to 3139. In contrast, the crowds in Part B are sparser with counts varying from 9 to 578 and are captured from streets of Shanghai. Part A has 482 images, of which 300 images are used for training and the rest are used for testing. Similarly, the 716 images of Part B are partitioned into chunks of 400 training and 316 testing images. Gaussian kernels with fixed sigma are used to generate the ground truth density maps.</p><p>For both Part A and Part B, we grow IG-CNN to 3 levels resulting in 8 expert regressors. <ref type="table">Table 1</ref> tabulates the performance metrics for IG-CNN on the dataset along with that of other models. It can be observed that IG-CNN outperforms all other methods in Part B by a significant margin both in terms of MAE and MSE. IG-CNN achieves better count accuracy in Part A as well. Though our model narrowly outperforms CP-CNN <ref type="bibr" target="#b16">[16]</ref>, it is to be noted that the authors of CP-CNN use adversarial training to boost their base performance from 76.1 to 73.6. <ref type="figure" target="#fig_5">Figure 4</ref> shows density maps predicted by IG-CNN and the base CNN along with the corresponding ground truths. The predicted density maps closely resemble the ground truth as well as have accurate count estimates. This demonstrates the ability of IG-CNN to better capture the crowd density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">UCF CC 50 dataset</head><p>UCF CC 50 dataset introduced by <ref type="bibr" target="#b3">[4]</ref>, serves to be a hard challenge because of the relatively small size of the dataset and the large variability of crowd density in the images. The crowd count in 50 images of the dataset vary considerably from 94 to 4543. The ground truth density maps are created with fixed variance Gaussian kernel. We follow 5-fold cross-validation protocol adopted by <ref type="bibr" target="#b3">[4]</ref> to evaluate the model on the dataset. IG-CNN hierarchical growing is done for two levels, creating 4 expert regressor on UCF CC 50 dataset. It can be seen from <ref type="table">Table 3</ref> that IG-CNN has the lowest count error. Despite being a challenging dataset, our model delivers an improvement of 4.4 points in MAE and has comparable performance in MSE metric also.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MAE MSE Lempitsky et al. <ref type="bibr" target="#b7">[8]</ref> 493.4 487. <ref type="bibr" target="#b0">1</ref> Idrees et al. <ref type="bibr" target="#b3">[4]</ref> 419.5 541.6 Zhang et al. <ref type="bibr" target="#b25">[25]</ref> 467.0 498.5 CrowdNet <ref type="bibr" target="#b1">[2]</ref> 452.5 -MCNN <ref type="bibr" target="#b26">[26]</ref> 377.6 509.1 Hydra2s <ref type="bibr" target="#b9">[10]</ref> 333.7 425.3 SCNN <ref type="bibr" target="#b13">[13]</ref> 318.1 439.2 Cascaded-MTL <ref type="bibr" target="#b15">[15]</ref> 322.8 397. <ref type="bibr" target="#b8">9</ref> CP-CNN <ref type="bibr" target="#b16">[16]</ref> 295.8 320.9 IG-CNN 291.4 349.4 <ref type="table">Table 3</ref>. Comparison of IG-CNN with other methods on UCF CC 50 dataset <ref type="bibr" target="#b3">[4]</ref>. Our model gives lower error than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">WorldExpo'10 dataset</head><p>The World Expo'10 dataset introduced by <ref type="bibr" target="#b25">[25]</ref> is a large dataset containing 1132 video sequences captured by 108 surveillance cameras covering a set of scenes. On an average, each image has 50 people in it. The dataset is divided into two parts for training and testing. Training data consists of 3380 frames from 103 different scenes, whereas the test data comprising of 5 different scenes has a total of 600 frames. Along with the images, the authors have also provided the Region of Interest (RoI) and the perspective maps. The RoIs are used during training to backpropogate only in those regions. Also, only the prediction in the RoI is used to report crowd count while testing. <ref type="table" target="#tab_1">Table 2</ref> lists the performance of all major methods. IG-CNN is grown only just one level with two experts. World Expo'10 dataset proves to be extremely challenging for our model due to the sparse nature of the crowd with the lack of significant variability in crowd density. This affects the ability of our model to generate experts catering to different crowd types. Despite these limitations, our model shows comparable performance with respect to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis and Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of Growing</head><p>In this section, we study the effect of the hierarchical CNN tree growing on the oracle accuracy and the final accuracy at test time. All ablations are performed on Part A of the Shanghaitech dataset <ref type="bibr" target="#b26">[26]</ref> as it is sufficiently large and has high variation in crowd density. <ref type="table">Table 4</ref> lists count errors for the base CNN along with that of the IG-CNN at different levels of growth. It also shows for each level, the classifier accuracy and Oracle MAE. This oracle error is the MAE that the model would achieve if the expert classifier is 100% accurate. There is significant improvement of MAE for IG-CNN at higher levels than the base CNN but saturates after level 3. Although the oracle error decreases down drastically with each increment of the growth, the expert classifier is unable to keep up and causes more switching error as evident from the <ref type="table">Table 4</ref>. This is primarily due to the reduction in number of training samples per expert regressor at higher tree levels. For example at level 2, the distribution of samples for the four regressors is so skewed that one of the expert gets only 2.9% of the total test patches. This is more severe for level 3 with only 0.5% for the expert with the least number of samples and the corresponding class wise classifier accuracy is just around 2%. The number of samples for some of the regressors are so small that the classifier is unable to generalize significant discriminative features for the specialties. We also show in <ref type="table">Table 4</ref>, the performance when the regressor with the least number of samples is not split, leading to an unbalanced tree. In this way, there are only 7 expert regressors at level 3 instead of 8 experts. The MAE in this case is comparable to IG-CNN at level 3, but higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Expert Specialty Characteristics</head><p>It is important to shed more light on the specialization process involved in the IG-CNN training. Hence, we analyse the features of specialty groups automatically inferred in the hierarchical differential training. The specialty groupings might be based on some latent features such that the  <ref type="bibr" target="#b26">[26]</ref> is used. <ref type="figure" target="#fig_6">Figure 5</ref> indicates a possible clustering of crowd patches based on count. Note that patches with few people go to one regressor while more denser ones get distributed across the other experts. This multichotomy observed in the specialties reinforces the fact that IG-CNN training creates experts based on certain latent factors. Some of the factors could be correlated with crowd density as density variation accounts for much of the variability seen in crowd images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Hierarchical Training Vs Baseline Methods</head><p>In short, IG-CNN training mines latent specialties hierarchically and creates a set of expert regressors. Here we compare this methodology with other similar methods. The standard mixture of experts (MoE) approach uses a gating network to weigh the output of the set of regressors. In the same setting as that of IG-CNN, we use VGG-16 classifier as gating CNN to output softmax confidences. The 8 regressors are initialized with base CNN weights and their outputs are multiplied by the classifier confidences. <ref type="table" target="#tab_3">Table 5</ref> shows MAE numbers for MoE and is clearly inferior to IG-CNN. MoE is unable to bring significant specialization among the regressors.</p><p>We also compare with differential training introduced in <ref type="bibr" target="#b13">[13]</ref>. Instead of performing hierarchical training, N-way differential training is done on the set of regressors as in <ref type="bibr" target="#b13">[13]</ref>. For this ablation, we use four and eight regressors which are exact copies of the base CNN, is comparable to IG-CNN with the same number of experts. The oracle loss of the expert set is minimized by selectively fine-tuning the best regressor for the given training sample. It can be observed from <ref type="table" target="#tab_3">Table 5</ref>, that the oracle MAE is lower for IG-CNN than that of N-way differentially trained model. In fact, the final performance with the expert classifier is also inferior in the case of N-way differential training. This emphasizes that the hierarchical training creates specialties with better discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Oracle </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We address the problem of better capturing large diversity seen in crowd scenes for accurate regression of crowd density. The proposed model, IG-CNN iteratively expands its model capacity based on the complexity of the training data. IG-CNN starts growing from a base CNN, which is trained to regress crowd density. The base CNN is replicated into two child regressors, each of which are imposed specialization with differential training and recursively divided again forming a CNN tree. The regressors at the leaf nodes of the tree are finer experts on certain specialties mined without any manually specified criteria. An expert classifier predicts the right expert for a given test patch. We evaluate on standard benchmarks and show better performance for the model. Additionally, analysis of the specialties created by IG-CNN reveals correlation with observable crowd characteristics such as crowd density.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Predictions of a typical regressor fine-tuned for sparse or dense crowds. Models perform better on their own specialties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Hierarchical Differential Training in IG-CNN. Regressors are recursively replicated and specialized forming a CNN tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Test time architecture of IG-CNN. The expert classifier routes crowd patches to the appropriate specialized regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>leaf [r]; end end /* Training of Expert Classifier */ Initialize Θ c with VGG-16 weights; for X, M in D 0 do r = argmin R∈R leaf |C R X − C GT X |; Add (X, r) to D c ; end Train classifier with D c and update Θ c ; Break if validation Actual MAE stagnates; end Algorithm 1: IG-CNN training algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Predictions made by IG-CNN on images of Shanghaitech dataset<ref type="bibr" target="#b26">[26]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Mean and standard deviation of crowd count distribution preferred by expert regressors at different hierarchies of IG-CNN. Computed on patches from Shanghaitech<ref type="bibr" target="#b26">[26]</ref> Part A test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b26">26]</ref> and delivers reasonable perfor-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2-CONV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3 | 64</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAX-POOL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2x2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2-CONV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3 | 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAX-POOL</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">CLASSIFIER EXPERT</cell><cell>3x3 | 256 3-CONV 2x2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAX-POOL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>"R1"</cell><cell>3-CONV 2x2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3x3 | 512</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAX-POOL</cell></row><row><cell></cell><cell>LAYER ROUTE</cell><cell>...</cell><cell></cell><cell></cell><cell>3x3 | 512 3-CONV 2x2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GAP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>512</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SOFTMAX</cell></row><row><cell>R1</cell><cell>R2</cell><cell>R3</cell><cell>...</cell><cell>Rn</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1-CONV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9x9 | 16</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAX-POOL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2x2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1-CONV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7x7 | 32</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAX-POOL</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2x2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DENSITY</cell><cell></cell><cell>1-CONV</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MAP</cell><cell></cell><cell>7x7 | 16 1-CONV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7x7 | 8</cell></row><row><cell></cell><cell></cell><cell>Σ</cell><cell></cell><cell></cell><cell>1-CONV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1x1 | 1</cell></row><row><cell></cell><cell></cell><cell cols="2">ESTIMATED COUNT</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>MAEs obtained by models for the 5 test scenes of WorldExpo'10 dataset<ref type="bibr" target="#b25">[25]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">Part A MAE MSE MAE MSE Part B</cell></row><row><cell>Zhang [25]</cell><cell>181.8 277.7 32.0</cell><cell>49.8</cell></row><row><cell>MCNN [26]</cell><cell>110.2 173.2 26.4</cell><cell>41.3</cell></row><row><cell>SCNN [13]</cell><cell>90.4 135.0 21.6</cell><cell>33.4</cell></row><row><cell cols="2">TDF-CNN [12] 97.5 145.1 20.7 CP-CNN [16] 73.6 106.4 20.1 IG-CNN 72.5 118.2 13.6</cell><cell>32.8 30.1 21.1</cell></row></table><note>Table 1. Performance of IG-CNN on Part A and Part B of Shang- haitech dataset [26]. IG-CNN outperforms other methods in MAE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Comparison of IG-CNN with other specialization based methods on Part A of Shanghaitech [26] dataset. IG-CNN outperforms other architectures.</figDesc><table><row><cell></cell><cell>MAE</cell><cell>Actual MAE</cell></row><row><cell>Mixture of Experts</cell><cell>-</cell><cell>281.8</cell></row><row><cell>4-way Differential Training</cell><cell>20.6</cell><cell>99.0</cell></row><row><cell>8-way Differential Training IG-CNN (Level 3)</cell><cell>9.9 8.5</cell><cell>75.1 72.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work is supported by SERB, Department of Science and Technology (DST), Government of India (Proj No. SB/S3/EECE/0127/2015). We also acknowledge NVIDIA AI Tech Centre (NVAITC) for their assistance.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network of experts for large-scale image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowdnet: A deep convolutional network for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="640" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Growing neural networks using soft competitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Ahlawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="page" from="975" to="8887" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hinton. Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixture of counting CNNs: Adaptive integration of CNNs specialized to specific appearance for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09393</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic multiple choice learning for training diverse deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P S</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2119" to="2127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image classification with growing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mrazova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kukacka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Theory and Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">422</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Onoro-Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An overview of some classical growing neural networks and new developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Education Technology and Computer (ICETC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CNN-based cascaded multitask learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>14th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04878</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to count with CNN boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3401" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Growing a brain: Fine-tuning by increasing model capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2471" to="2480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning fine-grained features via a CNN tree for large-scale classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HD-CNN: hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Singleimage crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
