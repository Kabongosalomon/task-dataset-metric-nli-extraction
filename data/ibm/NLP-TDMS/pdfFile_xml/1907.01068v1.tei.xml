<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmenting and Tuning Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bamler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farnood</forename><surname>Salehi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92617</postCode>
									<settlement>Irvine Irvine</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Ecole Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>9261</postCode>
									<settlement>Irvine Irvine</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Augmenting and Tuning Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph embeddings rank among the most successful methods for link prediction in knowledge graphs, i.e., the task of completing an incomplete collection of relational facts. A downside of these models is their strong sensitivity to model hyperparameters, in particular regularizers, which have to be extensively tuned to reach good performance <ref type="bibr" target="#b19">[Kadlec et al., 2017]</ref>. We propose an efficient method for large scale hyperparameter tuning by interpreting these models in a probabilistic framework. After a model augmentation that introduces perentity hyperparameters, we use a variational expectation-maximization approach to tune thousands of such hyperparameters with minimal additional cost. Our approach is agnostic to details of the model and results in a new state of the art in link prediction on standard benchmark data. (h,r,t)∈S hrt (E, R) + λ p ||E h || p p +||E t || p p +||R r || p p .</p><p>Here, E h , E t , and R r is the embedding for entity h, entity t, and relation r, respectively. Boldface E and R is shorthand for all entity and relation embeddings, respectively, and one typically uses a p-norm regularizer with p ∈ {2, 3}.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In 2012, Google announced that it improved the quality of its search engines significantly by utilizing knowledge graphs <ref type="bibr" target="#b12">[Eder, 2012]</ref>. A knowledge graph is a data set of relational facts represented as triplets <ref type="bibr">(head, relation, tail)</ref>. The head and tail symbols represent real-world entities, such as people, objects, or places. The relation describes how the two entities are related to each other, e.g., ' head was founded by tail ' or ' head graduated from tail '.</p><p>While the number of true relational facts among a large set of entities can be enormous, the amount of data points in empirical knowledge graphs is often rather small. It is therefore desirable to complete missing facts in a knowledge graph algorithmically based on patterns detected in the data set of known facts <ref type="bibr" target="#b27">[Nickel et al., 2016a]</ref>. Such * joint first authorship. link prediction in relational knowledge graphs has become an important subfield of artificial intelligence <ref type="bibr" target="#b7">[Bordes et al., 2013</ref><ref type="bibr" target="#b40">, Wang et al., 2014</ref><ref type="bibr" target="#b24">, Lin et al., 2015</ref><ref type="bibr" target="#b28">, Nickel et al., 2016b</ref><ref type="bibr" target="#b37">, Trouillon et al., 2016</ref><ref type="bibr" target="#b41">, Wang and Li, 2016</ref><ref type="bibr" target="#b17">, Ji et al., 2016</ref><ref type="bibr" target="#b32">, Shen et al., 2016</ref><ref type="bibr" target="#b43">, Xiao et al., 2017</ref><ref type="bibr" target="#b33">, Shi and Weninger, 2017</ref><ref type="bibr" target="#b22">, Lacroix et al., 2018</ref>.</p><p>A popular approach to link prediction is to fit an embedding model to the observed facts <ref type="bibr" target="#b19">[Kadlec et al., 2017</ref><ref type="bibr" target="#b26">, Nguyen, 2017</ref><ref type="bibr" target="#b39">, Wang et al., 2017</ref>. A knowledge graph embedding model represents each entity and each relation by a lowdimensional semantic embedding vector. Over the past six years, these models have made significant progress on link prediction <ref type="bibr" target="#b7">[Bordes et al., 2013</ref><ref type="bibr" target="#b28">, Nickel et al., 2016b</ref><ref type="bibr" target="#b37">, Trouillon et al., 2016</ref><ref type="bibr" target="#b22">, Lacroix et al., 2018</ref>. However, <ref type="bibr" target="#b19">Kadlec et al. [2017]</ref> pointed out that these models are highly sensitive to hyperparameters, specifically the regularization strength. This is not surprising since even large knowledge graphs often contain only few data points per entity (i.e., per embedding vector), and so the regularizer plays an important role. <ref type="bibr" target="#b19">Kadlec et al. [2017]</ref> showed that a simple baseline model can outperform more modern models when using carefully tuned hyperparameters.</p><p>In addition to being highly sensitive to the regularization strength, knowledge graph embedding models also need vastly different regularization strengths for different embedding vectors. Knowledge graph embedding models are typically trained by minimizing some function hrt of the embedding vectors for each triplet fact (h, r, t) (short or head, relation, and tail) in the training set S. One typically adds a regularizer with some strength λ &gt; 0 as follows, the frequency of entities and relations in the data set S since the regularizer is inside the sum over training points. This implies vastly different regularization strengths for different embedding vectors since the frequencies of entities and relations vary over a wide range ( <ref type="figure" target="#fig_1">Figure 1</ref>). As we show in this paper, the general idea to use stronger regularization for more frequent entities and relations can be justified from a Bayesian perspective (for empirical evidence, see <ref type="bibr" target="#b35">[Srebro and Salakhutdinov, 2010]</ref>). However, the specific choice to make the regularization strength proportional to the frequency seems more like a historic accident.</p><p>Rather than imposing a proprotional relationship between frequency and regularization strength, we propose to augment the model family such that each embedding E e and R r has its individual regularization strength λ E e and λ R r , respectively. This replaces the loss function from Eq. 1 with Here, the last two sums run over each entity e and each relation r exactly once (there is only one sum over entities since the same entity embedding vector E e is used for an entity e in either head or tail position).</p><p>The loss in Eq. 2 contains a macroscopic number of hyperparameters {λ E e } and {λ R r }: over 16,000 in our largest experiments. It would be impossible to tune such a large number of hyperparameters with traditional grid search, which scales exponentially in the number of hyperparameters. To solve this issue, we propose in this work a probabilistic interpretation of knowledge graph embedding models. The probabilistic interpretation admits efficient hyperparameter tuning with variational expectation-maximization <ref type="bibr" target="#b10">[Dempster et al., 1977</ref><ref type="bibr" target="#b4">, Bernardo et al., 2003</ref>. This allows us to optimize over all hyperparameters in parallel, and it leads to models with better predictive performance.</p><p>Besides improving performance, our approach also has the potential to accelerate research on new knowledge graph embedding models. Researchers who propose a new model architecture currently have to invest considerable resources into hyperparameter tuning to prove competitiveness with existing, highly tuned models. Our cheap large-scale hyperparameter tuning speeds up iteration on new models.</p><p>In detail, our contributions are as follows:</p><p>• We consider a broad class of knowledge graph embedding models containing ComplEx <ref type="bibr" target="#b37">[Trouillon et al., 2016]</ref> and DistMult . We interpret these models as generative models of facts with a corresponding generative process ( <ref type="figure" target="#fig_2">Figure 2</ref>).</p><p>• We first augment these models by introducing separate priors for each entity and relationship vector. In a nonprobabilistic picture, these correspond to regularizers. This augmentation makes the models more flexibile, but it introduces thousands of new hyperparameters (regularizers) that need to be optimized.</p><p>• We then show how to efficiently tune such augmented models. The large number of hyperparameters rules out both grid search with cross validation and Bayesian optimization, calling for gradient-based hyperparameter optimization. Gradient-based hyperparameter optimization would lead to singular solutions in classical maximum likelihood training. Instead, we propose variational expectation-maximization (EM), which avoids such singularities.</p><p>• We evaluate our proposed hyperparameter optimization method experimentally for augmented versions of DistMult and ComplEx. <ref type="bibr">1</ref> The high tunability of the proposed models combined with our efficient hyperparameter tuning method improve the predictive performance over the previous state of the art.</p><p>The paper is structured as follows: Section 2 summarizes a large class of knowledge graph embedding models and presents our probabilistic perspective on these models in terms of a generative probabilistic process. Section 3 describes our algorithm for hyperparameter tuning. We present experiments in Section 4, compare our method to related work in Section 5, and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GENERATIVE KNOWLEDGE GRAPH EMBEDDING MODELS</head><p>this constraint and allow each embedding vector to be regularized differently (Section 2.2). Second, we show that the loss functions of conventional KG embeddings as well as our augmented model class can be obtained as point estimates of a probabilistic generative process of the data (Section 2.3). Drawing on this probabilistic perspective, we can optimize all hyperparameters efficiently using variational expectation-maximization (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CONVENTIONAL KG EMBEDDINGS</head><p>We introduce our notation for a large class of knowledge graph embedding models (KG embeddings) from the literature, such as DistMult , Com-plEx <ref type="bibr" target="#b37">[Trouillon et al., 2016]</ref>, and Holographic Embeddings <ref type="bibr" target="#b28">[Nickel et al., 2016b]</ref>.</p><p>Knowledge graphs are sets of triplet facts (h, r, t) where the 'head' h and 'tail' t both belong to a fixed set of N e entities, and r describes which one out of a set of N r relations holds between h and t. KG embeddings represent each entity e ∈ [N e ] and each relation r ∈ [N r ] by an embedding vector E e and R r , respectively, that lives in a semantic embedding space V with a low dimension K. A model is defined by a real valued score function f (E h , R r , E t ). One fits the embedding vectors such that f assigns a high score to observed triplet facts (h, r, t) ∈ S in the training set S and a low score to triplets that do not appear in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples.</head><p>We give examples of the two models that reach highest predictive performance to the best of our knowledge. For more models, see <ref type="bibr" target="#b19">[Kadlec et al., 2017]</ref>.</p><p>In the DistMult model , the embedding space V = R K is real valued, and the score is defined as</p><formula xml:id="formula_0">f (E h , R r , E t ) = K k=1 E hk R rk E tk (DistMult)<label>(3)</label></formula><p>where, e.g., E hk ∈ R is the k th entry of the vector E h .</p><p>The ComplEx model <ref type="bibr" target="#b37">[Trouillon et al., 2016]</ref> uses a complex embedding space V = C K , and defines the score</p><formula xml:id="formula_1">f (E h , R r , E t ) = K k=1 Re[E hk R rkĒtk ] (ComplEx) (4)</formula><p>where Re[ · ] denotes the real part of a complex number, andĒ tk is the complex conjugate of E tk ∈ C.</p><p>Tail And Head Prediction. Typical benchmark tasks for KG embeddings are 'tail prediction' and 'head prediction', i.e., completing queries of the form (h, r, ?) and (?, r, t), respectively, by ranking potential completions (h, r, t) by their score f (E h , R r , E t ). Most proposals for KG embeddings train a single model for both tail and head prediction. Thus, the loss function is given by Eq. 1, where hrt (E, R)</p><p>is a sum of two terms to train for tail and head prediction, respectively. While early works (e.g., <ref type="bibr" target="#b7">[Bordes et al., 2013</ref><ref type="bibr" target="#b40">, Wang et al., 2014</ref>) trained by maximizing a margin over negative samples, the more recent literature <ref type="bibr" target="#b19">[Kadlec et al., 2017</ref><ref type="bibr" target="#b23">, Liang et al., 2018</ref> suggests that the softmax loss leads to better predictive performance,</p><formula xml:id="formula_2">hrt (E, R) = − f (E h , R r , E t ) + log Ne t =1 e f (E h ,Rr,E t ) − f (E h , R r , E t ) + log Ne h =1 e f (E h ,Rr,Et) .<label>(5)</label></formula><p>Here, the first line (with the sum over tails t ) is the softmax loss for tail prediction, while the second line (with the sum over heads h ) is the softmax loss for head prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">REGULARIZATION IN KG EMBEDDINGS</head><p>Knowledge graph embedding models are highly sensitive to hyperparameters, especially to the strength of the regularizer <ref type="bibr" target="#b19">[Kadlec et al., 2017]</ref>. This can be understood since even large knowledge graphs typically contain only few data points per entity. For example, the FB15K data set contains 483,000 data points, but 88% of all entities e ∈ [N e ] appear fewer than 100 times as head or tail of a training point. Moreover, the amount of training data varies strongly across entities and relations (see <ref type="figure" target="#fig_1">Figure 1</ref>), suggesting that the regularization strength for embedding vectors E e and R r should depend on the entity e and relation r.</p><p>The loss function for conventional KG embeddings in Eq. 1 regularizes all embedding vectors with the same strength λ. We propose to replace λ by individual regularization strengths λ E e and λ R r for each entity e and relation r, respectively, and to fit models with the loss function in Eq. 2. It generalize Eq. 1, which one obtains for</p><formula xml:id="formula_3">λ E e = λn E e ; λ R r = λn R r ; (conventional models) (6)</formula><p>where n E e and n R r denote the number of times that entity e or relation r appears in the training data, respectively. The proposed augmented models described by Eq. 2 are more flexible as they do not impose a linear relationship between n E/R e/r and the regularization strength λ E/R e/r . The downside of the augmented KG embedding models is that one has to tune a macroscopic number of hyperparameters {λ E e } and {λ R r }: more than 16,000 in the popular FB15K data set. Tuning such a large number of hyperparameters would be far too computationally expensive in a conventional setup that fits point estimates by minimizing the loss function. For point estimated models, it is well known that one cannot fit hyperparameters to the training data as this would lead to overfitting (see also Supplementary Material). To avoid overfitting, knowledge graph </p><formula xml:id="formula_4">{(h i , r i , t i )} n i=1</formula><p>. Hyperparameters (regularizer strengths) highlighted in blue. Left: proposed class of models augmented with individual regularizer strengths λ E/R e/r . Right: probabilistic interpretation of conventional models (Eq. 1). The conventional models fix the relative strength of regularizers by reducing all λ E/R e/r to a single scalar λ.</p><p>embedding models are conventionally tuned by cross validation on heldout data. This requires training a model from scratch for each new hyperparameter setting. Cross validation does not scale beyond models with a handful of hyperparameters, and it is expensive even there (see, e.g., <ref type="bibr" target="#b19">[Kadlec et al., 2017</ref><ref type="bibr" target="#b22">, Lacroix et al., 2018</ref>).</p><p>Probabilistic models, by contrast, allow tuning of many hyperparameters in parallel using the empirical Bayes method <ref type="bibr" target="#b10">[Dempster et al., 1977</ref><ref type="bibr" target="#b25">, Maritz, 2018</ref>. We propose a probabilistic formulation of augmented KG embeddings in the next section, and we present a method for efficient hyperparameter tuning in these models in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PROBABILISTIC KG EMBEDDINGS</head><p>We now present our probabilistic version of KG embeddings. The probabilistic formulation enables efficient optimization over thousands of hyperparameters, see Section 3.</p><p>Reciprocal Facts. The KG embedding models discussed in Sections 2.1 and 2.2 make a direct interpretation as a generative probabilistic process difficult. Training a single model for both head and tail prediction introduces cyclic causal dependencies. As will become clear below, the tail prediction part in Eq. 5 (first line on the right-hand side) corresponds to a generative process where the head h causes the tail t. However, the head prediction part (second line) corresponds to a generative process where t causes h.</p><p>To solve this issue, we employ a data augmentation due to <ref type="bibr" target="#b22">Lacroix et al. [2018]</ref> that goes as follows. For each relation r ∈ [N r ], one introduces a new symbol r −1 , which has the interpretation of the inverse of r, but whose embedding vector is not tied to that of r. One then constructs an augmented training set S by adding the reciprocal facts,</p><formula xml:id="formula_5">S := S ∪ {(t, r −1 , h)} (h,r,t)∈S (7)</formula><p>One trains the model by minimizing the loss in Eq. 1 or Eq. 2, where the sum over data points is now over S instead of S, and where hrt is given by only the first line of Eq. 5. When evaluating the model performance on a test set, one answers head prediction queries (?, r, t) by answering the corresponding tail prediction query (t, r −1 , ?). This data augmentation was introduced in <ref type="bibr" target="#b22">[Lacroix et al., 2018]</ref> to improve performance. As we show next, it also has the advantage of enabling a probabilistic interpretation by establishing a causal order where h comes before t.</p><p>Generative Process. With the above data augmentation, minimizing the loss function in Eq. 2 is equivalent to point estimating the parameters of the probabilistic graphical model shown in <ref type="figure" target="#fig_2">Figure 2</ref> (left). The generative process is:</p><formula xml:id="formula_6">• For each entity e ∈ [N e ] and each relation r ∈ [N r ], draw an embedding E e , R r ∈ V from the priors p(E e |λ E e ) ∝ e − λ E e p ||Ee|| p p ; p(R r |λ R r ) ∝ e − λ R r p ||Rr|| p p .</formula><p>(8) Here, p ∈ {2, 3} specifies the norm, and the omitted proportionality constant follows from normalization.</p><p>• Repeat for each data point to be generated:</p><p>-Draw a head entity h and a relation r from some discrete distribution P (h, r). The choice of this distribution has no influence on inference since h and r are both directly observed.</p><formula xml:id="formula_7">-Draw a tail entity t ∼ Categorical(softmax t (f (E h , R r , E t )) (9)</formula><p>where f is the score (e.g., Eq. 3 or 4), and</p><formula xml:id="formula_8">softmax t (f (E h , R r , E t )) = e f (E h ,Rr,Et) t e f (E h ,Rr,E t ) . -Add the triplet fact (h, r, t) to the data set S .</formula><p>This process defines a log joint distribution over E, R, and the data S , conditioned on the hyperparameters {λ E/R e/r }, which we denote collectively by the boldface symbol λ,</p><formula xml:id="formula_9">log p(E, R, S |λ) = = e∈[Ne] log p(E e |λ E e ) + r∈[Nr] log p(R r |λ R r ) + (h,r,t)∈S log P (h, r) + log p(t|h, r, E, R) .<label>(10)</label></formula><p>Using Eq. 9, it is easy to see that log p(t|h, r, E, R) is the negative of the first line of Eq. 5. Thus, up to an additive </p><formula xml:id="formula_10">:= − log p(E, R, S |λ). 5 Update E ← E − α∇ ELB and R ← R − α∇ RLB . until convergence (see Section 3.2)</formula><p>term that depends only on λ, the log joint distribution in Eq. 10 is the negative of the loss function, and minimizing the loss over E and R is equivalent to a maximum a posteriori (MAP) approximation of the probabilistic model. <ref type="figure" target="#fig_2">Figure 2</ref> compares the generative process of the augmented KG embeddings proposed in Section 2.2 (left part of the <ref type="figure">figure)</ref> to the generative process for conventional KG embeddings that one obtains by setting λ E e and λ R r as in Eq. 6 (right). The augmented models are more flexible due to the large number of hyperparameters λ E/R e/r . We discuss next how the probabilistic interpretation allows us to efficiently optimize over this large number of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HYPERPARAMETER OPTIMIZATION</head><p>We now describe the proposed method for hyperparameter tuning in the probabilistic knowledge graph embedding models introduced in Section 2.3. The method is based on variational expectation-maximization (EM). We first derive an approximate coordinate update equation for the hyperparameters (Section 3.1) and then cover details of the parameter initialization (Section 3.2).</p><p>Variational EM optimizes a lower bound to the marginal likelihood of the model over hyperparameters λ, with model parameters E and R integrated out. As we show in the supplementary material, the naive alternative of simultaneously optimizing the original model's loss function over model parameters and hyperparameters would lead to divergent solutions. Variational EM avoids such divergent solutions by keeping track of parameter uncertainty. We elaborate on the role of parameter uncertainty in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VARIATIONAL EM FOR KNOWLEDGE GRAPH EMBEDDING MODELS</head><p>Our proposed algorithm based on variational EM can easily be implemented in an existing model architecture by mak-  Algorithm 2 shows the modifications that are necessary to implement hyperparameter optimization. We describe the algorithm in detail below. In summary, one has to: (i) inject noise into the loss estimateL B (lines 4-5); (ii) learn the optimal amount ξ of noise via SGD (line 7); and (iii) update the hyperparameters λ (line 9).</p><formula xml:id="formula_11">(µ E + e ξ E E , µ R + e ξ R R |λ) (" " denotes elementwise multiplication). 6 Update µ ← µ − α µ ∇ µLB . 7 Update ξ ← ξ − α ξ (∇ ξLB − 1). 8 if t &gt; T E then 9 Update λ ← (1 − α λ )λ −1 + α λλ −1 −1 . M-step (see Eq. 17).</formula><p>Variational Expectation-Maximization. Our probabilistic interpretation of knowledge graph embedding models allows us to optimization over all hyperparameters {λ E e } and {λ R r } in parallel via the expectationmaximization (EM) algorithm <ref type="bibr" target="#b10">[Dempster et al., 1977]</ref>. This algorithm treats the model parameters E and R as latent variables that have to be integrated out. The EM algorithm alternates between a step in which the latent variables are integrated out ('E-step'), and an update step for the hyperparameters λ ('M-step'). We use a version of EM based on variational inference, termed variational EM <ref type="bibr" target="#b4">[Bernardo et al., 2003]</ref>, that avoids the integration step. We further derive an approximate coordinate update equation for the hyperparameters λ, which lead to a significant speedup over gradient updates in our experiments.</p><p>Each choice of hyperparameters λ defines a different variant of the model. The marginal likelihood of the data,</p><formula xml:id="formula_12">p(S |λ) = p(E, R, S |λ) dE dR<label>(11)</label></formula><p>quantifies how well a given model variant describes the data S . Maximizing p(S |λ) over λ thus yields the model variant that fits the data best. However, p(S |λ) is unavailable in closed form as the integral in Eq. 11 is intractable.</p><p>To circumvent the problem of the intractable marginal likelihood, we use variational inference (VI) <ref type="bibr" target="#b18">[Jordan et al., 1999]</ref>. Rather than integrating over the entire space of model parameters E and R, we maximize a lower bound on the marginal likelihood. We introduce a so-called variational family of Gaussian probability distributions,</p><formula xml:id="formula_13">q µ,σ (E, R) = q µ E ,σ E (E) q µ R ,σ R (R)<label>(12)</label></formula><p>with</p><formula xml:id="formula_14">q µ E ,σ E (E) = e∈[Ne] K k=1 N (E ek ; µ ek , σ 2 ek )<label>(13)</label></formula><p>and analogously for q µ R ,σ R (R). Here, the means µ ≡ (µ E , µ R ) and the standard deviations σ ≡ (σ E , σ R ) are so-called variational parameters over which we optimize. </p><p>Here, in the second step, we identified the log joint probability as the negative of the loss L of the corresponding point estimated model, and H[q µ,σ ] is the entropy of q µ,σ .</p><p>The bound in Eq. 14 is tight if the variational distribution q µ,σ is the true posterior of the model for given λ.</p><p>Since it is a lower bound, maximizing the ELBO over µ and σ minimizes the gap and yields the best approximation of the marginal likelihood. We thus take the ELBO as a proxy for the marginal likelihood, and we maximize it also over λ to find near-optimal hyperparameters.</p><p>Gradient updates for µ and σ. We maximize the ELBO concurrently over both variational parameters µ and σ as well as over hyperparameters λ. Updating the variational parameters is called the "E-step". Here, we use gradient updates using Black Box reparameterization gradients <ref type="bibr" target="#b21">[Kingma and</ref><ref type="bibr">Welling, 2014, Rezende et al., 2014]</ref>. This has the advantage of being agnostic to the model architecture as long as the score f (E h , R r , E t ) (e.g., Eqs. 3-4) is differentiable, and it requires only few changes compared to the standard SGD training loop in Algorithm 1.</p><p>To make sure that the standard deviations are always positive, we parameterize them by their logarithms ξ,</p><formula xml:id="formula_16">σ = e ξ (elementwise),<label>(15)</label></formula><p>and we optimize over µ and ξ using SGD. We obtain an unbiased estimate of the term E qµ,σ [L] in Eq. 14 by drawing a single sample from q µ,σ (lines 4-5 in Algorithm 2). The reparameterization gradient trick uses the fact that for, e.g., noise E ek ∼ N (0, 1) from a standard normal distribution,</p><formula xml:id="formula_17">µ E ek + σ E ek E ek is distributed as N µ E ek , (σ E ek ) 2 .</formula><p>The entropy part H[q µ,σ ] of the ELBO (Eq. 14) can be calculated analytically. Up to an additive constant, it is given by the sum over all log standard deviations ξ E ek and ξ R rk . Thus, its gradient with respect to ξ has the constant value of one in each coordinate direction, which we denote by the bold face term "1" on line 7 of Algorithm 2.</p><p>Coordinate updates for λ. Optimizing the ELBO over λ leads to an improved set of hyperparameters provided that the ELBO is a good approximation of the marginal likelihood p(S |λ). However, this is typically not the case at the beginning of the optimization when the variational distribution is still a poor fit of the posterior. We therefore begin the optimization with some number T E of pure "Estep" updates during which we keep λ fixed. After T E "Esteps", we alternate between "E" and "M" steps, where the latter update the hyperparameters λ. In our experiments, we found that the optimization converged slowly when we used gradient updates for λ. To speed up convergence, we therefore derive approximate coordinate updates for λ.</p><p>To simplify the notation, we derive the update equation only for a single hyperparameter λ E e . Updates for λ R r are analogous. The only term in the ELBO (Eq. 14) that depends on λ E e is the expected log prior, E qµ,σ [log p(E e |λ E e )]. Since this term is independent of the data we can write it out explicitly. The omitted proportionality constant in the prior (Eq. 8) is dictated by normalization. We find,</p><formula xml:id="formula_18">log p(E e |λ E e ) = K p log λ E e − λ E e p ||E e || p p + const,<label>(16)</label></formula><p>where K = K for a real-valued embedding space V = R K (as in DistMult) and K = 2K if V = C K (as in ComplEx). Setting ∇ λ E e E qµ,σ [log p(E e |λ E e )] to zero we find that the regularizer strengthλ E e that maximizes the ELBO for given µ and σ satisfies</p><formula xml:id="formula_19">1 λ E e = 1 K E qµ,σ ||E e || p p<label>(17)</label></formula><p>In moderately high embedding dimensions K, we can approximate the right-hand side of Eq. 17 accurately by sampling from q µ,σ . It is the expectation of the average of a large number of independent random variables, and therefore follows a highly peaked distribution. The update step on line 9 of Algorithm 2 uses a conservative weighted average between the current and the optimal value of 1/λ E e with a learning rate α λ ∈ (0, 1]. This effectively averages the estimates over past training steps with a decaying weight. Note that, for V = R K , Eq. 17 has a closed form solution for p = 2 and p = 3, but we found it unnecessary in our experiments to implement specialized code for these cases.</p><p>Absence of overfitting. While the variational EM algorithm keeps track of uncertainty of model parameters, it fits only point estimates for the hyperparameters λ. This is justified in our setup since there are much fewer hyperparameters than model parameters: each entity e and each relation r has an embedding vector E e or R r with K = 2,000 scalar components in our experiments, but only a single scalar hyperparameter λ E e or λ E r . We therefore expect much smaller posterior uncertainty for λ than for E and R, which justifies point estimating λ. Had we instead chosen a very flexible prior distribution with many hyperparameters per entity and relation, the EM algorithm would have essentially fitted the prior to the variational distribution, leading to an ill-posed problem. Judging from learning curves on the validation set, we did not detect any overfitting in variational EM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PRE-AND RE-TRAINING</head><p>Variational EM (Algorithm 2) converges more slowly than fitting point estimates (Algorithm 1) because the injected noise increases the variance of the gradient estimator. To speed up convergence, we train the model in three consecutive phases: pre-training, variational EM, and re-training.</p><p>In the pre-training phase, we keep the hyperparameters λ fixed and fit point estimates E and R to the model using standard SGD (Algorithm 1). We found the final predictive performance (after the variational EM and re-training phases) to be insensitive to the initial hyperparameters. We use early stopping based on the mean reciprocal rank (see Eq. 18 in Section 4 below), evaluated on the validation set.</p><p>In the variational EM phase (Algorithm 2 and Section 3.1), we initialize the variational distribution q µ,σ around the pre-trained model parameters E and R. In detail, we initialize µ E ← E and µ R ← R, and we initialize the components of σ with a value that is small compared to the typical components of µ (0.2 in our experiments).</p><p>In the re-training phase, we fit again point estimates E and R with Algorithm 1, this time using the optimized hyperparameters λ. We use the resulting models to evaluate the predictive performance, see results in Section 4.</p><p>Alternatively to re-training a point estimated model, one could also perform predictions by averaging predictive probabilities over samples from the variational distribution q µ,σ . If q µ,σ is a good approximation of the model posterior then this results in an approximate Bayesian form of link prediction. In our experiments, we found that, in low embedding dimensions K 100, predictions based on samples from q µ,σ outperformed predictions based on point estimates. In higher embedding dimensions, however, the point estimated models from the re-training phase had better predictive performance. We interpret this somewhat counterintuitive observation as a failure of the fully factorized Gaussian variational approximation to adequately approximate the true posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>We test the performance of the proposed model augmentation and the scalable hyperparameter tuning algorithm with two models and four different data sets. In this section, we report results using standard benchmark metrics and we compare to the previous state of the art. We also analyze the relationship between the optimized regularizer strengths and the frequency of entities and relations.</p><p>Model architectures and baselines. We report results for the DistMult model  and the Com-plEx model <ref type="bibr" target="#b37">[Trouillon et al., 2016]</ref>. We follow <ref type="bibr" target="#b22">[Lacroix et al., 2018]</ref> for details of the model architecture: we use reciprocal facts as described at the end of Section 2.3, (p = 3)-norm regularizers, and an embedding dimension of K = 2000. We compare our results to the previous state of the art: <ref type="bibr" target="#b11">[Dettmers et al., 2018</ref><ref type="bibr" target="#b19">, Kadlec et al., 2017</ref> for DistMult and <ref type="bibr" target="#b22">[Lacroix et al., 2018]</ref> for ComplEx.</p><p>Data sets. We used four standard data sets. The first two are FB15K from the Freebase project <ref type="bibr" target="#b6">[Bollacker et al., 2008]</ref> and WN18 from the WordNet database <ref type="bibr" target="#b8">[Bordes et al., 2014]</ref>. The other two data sets, FB15K-237 and WN18RR, are modified versions of FB15K and WN18 due to <ref type="bibr" target="#b36">[Toutanova and</ref><ref type="bibr">Chen, 2015, Dettmers et al., 2018]</ref>. The motivation for the modified data sets is that FB15K and WN18 contain near duplicate relations that lead to leakage into the test set, which makes link prediction trivial for some facts, thus encouraging overfitting. In FB15K-237 and WN18RR these near duplicates were removed.</p><p>Metrics. We report two standard metrics used in the KG embedding literature: mean reciprocal rank (MRR) and Hits@10. We average over head and tail prediction on the test set S test , which is equivalent to averaging only over tail prediction on the augmented test set S test , see Eq. 7.</p><p>All results are obtained in the 'filtered' setting introduced in <ref type="bibr" target="#b7">[Bordes et al., 2013]</ref>, which takes into account that more than one tail may be a correct answer to a query (h, r, ?).  <ref type="bibr" target="#b22">Lacroix et al. [2018]</ref> report performance metrics only with two decimals. In order to show three decimals, we reproduced their results using the code provided by the authors. When rounding to two digits, we recover all values reported in <ref type="bibr" target="#b22">[Lacroix et al., 2018]</ref> except that the MRR for FB15K-237 is reported there as 0.37. The small discrepancy in the last decimal may be explained by different random seeds. When calculating the rank of the target tail t one therefore ignores any competing tails t if the corresponding fact (h, r, t ) exists in either the training, validation, or test set. More formally, the fitlered rank, denoted below as rank filt. (t|h, r), is defined as one plus the number of 'incorrect' facts (h, r, t ) with the the given h and r for which</p><formula xml:id="formula_20">f (E h , R r , E t ) ≥ f (E h , R r , E t ).</formula><p>Here, candidate facts (h, r, t ) are considered 'incorrect' if they appear neither in the training nor in the validation or test set.</p><p>Given a test set S test with reciprocal facts (Eq. 7), the mean reciprocal rank (MRR) is (see, e.g., ),</p><formula xml:id="formula_21">MRR = 1 |S test | (h,r,t)∈S test 1 rank filt. (t|h, r) .<label>(18)</label></formula><p>With 'Hits@10', we denote the fraction of test queries for which the filtered rank is at most 10,</p><formula xml:id="formula_22">Hits@10 = |{(h, r, t) ∈ S test : rank filt. (t|h, r) ≤ 10}| |S test | .<label>(19)</label></formula><p>Quantitative results. For the ComplEx model, the performance improvements are less pronounced (lower half of <ref type="table" target="#tab_2">Table 1</ref>). This may be explained by the fact that the results in <ref type="bibr" target="#b22">[Lacroix et al., 2018]</ref> were already obtained after large scale expensive hyperparameter tuning using grid search. By contrast, the hyperparameter search with our proposed method required only a single run per data set. Even for the largest data set FB15K, the variational EM phase took less than three hours on a single GPU. Despite the much cheaper hyperparameter optimization, our models slightly outperform the previous state of the art on three out of the four considered data sets, with only a small degradation on the fourth. Qualitative results. Finally, we study the relationship between optimized hyperparameters and frequencies of entities in the training data. <ref type="figure" target="#fig_5">Figure 3</ref> shows the learned λ E e for all entities e as a function of the number of times n R e that each entity e appears in the training corpus of the FB15K data set. The red line is the best proportional fit λ E e ∝ n E e to the results, as is imposed by conventional models (Eq. 6).</p><p>Our findings confirm the general idea to use stronger regularization for entities with more training data. The Bayesian interpretation can explain this observation: a small amount of training data typically leads to high posterior uncertainty, which leads to smallλ E e in Eq. 17. However, our results indicate that imposing a proportionality between λ E e and n E e would be a poor choice that significantly underregularizes infrequent entities and overregularizes frequent entities (note the logarithmic scale in <ref type="figure" target="#fig_5">Figure 3)</ref>. Our empirical findings may inspire future theoretical work that derives an optimal frequency dependency of the regularization strength in tensor factorization models.</p><p>Related work to this paper can be grouped into link prediction algorithms and variational inference.</p><p>Link Prediction. Link prediction in knowledge graphs has gained a lot of attention as it may guide a way towards automated reasoning with real world data. For a review see <ref type="bibr" target="#b27">[Nickel et al., 2016a]</ref>. Two different approaches for link prediction are predominant in the literature. In statistical relational learning, one infers explicit rules about relations (such as transitivity or commutativity) by detecting statistical patterns in the training set. One then uses these rules for logic reasoning <ref type="bibr" target="#b13">[Friedman et al., 1999</ref><ref type="bibr" target="#b20">, Kersting et al., 2011</ref><ref type="bibr" target="#b29">, Niu et al., 2012</ref><ref type="bibr" target="#b30">, Pujara et al., 2015</ref>.</p><p>Our work focuses on a complementary approach that builds on knowledge graph embedding models. This line of research started with the proposal of the TransE model <ref type="bibr" target="#b7">[Bordes et al., 2013]</ref>, which models relational facts as vector additions in a semantic space. More recently, a plethora of different knowledge graph embedding models based on tensor factorizations have been proposed. We summarize here only the path that lead to the current state of the art. Different models make different trade-offs between generality and effective use of training data.</p><p>Canonical tensor decomposition <ref type="bibr" target="#b14">[Hitchcock, 1927]</ref> uses independent embeddings for entities in the head or tail position of a fact. DistMult <ref type="bibr">[Yang et al., 2015, Toutanova and</ref><ref type="bibr" target="#b36">Chen, 2015]</ref>, by contrast, uses the same embeddings for entities in head and tail position, thus making use of more training data per entity embedding, but restricting the model to symmetric relations. The ComplEx model <ref type="bibr" target="#b37">[Trouillon et al., 2016]</ref> lifts this restriction by multiplying the head, relation, and tail embeddings in an asymmetric way. To the best of our knowledge, the current state of the art was presented by <ref type="bibr" target="#b22">Lacroix et al. [2018]</ref>, who improved upon the ComplEx model by introducing reciprocal relations and using a better regularizer.</p><p>The sensitivity of KG embeddings to the choice of hyperparameters, such as regularizer strengths, was first pointed out in <ref type="bibr" target="#b19">[Kadlec et al., 2017]</ref>. A popular heuristic is to regularize each embedding every time it appears in a minibatch, thus effecitvely regularizing embeddings proportionally to their frequency <ref type="bibr" target="#b35">[Srebro and</ref><ref type="bibr">Salakhutdinov, 2010, Lacroix et al., 2018]</ref>. In contrast, we propose to learn entity-dependent regularization strengths without relying on heuristics. <ref type="bibr" target="#b38">Vilnis et al. [2018]</ref> proposed a new model that is probabilistic in the sense that it assigns probabilities to the results of queries. However, in contrast to our proposal, the model is not a probabilistic generative model of the data set.</p><p>Variational Inference. Variational inference (VI) is a powerful technique to approximate a Bayesian posterior over latent variables given observations <ref type="bibr" target="#b18">[Jordan et al., 1999</ref><ref type="bibr" target="#b5">, Blei et al., 2017</ref><ref type="bibr" target="#b45">, Zhang et al., 2018</ref>. Besides approximating the posterior, VI also estimates the marginal likelihood of the data. This allows for iterative hyperparameter tuning, (variational EM) <ref type="bibr" target="#b4">[Bernardo et al., 2003]</ref>, which is the main benefit of the Bayesian approach used in this paper.</p><p>Our paper builds on recent probabilistic extensions of embedding models to Bayesian models, such as word <ref type="bibr" target="#b3">[Barkan, 2017]</ref> or paragraph <ref type="bibr" target="#b16">[Ji et al., 2017]</ref> embeddings. In these works, the words are embedded into a K-dimensional space. It has been shown that using a probabilistic approach leads to better performance on small data sets, and allows these models to be combined with powerful priors, such as for time series modeling <ref type="bibr" target="#b15">, Jähnichen et al., 2018</ref>]. Yet, the underlying probabilistic models in these papers are very different from the ones considered in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian</head><p>Optimization. An alternative method that can be used for hyperparameter optimization is Bayesian optimization. However, Bayesian optimization does not scale to the large number of hyperparameters that we tune in this work. Most practical applications of Bayesian optimization (e.g., <ref type="bibr" target="#b34">[Snoek et al., 2012</ref><ref type="bibr" target="#b42">, Wang et al., 2013</ref>) tune only tens of hyperparameters, rather than ten thousands. This is because Bayesian optimization treats the model as a black box, which it can only train and then evaluate for a given choice of hyperparameters at a time. Each such evaluation contributes a single data point to fit an auxiliary model over the hyperparameters. By contrast, variational EM has access to gradient information to train all hyperparameters in parallel, and concurrently with the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We augmented a large class of popular knowledge graph embedding models in such a way that every entity embedding and every relationship embedding vector has their own regularizer, and showed that it is possible to tune these potentially thousands of hyperparameters in a scalable way. Our approach is motivated by the observation that sharing a common regularization parameter across all embeddings leads to over-regularization. The Role of Parameter Uncertainty in the Proposed Hyperparameter Optimization</p><p>Bayesian inference and the idea of measuring uncertainty are somewhat uncommon in the literature for knowledge graph embeddings. To clarify why uncertainty is important in the proposed hyperparameter optimization (Section 3.1 of the main text), we compare the method here to a more naive approach that will turn out to fail because it ignores uncertainty. We discuss the failure of the naive approach and the benefit of estimating parameter uncertainty first intuitively and then more formally.</p><p>Intuitive picture. The variational EM algorithm maximizes (a lower bound on) the marginal likelihood p(S |λ), Eq. 14 of the main text. In a more naive attempt to hyperparameter tuning without cross validation, one might be tempted to skip the marginalization over model parameters E and R. Instead, one might try to directly maximize the log joint probability log p(E, R, S |λ), i.e., minimize the loss L = − log p(E, R, S |λ), over E, R, and hyperparameters λ. This approach, however, would lead to divergent solutions because the log joint probability is unbounded.</p><p>The log joint probability contains the log priors (first two terms on the right-hand side of Eq. 10 of the main text). <ref type="figure" target="#fig_1">Figure S1</ref> shows the prior p(E ek |λ E e ), Eq. 8 of the main text, of a single component E ek of an entity embedding assuming, for simplicity, a real embedding space. With growing regularizer strength (increasing λ E e ), the prior becomes narrower and narrower. As the peak narrows, it also grows higher due to the normalization constraint. In the limit λ E e → ∞, the prior collapses to an infinitely narrow and high δ-peak at zero.</p><p>A hyperparameter tuning method that ignores posterior uncertainty can exploit the unbounded growth of the maximum of the prior to send the log joint distribution to infinity (i.e., the loss L → −∞). Without any posterior uncertainty, one can set the model parameter E ek precisely to * joint first authorship. Figure S1: Prior (Eq. 8 of the main text with p = 3) for a single model parameter E ek . As the prior gets more peaked for growing regularizer strength λ E e , the height of the peak grows unboundedly. An (incorrect) hyperparameter optimization method that ignores parameter uncertainty could end up exploiting this unboundedness and diverge to λ E e → ∞.</p><p>zero and then make the value of p(E ek |λ E e ) arbitrarily large by sending λ E ek → ∞. We prevent this collapse of the prior to a δ-peak by keeping track of parameter uncertainty. Admitting a nonzero uncertainty for E ek no longer allows us to set E ek precisely and deterministically to zero. Any slightly nonzero value of E ek would have no support under a δ-peaked prior.</p><p>Formal derivation. We now formalize the above intuitive picture and show that the specific variational approximation chosen in Eqs. 12-13 indeed suffices to prevent any divergent solutions.</p><p>Assuming again a real embedding space, the log prior of a single model parameter E ek is given by (cf., Eq. 16 of the main text) log p(E e |λ E e ) = 1 p log λ E e − λ E e |E ek | p + const. (S1)</p><p>As discussed above, setting E ek = 0 and sending λ E e → ∞ sends the right-hand side of Eq. S1 to infinity. This can</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Number of training points (triplet facts) for each entity (left) and relation (right) in the FB15K data set (we sorted entity and relation IDs by frequency). The large variation on the y-axis motivates entity/relation-dependent regularization strengths as proposed in Eq. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Two different generative processes of triplet facts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>end end ing a few modifications. Algorithm 1 shows the conventional way to train a knowledge graph embedding model using stochastic gradient descent (SGD). The log joint distribution in Eqs. 8-10 defines a loss function L(E, R|λ) := − log p(E, R, S |λ) of the form of Eq. 2. In SGD, one repeatedly calculates an estimateL B of this loss function based on a minibatch B of training points, and one obtains gradient estimates by backpropagating throughL B .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Evoking Jensen's inequality, the log marginal likelihood is then lower-bounded by the evidence lower bound [Blei et al., 2017, Zhang et al., 2018], or ELBO: log p(S |λ) ≥ E E,R∼qµ,σ log p(E, R, S |λ) − log q µ,σ (E, R) = −E E,R∼qµ,σ L(E, R, S |λ)] + H[q µ,σ ] =: ELBO(λ, µ, σ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Relationship between the learned regularizer strengths λ E e and the frequency of each entity. The conventional proportional scaling (red line) overregularizes frequent entities and underregularizes infrequent ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: Conventional training of knowledge graph embedding models (stochastic gradient descent). Input: Augmented data set S , see Eq. 7. Model p(E, R, S |λ) as defined by Eqs. 8-10. Hyperparameters λ. Learning rate α. Output: Trained embedding vectors E and R. Initialize embedding vectors E and R randomly. Calculate a minibatch estimateL B of the loss L(E, R|λ)</figDesc><table><row><cell>2 repeat 3 Draw a minibatch B ⊂ S .</cell></row></table><note>14</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2: Variational EM for knowledge graph embedding models with coordinate updates for λ. Input: Augmented data set S , see Eq. 7. Model p(E, R, S |λ) as defined by Eqs. 8-10. Initial values for µ, ξ, λ, see Section 3.2. Numbers T E , T EM of E-steps and EM-steps. Learning rates α µ , α ξ &gt; 0 and α λ ∈ (0, 1]. Output: Optimized hyperparameters λ, embedding vectors µ E/R ± e ξ E/R with uncertainties. Initialize means µ E/R and log standard deviations ξ E/R around a pretrained model, see Section 3.2.</figDesc><table /><note>12 for t ← 1 to T E + T EM do3 Draw a minibatch B ⊂ S .4 Draw Gaussian noise E , R ∼ N (0, I).5 Calculate a minibatch estimateL B of the loss with injected noise, L</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Model performances (in 'filtered' setting; see.</figDesc><table /><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 1 summarizes our quantitative results. The top half of the table shows results for the DistMult model. Our models with individually optimized regularization strengths significantly outperform the previous state of the art across all four data sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Supplementary Material to "Augmenting and Tuning Knowledge Graph Embeddings"</figDesc><table><row><cell>Robert Bamler  *  Department of Computer Science University of California, Irvine Irvine, CA 92617</cell><cell>Farnood Salehi  *  Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland Department of Computer Science Stephan Mandt University of California, Irvine Irvine, CA 9261</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">In this section, we introduce our notation for a large class of knowledge graph embedding models (KG embeddings) from the literature (Section 2.1), and we then generalize these models in two aspects. First, while conventional KG embeddings typically share the same regularization strength across all entities and relationship vectors, we lift 1 Source code: https://github.com/mandt-lab/ knowledge-graph-tuning</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We treat knowledge graph embeddings as generative probabilistic models, making them amenable to Bayesian model selection. We derived approximate coordinate updates for the hyperparameters in the framework of variational EM. We applied our method to generalizations of the DistMult and ComplEx models and outperformed the state of the art for link prediction. The approach can be applied to a wide range of models with minimal modifications to the training routine. In the future, it would be interesting to investigate whether tighter variational bounds <ref type="bibr" target="#b9">[Burda et al., 2016</ref>] may further improve model selection.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>even be relaxed: the log prior diverges for λ E e → ∞ as long as we keep E ek small enough, i.e., as long as E ek = O((λ E e ) −1/p ). This is why maximizing the log joint distribution over λ leads to divergend solutions.</p><p>Instead of maximizing the log joint distribution, the variational EM algorithm maximizes the ELBO, Eq. 14 of the main text. Note first that the ELBO itself is bounded from above by zero: the ELBO is a lower bound on the marginal log likelihood log p(S |λ), where p(S |λ) ≤ 1 since it is a discrete probability distribution.</p><p>Further, the ELBO has a maximum at finite values for the variational parameters and hyperparameters. Maximizing the ELBO ensures that each model parameter is associated with a nonzero uncertainty σ E/R e/r &gt; 0 since the entropy term</p><p>imposes an infinite penalty if any σ E/R e/r → 0. The entropy term in the ELBO thus has an additional regularizing effect. Combined with the other regularizing term in the ELBO, the expected log prior, we obtain for a given model parameter E ek using Eq. S1, up to an additive constant,</p><p>where, in the last equality, we made the dependency on σ E ek explict by reparameterizing the normal distributed random variable E ek = µ E ek + σ E ek in terms of a standard-normal distributed variable .</p><p>Maximizing the right-hand side of Eq. S3 over µ E ek yields µ E ek = 0 by symmetry, thus simplifying the objective to</p><p>with some numerical constant c p &gt; 0. The right-hand side of Eq. S4 is structurally similar to the right-hand side of Eq. S1, which had divergent solutions: both are a difference between a logarithmic and a linear term in λ E e . However, while in Eq. S1, we were able to send the logarithmic term to infinity and still keep the linear term bounded, this is not possible in Eq. S4, which has the same argument (up to a constant c p ) for the logarithmic and the linear term. The right-hand side of Eq. S4 has a maximum at a finite value for λ E e σ E ek p . Thus, the variational EM algorithm avoids divergent solutions.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving optimization in models with continuous symmetry breaking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perturbative black box variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian neural word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Barkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3135" to="3143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The variational Bayesian EM algorithm for incomplete data: with application to scoring graphical model structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Bayarri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="464" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Knowledge graph based search system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Scott</forename><surname>Eder</surname></persName>
		</author>
		<idno>App. 13/404</idno>
		<imprint>
			<date type="published" when="2012-06-21" />
			<biblScope unit="page">109</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning probabilistic relational models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1300" to="1309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable generalized dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jähnichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<title level="m">Bayesian paragraph vectors. Symposium on Advances in Approximate Bayesian Inference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical relational ai: logic, probability and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriraam</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR11)</title>
		<meeting>the 11th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jebara</surname></persName>
		</author>
		<idno>978-1-4503-5639-8</idno>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 World Wide Web Conference, WWW &apos;18</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Empirical Bayes Methods with Applications: 0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An overview of embedding models of entities and relationships for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomaso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepdive: Web-scale knowledge-base construction using statistical learning and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDS</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="25" to="28" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Using semantics and statistics to turn data into knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AI Magazine</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Implicit reasonet: Modeling large-scale structured relationships with shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Proje: Embedding projection for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collaborative filtering in a non-uniform world: Learning with the weighted trace norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2056" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic embedding of knowledge graphs with box lattice measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Text-enhanced representation learning for knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Zi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bayesian optimization in high dimensions via random embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masrour</forename><surname>Zoghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Matheson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ssp: Semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="3104" to="3110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Advances in variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Butepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Kjellstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
