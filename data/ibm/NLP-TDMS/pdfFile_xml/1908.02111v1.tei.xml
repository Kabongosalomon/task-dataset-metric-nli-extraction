<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point Cloud Super Resolution with Adversarial Residual Graph Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikai</forename><surname>Wu</surname></persName>
							<email>huikai.wu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
							<email>jgzhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
							<email>kaiqi.huang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Point Cloud Super Resolution with Adversarial Residual Graph Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point cloud super-resolution is a fundamental problem for 3D reconstruction and 3D data understanding. It takes a low-resolution (LR) point cloud as input and generates a high-resolution (HR) point cloud with rich details. In this paper, we present a data-driven method for point cloud super-resolution based on graph networks and adversarial losses. The key idea of the proposed network is to exploit the local similarity of point cloud and the analogy between LR input and HR output. For the former, we design a deep network with graph convolution. For the latter, we propose to add residual connections into graph convolution and introduce a skip connection between input and output. The proposed network is trained with a novel loss function, which combines Chamfer Distance (CD) and graph adversarial loss. Such a loss function captures the characteristics of HR point cloud automatically without manual design. We conduct a series of experiments to evaluate our method and validate the superiority over other methods. Results show that the proposed method achieves the state-of-theart performance and have a good generalization ability to unseen data. Code is available at https://github. com/wuhuikai/PointCloudSuperResolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When modeling an object from the real world for 3D printing or animation, a common way is to first obtain the point cloud with depth scanning devices or 3D reconstruction algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> and then recover the mesh from the point cloud <ref type="bibr" target="#b2">[3]</ref>. However, the captured point cloud is usually sparse and noisy due to the restrictions of devices or the limitations of algorithms, which leads to a low-quality mesh.</p><p>The key to improving the quality of the recovered mesh is point cloud super-resolution, which takes a LR point cloud as input and generates a HR point cloud with rich details and few noisy points, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Most existing methods are optimization based without learning from  <ref type="bibr" target="#b36">[37]</ref> and our method respectively. Ours is sharper at edges with fewer noisy points. Best viewed in color.</p><p>data, which have strong assumptions about the underlying surface of the HR point cloud <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. As for data-driven methods, few prior works study deep learning on this problem. <ref type="bibr" target="#b36">[37]</ref> propose PU-Net for point cloud super-resolution, which is a pioneering work. By employing deep neural networks, it outperforms many traditional methods such as EAR <ref type="bibr" target="#b12">[13]</ref> and achieves the state-of-the-art performance.</p><p>In this paper, we aim to advance the performance of point cloud super-resolution by overcoming the defects of PU-Net. The first problem is that PU-Net directly regresses the point coordinates without exploiting the similarity between LR and HR point cloud, which makes it hard to train. The second problem is that PU-Net proposes a complicated loss function with a strong assumption on the uniform distribution of HR point cloud. Manually designed loss functions tend to overfit human priors, which fail to capture many other properties of HR point cloud, such as continuity.</p><p>Recent work <ref type="bibr" target="#b15">[16]</ref> in image super-resolution shows that predicting the residual between the LR and HR image is a more desirable way to achieve better accuracy. Thus, to solve the first problem, we propose to introduce residual connections into graph convolution networks (GCNs) <ref type="bibr" target="#b4">[5]</ref> and add a skip connection between the input layer and the output layer. Employing GCNs to process point cloud is not new <ref type="bibr" target="#b37">[38]</ref>. However, the GCN in our method is unique in two aspects compared to that in <ref type="bibr" target="#b37">[38]</ref>: <ref type="bibr" target="#b0">(1)</ref> The architecture of our GCN is designed for generating point cloud while that in <ref type="bibr" target="#b37">[38]</ref> aims at aggregating information for classification. <ref type="bibr" target="#b1">(2)</ref> We propose an un-pooling layer for the GCN to upsample the input point cloud.</p><p>To solve the second problem, we design a graph adversarial loss based on LS-GAN <ref type="bibr" target="#b22">[23]</ref>. The proposed loss function is more expressive than manually designed ones, which can capture the characteristics of HR point cloud automatically. Pan et al. also introduce adversarial loss into graph networks <ref type="bibr" target="#b24">[25]</ref>. However, they focus on learning the distribution of graph embeddings. Thus, a multi-layer perceptron is employed as the discriminator to process the input vector. Differently, we aim at distinguishing real and fake point clouds. To achieve this, we propose a GCN as the discriminator to process the generated point cloud, which is significantly different from <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this way, we propose a novel method for point cloud super-resolution, named Adversarial Residual Graph Convolution Network (AR-GCN). Experiments show that the proposed method achieves the state-of-the-art performance on both seen dataset <ref type="bibr" target="#b36">[37]</ref> and unseen dataset (SHREC15). The contributions of our method are three-folds. First, we propose a novel architecture for point cloud superresolution. Second, we introduce the graph adversarial loss to replace manually designed loss functions. Third, we advance the state-of-the-art performance on both seen and unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Point Cloud Super Resolution</head><p>Point cloud super-resolution is formulated as an optimization problem in most earlier works. To upsample a point cloud, <ref type="bibr" target="#b1">[2]</ref> first compute the Voronoi diagram on the moving least squares surface and then add points at the vertices of this diagram. <ref type="bibr" target="#b21">[22]</ref> present a locally optimal projection operator for surface approximation based on L 1 median, which is parameter-free and robust to noisy points. These methods have a strong assumption on the smoothness of the underlying surface, which tend to have vague edges. Thus, <ref type="bibr" target="#b12">[13]</ref> introduce an edge-aware point cloud upsampling method, which first samples away from the edges and then progressively samples the point cloud while approaching the edge singularities.</p><p>However, all these methods have strong assumptions about the underlying surface based on human priors. To exploit the massive 3D data, <ref type="bibr" target="#b36">[37]</ref> propose a data-driven method that first learns multi-level features per point with PointNet++ <ref type="bibr" target="#b26">[27]</ref> and then expands the point set via a multi-branch convolution. Through end-to-end learning, this method outperforms the optimization based methods on multiple datasets, which achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning for 3D Data</head><p>There is a rising interest in 3D data processing recently. Most existing works transform 3D data into volumetric grids, which are then processed by 3D CNNs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b27">28]</ref>. Particularly, <ref type="bibr" target="#b29">[30]</ref> propose to upsample 3D objects in voxel space, which is similar to point cloud superresolution.</p><p>3D CNNs are memory and time consuming. Thus, following works propose to process point cloud directly instead of the volumetric grid <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14]</ref>. <ref type="bibr" target="#b25">[26]</ref> propose a point-wise network for 3D object classification and segmentation. Successively, <ref type="bibr" target="#b26">[27]</ref> introduce a hierarchical feature learning architecture to capture local and global context. <ref type="bibr" target="#b19">[20]</ref> introduce a convolution operator named Xconv that is capable of leveraging spatially-local correlation for point cloud classification.</p><p>Besides 3D scene understanding, multiple works focus on employing deep learning for single image 3D reconstruction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b14">15]</ref>. <ref type="bibr" target="#b6">[7]</ref> present a conditional shape sampler to generate multiple plausible point clouds from a single image. Differently, <ref type="bibr" target="#b33">[34]</ref> propose a graph CNN for producing the 3D triangular mesh from a single color image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Image Super Resolution</head><p>Super-resolution for 2D images is a well-studied problem, which is closely related to point cloud superresolution. Modern approaches are usually built on deep learning in a data-driven manner <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. <ref type="bibr" target="#b5">[6]</ref> first upsample the low-resolution image with bicubic interpolation and then use a shallow CNN to recover the details and textures. Instead of manually designed interpolation, <ref type="bibr" target="#b34">[35]</ref> propose to jointly learn the interpolation and detail recovery. To generate realistic images, <ref type="bibr" target="#b17">[18]</ref> present a generative adversarial network, which pushes the generated images close to natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first define point cloud superresolution formally and then introduce our method AR-GCN in detail. Our method is a novel approach that contains three major components: the adaptive adversarial loss function L G , the residual GCN G, and the graph discriminator D. As shown in <ref type="figure">Figure 2</ref>, the LR point cloud x is directly fed into G to generate the HR outputŷ. Then,ŷ is sent into D to produce L G , while another loss L cd is calculated based onŷ and the ground truth y.  <ref type="figure">Figure 2</ref>: Framework Overview. The proposed AR-GCN consists of a generator G and a discriminator D. G is a residual GCN that upsamples the input point cloud progressively with the upsampling ratio 2×. D is also a residual GCN, which learns to distinguish fake HR point cloud from the real one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Point Cloud Super Resolution</head><p>Formally, given a point cloud x with shape n × 3, the goal of point cloud super-resolution is to generate a point cloudŷ with shape N × 3 (N = γn, γ &gt; 1). Each point of y lies on the underlying surface described by x, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Method Overview: AR-GCN</head><p>As shown in <ref type="figure">Figure 2</ref>, our method AR-GCN consists of two networks, the generator G and the discriminator D. G aims to generate the HR point cloud by upsampling the LR input progressively, while D is responsible for distinguishing the fake HR point cloud from the real one. To train G and D simultaneously, we propose a joint loss function as shown in Equation 1:</p><formula xml:id="formula_0">L(x, y) = λL cd (G(x), y) + L G (G(x)),<label>(1)</label></formula><p>where λ controls the balance between L cd and L G . L cd measures the distance between y andŷ, which is similar to L 2 loss in image super-resolution. As shown in Equation 2:</p><formula xml:id="formula_1">L cd (ŷ, y) = Σ p∈y min q∈ŷ ||p − q|| 2 2 ,<label>(2)</label></formula><p>which is a variant of Chamfer Distance. The original Chamfer Distance consists of two parts: L cd and Lĉ d , where Lĉ d is symmetric with L cd and defined as Equation <ref type="bibr" target="#b2">3</ref>:</p><formula xml:id="formula_2">Lĉ d (ŷ, y) = Σ q∈ŷ min p∈y ||p − q|| 2 2 .<label>(3)</label></formula><p>Lĉ d encouragesŷ to be identical to the LR input, which leads to duplication points in the output point cloud. Thus, we remove Lĉ d and only employ L cd as our loss function.</p><p>L cd measures the point-wise distance between y andŷ, which ignores high-order properties defined by a cluster of points, such as continuity. Traditional methods usually manually design a complex function as the loss, which is inefficient and has strong assumptions about the underlying surface. Alternatively, we propose a loss function L G that is defined by a network and learned from data automatically. Concretely, L G is a graph adversarial loss that is inspired by generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref>. In this paper, we employ LS-GAN <ref type="bibr" target="#b22">[23]</ref> as the adversarial loss for its simplicity and effectiveness. L G is defined as follows:</p><formula xml:id="formula_3">L G (ŷ) = ||1 − D(ŷ)|| 2 2 ,<label>(4)</label></formula><p>where D is the discriminator that aims to distinguish real and fake HR point cloud by minimizing the following loss:</p><formula xml:id="formula_4">L D (ŷ, y) = 1 2 ||D(ŷ)|| 2 2 + 1 2 ||1 − D(y)|| 2 2 .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Residual Graph Convolution Generator</head><p>The generator G is built on the Graph Convolution Network (GCN) <ref type="bibr" target="#b3">[4]</ref>, which aims to progressively upsample the LR point cloud. It consists of three building blocks, namely residual graph convolution block, unpooling block and feature net, as shown in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Residual Graph Convolution Block</head><p>PU-Net employs PointNet++ to generate HR point cloud, which treats the central point and the neighbor points equally. This limits the learning ability of the network. Alternatively, we build our method on graph convolution <ref type="bibr" target="#b3">[4]</ref>, as shown in <ref type="figure">Figure 3</ref>.</p><formula xml:id="formula_5">ReLU G-conv ! "# $ "# + ReLU ReLU G-conv + $ %&amp;' ! %&amp;' Figure 3: Residual Graph Convolution Block. x in is used for querying the k nearest neighbors. x out is the same as x in .</formula><p>The core of graph convolution, G-conv, is defined on a graph G=(υ, ε) and calculated as follows,</p><formula xml:id="formula_6">f p l+1 = w 0 f p l + w 1 Σ q∈N (p) f q l , ∀p ∈ υ,<label>(6)</label></formula><p>where w is the learn-able parameters and f p l represents the feature of vertex p at layer l. N (p) is the vertices that connect to p as defined by the adjacency matrix ε. However, there's no predefined adjacency matrix for a point cloud. To solve this problem, we define N (p) as the k nearest neighbors of p in Euclidean space, of which the coordinates are defined by x in .</p><p>Besides G-conv, we also introduce residual connections into our block, because residual networks usually lead to faster convergence and better results. It also helps to exploit the similarity between LR point cloud and the corresponding HR point cloud.</p><p>In our experiment, the number of neighbors k is set to 8. All the G-conv operators inside the block have the same number of channels, which is 128. The input feature f in and point cloud x in are processed by 12 residual layers to obtain f out , while x out is the same as x in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Unpooling Block</head><p>The unpooling block takes point cloud x in and the corresponding features f in as inputs. It first transforms f in with shapen × c to a tensor with shapen × 6 by a G-conv layer. The tensor is then reshaped ton × 2 × 3, which is noted as δx. The upsampled point cloud x out is obtained by adding x in and δx point-wisely, where each point is transformed into 2 points. The unpooling block is designed to predict the residual between x in and x out instead of regressing x out directly. This exploits the similarity between x in and x out , which leads to a faster convergence and better performance.</p><p>The feature of the output point cloud, f out , are obtained by the following equation:</p><formula xml:id="formula_7">f p out = 1 k Σ q∈N [xin](p) f q in , ∀p ∈ x out ,<label>(7)</label></formula><p>where N [x in ](p) means the k nearest neighbors of point p in point cloud x in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Feature Net</head><p>As shown in <ref type="figure">Figure 3</ref>, a residual graph convolution block takes both the point cloud and the corresponding feature as inputs. However, the generator only have one input, the point cloud x. To obtain the other input, the corresponding feature f , we design a simple block named feature net, which takes the point cloud x as input. Specifically, for each point p ∈ x with shape 1 × 3, we first obtain its k nearest neighbors P with shape k × 3. Then, a series of point-wise convolutions with a max-pooling layer transformP = P −p into f p with shape 1 × c.</p><p>In our experiment, k is set to 8 while c is set to 128. The number of convolution layers is set to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Progressive Super Resolution</head><p>Instead of directly upsampling the LR point cloud with the desired upscale ratio, we choose to generate the HR point cloud step by step. The point cloud is upsampled by 2 times at each step, as shown in <ref type="figure">Figure 2</ref>. Our experiment shows that such an approach results in better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Graph Discriminator</head><p>To generate more realistic HR point cloud, we present a graph adversarial loss for point cloud, which is defined by the discriminator D. As shown in <ref type="figure">Figure 2</ref>, D is composed of feature net, residual graph convolution block, and pooling block. For feature net, k is set to 8 while c is set to 64. The number of convolution layers is set to 2. For residual graph convolution block, k is set to 8 while c is set to 64. The number of layers is set to 4.</p><p>Pooling Block Given the input point cloud x in with shape 4n × 3, we first employ farthest point sampling (FPS) to generate x out with shape n × 3. The corresponding features f out is then obtained as follows:</p><formula xml:id="formula_8">f p out = max q∈N [xin](p) f q in , ∀p ∈ x out ,<label>(8)</label></formula><p>where max is applied element-wisely.</p><p>Graph Patch GAN Most discriminators downsample the input progressively to obtain a single flag for the whole input. Such a design usually leads to blurry and unpleasant artifacts. Instead of employing a global discriminator, we build a graph patch GAN based on <ref type="bibr" target="#b28">[29]</ref>. Specifically, our discriminator downsamples the input multiple times so that the output contains more than 1 point. Graph patch GAN forces every local patch of the generated point cloud to lie on the distribution of the real HR point cloud. In our experiment, we set the number of the output points to 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we first introduce the datasets for training and testing, as well as the details of our implementation.  The quantitative and qualitative results are then presented to show the effectiveness of our method. To demonstrate the effect of each component in AR-GCN, we also conduct a comprehensive ablation study. To further show the potential applications of our method, we test AR-GCN in an iterative setting and apply it to 3D reconstruction task. Besides, we also employ our method to assist LR point cloud classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We utilize two datasets for our experiments. One is the train-test dataset, which our method is trained with and tested on. The other is the unseen dataset, where our method is directly tested without training or finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train-Test Dataset</head><p>We use the dataset proposed in PU-Net <ref type="bibr" target="#b36">[37]</ref> for training and testing. This dataset contains 60 different models from the Visionair repository. Following the protocol in PU-Net, we use 40 models for training while the other 20 models are used for testing. For training, 100 patches are extracted from each model as the ground truth, which contains 4, 096 points. The input patch is randomly sampled from the ground truth patch at each iteration of training, which contains 1, 024 points. For testing, we sample 20, 000 points uniformly per model as the ground truth, while sampling 5, 000 points as the input.</p><p>Unseen Dataset: SHREC15 To further validate the generalization ability of our method, we directly test AR-GCN on SHREC15 <ref type="bibr" target="#b20">[21]</ref> after training with the train-test dataset without finetuning. For SHREC15, there are 50 categories in total and 24 models in each category. We randomly choose one model from each category for testing, since the models in each category only differ in the pose. Same as the train-test dataset, the ground truth contains 20, 000 points, while the input contains 5, 000 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our method is implemented in Tensorflow <ref type="bibr" target="#b0">[1]</ref> and runs on a single Titan-Xp GPU. To avoid overfitting, we augment the training data by randomly rotating, shifting and scaling the data. For optimization, we use Adam <ref type="bibr" target="#b16">[17]</ref> as the optimizer, where the batch size is 28 and the learning rate is 0.001. The network is firstly trained with L cd for 80 epochs at the speed of 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Results</head><p>Evaluation Metrics We adopt standard 3D reconstruction metrics for point cloud super-resolution because the output of both tasks is point cloud. To measure the difference betweenŷ and y point-wisely, we utilize the standard Chamfer Distance (CD) and Earth Mover's Distance (EMD), for which smaller is better.</p><p>CD and EMD are heavily influenced by the outliers. Thus, we also report F-score <ref type="bibr" target="#b30">[31]</ref> by treating point cloud super-resolution as a classification problem. Specifically, precision and recall are first evaluated by checking the percentage of points inŷ or y that can find a neighbor from the other within certain threshold τ . The F-score is then calculated as the harmonic mean of precision and recall. For this metric, larger is better.</p><p>The metrics in <ref type="bibr" target="#b36">[37]</ref> are also employed to evaluate our method. We use Deviation to measure the difference between the predicted point cloud and the ground truth mesh, while normalized uniformity coefficient (NUC) is evaluated for measuring the uniformity. For these two metrics, smaller is better. Notably, the original mesh is used as the ground truth instead of the sampled 20, 000 points.   Comparison with Other Methods The performance of different methods on the train-test dataset is reported in Table 1. We first report the performance of the LR input as a preliminary baseline. We then report the performance of Moving Least Squares (MLS) <ref type="bibr" target="#b1">[2]</ref>, which is a traditional method. <ref type="bibr" target="#b0">1</ref> The performance of PU-Net is then reported, which is the state-of-the-art method for point cloud superresolution. <ref type="bibr" target="#b1">2</ref> As shown in <ref type="table" target="#tab_1">Table 1</ref>, our method outperforms all the other methods under most metrics. Particularly, our method improves the F-score by more than 10%, while advances CD a large step. Surprisingly, our method even outperforms PU-Net in NUC, although it is not trained with the repulsion loss <ref type="bibr" target="#b36">[37]</ref>, which forces the generated point cloud uniform.</p><p>We attribute this to the effect of our adversarial loss. As for MLS, it performs well in Deviation but has the lowest NUC scores. The reason is that MLS tends to produce new points close to the points in the input point cloud, which leads to a non-uniformly distributed point cloud, resulting in poor performance on NUC metric. However, the mean deviation to the ground truth is small. Similar results are obtained on SHREC15 as shown in <ref type="table" target="#tab_2">Table 2</ref>, which shows the generalization ability of our method on the unseen dataset. As for NUC and Deviation, our method outperforms the baseline methods by a large margin.</p><p>Since both our method and PU-Net are deep learning based method, we also compare the number of parameters in each model. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our model contains about the same number of learnable parameters as PU-Net while achieves much better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>The evaluation metrics reflect the shape quality to some degree. However, they mainly focus on point-wise distance and fail to reflect the surface properties such as smoothness and high-order details. Since there are barely any standard metrics to measure these aspects, we present a series of qualitative results to show the advantage of our method. <ref type="figure">Figure 4</ref> presents the point clouds in both datasets visually, where the 1st row is from the train-test dataset and the 2nd row is from SHREC15. Compared to PU-Net, the HR point clouds of our method have richer details and sharper edges. Besides, ours are more uniformly distributed in the smooth area with fewer noisy points. On the contrary, the results from PU-Net are noisy and blurry on the edges with little details. It tends to outspread uniformly without preserving the underlying structure. Notably, the differences around the legs, feet and horns are most obvious, as shown in the red boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>To further verify the effect of each component in our method, we conduct an ablation study on both datasets and present the results in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>We first present the results of a very simple baseline GCN 4×</p><p>points . Compared to AR-GCN, there are mainly three modifications. First, we remove the residual connections from the proposed generator, as well as the skip connection between input and output. Second, it regresses the coordinates directly under the supervision of a single loss L cd . Third, instead of progressively upscaling, it upsamples the point cloud by 4× directly with the number of G-conv layers unchanged. Without all the key features in our method, it is not surprising that this simple baseline does not work at all. We then put back the skip connection between input and output, which forces the model to predict the residual δx instead of directly regressing the point coordinates. This simple change largely improves the stability of learning, which results in a model with a reasonable performance, as shown by GCN 4× . We further transform graph convolution into residual graph convolution by putting back the residual connections. Such a modification improves F-score by about 3% on SHREC15 and 1.5% on the train-test dataset, as shown by ResGCN 4× . By enabling progressive superresolution, we obtain the proposed generator. As shown by ResGCN, the F-score increases by nearly 4% on the traintest dataset. The Deviation also improves a lot on both datasets. When changing L cd into the loss proposed in <ref type="bibr" target="#b36">[37]</ref>, the F-score decreases by about 8% as shown by ResGCN + L pu and ResGCN, which demonstrates the effectiveness of L cd . Compared to PU-Net, ResGCN + L pu improves the F-score by more than 17% because of the proposed residual graph network ResGCN. By replacing L pu with the proposed loss, the F-score is further improved by around 9%,  <ref type="table">Table 4</ref>: Experiments on noisy data and uneven data.</p><p>which achieves the state-of-the-art performance, as shown by AR-GCN.</p><p>We also take an experiment to compare different training strategies. AR-GCN w/o FT is trained with the proposed loss L(x, y) for 120 epochs, while AR-GCN is trained with L cd for 80 epochs and then finetuned with L(x, y) for another 40 epochs. As shown in the table, AR-GCN outperforms AR-GCN w/o FT consistently, which shows the superiority of the 2-step training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Robustness of AR-GCN</head><p>To show the robustness of our method, we take experiments on noisy point clouds and non-uniformly sampled point clouds separably. The quantitative results on the Train-Test dataset and SHREC15 are shown in <ref type="table">Table 4</ref>. Ours+noisy represents applying AR-GCN on noisy point clouds (Gaussian noise z, where z ∼ N (0, 0.01)). Results show that our method with noisy point clouds outperforms PU-Net with clean ones. Ours+uneven means employing our method on non-uniformly sampled point clouds. Our method with uneven point clouds achieves similar performance to PU-Net with uniform ones on all the metrics except NUC. We attribute this to the non-uniform distribution of the input point clouds. Our method can only make it more uniform to a certain degree. We plan to solve this problem in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Applications</head><p>Iterative Super Resolution Our method is trained to upscale a point cloud by a fixed ratio, which is 4× in our setting. To demonstrate the ability of upsampling a point cloud by more than 4×, we conduct an experiment that takes the output of the previous iteration as input and upsamples it by 4× again with AR-GCN. The initial point cloud has 1, 024 points, which are upsampled by 16× after 2 iterations. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, our method not only handles a relatively sparse point cloud but also upsamples it by more than 4× iteratively. Although the resulting point cloud is not as good as that in <ref type="figure">Figure 4</ref>, it recovers many details from the point cloud with only 1, 024 points, which is promising.  3D Reconstruction In 3D reconstruction, the quality and density of the point cloud have a huge impact on the quality of the reconstructed mesh. However, due to the limitations of scanning devices, the point cloud is usually sparse and noisy. Thus, point cloud super-resolution is the key to improve the quality of 3D reconstruction. We employ our method and PU-Net to generate the HR point cloud, which is then fed into Ball pivoting algorithm <ref type="bibr" target="#b2">[3]</ref> for mesh reconstruction. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, the mesh reconstructed from our method contains richer details compared to that from the LR input. Besides, ours is smoother in the flat area and sharper at the edges, while the mesh from PU-Net is noisy with many unpleasant artifacts, especially in the red boxes.  LR Point Cloud Classification For 3D understanding, the classification accuracy of the LR point cloud is usually lower than that of the HR point cloud. To improve the performance of LR point cloud classification, one possible way is transforming it to HR point cloud with the point cloud super-resolution method. To show the effectiveness of point cloud super-resolution, we employ PointNet++ <ref type="bibr" target="#b26">[27]</ref> on ModelNet40 dataset <ref type="bibr" target="#b35">[36]</ref> for point cloud classification. As shown in <ref type="table" target="#tab_6">Table 5</ref>, PointNet++ achieves 91.05% in classification accuracy with 1, 024 points as input. When we randomly sample 256 points from the 1, 024 points and send them to PointNet++, the performance drops from 91.05% to 46.96%. We then upsample the 256 points by 4× with our method and send the 1, 024 points to PointNet++, resulting in 79.34% in accuracy, which outperforms 46.96% by a large margin. This experiment shows that point cloud superresolution is important for understanding LR point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-Scanned Point Cloud</head><p>We also conduct an experiment on real-scanned and un-even point clouds. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, our method generates a denser point cloud with more uniformly distributed points, while maintains the underlying structure such as the striped texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a graph convolution network AR-GCN for point cloud super-resolution, which is composed of a residual generator, a graph discriminator, and a graph adversarial loss. With comprehensive experiments, we demonstrated that residual connections and residual prediction are effective for stable training and better performance. With the proposed graph adversarial loss, our method generates more realistic HR point cloud compared to the manually designed loss function. The experiment on train-test dataset showed that our method outperforms other methods. The experiment on the unseen dataset SHREC15 further demonstrated the better performance and generalization ability of our method. Notably, our method is not designed for completion, thus it can not fill large holes or missing parts. We'd like to solve the limitations in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Point Cloud Super Resolution. (a) is the input LR point cloud with sparse distribution. (b) is the corresponding HR point cloud with dense distribution. (c) and (d) are the HR point cloud generated by PU-Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2 min/epoch. Then we add L G and finetune the network for another 40 epochs at the speed of 4.9 min/epoch. The training process takes about 6.2 hours in total, while it takes about 4.8 hours for training PU-Net under the same setting (2.4 min/epoch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Iterative Super Resolution. (a) is the input point cloud with 1, 024 points. (b) and (c) are the generated HR point clouds after the 1st and 2nd iterations. At each iteration, the output from the previous iteration is upsampled by 4 times with our method. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Mesh Reconstruction from Point Cloud. The differences inside the red boxes are most obvious. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Employ our method on real-scanned and un-even point cloud. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative Comparison on the Train-Test Dataset.</figDesc><table><row><cell>Method</cell><cell>CD</cell><cell>EMD</cell><cell cols="5">F-score τ = 0.01 0.2% 0.4% 0.6% 0.8% 1.0% mean NUC with different p Deviation (1e-2) Params std</cell></row><row><cell>Input</cell><cell cols="2">0.0120 0.0036</cell><cell></cell><cell>41.33%</cell><cell>0.315 0.224 0.185 0.163 0.150</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MLS [2]</cell><cell cols="2">0.0117 0.0043</cell><cell></cell><cell>57.70%</cell><cell>0.364 0.272 0.229 0.204 0.186</cell><cell>0.18</cell><cell>0.34</cell><cell>-</cell></row><row><cell cols="3">PU-Net [37] 0.0118 0.0041</cell><cell></cell><cell>43.24%</cell><cell>0.206 0.165 0.147 0.137 0.131</cell><cell>0.78</cell><cell>0.66</cell><cell>0.777M</cell></row><row><cell>AR-GCN</cell><cell cols="2">0.0084 0.0035</cell><cell></cell><cell>70.28%</cell><cell>0.204 0.164 0.145 0.134 0.128</cell><cell>0.26</cell><cell>0.30</cell><cell>0.785M</cell></row><row><cell>Method</cell><cell>CD</cell><cell cols="2">EMD</cell><cell cols="4">F-score τ = 0.01 0.2% 0.4% 0.6% 0.8% 1.0% mean NUC with different p Deviation (1e-2) std</cell></row><row><cell>Input</cell><cell cols="6">0.0077 0.0031 27.86% 0.310 0.220 0.183 0.163 0.151</cell><cell>-</cell><cell>-</cell></row><row><cell>MLS [2]</cell><cell cols="7">0.0067 0.0032 84.69% 0.253 0.199 0.173 0.159 0.150 0.33</cell><cell>0.46</cell></row><row><cell cols="8">PU-Net [37] 0.0103 0.0050 56.39% 0.283 0.230 0.204 0.189 0.180 0.90</cell><cell>0.73</cell></row><row><cell>AR-GCN</cell><cell cols="7">0.0054 0.0031 93.07% 0.201 0.162 0.144 0.135 0.130 0.18</cell><cell>0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative Comparison on SHREC15.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study Results on the Train-Test Dataset and the Unseen Dataset, SHREC15.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracy on the test set of Model-Net40 with PointNet++. 1024 (from 256 points) is obtained by upsampling the 256 points 4 times with our method.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The result of MLS is reproduced with Point Cloud Library (PCL).<ref type="bibr" target="#b1">2</ref> The result of PU-Net is reproduced with the official code by following the author's instructions.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing and rendering point set surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ball-pivoting algorithm for surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mittleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image super-resolution via dual-state recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Consolidation of unorganized point clouds for surface reconstruction. TOG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Edge-aware point set resampling. TOG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pointsift: A sift-like network module for 3d point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00652</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-rigid 3d shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elnaghy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>El-Sana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giachetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Limberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">U</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pevzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DOR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Parameterization-free projection for geometry reconstruction. TOG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tal-Ezer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smolley. Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09987</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>3d object superresolution</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond accuracy, f-score and roc: a family of discriminant measures for performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pix3d: Dataset and methods for single-image 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01654</idno>
		<title level="m">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">End-to-end image super-resolution via deep and shallow convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07680</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Punet: Point cloud upsampling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A graph-cnn for 3d point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
