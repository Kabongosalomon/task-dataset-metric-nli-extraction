<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BYTECOVER: COVER SONG IDENTIFICATION VIA MULTI-LOSS TRAINING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bytedance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhesong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bytedance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bytedance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejun</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Bytedance AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BYTECOVER: COVER SONG IDENTIFICATION VIA MULTI-LOSS TRAINING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Cover song identification</term>
					<term>instance-batch normalization (IBN)</term>
					<term>BNNeck</term>
					<term>classification loss</term>
					<term>triplet loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present in this paper ByteCover, which is a new feature learning method for cover song identification (CSI). Byte-Cover is built based on the classical ResNet model, and two major improvements are designed to further enhance the capability of the model for CSI. In the first improvement, we introduce the integration of instance normalization (IN) and batch normalization (BN) to build IBN blocks, which are major components of our ResNet-IBN model. With the help of the IBN blocks, our CSI model can learn features that are invariant to the changes of musical attributes such as key, tempo, timbre and genre, while preserving the version information. In the second improvement, we employ the BN-Neck method to allow a multi-loss training and encourage our method to jointly optimize a classification loss and a triplet loss, and by this means, the inter-class discrimination and intra-class compactness of cover songs, can be ensured at the same time. A set of experiments demonstrated the effectiveness and efficiency of ByteCover on multiple datasets, and in the Da-TACOS dataset, ByteCover outperformed the best competitive system by 18.0%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Cover song identification (CSI), which aims at finding cover versions of a given music track in a music database, is an important task in the field of music information retrieval (MIR), with a number of potential applications such as music organization, retrieval, recommendation and music license management. However, cover versions may be different from the original music in many aspects such as key, tempo, structure, instrumentation or even genre, which makes CSI a challenging problem <ref type="bibr" target="#b0">[1]</ref>.</p><p>Traditional CSI methods based on the extraction of handcrafted features (e.g., predominant melody, chroma sequence, chord progression, and harmonic pitch class profiles (HPCP)) and the use time series matching techniques (e.g., dynamic time warping (DTW)) have achieved promising results on small datasets <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. However, most of these methods contain exhaustive iteration and matching, which makes it difficult for them to scale to large datasets consisting of hundreds of thou-sands of or even millions of music tracks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">7]</ref>. This limits the use of CSI in real-world industry applications. Recently, with their popularity in computer vision and other MIR tasks, deep learning methods have been introduced to solve the CSI problem. Roughly speaking, existing deep learning methods to CSI can be classified into two categories. The first category of methods, e.g., <ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref>, treats CSI as a multi-class classification problem, where each version group is considered as an unique class. Convolutional neural networks (CNNs) are trained to classify music tracks in the training set and during retrieval, the network's penultimate layer is used to generate feature for audio matching. The second category of methods, on the other hand, treats CSI as a metric learning problem. These methods, e.g., <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11]</ref>, mostly use triplet loss as the objective function and train CNN-based models to minimize the distances between cover pairs and maximize the distances between different covers. After training, the convolutional part of the network is used as feature extractor for the further identification.</p><p>Experiments in the existing works have proved that deep learning models can boost the performance of CSI by a large margin over traditional methods. Moreover, the feature representations learned by deep models are usually fixed-length vectors where the length is unrelated to the original music track. This raises an advantage that the similarity between features can be directly measured by Euclidean distance or Cosine distance, and therefore the features can be efficiently indexed and retrieved using existing nearest-neighbor search libraries, even for a large dataset. In this paper, following the success of deep learning models introduced above, we present the Bytedance Cover Song Identification System (ByteCover), which is a new feature learning system that aims at learning fixed-length feature representations for CSI. ByteCover is built based on the classical CNN model ResNet50 <ref type="bibr" target="#b12">[12]</ref>, and includes two major improvements that make the model more powerful for CSI. Together with other minor designs such as generalized mean pooling (GeMPool), these two improvements helped ByteCover beat all the existing state-of-the-art methods on all the widely-used datasets in our experiments. Especially, on the Da-TACOS dataset, Byte-Cover achieved a mean average precision of 71.4%, which is 18.0% higher than that the best competitive system.</p><p>Our first improvement is that we introduce instance normalization <ref type="figure">(</ref>  <ref type="bibr" target="#b13">[13]</ref> to joint utilize the ability of IN for learning features that are invariant to appearance changes, such as colors, styles, and virtuality/reality, and the ability of BN for preserving content related information. In CSI, we also have the demand of keeping the version information of music, while designing features that are robust to the changes of key, tempo, timbre, and genre, among others. We believe that the use of IBN would also be beneficial for CSI, and our ablation experiments will show that, by integrating IN with BN in ResNet50, the performance of CSI can be significantly improved.</p><p>Our second improvement is that, different from existing works which either formulate CSI as a classification problem or treat it as a metric learning problem, ByteCover attempts to learn features by jointly optimizing a classification loss and a triplet loss. It has been testified that classification loss emphasizes more on inter-class discrimination while triplet loss emphasizes more on intra-class compactness <ref type="bibr" target="#b14">[14]</ref>. In the literature, a few studies have been conducted to combine these two losses, and the experimental results showed that the combination can result in better performance than using a single loss, in tasks such as person re-identification <ref type="bibr" target="#b15">[15]</ref>, fine-grained visual recognition and ego-motion action recognition <ref type="bibr" target="#b14">[14]</ref>. In this paper, we investigate the use of BNNeck proposed in <ref type="bibr" target="#b15">[15]</ref> to fuze the classification and triplet losses for CSI, and by using this multi-loss training, feature representations that are both robust and discriminative can be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BYTECOVER APPROACH</head><p>Our aim of using ByteCover is to derive from each input music track a single global feature that encodes the version information, and by comparing the features of any two music tracks using Cosine distance, we can obtain an estimation of whether these two music tracks are cover versions of each other. <ref type="figure">Figure 1</ref> shows the model structure of ByteCover. As shown in the figure, ByteCover takes as input a constant-Q transform (CQT) spectrogram and employs a CNN-based model, i.e., ResNet-IBN, for feature learning. The learned feature map is then compressed by GeMPool to form a fixedlength vector, which is then used to calculate the triplet loss and Softmax loss jointly. The joint training of these two losses are achieved by using the BNNeck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">ResNet-IBN</head><p>For the simplification of the overall pipeline, we use the CQT spectrogram rather than the sophisticated features widely used in the CSI <ref type="bibr" target="#b7">[7]</ref>. In terms of the parameters of the CQT, the number of bins per octave is set as 12 and Hann window is used for extraction with a hop size of 512. Besides this, the audio is resampled to 22050 Hz before the feature extraction. Afterwards, the CQT is downsampled with an averaging factor of 100 along the temporal dimension. This size reduction of input feature improves the efficiency of the model and reduces the latency of our CSI system. As a result, the input audio is processed to a compressed CQT spectrogram S ∈ R 84×T , where the T depends on on the duration of the input music.</p><p>In the CSI task, the robustness against the change of various musical elements is highly related to the model's generalization on large-scale music corpus. At the same time, we also hope that this robustness will not undermine the ability of the model to identify covers. Inspired by the success of instance batch normalization (IBN) networks <ref type="bibr" target="#b13">[13]</ref> in learning an invariant representation of an image and preserving the discrimination for person re-identification, we replaced the residual block in the vanilla ResNet <ref type="bibr" target="#b12">[12]</ref> with IBN block.</p><p>The original ResNet <ref type="bibr" target="#b12">[12]</ref> consists of four stages, each having 3, 4, 6, and 3 basic residual blocks in turn. In addition, the first stage has a 7 × 7 convolution and 3 × 3 pooling layer to process the input. To transform a ResNet to a model equipped with IBN module for learning an invariant embedding, the residual block which is the basic elements of the model, are replaced with IBN blcok. As <ref type="figure" target="#fig_1">Figure 2</ref> shows, the BN is applied to half of the channels and IN normalized the other channels after the first convolution for every residual block. The IN modules allow the model to learn style invariant features for music so that it could make better use of the music performances with a high diversity of styles within a single dataset. Besides, to prevent that the model's learning capacity is degraded by too many IN layers, the last module  has not been modified and remains the same. Furthermore, to enlarge the spatial size of the output feature map, the stride of the first convolution layer in the last residual block is changed to 1. Thus the input spectrogram has been processed to a 3-D embedding X ∈ R K×H×W after being downsampled four times and the channel expansion. Here, K = 2048 is the number of output channel, H = 6 and W = T /8 are the spatial size along the frequency and time axis, respectively.</p><p>To compress the feature map X to a fixed-length vector f, a spatial aggregation module is needed. Average pooling and max pooling are two common methods for aggregating local feature, and the several CSI models <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref> attach a average pooling layer after the feature extractor. To boost the performance of our CSI model, we adopt a novel GemPool module, which unifies these two aggregation methods. The compression results f can be given by</p><formula xml:id="formula_0">f = [f 1 . . . f k . . . f K ] , f k = 1 |X k | x∈X k x p 1 p . (1)</formula><p>The behavior of the GeM pooling can be controlled by adjusting parameter p : the GeM pooling is equivalent to the average pooling when p = 1 and max pooling for p → ∞. The p is set to a trainable parameter to fully exploit the advantage of deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">BNNeck for Multi-Loss Training</head><p>In the field of CSI, there are two paradigms of training the model: they are classification and metric learning paradigms <ref type="bibr" target="#b16">[16]</ref>. The former <ref type="bibr" target="#b10">[10]</ref> uses an additional fully connected layer as a classifier to learn embeddings for cover detection through a proxy classification task. The latter <ref type="bibr" target="#b7">[7]</ref> directly uses metric learning methods such as triplet loss to make the Euclidean distance between cover pairs smaller than non-cover pairs.</p><p>A straightforward method to merge these two training paradigms is calculating the classification loss and the triplet loss on the same feature. However, the experimental results in Section 3 show that this naive approach cannot effectively improve the performance of the CSI model. The disharmony between classification loss and triplet loss may be the reason for the unsatisfactory performance of joint learning. The work of Hao et al. <ref type="bibr" target="#b15">[15]</ref> indicates that the targets of these two losses are inconsistent in the embedding space.</p><p>For sample pairs in the embedding space, classification loss mainly optimizes the cosine distance while triplet loss focuses on the Euclidean distance. If we use these two losses to optimize a feature vector simultaneously, their goals may be inconsistent. Consequently, triplet loss may influence the clear decision surfaces of classification loss, and classification loss may reduce the intra-class compactness of triplet loss <ref type="bibr" target="#b15">[15]</ref>.</p><p>Thus the BNNeck module inserts a BN layer before the classifier, which is a no-biased FC layer with weights W, as <ref type="figure">Figure 1</ref> depicts. The feature yield by the GeM module is denoted as f t here. We let f t pass through a BN layer to generate the feature f c . In the training stage, f t and f c are used to compute triplet and classification losses, respectively.</p><p>Because the normalized features are more suitable for the calculation of cosine metric, we use the f c to do the computation of the similarity among the performances during the inference and retrieval phases. Overall, the objective function used for training our model can be derived by</p><formula xml:id="formula_1">L = L cls (f c ) + L tri (f t ) (2) = CE(Sof tmax(Wf c ), y) + [d p − d n + α] + ,<label>(3)</label></formula><p>where d p and d n are feature distances of positive pair and negative pair. α is the margin of triplet loss, and [z] + equals to max(z, 0). In this paper, α is set to 0.3. In terms of the classification loss L cls , the CE refers the cross entropy function and y is the ground truth label. Finally, we train the model with Adam optimizer. The batch size and the learning rate are 32 and 0.0004, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Settings</head><p>To evaluate the performance of our ByteCover model, we conducted several experiments on four publicly available benchmark datasets, including SHS100K <ref type="bibr" target="#b8">[8]</ref>, Youtube350 <ref type="bibr" target="#b19">[19]</ref>, Covers80 <ref type="bibr" target="#b2">[3]</ref>, and Da-TACOS <ref type="bibr" target="#b20">[20]</ref>. SHS100K, which is collected from Second Hand Songs website by <ref type="bibr" target="#b8">[8]</ref>, consisting of 8858 songs with various covers and 108523 recordings. For this data, we follow the settings of <ref type="bibr" target="#b10">[10]</ref>. The dataset is split into the training, validation, and test sets with a ratio of 8:1:1. Our ByteCover model was MAP P@10 MR1</p><p>Results on Youtube350 SiMPle <ref type="bibr" target="#b17">[17]</ref> 0.591 0.140 7.91 TPPNet <ref type="bibr" target="#b9">[9]</ref> 0.859 0.188 2.85 CQT-Net <ref type="bibr" target="#b10">[10]</ref> 0.917 0.192 2.50 MOVE <ref type="bibr" target="#b7">[7]</ref> 0.888 -3.00 ByteCover 0.955 0.196 1.60 Results on Covers80 Qmax <ref type="bibr" target="#b4">[5]</ref> 0.544 0.061 -TPPNet <ref type="bibr" target="#b9">[9]</ref> 0.744 0.086 6.88 CQT-Net <ref type="bibr" target="#b10">[10]</ref> 0.840 0.091 3.85 ByteCover 0.906 0.093 3.54 Results on Da-TACOS Qmax <ref type="bibr" target="#b4">[5]</ref> 0.365 -113 SiMPle <ref type="bibr" target="#b17">[17]</ref> 0.332 -142 MOVE <ref type="bibr" target="#b7">[7]</ref> 0.507 -40 Re-MOVE <ref type="bibr" target="#b18">[18]</ref> 0 Youtube350 <ref type="bibr" target="#b19">[19]</ref> is collected from the YouTube, containing 50 compositions of multiple genres <ref type="bibr" target="#b19">[19]</ref>. Each song in Youtube350 has 7 versions, with 2 original versions and 5 different versions and thus results in 350 recordings in total. In our experiment, we used the 100 original versions as references and the others as queries following the same as <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b17">17]</ref>.</p><p>Covers80 <ref type="bibr" target="#b2">[3]</ref> is a widely used benchmark dataset in the literature. It has 80 songs, with 2 covers for each song, and has 160 recordings in total. To compare with existing methods, we computed the similarity of any pair of recordings.</p><p>Da-TACOS <ref type="bibr" target="#b20">[20]</ref> consists of 15000 performances, of which 13000 performances belong to 1000 cliques, with 13 samples in each clique; the remaining 2000 pieces do not belong to any clique. It is worth mentioning that there are 8373 samples of the Da-TACOS which are also belongs to the training set of SHS100K. Thus we removed these samples from the training set and report the results of retrained model.</p><p>During the retrieval phase, the cosine distance metric was used to estimate the similarity between two musical performances. Following the evaluation protocol of the Mirex Audio Cover Song Identification Contest 1 , the mean average precision (mAP), precision at 10 (P@10), and the mean rank of the first correctly identified cover (MR1) are reported as 1 https://www.music-ir.org/mirex/wiki/2020: Audio_Cover_Song_Identification   <ref type="table">Table 3</ref>. The ablation study of ByteCover. Baseline is a ResNet based network trained by the classification loss only. The improvements are aggregated in turn. evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison and Ablation Study</head><p>We compared ByteCover against published state-of-theart methods on four datasets, which are mentioned above.</p><p>Our ByteCover model outperformed all previous methods, achieving a new state-of-the-art performance by 18.0% mAP over Re-MOVE <ref type="bibr" target="#b18">[18]</ref> on the largest benchmark dataset Da-TACOS. We found that our model can often achieve more significant advantages on datasets with more samples, more varied styles, and more diverse genres, like SHS100K or Da-TACOS. To analyze the importance of different parts of the Byte-Cover system quantitatively, we trained the models with several settings. The baseline was the original ResNet-50 based model trained by the classification loss only, and a global average pooling module produced the final output. Other settings and results are presented in <ref type="table">Table 3</ref>.</p><p>To prove the scalability of ByteCover model and a fairer comparison with Re-MOVE <ref type="bibr" target="#b18">[18]</ref>, we report our method's performances with the embedding vectors of different lengths. We utilized a simple fully-connected layer as the projection head to reduce feature vector size from 2048 to 128 or 256, rather than the sophisticated knowledge distillation used in <ref type="bibr" target="#b18">[18]</ref>. As the <ref type="table" target="#tab_3">Table 2</ref> shows, our model still surpasses the stateof-the-art method Re-MOVE <ref type="bibr" target="#b18">[18]</ref> by a large margin while the length of our feature vector is half of <ref type="bibr" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we propose a simple and efficient design of the cover song identification system. The combination of classification and metric learning training schemes encourages our model to learn a discriminative feature embedding. The utilization of IBN leads the model to be robust against the musical variations. The results show that our ByteCover system outperforms all previous state-of-the-art cover detection methods by a significant margin on four public benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The Structure of IBN Block and ResNet-IBN 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance on different datasets (-indicates the results are not shown in original works). trained on the training subset of SHS100K, and the results on the test subset are reported.</figDesc><table><row><cell></cell><cell>.534</cell><cell>-</cell><cell>38</cell></row><row><cell>ByteCover</cell><cell>0.714</cell><cell>0.801</cell><cell>23.0</cell></row><row><cell></cell><cell cols="3">Results on SHS100K-TEST</cell></row><row><cell>Ki-CNN [8]</cell><cell>0.219</cell><cell>0.204</cell><cell>174</cell></row><row><cell>TPPNet [9]</cell><cell>0.465</cell><cell>0.357</cell><cell>72.2</cell></row><row><cell>CQT-Net [10]</cell><cell>0.655</cell><cell>0.456</cell><cell>54.9</cell></row><row><cell>ByteCover</cell><cell>0.836</cell><cell>0.534</cell><cell>47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>mAP on Da-Tacos for different embedding sizes K.</figDesc><table><row><cell></cell><cell cols="2">mAP on Da-TACOS mAP on SHS100K-TEST</cell></row><row><cell>Baseline</cell><cell>0.546</cell><cell>0.683</cell></row><row><cell>+ IBN</cell><cell>0.617</cell><cell>0.734</cell></row><row><cell>+ BNNeck</cell><cell>0.698</cell><cell>0.802</cell></row><row><cell>+ GeMPool</cell><cell>0.714</cell><cell>0.836</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Audio cover song identification and similarity: background, approaches, evaluation, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="307" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A mid-level melody-based representation for calculating audio similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Music Information Retrieval</title>
		<meeting>of the Int. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="280" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying &apos;cover songs&apos; with chroma features and dynamic programming beat tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Poliner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<meeting>of the IEEE Int. Conf. on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="page" from="1429" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chroma binary similarity and local alignment applied to cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilia</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serrà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1138" to="1151" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross recurrence quantification for cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Andrzejak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">93017</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective cover song identification based on skipping bigrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="96" to="100" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate and scalable version identification using musically-motivated embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yesiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<meeting>of the IEEE Int. Conf. on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Keyinvariant convolutional neural network toward efficient cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal pyramid pooling convolutional neural network for cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhesong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4846" to="4852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a representation for cover song identification using convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="541" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cover detection using dominant melody embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Doras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffroy</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018, Proceedings, Part IV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Boosting standard classification architectures through a ranking regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruhisa</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="758" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining musical features for cover detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Doras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furkan</forename><surname>Yesiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilia</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffroy</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simple: assessing music similarity using subsequences joins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Chin M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Enrique</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Almeida Prado Alves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Less is more: Faster and better music version identification with embedding distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furkan</forename><surname>Yesiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilia</forename><surname>Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Music shapelets for fast cover song recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Music Information Retrieval</title>
		<meeting>of the Int. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="441" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Da-TACOS: a dataset for cover song identification and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yesiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tralie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Correya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tovstogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
