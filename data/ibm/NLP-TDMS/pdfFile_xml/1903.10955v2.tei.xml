<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
							<email>zengxingyu@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an efficient 3D object detection framework based on a single RGB image in the scenario of autonomous driving. Our efforts are put on extracting the underlying 3D information in a 2D image and determining the accurate 3D bounding box of the object without point cloud or stereo data. Leveraging the off-the-shelf 2D object detector, we propose an artful approach to efficiently obtain a coarse cuboid for each predicted 2D box. The coarse cuboid has enough accuracy to guide us to determine the 3D box of the object by refinement. In contrast to previous state-ofthe-art methods that only use the features extracted from the 2D bounding box for box refinement, we explore the 3D structure information of the object by employing the visual features of visible surfaces. The new features from surfaces are utilized to eliminate the problem of representation ambiguity brought by only using a 2D bounding box. Moreover, we investigate different methods of 3D box refinement and discover that a classification formulation with quality aware loss has much better performance than regression. Evaluated on the KITTI benchmark, our approach outperforms current state-of-the-art methods for single RGB image based 3D object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection is one of the key components of autonomous driving. It has drawn increasing attention in the recent computer vision community. With 3D LIDAR laser scanners, discrete 3D location data of objects in the form of point cloud can be fetched, but the equipment is quite expensive. On the contrary, on-board color cameras are cheaper and more flexible for most vehicles, whereas they can only provide 2D photos. Thus 3D object detection with a single RGB camera becomes important as well as challenging for economical autonomous driving systems. This paper focuses on detecting complete 3D object content using only monocular RGB image.</p><p>This paper proposes an efficient framework based on 3D guidance and using the surface feature for refinement (GS3D) to detect complete 3D object content using only monocular RGB image.</p><p>(a) (b) (c) <ref type="figure">Figure 1</ref>. The key idea of our method: (a) We first predict reliable 2D box and its observation orientation. (b) Based on the predicted 2D information, we utilize artful techniques to efficiently determine a basic cuboid for the corresponding object, called guidance. (c) Features extracted from the visible surfaces of projected guidance as well as the tight 2D bounding box of it will be utilized by our model to perform accurate refinement with classification formulation and quality-aware loss.</p><p>Typical single image 3D detection methods, e.g. Mono3d <ref type="bibr" target="#b1">[2]</ref>, adopt the framework of traditional 2D detection, where exhaustive sliding windows in 3D space are utilized as proposals and the task is to select those covering the objects well. The problem is that the 3D space is much larger than the 2D space, which costs much more computation and is not necessary.</p><p>Our first observation is that a 3D coarse structure can be recovered from 2D detection and prior knowledge on the scene. Since state-of-the-art 2D object detection methods can provide 2D bounding boxes with quite a high accuracy, proper utilization of them can significantly reduce the search space, which is already applied in several point cloud based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12]</ref>. Furthermore, with prior knowledge of the auto-driving scenario (e.g. the projection matrix), we can even obtain an approximate 3D bounding box (cuboid) for the object in the 2D box despite the lack of point cloud. Inspired by this, we design an algorithm to efficiently determine a basic cuboid for the predicted object by a 2D detector. Although coarse, the basic cuboid has acceptable accuracy and can guide us to determine the 3D setting, size (height, width, length) and orientation of the object. Thus the basic coarse cuboid is called Guidance by us. <ref type="figure">Figure 2</ref>. An example of the feature representation ambiguity caused by only using 2D bounding box. The 3D boxes vary largely from each other and only the left one is correct, but their corresponding 2D bounding box are exactly the same.</p><p>As our second observation, the underlying 3D information can be utilized by investigating the visible surfaces of the 3D box. Based on the guidance, a further classification for eliminating false positives and appropriate refinement for better localization are necessary in order to achieve high accuracy. However, the information missing when using only the 2D bounding box for feature extraction brings a problem of representation ambiguity. As shown in <ref type="figure">Fig.2</ref>, different 3D boxes varying largely from each other can just have the same corresponding 2D bounding box. Therefore the model will take the same feature as input, but the classifier is expected to predict different confidences for them (high confidence for the left one and low confidences for the others in <ref type="figure">Fig.2</ref>), which is conflict. And the residual (∆x, ∆y and etc.) prediction is also difficult. From only the 2D bounding box, the model can hardly know what the original parameters (of the guidance) are, but it aims to predict the residual based on them. So training is quite ineffective. To handle this problem, we explore the underlying 3D information in the 2D image and propose a new approach that employs features parsed from visible surfaces of the projection of the 3D box. As shown in <ref type="figure">Figure.</ref>1 (c), features in the visible surfaces are extracted respectively and then incorporated, so that structural information is utilized to distinguish different forms of 3D boxes.</p><p>For 3D box refinement, we reformulate the conventional regression form into a classification form, and a qualityaware loss is designed for it, which significantly improves the performance.</p><p>Our main contributions are as follows:</p><p>1. We propose a purely monocular data based approach to efficiently obtain a coarse basic cuboid for the object, based on reliable 2D detection results. The basic cuboid provides a reliable approximation of the location, size, and orientation of the object and works as the guidance for further refinement.</p><p>2. We exploit the potential 3D structural information in the visible surfaces of the projected 3D box on 2D images and propose to utilize the features extracted from these surfaces to overcome the problem of feature ambiguity in previous methods when only features from the 2D box are used. With the fusion of surface features, the model achieves the better ability of judgment and the refinement accuracy is improved.</p><p>3. We design and investigate several methods for refinement. And we draw a conclusion that discrete classification based methods with quality aware loss perform much better than direct regression approaches for the task of 3D box refinement.</p><p>We evaluate our method on the KITTI object detection benchmark <ref type="bibr" target="#b6">[7]</ref>. Experiments show that our method surpasses current state-of-the-art methods using only a single RGB image and is even comparable to those using stereo data. To facilitate comparison with our works, we make our results on val 1 and val 2 available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>As 3D understanding of object and scene is drawing more and more attention. Early works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5]</ref> use low-level feature or statistics analysis to tackle 3D recognition or recover tasks. While the 3D object detection task is more challenging <ref type="bibr" target="#b6">[7]</ref>.</p><p>3D object detection methods can be divided into 3 categories by data, i.e. point cloud, multi-view images (video or stereo data) and monocular image. Point cloud based methods, e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>, can directly fetch the coordinates of the points on the surfaces of objects in 3D space, so they can easily achieve much higher accuracy than the methods without point cloud. Multi-view based methods, e.g. <ref type="bibr" target="#b2">[3]</ref>, can obtain a depth map using the disparity computed from the images of different views. Although point cloud and stereo methods have more accurate information for 3D inference, the equipment of monocular RGB camera is more convenient and much cheaper.</p><p>The works that most related to ours are those using a single RGB image for 3D object detection in autonomous driving scenes. This setting is most challenging for the lack of 3D space information. Many recent works focus on this setting because it is a fundamental problem with great impact. Mono3d <ref type="bibr" target="#b1">[2]</ref> addresses this problem through the usage of 3D sliding windows. It exhaustively samples 3D proposals from several predefined 3D regions where the objects may appear. Then it utilizes complex features of segmentation, shape, context, and location to filter out the impossible proposals and finally select the best candidates by a classifier. The complexity of Mono3d brings a serious problem of inefficiency. Whereas we design a pure projective geometry based method with a reasonable assumption, which can efficiently generate 3D candidate boxes with a much smaller number but even higher accuracy.</p><p>Since state-of-the-art 2D detectors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15]</ref> can provide reliable 2D bounding boxes for objects, several works use 2D box as a prior to reduce the search region of 3D box <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>. <ref type="bibr" target="#b0">[1]</ref> uses a CNN to predict the parts coordinates, visibility and template similarity based on the 2D box, and match the best corresponding 3D template. While <ref type="bibr" target="#b17">[18]</ref> first uses a CNN to predict the size and orientation based on the cropped 2D box region, and then determine the location coordinates by the constraint that the 3D box after projection should tightly fit in the 2D detection box. These methods just extract features from the 2D bounding box, which brings the problem of representation ambiguity. While we utilize surface features to eliminate the problem.</p><p>State-of-the-art monocular based methods pay more attention to the extra 3D information in order to facilitate the detection. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14]</ref> try to utilize more 3D information by learning sub-categories or 3D key-points or parts in their intermediate stages. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> use 2D-3D matching to determine the 3D coordinate of objects. They both need CAD models with extra labels of structure or key-points. <ref type="bibr" target="#b25">[26]</ref> uses the depth information generated from disparity prediction to obtain approximate point cloud, and then use the fusion of 2D box feature and point cloud to determine the 3D box. Although only the monocular image is used in prediction, the training of the disparity model requires stereo data. In contrast to these methods, our work takes advantage of 3D structural information in the monocular image without extra data or labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>We adopt the 3D coordinate system from KITTI data set: the origin of the coordinate is on the camera center; x axis points to right on the 2D image plane; y axis points down; and z axis points to the inner direction orthogonal to the image plane and stands for depth. 3D bounding box is represented as B = (w, h, l, x, y, z, θ, φ, ψ). Here w, h, l are the size of the box (width, height, and length respectively) and x, y, z are the coordinates of the bottom center, which is following the KITTI annotation. The size and center coordinate are measured in meter. θ, φ, ψ are the rotation around y axis, x axis and z axis respectively. Since our target objects are all on the ground, we only consider the θ rotation as all previous works do. 2D bounding box is noted with a specified mark, i.e. B 2d = (x 2d , y 2d , w 2d , h 2d ), where (x 2d , y 2d ) is the center of box. <ref type="figure" target="#fig_4">Fig. 5</ref> shows an overview of the proposed framework. This framework takes a single RGB image as input and consists of the following steps: 1) A CNN based detector is leveraged to obtain reliable 2D bounding boxes and obser-vation orientations of objects. This sub-network is referred as 2D+O subnet. 2) The obtained 2D bounding box and orientation are utilized together with the prior knowledge on the driving scenario to generate a basic cuboid called guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GS3D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>3) The guidance is projected on the image plane. Features are extracted from its 2D bounding box and visible surfaces. These features are fused as the distinguishable structural information for eliminating feature ambiguity. 4) The fused features are used by another CNN called 3D subnet to refine the guidance. The 3D detection is considered as a classification problem and quality aware classification loss is used for learning the classifiers and the CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">2D Detection and Orientation Prediction</head><p>For 2D detection, we modify the faster R-CNN framework by adding a new branch of orientation prediction. The details is illustrated in <ref type="figure" target="#fig_1">Fig.3</ref>.  Specifically, a CNN called 2D+O subnet is used for extracting features from the image, then the region proposal net generates candidate 2D box proposals. From these proposals, ROI-pooling is used for extracting the RoI features, which are then used for classification, bounding box regression, and orientation estimation. The orientation estimated in the 2D+O subnet is the observation angle of the object which is directly related to the appearance of the object. We denote the observation angle as α in order to distinguish it from the global rotation, θ. Both α and θ are annotated in the KITTI data set and their geometry relationship is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Guidance Generation</head><p>Based on reliable 2D detection results, we can estimate a 3D box for each 2D bounding box. Specifically, our target is to obtain the guidance B g = (w g , h g , l g , x g , y g , z g , θ g ), given the 2D box B 2d = (x 2d , y 2d , h 2d , w 2d ), the observation angle α and the camera intrinsic matrix K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Obtaining Guidance Size</head><formula xml:id="formula_0">(w g , h g , l g )</formula><p>In the auto-driving scenario, the distribution of the object sizes for instances of the same category is low-variance and unimodal. Since the object class is predicted by 2D subnet, we simply use the guidance size (w,h,l) of a certain class calculated on the training data for the guidances with the same class. So we have (w g , h g , l g ) = (w,h,l), which is class dependent (class does not appear in the equation for convenient notation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Estimating Guidance Location</head><formula xml:id="formula_1">(x g , y g , z g )</formula><p>As formulated in Section.3, (x g , y g , z g ) is the bottom surface center of the guidance, denoted as C b . So we study the characteristic of the bottom center and propose a wellworked approaches.</p><p>Our estimation approach is based on the discovery in the auto-driving settings. The top center of the object 3D box has a stable projection on the 2D plane that is very close to the top midpoint of the 2D bounding box, and the 3D bottom center has a similar stable projection that is above and close to the 2D bounding box. This discovery can be explained by the fact that the top positions of most objects have the projection that are very close to the vanishing line of the 2D image since the camera is set on the top of the data collecting vehicle and other objects in the driving scenario have similar height to it.</p><p>With the predicted 2D box (x 2d , y 2d , w 2d , h 2d ), where (x 2d , y 2d ) is the box center, we have the top midpoint M 2d t = (x 2d , y 2d − h 2d /2) and bottom midpoint M 2d b = (x 2d , y 2d + h 2d /2). Then we approximately have the homogeneous form of projected top center C 2d</p><formula xml:id="formula_2">t = (M 2d t , 1) = (x 2d , y 2d − h 2d /2, 1) and bottom center C 2d b = (M 2d b , 1) − (0, λh 2d , 0) = (x 2d , y 2d + ( 1 2 − λ)h 2d , 1)</formula><p>, where λ is from the statistics on training data. With the known camera intrinsic matrix K, we can obtain the normalized 3D coordi-natesC b = (x b ,ỹ b , 1) for the guidance bottom center C b andC t = (x t ,ỹ t , 1) for the top center C t as follows:</p><formula xml:id="formula_3">C b = K −1 C 2d b ,C t = K −1 C 2d t .<label>(1)</label></formula><p>If the depth d is known, C b can be obtained by:</p><formula xml:id="formula_4">C b = dC b .<label>(2)</label></formula><p>So our target now is to obtain d. We can calculate the normalized 3D coordinate of top centerC t = (x t ,ỹ t , 1) by Equation <ref type="formula" target="#formula_3">(1)</ref>. With both the bottom center and the top center, we have the normalized heighth =ỹ b −ỹ t . Since the guidance height h g =h is already obtained, we have d = h g /h. And finally we have (x g , y g , z g ) = C b = (dx b , dỹ b , d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Calculating Guidance Orientation θ</head><p>From <ref type="figure" target="#fig_3">Fig.4</ref> we can see that the relationship between the observed angle α and global rotation angle θ is</p><formula xml:id="formula_5">θ = α + arctan x z<label>(3)</label></formula><p>Since x g , z g and α are available through previous estimation, we can obtain θ g by Equation.</p><p>3 now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Surface Feature Extraction</head><p>We use the projected surface regions of the given 3D box (guidance) to extract 3D structure specified features for more accurate determination. An example is illustrated in <ref type="figure" target="#fig_5">Fig.6</ref>, the visible projected surfaces correspond to the top, left side and back of the object shown in light red, green and blue respectively.</p><p>Since all the target objects are on the ground, the bottom surface is always invisible, we use the top surface to extract features. For the other 4 surfaces, the visibility of them can be determined by the observation orientation α of the object. In the KITTI coordinate system illustrated in <ref type="figure" target="#fig_3">Fig.4</ref>, we have α ∈ (−π, π] with the right-hand direction of observer as zero angle (α = 0) and the clockwise direction as positive rotation. So when α &gt; 0 the front surface is visible and when α &lt; 0 the back surface is visible. The right side is visible when − π 2 &lt; α &lt; π 2 , and otherwise the left side is visible.</p><p>Features in visible surface regions are warped to a regular shape (e.g. 5x5 feature map) by perspective transformation. Specifically, for a visible surface F , we first use the camera projection matrix to obtain the quadrilateral F 2d in the image plane and then calculate the scaled quadrilateral F 2d s on the feature map according to the stride of the network. With the coordinates of the 4 corners of F 2d s and the target 4 corners of the 5x5 map, we can obtain the perspective transformation matrix P . Let X, Y represents the feature maps before and after the perspective transformation respectively. The value of the element on Y with coordinate (i,j) is computed by the following equations:</p><formula xml:id="formula_6">Y i,j = X u,v (u, v, 1) = P −1 (i, j, 1)<label>(4)</label></formula><p>Usually (u,v) is not an integer coordinate and we use the 4 nearest integer coordinates with bi-linear interpolation to obtain the value X u,v . The extracted features of visible surfaces are concatenated and we use convolution layers to compress the number of channels and fuse the information on different surfaces. As shown in <ref type="figure" target="#fig_6">Fig.7</ref>, we also extract features from 2D bounding box to provide context information. The 2D box features are concatenated with fused surface features, and they are finally used for refinement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Refinement Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Residual Regression</head><p>With the candidate box (w, h, l, x, y, z, θ) and target ground truth (w * , h * , l * , x * , y * , z * , θ * ), the residuals are encoded as:</p><formula xml:id="formula_7">∆x = x * − x √ l 2 + w 2 , ∆y = y * − y √ l 2 + w 2 , ∆z = z * − z h , ∆l = log( l * l ), ∆w = log( w * w ), ∆h = log( h * h ), ∆θ = θ * − θ<label>(5)</label></formula><p>And the commonly used method is to predict the encoded residuals by regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Classification Formulation</head><p>Regression in a large scope usually performs no better than discrete classification, so we transform the residual regression into a classification formulation for 3D box refinement. The main idea is to divide the residual range into several intervals and classify the residual value into one interval. Denote ∆d i = d gt i −d gd i as the difference of the ith guidance and its corresponding ground-truth 3D setting descriptor d where d ∈ {w, h, l, x, y, z, θ}. The standard deviation σ(d) of ∆d on the training data is calculated. Then we assign (0, ±σ(d), ±2σ(d), ..., ±N (d)σ(d)) as the center for the intervals of descriptor d and each interval has a length of σ(d). N (d) is chosen according to the range of ∆d.</p><p>Since the guidance may come from a false positive 2D box, we treat the intervals as multiple binary classification problems. During training, if the 2D bounding box of the guidance cannot be matched with any ground-truth, the probability for all the intervals will be close to 0. In this way, we can consider the guidance to be a background and reject it during inference if the confidences of all classes are very low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Classification after Shift</head><p>Since mapping 2D regions to 3D space is an underdetermined problem, we further consider starting from deviations directly in the 3D coordinate. Specifically, each class (residual interval) uses the most correlated region (the projection of guidance after corresponding residual shift) to extract individual features for itself. And all the classifiers of residual intervals can share parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Quality Aware Loss</head><p>We expect the confidence predicted in classification to reflect the quality of the target box of corresponding class, so that the more accurate target box gets the higher score. This is important because AP (average precision) is computed by sorting the candidates with respect to their scores. However, the common used 0/1 label is improper for the purpose because the model is forced to predict 1 for all positive candidates regardless of their variation in quality. Inspired by loss in 2D detection <ref type="bibr" target="#b10">[11]</ref>, we change the 0/1 label to a quality aware form:</p><formula xml:id="formula_8">q =      1 if ov &gt; 0.75 0 if ov &lt; 0.25 2ov − 0.5 otherwise (6)</formula><p>where ov is the 3D overlap between the target box and ground-truth. And we use BCE as the loss function:</p><formula xml:id="formula_9">L quality = −[q log(p) + (1 − q) log(1 − p)].<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our framework on KITTI object detection benchmark <ref type="bibr" target="#b6">[7]</ref>. It consists of 7,481 training and 7,518 test images. We follow <ref type="bibr" target="#b0">[1]</ref> to use two train/val splits. Among the previous works, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b17">18]</ref> use val 1 , and <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> use val 2 , and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref> use them both. Our experiments are focused on the car category like most previous works do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Network Setup:</head><p>Both our 2D sub-net and 3D sub-net are based on the VGG-16 <ref type="bibr" target="#b21">[22]</ref> network architecture. The 2D sub-net takes a classification model pre-trained on ImageNet data set to initialize its parameters. And the trained model of 2D sub-net is used to initialize the parameters of 3D sub-net in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Optimization</head><p>We use the Caffe deep learning framework <ref type="bibr" target="#b9">[10]</ref> for training and evaluation. During training, we upscale the image by a factor of 2, and use 4 GPUs with one image on each. We run SGD solver with a base learning rate of 0.001 for the first 30K iterations and reduce it to 0.0001 for another 10K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">2D Detection and Orientation</head><p>Since our efforts are focused on 3D detection, we spare no time for tunning the hyper-parameters (e.g. loss weight, anchor size) for best performance of the 2D model and just train the 2D subnet without bells and whistles. We evaluate the Average Precision (AP) and Average Orientation Similarity (AOS) of our 2D model, following the standard KITTI setup. The results are shown and compared with other stateof-the-art works in <ref type="table">Table.</ref>2. Despite Deep3DBox <ref type="bibr" target="#b17">[18]</ref> with much higher AP, our result is better than or comparable to other works. Moreover, although Deep3DBox use better 2D box for 3D box estimation, our 3D results surpasses theirs by a large margin <ref type="table">(Table.</ref>1), which highlights the strength of our 3D box determination method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Guidance Generation</head><p>Based on the statistics on training data, we setw = 1.62, h = 1.53,l = 3.89 as the size of guidance and λ = 0.07 for the shift of the projected bottom center.</p><p>To better evaluate the accuracy of the guidance, we use the metric of Recall loc as well as Recall 3D . For Recall loc , the Euclidean distance between box centers of candidates and ground truths is calculated, and the ground-truth box is recalled if there is an candidate whose distance from it is within a threshold. While Recall 3D is similar with the criteria changed from distance to 3D overlap.</p><p>As shown in <ref type="table">Table.</ref>3, we also compare our guidance recall with the proposals recall of Mono3D <ref type="bibr" target="#b1">[2]</ref> for their similar roles in the 3D detection framework. The evaluation is performed on val 2 . more efficient than the complex method of proposal generating of Mono3D.</p><p>Note that the number of guidance is just equals to the number of 2D detected boxes, which is of the same order of magnitude as ground-truth. So the Recall 3D of guidance is similar to AP 3D , and our refined 3D boxes can achieve an AP that surpasses the value of guidance Recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Refinement</head><p>The ablation study of the contribution of surface feature, classification formulation and quality aware loss are shown in <ref type="table">Table.</ref>5.</p><p>We first train a baseline model using direct residual regression following previous works e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. And the baseline only uses guidance region (bounding box) features pooled from the feature map of the image.</p><p>Then we adopt the network architecture in <ref type="figure" target="#fig_6">Fig.7</ref> and train a surface feature aware model. With the surface feature providing 3D structurally distinguishable information, the regression accuracy is improved (seen in the line of "+surf").</p><p>For the classification formulated refinement, the distributions of ∆d for each dimension on the training set are analyzed as shown in <ref type="table">Table.</ref>4. As stated in Section.4.5.2, we set the interval length for each dimension as the σ d . And we choose N d = 5 for d ∈ {w, h, l, y, θ} and N x = N z = 10, mainly according to the range over std ratio.</p><p>With the parameters for classes settled, we perform experiments with the classification formulation instead of the direct regression. Comparison experiments using the features after shift for classification are also conducted. In Ta-   <ref type="table">Table 4</ref>. Distribution analysis of ∆d on training data. ble.5, "+cls" and "+scls" represent these two methods respectively. We can see the two class formulated methods both surpass the regression method. The fixed feature based method performs better in AP@0.5, while the shift feature based one performs better in AP@0.7.</p><p>Finally we change the 0-1 label based loss to the quality aware form introduced in Section.4.5.4. Significant gain is achieved in both classification based methods (seen in the line "+qua" of Table.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with Other Methods</head><p>We compare our work with state-of-the-art RGB image based 3D object detection methods: Mono3D <ref type="bibr" target="#b1">[2]</ref>, Deep3DBox <ref type="bibr" target="#b17">[18]</ref>, DeepManta <ref type="bibr" target="#b0">[1]</ref>, MF3D <ref type="bibr" target="#b25">[26]</ref> and 3DOP <ref type="bibr" target="#b2">[3]</ref>.</p><p>Most of these methods requires extra data or label in addition to single RGB image and the KITTI official anno-  <ref type="table">Table 5</ref>. Ablation study of 3D detection results for car category on KITTI val2 set. "Modr" means moderate here. And "+surf", "+cls", "+scls", "+qua" represent the usage of surface feature, class formulation, shift based class formulation and quality aware loss respectively. tation for training. 3DOP is a stereo data based method. Mono3D need segmentation data for the mask prediction. DeepManta need 3D CAD data and vertices for their 3D model prediction. MF3D adopts the model in MonoDepth <ref type="bibr" target="#b7">[8]</ref> for their disparity prediction, which is actually trained on stereo data. Whereas only Deep3DBox, as well as our work, requires no extra data or label.</p><p>AP 3D : The major metric for our 3D detection evaluation is the KITTI official 3D Average Precision (AP 3D ): a detection box is considered as true positive if it has a overlap (IoU) with the ground truth box larger than the threshold IoU=0.7. We also show result comparison with IoU=0.5. As we can see in <ref type="table">Table.</ref>1, our method surpasses other works by a large margin in the official metric (IoU=0.7), while 3DOP has a better performance evaluated with IoU=0.5. This indicates that our method can achieve fine refinement result for certain good guidances but is not good at correcting the largely deviated guidances. The inference time is also shown in this table, which demonstrates the efficiency of our method.</p><p>ALP: Since DeepMANTA only provides their results evaluated in Average Localization Precision (ALP) metric <ref type="bibr" target="#b0">[1]</ref>, we also preform results comparison in this metric. As shown in <ref type="table">Table.</ref>6, our method is outstanding among current state of the art works, except that 3DOP outperforms us in this metric. Since ALP focus only on the location accuracy and the size and rotation is not taken into consideration, its ability of reflecting the true quality of the 3D box may be   <ref type="table">Table 6</ref>. 3D detection for car category evaluated using the metric of ALP . Results on the two validation sets val1 / val2. "Extra" means the extra data or label used in training.</p><p>Results on Test Set: Among all published monocular 3D detection works, only MF3D <ref type="bibr" target="#b25">[26]</ref> shows the results evaluated on the official test set. The comparison between their results and ours is shown in <ref type="table">Table.</ref>7.</p><p>We only submit once so there is no trick of hyperparameter search. But even so, our method outperforms the other work. Note that both the results of MF3D and ours on test set have a gap compared with those on validation set <ref type="table">(Table.</ref>1). And this is most probably caused by the gap of data distribution between training and testing set, since KITTI training set is really small. <ref type="figure" target="#fig_7">Fig. 8</ref> shows some qualitative results of our approach. Our method can handle different scenes. It is robust to object in different distances from the camera. And when the scene is crowded, our method still performs well in most <ref type="bibr">Method</ref> AP 3D (IoU=0.7) Easy Moderate Hard MF3D <ref type="bibr" target="#b25">[26]</ref> 7.08 5.18 4.68 GS3D (Ours) 7.69 6.29 6.16 <ref type="table">Table 7</ref>. Our 3D detection results on official test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Results</head><p>cases. The red box in the two images in the last row shows a typical failure cases of our work. In the left image, the location of the box (in red) of the car on the bottom right corner has an obvious deviation from the true car. In the right image, the red dashed box is mistaken for negative box by our model. Our approach is not good at handling the objects on the boundary of the image (usually with occlusion or truncation). Further efforts is in need to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have proposed a monocular 3D object detection framework for autonomous driving. We utilize the mature 2D detection technology and projection knowledge to efficiently generate basic 3D box called guidance. Based on the guidance, further refinement is performed to achieve high accuracy. We take advantage of potential 3D structure information in surface feature that eliminate the representation ambiguity brought by only using 2D bounding box. And we reformulate the hard residual regression problem into classification, which is easier to be well-trained. And we use a quality aware loss to enhance the discriminative ability of model. Experiment shows that our framework achieves new state-of-the-art as a method using single RGB image without any extra data or label for training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Details of the head of 2D+O subnet. All line connections represent fully connected layers here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Top view of observation angle α and global rotation angle θ. The blue arrows represent the observation axes and the red arrow indicates the heading of the car. Since it is a right-handed coordinate system, the positive direction of rotation is clockwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Overview of the proposed 3D object detection paradigm. A CNN based model (2D+O subnet) is used to obtain a 2D bounding box and observation orientation of the object. The guidance is then generated by our proposed algorithm using the obtained 2D box and orientation with the projection matrix. And features extracted from visible surfaces as well as the 2D bounding box of the projected guidance are utilized by the refinement model (3D subnet). Instead of direct regression, the refinement model adopts classification formulation with the quality-aware loss for a more accurate result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of feature extraction from the projected surfaces of 3D box by perspective transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Details of the head of 3D subnet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative illustration of our 3D detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1. 3D detection accuracy on KITTI for car category evaluated using the metric of AP3D. Results on the two validation sets val1 / val2. "Extra" means the extra data or label used in training. "scls" represents the method using shift feature for classification.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Extra</cell><cell>Time</cell><cell>Easy</cell><cell cols="2">AP3D (IoU=0.5) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP3D (IoU=0.7) Moderate</cell><cell>Hard</cell></row><row><cell cols="2">Deep3DBox [18]</cell><cell cols="2">None</cell><cell>-</cell><cell>27.04/ -</cell><cell cols="2">20.55/ -</cell><cell>15.88/ -</cell><cell>5.85 / -</cell><cell>4.10 / -</cell><cell>3.84 / -</cell></row><row><cell>Mono3D [2]</cell><cell></cell><cell cols="2">Mask</cell><cell>4.2 s</cell><cell cols="2">-/25.19</cell><cell>-/18.20</cell><cell>-/15.52</cell><cell>-/ 2.53</cell><cell>-/ 2.31</cell><cell>-/ 2.31</cell></row><row><cell>3DOP [3]</cell><cell></cell><cell cols="2">Stereo</cell><cell>3 s</cell><cell cols="2">-/46.04</cell><cell>-/34.63</cell><cell>-/30.09</cell><cell>-/ 6.55</cell><cell>-/ 5.07</cell><cell>-/ 4.10</cell></row><row><cell>MF3D [26]</cell><cell></cell><cell cols="2">Stereo</cell><cell>-</cell><cell cols="4">47.88/44.57 29.48/30.03 26.44/23.95</cell><cell>10.53/ 7.85</cell><cell>5.69 / 5.39</cell><cell>5.39 / 4.73</cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">None</cell><cell cols="5">2.3 s 34.72/33.11 30.06/27.16 24.78/23.57</cell><cell>9.12 / 8.71</cell><cell>6.71 / 6.64</cell><cell>6.31 / 6.11</cell></row><row><cell>Ours (scls)</cell><cell></cell><cell cols="2">None</cell><cell cols="5">2.3 s 32.15/30.60 29.89/26.40 26.19/22.89</cell><cell>13.46/11.63 10.97/10.51 10.38/10.51</cell></row><row><cell cols="2">Method</cell><cell></cell><cell cols="2">AP 2D</cell><cell>AOS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mono3D [2]</cell><cell></cell><cell cols="2">-/88.67</cell><cell cols="2">-/86.28</cell><cell></cell><cell></cell></row><row><cell cols="2">3DOP [3]</cell><cell></cell><cell cols="2">-/88.07</cell><cell cols="2">-/85.80</cell><cell></cell><cell></cell></row><row><cell cols="3">Deep3DBox [18]</cell><cell cols="2">97.20/ -</cell><cell>96.68/</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DeepMANTA [1]</cell><cell cols="2">91.01/90.89</cell><cell cols="2">90.66/90.66</cell><cell></cell><cell></cell></row><row><cell cols="2">Ours</cell><cell></cell><cell cols="2">90.02/88.85</cell><cell cols="2">89.13/87.52</cell><cell></cell><cell></cell></row><row><cell cols="8">Table 2. Comparison of 2D detection and orientation results for</cell><cell></cell></row><row><cell cols="8">car category evaluated on val1 / val2 of KITTI data set. Only the</cell><cell></cell></row><row><cell cols="8">results under the moderate criteria, the primal metric of KITTI, are</cell><cell></cell></row><row><cell cols="4">shown for convenient size of table.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">Recall loc thr=2m thr=1m Easy Moderate Hard Recall 3D @IoU=0.5</cell><cell></cell><cell></cell></row><row><cell cols="5">Mono3D [2] 79.10 70.24 29.55</cell><cell>27.72</cell><cell>27.23</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell cols="4">89.80 85.78 35.52</cell><cell>28.74</cell><cell>25.02</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Recallloc and Recall3D of our results compared with Mono3D. The IoU threshold of Recall3D is 0.5. These are evaluated on val2 set.</figDesc><table><row><cell>Dimension</cell><cell>w</cell><cell>h</cell><cell>l</cell><cell>x</cell><cell>y</cell><cell>z</cell><cell>θ</cell></row><row><cell>std</cell><cell cols="3">0.10 0.13 0.41</cell><cell>0.48</cell><cell>0.10</cell><cell>1.65</cell><cell>0.05</cell></row><row><cell>range</cell><cell cols="7">-0.49, -0.44, -1.74, -10.89, -0.52, -12.78, -0.27, 0.40 0.90 1.27 6.22 0.69 27.06 0.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>33.11 27.16 23.57 8.71 6.64 6.11 +surf +scls +qua 30.60 26.40 22.89 11.63 10.51 10.51</figDesc><table><row><cell>Method</cell><cell>AP 3D @IoU=0.5 Easy Modr Hard Easy Modr Hard AP 3D @IoU=0.7</cell></row><row><cell>Baseline</cell><cell>21.66 15.47 14.75 2.75 1.99 1.86</cell></row><row><cell>+surf</cell><cell>25.81 20.41 17.70 3.75 2.99 2.86</cell></row><row><cell>+surf +cls</cell><cell>30.87 23.39 19.86 5.09 3.76 3.63</cell></row><row><cell>+surf +scls</cell><cell>28.57 18.81 17.63 7.41 4.51 4.51</cell></row><row><cell>+surf +cls +qua</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>not as good as 3D overlap. Ours None 71.09/66.23 63.77/58.01 50.97/47.43 Ours (scls) None 67.87/62.56 60.66/53.85 53.53/49.54</figDesc><table><row><cell>Method</cell><cell>Extra</cell><cell>Easy</cell><cell>ALP 1m Moderate</cell><cell>Hard</cell></row><row><cell>3DVP [23]</cell><cell cols="2">None 45.61/ -</cell><cell>34.28/ -</cell><cell>27.72/ -</cell></row><row><cell cols="3">Deep3DBox [18] None 35.71/ -</cell><cell>25.35/ -</cell><cell>23.03/ -</cell></row><row><cell>Mono3D [2]</cell><cell>Mask</cell><cell>-/48.31</cell><cell>-/38.98</cell><cell>-/34.25</cell></row><row><cell cols="5">DeepMANTA [1] CAD 70.90/65.71 58.05/53.79 49.00/47.21</cell></row><row><cell>3DOP [3]</cell><cell>Stereo</cell><cell>-/81.97</cell><cell>-/68.15</cell><cell>-/59.85</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://drive.google.com/file/d/188BxA_ jlhHHpxCXk3SxPBA5qkmk53PIt/view?usp=sharing</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teulière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding bayesian rooms using composite 3d object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bowdish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kermgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3-d shape recovery using distributed aspect matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="174" to="198" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analyzing 3d objects in cluttered images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11590</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IROS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient harmonized singlestage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep supervision with shape concepts for occlusion-aware 3d object parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zoom out-and-in network with map attention decision for region proposal and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="238" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent scale approximation for object detection in cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="571" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5632" to="5640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="924" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating the aspect layout of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3410" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detailed 3d representations for object recognition and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2608" to="2623" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
