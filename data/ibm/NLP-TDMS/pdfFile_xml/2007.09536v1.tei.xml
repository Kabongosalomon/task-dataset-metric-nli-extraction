<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Topic Mining via Joint Spherical Tree and Text Em-bedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu2chaozhang@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Topic Mining via Joint Spherical Tree and Text Em-bedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;20)</title>
						<meeting>the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;20) <address><addrLine>CA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Information systems → Data mining</term>
					<term>Document topic mod- els</term>
					<term>Clustering and classification</term>
					<term>• Computing methodologies → Natural language processing</term>
					<term>KEYWORDS Topic Mining</term>
					<term>Topic Hierarchy</term>
					<term>Text Embedding</term>
					<term>Tree Embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An example of Hierarchical Topic Mining. We aim to retrieve a set of representative terms from a given corpus for each category in a user-provided hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Topic models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, which uncover hidden semantic structure in a text corpus via generative modeling, have proven successful on automatic topic discovery. Hierarchical topic models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref> extend the classical ones by considering a latent topic hierarchy during the corpus generative process, motivated by the fact that topics are naturally correlated (e.g., "sports" is a super-topic of "soccer"). Due to their effectiveness of discovering organized topic structures automatically without human supervision, hierarchical topic models have been applied to a wide range of applications including political text analysis <ref type="bibr" target="#b9">[10]</ref>, entity disambiguation <ref type="bibr" target="#b14">[15]</ref> and relation extraction <ref type="bibr" target="#b0">[1]</ref>.</p><p>Despite being able to learn latent topic hierarchies from text corpora, the applicability of hierarchical topic models to learn a userinterested topic structure is limited seriously by their unsupervised nature: Unsupervised generative models maximize the likelihood of the observed data, tending to discover the most general and prominent topics from a text collection, which may not fit a user's particular interest, or provide a superficial summarization of the corpus. Furthermore, the inference algorithms of topic models yield local optimum solutions, resulting in instability and inconsistency across different runs. This issue even worsens in the hierarchical setting where a larger number of topics and their correlations need to be modeled.</p><p>In many cases, a user is interested in a specific topic structure, or has prior knowledge about the potential topics in a corpus. These topics, based on a user's interest or prior knowledge, may be easily described via a set of category names with a hierarchical structure. Such a user-provided category hierarchy will facilitate a more stable topic discovery process, yielding more desirable and consistent results that better cater to a user's needs. Therefore, we propose a new task, Hierarchical Topic Mining, which takes only a topic hierarchy described by category names as user guidance, and aims to retrieve a set of coherent and representative terms under each category to help users comprehend his/her interested topics. For example, as shown in <ref type="figure">Figure 1</ref>, a user may provide a hierarchy of interested concepts along with a corpus and rely on hierarchical topic mining to retrieve a set of representative terms from a text corpus (e.g., different music and dance genres, terminologies for different sports, as well as general descriptions for internal nodes) that provide a clear interpretation of the categories.</p><p>Several previous studies also focus on guiding topic discovery with word-level supervision. Seed-guided topic modeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> incorporates user-provided seed words to bias the generative process towards seed-related topics. A recent study CatE <ref type="bibr" target="#b23">[24]</ref> learns discriminative text embeddings guided by category names for representative term retrieval. However, none of the above methods handle hierarchical topic structures. Under the hierarchical setting, there are supervised <ref type="bibr" target="#b32">[33]</ref> and semi-supervised <ref type="bibr" target="#b21">[22]</ref> models that leverage category labels of documents to regularize the generative process. However, they rely on a large amount of annotated documents which may be costly to obtain. Under our setting, only a set of easy-to-provide category names that form a topic hierarchy is needed to guide the hierarchical topic discovery process.</p><p>In this paper, we propose JoSH, a novel Joint Spherical tree and text embedding model for Hierarchical Topic Mining. The userprovided category tree structure and text corpus statistics are simultaneously modeled via directional similarity in the spherical space, which facilitates effective estimation of category-word semantic correlations for representative term discovery. To train our model in the spherical space, we develop a principled EM optimization procedure based on Riemannian optimization.</p><p>Our contributions can be summarized as follows.</p><p>(1) We propose a new task for hierarchical topic discovery, Hierarchical Topic Mining, which requires a category hierarchy described by category names as the only supervision to retrieve a set of representative terms per category for effective topic understanding. <ref type="bibr" target="#b1">(2)</ref> We develop a joint embedding framework for hierarchical topic mining by simultaneously modeling the user-provided category tree structure and the text generation process. The model is defined in the spherical space, where directional similarity is employed to characterize semantic correlations among words, documents, and categories for accurate category representative term retrieval. <ref type="bibr" target="#b2">(3)</ref> We develop an EM algorithm to optimize our model in the spherical space that iterates between estimating the latent category of words and maximizing corpus generative likelihood while optimizing the category tree structure in the embedding space. (4) We conduct a comprehensive set of experiments on two public corpora from different domains on Hierarchical Topic Mining. Our model enjoys high efficiency and mines high-quality topics. The embeddings trained by our model can be directly used for weakly-supervised hierarchical text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>Definition 1 (Hierarchical Topic Mining). Given a text corpus D and a tree-structured hierarchy T where each node c i ∈ T is represented by the name of the category, Hierarchical Topic Mining aims to retrieve a set of terms C i = {w 1 , . . . , w m } from D for each category c i ∈ T such that C i provides a clear description of the category c i based on D.</p><p>Connection and difference between Hierarchical Topic Models. Similar to Hierarchical Topic Modeling <ref type="bibr" target="#b3">[4]</ref>, we also aim to capture the hierarchical correlations among topics during topic discovery. However, Hierarchical Topic Mining is weakly-supervised as it requires the user to provide the names of the hierarchy categories which serve as the minimal supervision and focuses on retrieving representative terms only for the provided categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPHERICAL TEXT AND TREE EMBEDDING</head><p>In this section, we introduce our model JoSH which jointly learns text embeddings and tree embeddings in the spherical space, where directional similarity is used to effectively characterize semantic correlations among words, documents and categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Mining representative terms relevant to a given category relies on accurate estimation of semantic similarity, on which directional similarity of text embeddings has proven most effective. For example, cosine similarity is empirically shown <ref type="bibr" target="#b17">[18]</ref> to better characterize word semantic similarity and dissimilarity. Motivated by the effectiveness of directional similarity for text analysis, several recent studies employ the spherical space for topic modeling <ref type="bibr" target="#b2">[3]</ref>, text embedding learning <ref type="bibr" target="#b24">[25]</ref> and text sequence generation <ref type="bibr" target="#b15">[16]</ref>. To learn text embeddings tailored for the given category tree, we propose to jointly embed the tree structure into the spherical space where each category is surrounded by its representative terms. Different from recent hyperbolic tree embedding models, such as Poincaré embedding <ref type="bibr" target="#b29">[30]</ref>, Lorentz model <ref type="bibr" target="#b30">[31]</ref> and hyperbolic cones <ref type="bibr" target="#b8">[9]</ref>, we do not preserve the absolute tree distance in the embedding space, but rather the relative category relationship reflected in the tree structure. For example, in the category hierarchy given in <ref type="figure">Figure 1</ref>, although the tree distance between "sports" and "arts" and that between "baseball" and "soccer" are both 2, the latter pair of categories should be embedded closer than the former pair due to higher semantic similarity. Therefore, the tree distance in the category hierarchy should not be preserved in an absolute manner, but treated as a relative metric, e.g., for category "soccer", its tree distance to "sports" is smaller than that to "baseball", so "soccer" should be embedded closer to "sports" than to "baseball".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spherical Tree Embedding</head><p>We propose a novel tree embedding method that preserves the relative category hierarchical structure in the spherical embedding space, meanwhile encouraging inter-category distinctiveness for clear topic interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>The Flat Case. We start with the simplest case where all categories are parallel and do not exhibit hierarchical structures. We aim to jointly embed categories and their representative terms       such that (1) the representative terms selected for each category 2 are semantically coherent and (2) the categories are distinctive from each other, which allows clear category interpretation. For example, in <ref type="figure">Figure 1</ref>, one can clearly recognize and understand "baseball" and "soccer" thanks to the discriminative terms that are exclusively relevant to the corresponding category. Intra-Category Coherence. The representative terms of each category should be highly semantically relevant to each other, reflected by high directional similarity in the spherical space. To achieve this, we require the embeddings of representative terms to be placed near the category center direction within a local region by maximizing</p><formula xml:id="formula_0">m I K V s S P v Q 1 T S m E S g v n 4 Y b W 8 d a 6 V l h I v W L 0 Z q q v z d y G i k 1 i g I 9 G V E c q H l v I v 7 n d T M M r z w d K s 0 Q Y j Y 7 F G b C w s S a N G X 1 u A S G Y q Q J Z Z L r v 1 p s Q C V l u g V V 1 i U 4 8 5 E X S</formula><formula xml:id="formula_1">7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6 7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6 7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F O t i N e M a P X k c Y 5 + l U d 9 T X K Z 5 h m U = " &gt; A A A B 2 X i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b n Q p u H F Z w b Z C O 5 R M 5 k 4 b m s k M y R 2 h D H 0 B F 2 5 E f C 9 3 v o 3 p z 0 J b D w Q + z k n I v S c u l L Q U B N 9 e b W d 3 b / + g f u g f N f z j k 9 N m o 2 f z 0 g j s i l z l 5 j n m F p X U 2 C V J C p 8 L g z y L F f b j 6 f 0 i 7 7 + g s T L X T z Q r M M r 4 W M t U C k 7 O 6 o y a r a A d L M W 2 I V x D C 9 Y a N b + G S S 7 K D D U J x a 0 d h E F B U c U N S a F w 7 g 9 L i w U X U z 7 G g U P N M 7 R R t R x z z i 6 d k 7 A 0 N + 5 o Y k v 3 9 4 u K Z 9 b O s t j d z D h N 7 G a 2 M P / L B i W l t 1 E l d V E S a r H 6 K C 0 V o 5 w t d m a J N C h I z R x w Y a S b l Y k J N 1 y Q a 8 Z 3 H Y S b G 2 9 D 7 7 o d u m I e A 6 j D O V z A F Y R w A 3 f w A B 3 o g o A E X u H d</formula><formula xml:id="formula_2">U e X 9 G t W F K P t t 5 T i O B x 5 J l j G D r r N j 3 R 4 n i q Z k L t 5 V k E b P Y b w X t Y D l o W 4 R r 0 Y L 1 d G L / a 5 Q q U g g q L e H Y m G E Y 5 D Y q s b a M c L p o j A p D c 0 y m e E y H T k o s q I n K Z f M F u n J O i j K l 3 Z I W L d 3 f N 0 o s T F X O n R T Y T s x m V p n / Z c P C Z n d R y W R e W C r J 6 q G s 4 M g q V G F A K d O U W D 5 3 A h P N X F d E J l h j Y h 2 s h o M Q b n 5 5 W / R u 2 q E j 8 x R A H S 7 g E q 4 h h F u 4 h 0 f o Q B c I z O A F 3 u D d K 7 1 X 7 2 O F q + a t u Z 3 D n / E + f w D R i Z K c &lt; / l a t e x i t &gt; &lt; l a</formula><formula xml:id="formula_3">U e X 9 G t W F K P t t 5 T i O B x 5 J l j G D r r N j 3 R 4 n i q Z k L t 5 V k E b P Y b w X t Y D l o W 4 R r 0 Y L 1 d G L / a 5 Q q U g g q L e H Y m G E Y 5 D Y q s b a M c L p o j A p D c 0 y m e E y H T k o s q I n K Z f M F u n J O i j K l 3 Z I W L d 3 f N 0 o s T F X O n R T Y T s x m V p n / Z c P C Z n d R y W R e W C r J 6 q G s 4 M g q V G F A K d O U W D 5 3 A h P N X F d E J l h j Y h 2 s h o M Q b n 5 5 W / R u 2 q E j 8 x R A H S 7 g E q 4 h h F u 4 h 0 f o Q B c I z O A F 3 u D d K 7 1 X 7 2 O F q + a t u Z 3 D n / E + f w D R i Z K c &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " C i f n h 6 y U U G D m V 1 K u S r S 7 X H O y S D E = " &gt; A A A B + X i c b V C 7 T s M w F L 0 p r 1 J e A U Y W i w q J q U p Y Y K z E w l g k + p D a K H I c p 7 X q 2 J H t V K q i / g k L A w i x 8 i d s / A 1 O m w F</formula><formula xml:id="formula_4">6 R X K p B h D X l T N C u Y Y b T Q a Y o T i N O + 9 H 0 v v T 7 M 6 o 0 k + L J z D M a p H g s W M I I N l Y K X X c U S R 7 r e W q v g i x C F r p N r + U t g T a J X 5 E m V O i E 7 t c o l i R P q T C E Y 6 2 H v p e Z o M D K M M L p o j H K N c 0 w m e I x H V o q c E p 1 U C y T L 9 C V V W K U S G W P M G i p / t 4 o c K r L c H Y y x W a i 1 7 1 S / M 8 b 5 i a 5 C w o m s t x Q Q V Y P J T l H R q K y B h Q z R Y n h c 0 s w U c x m R W S C F S b G l t W w J f j r X 9 4 k v Z u W b 5 t 5 9 J p t r 6 q j D h d w C d f g w y 2 0 4 Q E 6 0 A U C M 3 i G V 3 h z C u f F e X c + V q M 1 p 9 o 5 h z 9 w P n 8 A K b K T 6 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6 7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6 7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6 7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6 7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6 7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6 7 W u a K 0 A 6 R X K p + h D X l T N C O Y Y b T f q Y o T i N O e 9 H k v v R 7 U 6 o 0 k + L J z D I a p H g k W M I I N l Y K X X c Y S R 7 r W W q v g s x D F r o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 C w o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 v G m 0 v K q O G p z D B V y B D 7 f Q g g d o Q w c I T O E Z X u H N K Z w X 5 9 3 5 W I 5 u O N X O G f y B 8 / k D K v K T 7 g = = &lt; / l a t e x i t &gt; c j</formula><formula xml:id="formula_5">L intra = c i ∈ T w j ∈ C i min(0, u ⊤ w j c i − m intra ),<label>(1)</label></formula><p>where u w is the word embedding of w; c i is the category center vector of c i . Note that u ⊤ w j c i = cos(u w j , c i ) since the vectors reside on the unit sphere S p−1 ⊂ R p . We set m intra = 0.9 which works well in general since it requires high cosine similarity between representative words and the category center.</p><p>When L intra is maximized (i.e., ∀w j ∈ C i , u ⊤ w j c i ≥ m intra ), the representative word embeddings of the corresponding category reside in a spherical sector centered around the category center vector.</p><p>Inter-Category Distinctiveness. We would like to encourage distinctiveness across different categories to avoid semantic overlaps so that the retrieved terms provide a clear and distinctive description of the category. To accomplish this, we enforce inter-category directional dissimilarity by requiring the cosine distance between <ref type="bibr" target="#b1">2</ref> We will discuss how to select representative terms in <ref type="bibr">Section 4.</ref> any two categories to be larger than m inter , i.e.,</p><formula xml:id="formula_6">∀c i , c j (c i c j ), 1 − c ⊤ i c j &gt; m inter .</formula><p>Therefore, we maximize the following objective:</p><formula xml:id="formula_7">L inter = c i ∈T c j ∈T\{c i } min(0, 1 − c ⊤ i c j − m inter ).<label>(2)</label></formula><p>We will introduce how to set m inter in Section 3.2.2. <ref type="figure" target="#fig_6">Figure 2(a)</ref> shows the configuration of category center vectors upon enforcing intra-category coherence and inter-category distinctiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Recursive Local Tree</head><p>Embedding. We generalize the ideas in the flat case to the hierarchical case and recursively embed local structures of the category tree such that the relative category relationship is preserved.</p><p>We first define the local tree structure that we work with at each recursive step: Definition 2 (Local Tree). A local tree T r rooted at node c r ∈ T consists of node c r and all its direct children nodes.</p><p>Preserving Relative Tree Distance Within Local Trees. Without a hierarchical structure, pairwise category distance is enforced by Eq. (2). With a local tree structure, the category distance in the embedding space should reflect the tree distance in a comparative way. Specifically, since the tree distance between two children nodes is larger than that between a children node and the parent node, a category should be closer to its parent category than to its sibling categories in the embedding space. To achieve this property, we employ the following objective for categories in a local tree T r :</p><formula xml:id="formula_8">L inter = c i ∈T r \{c r } c j ∈T r \{c r ,c i } min(0, c ⊤ i c r − c ⊤ i c j −m inter ),<label>(3)</label></formula><p>which generalizes Eq. (2) by forcing the directional similarity between a children category center vector and its parent category center vector to be higher than that between two sibling categories by m inter . Maximizing L inter results in two favorable tree embedding properties: (1) The children categories are placed near the parent category (by requiring higher value of c ⊤ i c r ), which reflects the semantic correlation between a sub-category and a super-category;</p><p>(2) Any two sibling categories are well-separated (by requiring lower value of c ⊤ i c j ), which encourages distinction between sibling categories (e.g., "baseball" vs. "soccer"). Recursively Embed Local Trees. We apply the idea of local tree embedding recursively to embed the entire category tree structure in a top-down manner: We first embed the local tree rooted at the ROOT node, and then proceed to the next level to embed the local trees of every node at the current level. We repeat this process until we reach the leaf nodes. <ref type="figure" target="#fig_6">Figures 2(b)</ref> and 2(c) illustrate the recursive embedding procedure, which can be realized via the following holistic objective which combines the objectives of every local tree:</p><formula xml:id="formula_9">L tree = c r ∈ T c i ∈ T r \{c r } c j ∈ T r \{c r ,c i } min(0, c ⊤ i c r − c ⊤ i c j − m inter ).</formula><p>We note that m inter needs to be set differently for different levels: As <ref type="figure" target="#fig_6">Figure 2(c)</ref> shows, the sibling categories are embedded in more localized regions as we proceed to the lower levels of the hierarchy to reflect their intrinsic semantic similarity. As a result, for each level L of T , we set m inter (L) to be the average difference between children-parent and inter-sibling embedding similarity across level L, i.e.,</p><formula xml:id="formula_10">m inter (L) = 1 N L c r ∈L c i ∈ T r \{c r } c j ∈ T r \{c r ,c i } c ⊤ i c r − c ⊤ i c j ,</formula><p>where N L is the total number of sibling pairs within each local tree in level L. For the simplicity of notations, we omit the argument of m inter in the rest of the paper, but it should be kept in mind that m inter is level-dependent. Finally, after embedding the category tree, we use the same objective as Eq. (1) to encourage intra-category coherence of retrieved terms so that the category embedding configuration can effectively guide the text embeddings to fit the tree structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spherical Text Embedding via Modeling Conditional Corpus Generation</head><p>We introduce how to learn text embeddings tailored for the given category hierarchy T in the spherical space by modeling the corpus generation process conditioned on the categories. Specifically, we assume the corpus D is generated following a three-step process:</p><p>(1) First, each document d i ∈ D is generated conditioned on one of the categories in the category hierarchy T . Since a category can cover a wide range of semantics, it is natural to model a category as a distribution in the embedding space instead of as a single vector. Therefore, we extend the previous representation of a category c i from a single center vector c i to a spherical distribution centered around c i , i.e., a von Mises-Fisher (vMF) distribution. Specifically, the vMF distribution of a category is parameterized by a mean vector c i and a concentration parameter κ c i . The probability density closer to c i is greater and the spread is controlled by κ c i . Formally, a unit random vector x ∈ S p−1 ⊂ R p has the p-variate vMF distribution vMF p (x; c i , κ c i ) if its probability density function is</p><formula xml:id="formula_11">f (x; c i , κ c i ) = n p (κ c i ) exp κ c i · cos(x, c i ) ,</formula><p>where ∥c i ∥ = 1 is the center direction, κ c i ≥ 0 is the concentration parameter, and the normalization constant n p (κ c i ) is given by</p><formula xml:id="formula_12">n p (κ c i ) = κ p/2−1 c i (2π ) p/2 I p/2−1 (κ c i ) ,</formula><p>where I r (·) represents the modified Bessel function of the first kind at order r . We define the generative probability of each document d i conditioned on its corresponding true category c i to be:</p><formula xml:id="formula_13">p(d i | c i ) = vMF(d i ; c i , κ c i ) = n p (κ c i ) exp κ c i · cos(d i , c i ) , (4)</formula><p>where d i is the document embedding of d i .</p><p>However, modeling category distribution via Eq. (4) is not directly helpful for our task, since our goal is to discover representative terms rather than documents for each category. For this reason, we further decompose p(d i | c i ) into category-word distribution:</p><formula xml:id="formula_14">p(d i | c i ) ∝ w j ∈d i p(w j | c i ) ∝ w j ∈d i vMF(u w j ; c i , κ c i ), (5)</formula><p>where each word is assumed to be generated independently based on the document category. Eq. (5) allows direct modeling of p(w j | c i ), from which category representative terms will be derived.</p><p>(2) Second, each word w j is generated based on the semantics of the document d i . Intuitively, higher directional similarity implies higher semantic coherence, thus higher probability of co-occurrence. We assume the probability of w j appearing in document d i to be:</p><formula xml:id="formula_15">p(w j | d i ) ∝ exp(cos(u w j , d i )).<label>(6)</label></formula><p>(3) Third, surrounding words w j+k in the local context window (−h ≤ k ≤ h, k 0, h is the local context window size) of w j are generated conditioned on the semantics of the center word w j . Similar to (2), we assume the probability of w j+k appearing in the local context window of w j to be:</p><formula xml:id="formula_16">p(w j+k | w j ) ∝ exp(cos(v w j+k , u w j )),<label>(7)</label></formula><p>where v w is the context word representation of w.</p><p>We summarize how the above three steps jointly model the text generation process by capturing both global and local textual contexts, conditioned on the given categories: Step (1) draws a connection between each document and one of the categories in T (i.e., topic assignment).</p><p>Step (2) models the semantic coherence between a word and the document it appears in (i.e., global contexts).</p><p>Step (3) models the semantic correlations of co-occurring words within a local context window (i.e., local contexts). We note that all three steps use directional similarity to model the correlations among categories, documents, and words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZATION</head><p>In this section, we introduce the optimization procedure for learning embedding in the spherical space via our model defined in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We first summarize the objectives of our optimization problem as follows (the derivation is based on maximum likelihood estimation; details can be found at Appendix B):</p><formula xml:id="formula_17">L = L tree + L text , L tree = c r ∈ T c i ∈ T r \{c r } c j ∈ T r \{c r ,c i } min(0, c ⊤ i c r − c ⊤ i c j − m inter ).<label>(8)</label></formula><formula xml:id="formula_18">L text = d i ∈ D w j ∈d i w j +k ∈d i −h ≤k ≤h,k 0 min 0, v ⊤ w j+k u w j + u ⊤ w j d i − v ⊤ w j+k u w ′ j − u ⊤ w ′ j d i − m + c i ∈ T w j ∈ C i log n p (κ c i ) + κ c i u ⊤ w j c i 1(u ⊤ w j c i &lt; m intra ). (9) s.t . ∀w, d, c, ∥u w ∥ = ∥v w ∥ = ∥d ∥ = ∥c ∥ = 1, κ c ≥ 0,</formula><p>where 1(·) is the indicator function; we set m = 0.25.</p><p>We note that our objective contains latent variables, i.e., the second term in Eq. (9) requires knowledge about the latent category of words. At the beginning, we only know that the category name provided by the user belongs to the corresponding category (e.g., w sports ∈ C sports ). The goal of Hierarchical Topic Mining is to discover the latent category assignment of more words such that they form a clear description of the category.</p><p>To solve the optimization problem involving latent variables, we develop an EM algorithm that iterates between the estimation of the latent category assignment of words (i.e., E-Step) and maximization of the embedding training objectives (i.e., M-Step). We detail the design of the EM algorithm below: E-Step. We update the estimation of words assigned to each category by</p><formula xml:id="formula_19">C (t ) i ← Top t {w }; u (t ) w , c (t ) i , κ (t ) c i ,<label>(10)</label></formula><p>where Top t ({w }; u w , c i , κ c i ) denotes the set of terms ranked at the top t positions according to vMF(u w ; c i , κ c i ) (i.e., we assign the t terms to c i that are most likely generated from its current estimated category distribution). In practice, we find that gradually increasing t (i.e., set t = 1 at the first iteration where C <ref type="bibr" target="#b0">(1)</ref> i contains only the category name w c i by initializing c</p><formula xml:id="formula_20">(1) i = u (1) w c i</formula><p>; increment t by 1 for the following iterations) works well. Therefore, here t also denotes the iteration index.</p><p>Note here that we only update the estimation of category assignment for the top t words per category, which will become the representative terms retrieved. The reason is that most of the terms in the vocabulary are not representative for any of the categories; assigning them to one of the category will have negative impact on accurate estimation of the category distribution. M-Step. We update the text embeddings and category embeddings by maximizing L tree and L text :</p><formula xml:id="formula_21">Θ (t +1) ← arg max L text Θ (t ) + L tree Θ (t ) ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_22">Θ (t ) = u (t ) w , v (t ) w , d (t )</formula><p>, c (t ) . Eq. (11) requires non-Euclidean stochastic optimization methods, which will be introduced in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Riemannian Optimization</head><p>Embedding learning is usually based on stochastic optimization techniques, but Euclidean optimization methods like SGD cannot be directly applied to our case, because the Euclidean gradient provides update directions in a non-curvature space, while the embeddings in our model must be updated on the spherical surface S p−1 with constant positive curvature.</p><p>For the above reason, we apply the Riemannian optimization method in the spherical space as described in <ref type="bibr" target="#b24">[25]</ref> to train text and tree embeddings. Specifically, the Riemannian gradient of a parameter θ is computed as</p><formula xml:id="formula_23">grad L(θ ) I − θθ ⊤ ∇L(θ ),</formula><p>where ∇L(θ ) is the Euclidean gradient of θ . For example, the Riemannian gradient of u w is computed as</p><formula xml:id="formula_24">grad L(u w j ) = I − u w j u ⊤ w j c ∈T 1(w j ∈ C)κ c c+ d i ,w j+k 1(pos d i ,w j ,w j+k − neg &lt; m) v w j+k + d i ,</formula><p>where 1(w j ∈ C) is the indicator function of whether w j belongs to category c; 1(pos d i ,w j ,w j +k − neg &lt; m) is the indicator function of whether the margin of the positive tuple over the negative one is achieved. The Riemannian gradient of the other embeddings can be derived similarly. Since we aim to maximize our objective, we update the parameters following the Riemannian gradient direction:</p><formula xml:id="formula_25">θ (t +1) ← R θ (t ) α · grad L θ (t ) ,</formula><p>where α is the learning rate; R x (z) is a first-order approximation of the exponential mapping at x which maps the updated parameters back to the sphere. We follow the definition in <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_26">R x (z) x + z ∥x + z∥ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Algorithm</head><p>We summarize the overall algorithm of Hierarchical Topic Mining in Algorithm 1.</p><p>Complexity. We analyze the computation cost of our algorithm with respect to the tree size n. The tree embedding objective (Eq. (8)) loops over every local tree T r ∈ T and every pair of sibling nodes in T r . Since the number of local trees is upper bounded by the number of total tree nodes, the complexity is O(nB 2 ) where B is the maximum branching factor in T . The text embedding objective (Eq. (9)) pushes each representative term into the spherical sector centered around the category center vector, whose complexity is O(nK). Overall, our algorithm scales linearly with the tree size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct empirical evaluations to demonstrate the effectiveness of our model. We also carry out case studies to </p><formula xml:id="formula_27">C i | n i=1 . u w , v w , d, c ← random initialization on S p−1 ; t ← 1; C (1) i ← w c i | n i=1</formula><p>▷ initialize with category names; while t &lt; K + 1 do t ← t + 1; // Representative term retrieval; show how the joint embedding space effectively models category tree structure and textual semantics.</p><formula xml:id="formula_28">C (t ) i | n i=1 ← Eq. (10) ▷ E-Step; // Embedding training; u w , v w , d, c ← Eq. (11) ▷ M-Step; for i ← 1 to n do C (t ) i ← C (t ) i \ {w c i } ▷ exclude category names; Return C (t ) i | n i=1 ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Datasets. We use two datasets from different domains with groundtruth category hierarchy: (1) The New York Times annotated corpus (NYT) <ref type="bibr" target="#b33">[34]</ref>; (2) arXiv paper abstracts (arXiv) <ref type="bibr" target="#b2">3</ref> . For both datasets, we first select the major categories (with more than 1, 000 documents) and then collect documents with exactly one ground truth category label. The dataset statistics can be found at <ref type="table" target="#tab_1">Table 1</ref>. Implementation Details and Parameters. We pre-process the corpora by discarding infrequent words that appear less than 5 times. We use AutoPhrase <ref type="bibr" target="#b34">[35]</ref> to extract quality phrases, which are treated as single words during embedding training. For fair comparisons with baselines, we set hyperparameters as below for all methods: Embedding dimension p = 100; local context window size h = 5; number of representative terms to retrieve per category K = 5; learning rate α is set to be 0.025 initially with linear decay. Other parameters (if any) are set to be the default values of the corresponding algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hierarchical Topic Mining</head><p>Compared Methods. We compare our model with the following baselines including unsupervised/seed-guided hierarchical topic models and unsupervised/seed-guided text embedding models. For baseline methods that require the number of topics n L at each level L as input, we vary n L in [n L , 2n L , . . . , 5n L ] where n L is the actual <ref type="bibr" target="#b2">3</ref> Data crawled from https://arxiv.org/. number of categories at level L and report the best performance of the method.</p><p>• hLDA <ref type="bibr" target="#b3">[4]</ref>: hLDA is a non-parametric hierarchical topic model. It assumes that documents are generated from the word distribution of a path of topics induced by the nested Chinese restaurant process.</p><p>Since hLDA is unsupervised and cannot take given category names as supervision, we manually match the most relevant topics to the provided category hierarchy.</p><p>• hPAM <ref type="bibr" target="#b28">[29]</ref>: hPAM generalizes the Pachinko Allocation Model <ref type="bibr" target="#b18">[19]</ref> by sampling topic paths from the Dirichlet-multinomial distributions of internal nodes. We perform manual matching of topics as we do for hLDA.</p><p>• JoSE <ref type="bibr" target="#b24">[25]</ref>: JoSE trains spherical text embeddings with Riemannian optimization. It outperforms Euclidean embedding models on textual similarity measurement. We retrieve the nearest-neighbor words of the category name in the spherical space as category representative words.</p><p>• Poincaré GloVe <ref type="bibr" target="#b35">[36]</ref>: Poincaré GloVe learns hyperbolic word embeddings based on the Euclidean GloVe model. It naturally encodes the latent hierarchical word semantic correlations (e.g., hypernymhyponym). We retrieve the nearest-neighbor words of the category name in the Poincaré space as category representative words.</p><p>• Anchored CorEx <ref type="bibr" target="#b7">[8]</ref>: CorEx discovers informative topics via total correlation maximization and can naturally model topic hierarchy via latent factor dependencies. Its anchored version incorporates user-provided seed words by balancing between compressing the original corpus and preserving anchor words related information. We provide the category names as seed words.</p><p>• CatE <ref type="bibr" target="#b23">[24]</ref>: CatE takes category names as input and learns discriminative text embeddings by enforcing distinctiveness among categories. We recursively run CatE on local trees since CatE assumes that the provided categories are mutually-exclusive semantically.</p><p>Quantitative Evaluation. We apply two metrics on the top-K (K = 5 in our experiments) words/phrases retrieved under each category to evaluate all methods: Topic coherence (TC) and Mean accuracy (MACC) as defined in <ref type="bibr" target="#b23">[24]</ref>. The accuracy metric is obtained from the averaged results given by five graduate students who independently label whether each retrieved term is highly relevant to the corresponding category. The quantitative results are reported in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Qualitative Results. We demonstrate the qualitative results of NYT in <ref type="figure" target="#fig_7">Figure 3</ref> and arXiv in <ref type="figure" target="#fig_9">Figure 5</ref> in Appendix A. Words in blue boxes are input category names; words in white boxes are retrieved representative terms of the corresponding category. Run Time. Since topic discovery is usually performed on largescale text corpus, algorithm efficiency is of great importance. Therefore, we report the run time of all methods in <ref type="table" target="#tab_4">Table 3</ref>. JoSH takes only slightly longer to train than JoSE, which only learns text embeddings.</p><p>Discussions. The two unsupervised baselines (hLDA and hPAM) do not perform well. Despite running with different parameters for multiple times, they still fail to generate high quality topics similar to the ground-truth category hierarchy, showing the limitations of unsupervised approaches. For the two unsupervised embedding baselines, JoSE outperforms Poincaré GloVe by a large margin, demonstrating that the spherical space is more suitable than the hyperbolic space on capturing textual semantic correlations for category representative term retrieval. CatE has strong performance on the two datasets, but it has to be run recursively on each set of sibling nodes since it requires all the input categories to be mutually exclusive. Therefore, run time will become a potential bottleneck of applying CatE to large-scale hierarchies. JoSH not only outperforms all models on Hierarchical Topic Mining quality, but also enjoys high efficiency via efficient joint modeling of category tree structure and text corpus statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Weakly-Supervised Hierarchical Text Classification</head><p>Hierarchical Topic Mining is also closely related to the task of text classification. Intuitively, having a good understanding of topics should lead to better categorization of documents. Similar to Hierarchical Topic Mining, the input to weakly-supervised hierarchical classification is also a word-described category tree. Since weakly-supervised classification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref> does not require training documents, it is especially favorable when manual annotation is expensive. Compared Methods. We compare the following weakly-supervised hierarchical models on their classification performance, evaluated on the two datasets.</p><p>• WeSHClass <ref type="bibr" target="#b26">[27]</ref>: WeSHClass leverages the provided keywords of each category to generate a set of pseudo documents for pretraining a hierarchical deep classifier, and self-trains the ensembled local classifiers on unlabeled data. It uses Word2Vec <ref type="bibr" target="#b27">[28]</ref> as word representation.</p><p>• JoSH: Since our model makes explicit generative assumption between topics and documents (Eq. (4)), we are able to build a generative classifier by assigning the document to the category with the highest probability that it gets generated from, i.e.,</p><formula xml:id="formula_29">y d = arg max c vMF(d; c, κ c ),</formula><p>where y d is the predicted category label for document d. • WeSHClass + CatE <ref type="bibr" target="#b23">[24]</ref>: It is shown in <ref type="bibr" target="#b23">[24]</ref> that the learned discriminative text embedding can be used as input feature to benefit classification model. We replace the Word2Vec embedding used in WeSHClass with CatE embeddings.</p><p>• WeSHClass + JoSH: We replace the Word2Vec embedding used in WeSHClass with word embeddings learned by JoSH. Since JoSH effectively leverages the category tree structure to guide text embedding configuration, it is expected to benefit hierarchical classification model as input features.</p><p>Quantitative Evaluation. We use two metrics for classification evaluation, Macro-F1 and Micro-F1, which are commonly used in multi-class classification evaluations. The results are reported in <ref type="table" target="#tab_5">Table 4</ref>.</p><p>Discussions. We demonstrate two potential usage of JoSH in weaklysupervised hierarchical text classification: (1) Directly build a generative classifier based on the model assumption;</p><p>(2) Use the learned embedding as input features to existing classification models. JoSH alone as a generative classifier even outperforms the WeSHClass model; when used as features to WeSHClass, JoSH significantly boosts the classification performance, proved to be more effective than CatE which does not model the category hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Joint Embedding Space Visualization</head><p>To understand how categories and words are distributed in the joint embedding space and how the category tree structure is modeled, we apply t-SNE <ref type="bibr" target="#b20">[21]</ref> to visualize the embedding space in <ref type="figure" target="#fig_8">Figure 4</ref>. Representative terms surround their category centers; sub-categories surround their super-categories which form a category tree structure. An interesting observation is that some subcategories under different super-categories are embedded closer, e.g., in <ref type="figure" target="#fig_8">Figure 4</ref>(b), "optimization" under "math" and "algorithm" under "computer science". Indeed, these two sub-categories are somewhat cross-domain-"optimization" and "algorithm" are relevant to both mathematics and computer science. This shows that JoSH not only models the given category tree structure, but also captures semantic correlation among categories via jointly training tree and text embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK 6.1 Hierarchical Topic Modeling</head><p>Hierarchical topic models extend their flat counterparts by capturing the correlations among topics and generate topic hierarchies. hLDA <ref type="bibr" target="#b3">[4]</ref> generalizes LDA <ref type="bibr" target="#b5">[6]</ref> with a non-parametric probabilistic model, the nested Chinese restaurant process, which induces a path from the root topic to a leaf topic. The documents are assumed to be generated by sampling words from the topics along this path.   Another famous hierarchical topic model, hPAM <ref type="bibr" target="#b28">[29]</ref> is built on the Pachinko Allocation Model <ref type="bibr" target="#b18">[19]</ref> which models documents as a mixture of distributions over a set of topics; the co-occurrences of topics are further represented via a directed acyclic graph. hPAM represents the topic hierarchical structure through the Dirichletmultinomial parameters of the internal node distributions. There are also supervised hierarchical topic models. HSLDA <ref type="bibr" target="#b32">[33]</ref> extends sLDA <ref type="bibr" target="#b4">[5]</ref> by incorporating a breadth first traversal in the label space during document generation. SSHLDA <ref type="bibr" target="#b21">[22]</ref> is a semi-supervised hierarchical topic model that not only explores new latent topics in the label space, but also makes use of the information from the hierarchy of observed labels. A seed-guided topic modeling framework, CorEx <ref type="bibr" target="#b7">[8]</ref>, learns informative topics that maximize total correlation. It is similar to our setting as it incorporates seed words by preserving seed relevant information. CorEx is able to generate topic hierarchy via latent factor dependencies. Different from the previous unsupervised and supervised topic models, our framework takes as guidance only a category hierarchy described by category names, and models category-word semantic correlation via joint spherical text and tree embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Text Embedding and Tree Embedding</head><p>Text embeddings <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref> effectively capture textual semantic similarity via distributed representation learning of words, phrases, sentences, etc. Several topic modeling frameworks, such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> leverage text embeddings to model contextualized semantic similarity of words, making up the bag-of-words generative assumption in classical topic models. Poincaré GloVe <ref type="bibr" target="#b35">[36]</ref> adapts the original GloVe model by training word embedding in the Poincaré space where the latent hierarchical semantic relations between words are naturally captured. A recent text embedding model CatE <ref type="bibr" target="#b23">[24]</ref> proposes to learn discriminative text embeddings for category representative term retrieval given a set of category names as user guidance, which is similar to our setting. CatE makes mutual exclusive assumption on category semantics, which does not hold when categories exhibit a hierarchical structure. None of the previous text embedding framework is able to model a given hierarchical category structure in the embedding space to guide text embedding learning. With the recent advances in hyperbolic embedding space, several frameworks have been developed to model tree structures. Poincaré embedding <ref type="bibr" target="#b29">[30]</ref> learns to model hierarchical structure in the Poincaré ball. Since the embedding distance directly corresponds to tree distance, Poincaré embedding can be used to infer lexical entailment relationship by embedding the tree structure of WordNet or perform link prediction by embedding networks. Later, Lorentz model <ref type="bibr" target="#b30">[31]</ref> brings a more principled optimization approach in the hyperbolic space to learn tree structures; hyperbolic cones <ref type="bibr" target="#b8">[9]</ref> are proposed to model hierarchical relations and admit an optimal shape with a closed form expression. These hyperbolic tree embedding methods, however, are not suitable for embedding category trees in a joint space with words. The reason is that hyperbolic embeddings preserve the absolute tree distance, i.e., similar embedding distances imply similar tree distances. In a category tree, lower-level sibling categories are generally more semantically similar than higher-level ones despite the same tree distance. Therefore, category embedding distances should not be solely determined by tree distances. In our model, text and category tree are jointly embedded, allowing the tree structure to better reflect the textual semantics of the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we propose a new task for hierarchical topic discovery guided by a user-provided category tree described with category names only. To effectively model the category tree structure while capturing text corpus statistics, we propose a joint spherical space embedding model JoSH that uses directional similarity to characterize semantic correlations among words, documents, and categories. We develop an EM algorithm based on Riemannian optimization for training the model in the spherical space. JoSH mines high-quality topics and enjoys high efficiency. We also show that JoSH can be applied to the task of weakly-supervised hierarchical classification, serving as either a generative classifier on its own, or input features to existing classification models.</p><p>In the future, we aim to extend JoSH to not only focus on a usergiven category structure, but also be able to discover other latent topics from a text corpus, probably by relaxing the assumption that a document is generated from one of the given topics or collaborating with other taxonomy construction algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Also, the promising results of our joint spherical space embedding model may shed light on future studies of embedding tree or graph structures along with textual data in the spherical space for mining structured knowledge from text corpora. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DERIVATION OF OBJECTIVE</head><p>The derivation of Eqs. <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula">(9)</ref> is provided as follows.</p><p>The conditional likelihood of the corpus given the category hierarchy is obtained by combining the assumptions described in Eqs. (5), (6) and <ref type="formula" target="#formula_16">(7)</ref>:</p><formula xml:id="formula_30">P(D | T ) = d i ∈ D p(d i | c i ) w j ∈d i p(w j | d i ) w j+k ∈d −h ≤k ≤h,k 0 p(w j+k | w j ) ∝ d i ∈ D w j ∈d i p(w j | c i )p(w j | d i ) w j+k ∈d i −h ≤k ≤h,k 0 p(w j+k | w j ),<label>(12)</label></formula><p>where c i is the latent true category of d i .</p><p>To make the learning of text embedding and category distribution explicit, we re-write Eq. (12) by re-arranging the product of p(w | c) over categories:</p><formula xml:id="formula_31">P(D | T ) ∝ c i ∈ T w j ∈c i p(w j | c i ) · d i ∈ D w j ∈d i p(w j | d i ) w j+k ∈d i −h ≤k ≤h,k 0 p(w j+k | w j ),</formula><p>Taking the log-likelihood as our objective to maximize, we have L = c i ∈T w j ∈c i log p(w j | c i )</p><formula xml:id="formula_32">+ d i ∈D w j ∈d i log p(w j | d i ) + d i ∈D w j ∈d i w j+k ∈d i −h ≤k ≤h,k 0 log p(w j+k | w j ) + constant.<label>(13)</label></formula><p>We omit the constant term and split Eq. (13) into category distribution modeling and corpus-based embedding learning objectives, plugging in the definition of the probability expressions given by Eqs. <ref type="bibr" target="#b4">(5)</ref>, <ref type="formula" target="#formula_15">(6)</ref> and <ref type="bibr" target="#b6">(7)</ref>. For category distribution modeling, we have:</p><formula xml:id="formula_33">L cat = c i ∈T w j ∈c i log p(w j | c i ) = c i ∈T w j ∈c i log n p (κ c i ) + κ c i · cos(u w j , c i ).<label>(14)</label></formula><p>Eq. <ref type="formula" target="#formula_5">(14)</ref> achieves the same effect as Eq. (1) on encouraging word representative terms to have high directional similarity with the category center vector, except that Eq. <ref type="formula" target="#formula_5">(14)</ref> does not incorporate an intra-category margin. Thus we extend Eq. (14) into the following:</p><formula xml:id="formula_34">L * cat = c i ∈T w j ∈ C i log n p (κ c i ) + κ c i u ⊤ w j c i 1(u ⊤ w j c i &lt; m intra ),<label>(15)</label></formula><p>where 1(·) is the indicator function.</p><p>For corpus-based embedding learning, we have: Directly maximizing the above objective results in trivial solution that all text embedding vectors are converged to the same point (so that the cosine similarity term is always maximized). To tackle this issue, we employ the same technique used in <ref type="bibr" target="#b24">[25]</ref> where the loglikelihood of a positive co-occurring tuple (w j , w j+k , d i ) is pushed over that of a negative tuple (w ′ j , w j+k , d i ) by a margin m, where w ′ j is a randomly sampled word from the vocabulary.</p><formula xml:id="formula_35">L</formula><p>L corpus = d i ∈D w j ∈d i w j+k ∈d i −h ≤k ≤h,k 0 min 0, −m + cos(v w j+k , u w j )+ cos(u w j , d i )) − cos(v w j+k , u w ′ j ) − cos(u w ′ j , d i ) . Finally, L text = L * cat + L corpus .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " Z J q 7 n 5 K 3 t d g q k v 1 b m q w w 8 H 5 G F E 0 = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k o i g h 4 L X j x W s K 3 Q h r D Z T t q l m 0 3 Y n Y g l B P w r X j w o 4 t X f 4 c 1 / 4 7 b N Q V s f D D z e m 2 F m X p g K r t F 1 v 6 3 K y u r a + k Z 1 s 7 a 1 v b O 7 Z + 8 f d H S S K Q Z t l o h E 3 Y d U g + A S 2 s h R w H 2 q g M a h g G 4 4 v p 7 6 3 Q d Q m i f y D i c p + D E d S h 5 x R t F I g X 3 U x x E g D f I + w i P m X K K i R R H Y d b f h z u A s E 6 8 k d V K i F d h f / U H C s h g k M k G 1 7 n l u i n 5 O F X I m o K j 1 M w 0 p Z W M 6 h J 6 h k s a g / X x 2 f u G c G m X g R I k y J d G Z q b 8 n c h p r P Y l D 0 x l T H O l F b y r + 5 / U y j K 5 8 8 1 O a I U g 2 X x R l w s H E m W b h D L g C h m J i C G W K m 1 s d N q K K M j S J 1 U w I 3 u L L y 6 R z 3 v B M M r c X 9 a Z b x l E l x + S E n B G P X J I m u S E t 0 i a M 5 O S Z v J I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s k f W J 8 / g v y W a g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z J q 7 n 5 K 3 t d g q k v 1 b m q w w 8 H 5 G F E 0 = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k o i g h 4 L X j x W s K 3 Q h r D Z T t q l m 0 3 Y n Y g l B P w r X j w o 4 t X f 4 c 1 / 4 7 b N Q V s f D D z e m 2 F m X p g K r t F 1 v 6 3 K y u r a + k Z 1 s 7 a 1 v b O 7 Z + 8 f d H S S K Q Z t l o h E 3 Y d U g + A S 2 s h R w H 2 q g M a h g G 4 4 v p 7 6 3 Q d Q m i f y D i c p + D E d S h 5 x R t F I g X 3 U x x E g D f I + w i P m X K K i R R H Y d b f h z u A s E 6 8 k d V K i F d h f / U H C s h g k M k G 1 7 n l u i n 5 O F X I m o K j 1 M w 0 p Z W M 6 h J 6 h k s a g / X x 2 f u G c G m X g R I k y J d G Z q b 8 n c h p r P Y l D 0 x l T H O l F b y r + 5 / U y j K 5 8 8 1 O a I U g 2 X x R l w s H E m W b h D L g C h m J i C G W K m 1 s d N q K K M j S J 1 U w I 3 u L L y 6 R z 3 v B M M r c X 9 a Z b x l E l x + S E n B G P X J I m u S E t 0 i a M 5 O S Z v J I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s k f W J 8 / g v y W a g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z J q 7 n 5 K 3 t d g q k v 1 b m q w w 8 H 5 G F E 0 = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k o i g h 4 L X j x W s K 3 Q h r D Z T t q l m 0 3 Y n Y g l B P w r X j w o 4 t X f 4 c 1 / 4 7 b N Q V s f D D z e m 2 F m X p g K r t F 1 v 6 3 K y u r a + k Z 1 s 7 a 1 v b O 7 Z + 8 f d H S S K Q Z t l o h E 3 Y d U g + A S 2 s h R w H 2 q g M a h g G 4 4 v p 7 6 3 Q d Q m i f y D i c p + D E d S h 5 x R t F I g X 3 U x x E g D f I + w i P m X K K i R R H Y d b f h z u A s E 6 8 k d V K i F d h f / U H C s h g k M k G 1 7 n l u i n 5 O F X I m o K j 1 M w 0 p Z W M 6 h J 6 h k s a g / X x 2 f u G c G m X g R I k y J d G Z q b 8 n c h p r P Y l D 0 x l T H O l F b y r + 5 / U y j K 5 8 8 1 O a I U g 2 X x R l w s H E m W b h D L g C h m J i C G W K m 1 s d N q K K M j S J 1 U w I 3 u L L y 6 R z 3 v B M M r c X 9 a Z b x l E l x + S E n B G P X J I m u S E t 0 i a M 5 O S Z v J I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s k f W J 8 / g v y W a g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z J q 7 n 5 K 3 t d g q k v 1 b m q w w 8 H 5 G F E 0 = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k o i g h 4 L X j x W s K 3 Q h r D Z T t q l m 0 3 Y n Y g l B P w r X j w o 4 t X f 4 c 1 / 4 7 b N Q V s f D D z e m 2 F m X p g K r t F 1 v 6 3 K y u r a + k Z 1 s 7 a 1 v b O 7 Z + 8 f d H S S K Q Z t l o h E 3 Y d U g + A S 2 s h R w H 2 q g M a h g G 4 4 v p 7 6 3 Q d Q m i f y D i c p + D E d S h 5 x R t F I g X 3 U x x E g D f I + w i P m X K K i R R H Y d b f h z u A s E 6 8 k d V K i F d h f / U H C s h g k M k G 1 7 n l u i n 5 O F X I m o K j 1 M w 0 p Z W M 6 h J 6 h k s a g / X x 2 f u G c G m X g R I k y J d G Z q b 8 n c h p r P Y l D 0 x l T H O l F b y r + 5 / U y j K 5 8 8 1 O a I U g 2 X x R l w s H E m W b h D L g C h m J i C G W K m 1 s d N q K K M j S J 1 U w I 3 u L L y 6 R z 3 v B M M r c X 9 a Z b x l E l x + S E n B G P X J I m u S E t 0 i a M 5 O S Z v J I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s k f W J 8 / g v y W a g = = &lt; / l a t e x i t &gt; ✓ inter &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 P Z R d i q C b 4 I H 3 j n e Z f h 1 x x g G x o M = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k o i g h 4 L X j x W s K 3 Q h r D Z T t q l m 0 3 Y n Y g l B P w r X j w o 4 t X f 4 c 1 / 4 7 b N Q V s f D D z e m 2 F m X p g K r t F 1 v 6 3 K y u r a + k Z 1 s 7 a 1 v b O 7 Z + 8 f d H S S K Q Z t l o h E 3 Y d U g + A S 2 s h R w H 2 q g M a h g G 4 4 v p 7 6 3 Q d Q m i f y D i c p + D E d S h 5 x R t F I g X 3 U x x E g D f I + w i P m X C K o o g j s u t t w Z 3 C W i V e S O i n R C u y v / i B h W Q w S m a B a 9 z w 3 R T + n C j k T U N T 6 m Y a U s j E d Q s 9 Q S W P Q f j 4 7 v 3 B O j T J w o k S Z k u j M 1 N 8 T O Y 2 1 n s S h 6 Y w p j v S i N x X / 8 3 o Z R l e + + S n N E C S b L 4 o y 4 W D i T L N w B l w B Q z E x h D L F z a 0 O G 1 F F m Q l B 1 0 w I 3 u L L y 6 R z 3 v B M M r c X 9 a Z b x l E l x + S E n B G P X J I m u S E t 0 i a M 5 O S Z v J I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s k f W J 8 / i Q e W b g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 P Z R d i q C b 4 I H 3 j n e Z f h 1 x x g G x o M = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k o i g h 4 L X j x W s K 3 Q h r D Z T t q l m 0 3 Y n Y g l B P w r X j w o 4 t X f 4 c 1 / 4 7 b N Q V s f D D z e m 2 F m X p g K r t F 1 v 6 3 K y u r a + k Z 1 s 7 a 1 v b O 7 Z + 8 f d H S S K Q Z t l o h E 3 Y d U g + A S 2 s h R w H 2 q g M a h g G 4 4 v p 7 6 3 Q d Q m i f y D i c p + D E d S h 5 x R t F I g X 3 U x x E g D f I + w i P m X C K o o g j s u t t w Z 3 C W i V e S O i n R C u y v / i B h W Q w S m a B a 9 z w 3 R T + n C j k T U N T 6 m Y a U s j E d Q s 9 Q S W P Q f j 4 7 v 3 B O j T J w o k S Z k u j M 1 N 8 T O Y 2 1 n s S h 6 Y w p j v S i N x X / 8 3 o Z R l e + + S n N E C S b L 4 o y 4 W D i T L N w B l w B Q z E x h D L F z a 0 O G 1 F F m Q l B 1 0 w I 3 u L L y 6 R z 3 v B M M r c X 9 a Z b x l E l x + S E n B G P X J I m u S E t 0 i a M 5 O S Z v J I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s k f W J 8 / i Q e W b g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 P Z R d i q C b 4 I H 3 j n e Z f h 1 x x g G x o M = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k o i g h 4 L X j x W s K 3 Q h r D Z T t q l m 0 3 Y n Y g l B P w r X j w o 4 t X f 4 c 1 / 4 7 b N Q V s f D D z e m 2 F m X p g K r t F 1 v 6 3 K y u r a + k Z 1 s 7 a 1 v b O 7 Z + 8 f d H S S K Q Z t l o h E 3 Y d U g + A S 2 s h R w H 2 q g M a h g G 4 4 v p 7 6 3 Q d Q m i f y D i c p + D E d S h 5 x R t F I g X 3 U x x E g D f I + w i P m X C K o o g j s u t t w Z 3 C W i V e S O i n R C u y v / i B h W Q w S m a B a 9 z w 3 R T + n C j k T U N T 6 m Y a U s j E d Q s 9 Q S W P Q f j 4 7 v 3 B O j T J w o k S Z k u j M 1 N 8 T O Y 2 1 n s S h 6 Y w p j v S i N x X / 8 3 o Z R l e + + S n N E C S b L 4 o y 4 W D i T L N w B l w B Q z E x h D L F z a 0 O G 1 F F m Q l B 1 0 w I 3 u L L y 6 R z 3 v B M M r c X 9 a Z b x l E l x + S E n B G P X J I m u S E t 0 i a M 5 O S Z v J I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s k f W J 8 / i Q e W b g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 P Z R d i q C b 4 I H 3 j n e Z f h 1 x x g G x o M = " &gt; A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k o i g h 4 L X j x W s K 3 Q h r D Z T t q l m 0 3 Y n Y g l B P w r X j w o 4 t X f 4 c 1 / 4 7 b N Q V s f D D z e m 2 F m X p g K r t F 1 v 6 3 K y u r a + k Z 1 s 7 a 1 v b O 7 Z + 8 f d H S S K Q Z t l o h E 3 Y d U g + A S 2 s h R w H 2 q g M a h g G 4 4 v p 7 6 3 Q d Q m i f y D i c p + D E d S h 5 x R t F I g X 3 U x x E g D f I + w i P m X C K o o g j s u t t w Z 3 C W i V e S O i n R C u y v / i B h W Q w S m a B a 9 z w 3 R T + n C j k T U N T 6 m Y a U s j E d Q s 9 Q S W P Q f j 4 7 v 3 B O j T J w o k S Z k u j M 1 N 8 T O Y 2 1 n s S h 6 Y w p j v S i N x X / 8 3 o Z R l e + + S n N E C S b L 4 o y 4 W D i T L N w B l w B Q z E x h D L F z a 0 O G 1 F F m Q l B 1 0 w I 3 u L L y 6 R z 3 v B M M r c X 9 a Z b x l E l x + S E n B G P X J I m u S E t 0 i a M 5 O S Z v J I 3 6 8 l 6 s d 6 t j 3 l r x S p n D s k f W J 8 / i Q e W b g = = &lt; / l a t e x i t &gt; ✓ intra  arccos(m intra )&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v + a 8 T t b E A + t N 7 K Z H K 5 Y h a D 0 5 z c 8 = " &gt; A A A C G n i c b V D L S g N B E J z 1 G e N r 1 a O X x S D E S 9 g V Q Y 8 B L x 4 j m A d k Q + i d d J I h s w 9 m e s W w 5 D u 8 + C t e P C j i T b z 4 N 0 6 S P W h i w U B R 1 U 1 P V Z B I o c l 1 v 6 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v 3 z 4 4 b O g 4 V R z r P J a x a g W g U Y o I 6 y R I Y i t R C G E g s R m M r q d + 8 x 6 V F n F 0 R + M E O y E M I t E X H M h I X d v z a Y g E 3 c w n f K B M R K R g M v E l + q A 4 j 3 U 5 X L D O u n b J r b g z O M v E y 0 m J 5 a h 1 7 U + / F / M 0 x I i 4 B K 3 b n p t Q J w N F g k u c F P 1 U Y w J 8 B A N s G x p B i L q T z a J N n F O j 9 J x + r M y L y J m p v z c y C L U e h 4 G Z D I G G e t G b i v 9 5 7 Z T 6 V x 0 T K k k J I z 4 / 1 E + l Q 7 E z 7 c n p C Y W c 5 N g Q 4 E q Y v z p 8 C A o 4 m T a L p g R v M f I y a Z x X P N P M 7 U W p 6 u Z 1 F N g x O 2 F l 5 r F L V m U 3 r M b q j L N H 9 s x e 2 Z v 1 Z L 1 Y 7 9 b H f H T F y n e O 2 B 9 Y X z 9 Q A q L S &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v + a 8 T t b E A + t N 7 K Z H K 5 Y h a D 0 5 z c 8 = " &gt; A A A C G n i c b V D L S g N B E J z 1 G e N r 1 a O X x S D E S 9 g V Q Y 8 B L x 4 j m A d k Q + i d d J I h s w 9 m e s W w 5 D u 8 + C t e P C j i T b z 4 N 0 6 S P W h i w U B R 1 U 1 P V Z B I o c l 1 v 6 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v 3 z 4 4 b O g 4 V R z r P J a x a g W g U Y o I 6 y R I Y i t R C G E g s R m M r q d + 8 x 6 V F n F 0 R + M E O y E M I t E X H M h I X d v z a Y g E 3 c w n f K B M R K R g M v E l + q A 4 j 3 U 5 X L D O u n b J r b g z O M v E y 0 m J 5 a h 1 7 U + / F / M 0 x I i 4 B K 3 b n p t Q J w N F g k u c F P 1 U Y w J 8 B A N s G x p B i L q T z a J N n F O j 9 J x + r M y L y J m p v z c y C L U e h 4 G Z D I G G e t G b i v 9 5 7 Z T 6 V x 0 T K k k J I z 4 / 1 E + l Q 7 E z 7 c n p C Y W c 5 N g Q 4 E q Y v z p 8 C A o 4 m T a L p g R v M f I y a Z x X P N P M 7 U W p 6 u Z 1 F N g x O 2 F l 5 r F L V m U 3 r M b q j L N H 9 s x e 2 Z v 1 Z L 1 Y 7 9 b H f H T F y n e O 2 B 9 Y X z 9 Q A q L S &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v + a 8 T t b E A + t N 7 K Z H K 5 Y h a D 0 5 z c 8 = " &gt; A A A C G n i c b V D L S g N B E J z 1 G e N r 1 a O X x S D E S 9 g V Q Y 8 B L x 4 j m A d k Q + i d d J I h s w 9 m e s W w 5 D u 8 + C t e P C j i T b z 4 N 0 6 S P W h i w U B R 1 U 1 P V Z B I o c l 1 v 6 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v 3 z 4 4 b O g 4 V R z r P J a x a g W g U Y o I 6 y R I Y i t R C G E g s R m M r q d + 8 x 6 V F n F 0 R + M E O y E M I t E X H M h I X d v z a Y g E 3 c w n f K B M R K R g M v E l + qA 4 j 3 U 5 X L D O u n b J r b g z O M v E y 0 m J 5 a h 1 7 U + / F / M 0 x I i 4 B K 3 b n p t Q J w N F g k u c F P 1 U Y w J 8 B A N s G x p B i L q T z a J N n F O j 9 J x + r M y L y J m p v z c y C L U e h 4 G Z D I G G e t G b i v 9 5 7 Z T 6 V x 0 T K k k J I z 4 / 1 E + l Q 7 E z 7 c n p C Y W c 5 N g Q 4 E q Y v z p 8 C A o 4 m T a L p g R v M f I y a Z x X P N P M 7 U W p 6 u Z 1 F N g x O 2 F l 5 r F L V m U 3 r M b q j L N H 9 s x e 2 Z v 1 Z L 1 Y 7 9 b H f H T F y n e O 2 B 9 Y X z 9 Q A q L S &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v + a 8 T t b E A + t N 7 K Z H K 5 Y h a D 0 5 z c 8 = " &gt; A A A C G n i c b V D L S g N B E J z 1 G e N r 1 a O X x S D E S 9 g V Q Y 8 B L x 4 j m A d k Q + i d d J I h s w 9 m e s W w 5 D u 8 + C t e P C j i T b z 4 N 0 6 S P W h i w U B R 1 U 1 P V Z B I o c l 1 v 6 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v 3 z 4 4 b O g 4 V R z r P J a x a g W g U Y o I 6 y R I Y i t R C G E g s R m M r q d + 8 x 6 V F n F 0 R + M E O y E M I t E X H M h I X d v z a Y g E 3 c w n f K B M R K R g M v E l + q A 4 j 3 U 5 X L D O u n b J r b g z O M v E y 0 m J 5 a h 1 7 U + / F / M 0 x I i 4 B K 3 b n p t Q J w N F g k u c F P 1 U Y w J 8 B A N s G x p B i L q T z a J N n F O j 9 J x + r M y L y J m p v z c y C L U e h 4 G Z D I G G e t G b i v 9 5 7 Z T 6 V x 0 T K k k J I z 4 / 1 E + l Q 7 E z 7 c n p C Y W c 5 N g Q 4 E q Y v z p 8 C A o 4 m T a L p g R v M f I y a Z x X P N P M 7 U W p 6 u Z 1 F N g x O 2 F l 5 r F L V m U 3 r M b q j L N H 9 s x e 2 Z v 1 Z L 1 Y 7 9 b H f H T F y n e O 2 B 9 Y X z 9 Q A q L S &lt; / l a t e x i t &gt; ✓ inter arccos(1 m inter ) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m l C v M L t + / A X 5 H a G + Y v K b T Z Y C e z 4 = " &gt; A A A C H H i c b V D L S s N A F J 3 4 r P U V d e k m W I S 6 s C Q q 6 L L g x m U F + 4 A m h M n 0 p h 0 6 e T B z I 5 b Q D 3 H j r 7 h x o Y g b F 4 J / 4 7 T N Q l s P D B z O u Z c 7 5 w S p 4 A p t + 9 t Y W l 5 Z X V s v b Z Q 3 t 7 Z 3 d s 2 9 / Z Z K M s m g y R K R y E 5 A F Q g e Q x M 5 C u i k E m g U C G g H w + u J 3 7 4 H q X g S 3 + E o B S + i / Z i H n F H U k m + e u z g A p H 7 u I j x g z m M E O R 6 7 f X C p Z C x R V e c 0 m j N P f L N i 1 + w p r E X i F K R C C j R 8 8 9 P t J S y L I E Y m q F J d x 0 7 R y 6 l E z g S M y 2 6 m I K V s S P v Q 1 T S m E S g v n 4 Y b W 8 d a 6 V l h I v W L 0 Z q q v z d y G i k 1 i g I 9 G V E c q H l v I v 7 n d T M M r z w d K s 0 Q Y j Y 7 F G b C w s S a N G X 1 u A S G Y q Q J Z Z L r v 1 p s Q C V l u g V V 1 i U 4 8 5 E X S e u s 5 u h m b i 8 q d b u o o 0 Q O y R G p E o d c k j q 5 I Q 3 S J I w 8 k m f y S t 6 M J + P F e D c + Z q N L R r F z Q P 7 A + P o B T o q j R w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m l C v M L t + / A X 5 H a G + Y v K b T Z Y C e z 4 = " &gt; A A A C H H i c b V D L S s N A F J 3 4 r P U V d e k m W I S 6 s C Q q 6 L L g x m U F + 4 A m h M n 0 p h 0 6 e T B z I 5 b Q D 3 H j r 7 h x o Y g b F 4 J / 4 7 T N Q l s P D B z O u Z c 7 5 w S p 4 A p t + 9 t Y W l 5 Z X V s v b Z Q 3 t 7 Z 3 d s 2 9 / Z Z K M s m g y R K R y E 5 A F Q g e Q x M 5 C u i k E m g U C G g H w + u J 3 7 4 H q X g S 3 + E o B S + i / Z i H n F H U k m + e u z g A p H 7 u I j x g z m M E O R 6 7 f X C p Z C x R V e c 0 m j N P f L N i 1 + w p r E X i F K R C C j R 8 8 9 P t J S y L I E Y m q F J d x 0 7 R y 6 l E z g S M y 2 6 m I K V s S P v Q 1 T S m E S g v n 4 Y b W 8 d a 6 V l h I v W L 0 Z q q v z d y G i k 1 i g I 9 G V E c q H l v I v 7 n d T M M r z w d K s 0 Q Y j Y 7 F G b C w s S a N G X 1 u A S G Y q Q J Z Z L r v 1 p s Q C V l u g V V 1 i U 4 8 5 E X S e u s 5 u h m b i 8 q d b u o o 0 Q O y R G p E o d c k j q 5 I Q 3 S J I w 8 k m f y S t 6 M J + P F e D c + Z q N L R r F z Q P 7 A + P o B T o q j R w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m l C v M L t + / A X 5 H a G + Y v K b T Z Y C e z 4 = " &gt; A A A C H H i c b V D L S s N A F J 3 4 r P U V d e k m W I S 6 s C Q q 6 L L g x m U F + 4 A m h M n 0 p h 0 6 e T B z I 5 b Q D 3 H j r 7 h x o Y g b F 4 J / 4 7 T N Q l s P D B z O u Z c 7 5 w S p 4 A p t + 9 t Y W l 5 Z X V s v b Z Q 3 t 7 Z 3 d s 2 9 / Z Z K M s m g y R K R y E 5 A F Q g e Q x M 5 C u i k E m g U C G g H w + u J 3 7 4 H q X g S 3 + E o B S + i / Z i H n F H U k m + e u z g A p H 7 u I j x g z m M E O R 6 7 f X C p Z C x R V e c 0 m j N P f L N i 1 + w p r E X i F K R C C j R 8 8 9 P t J S y L I E Y m q F J d x 0 7 R y 6 l E z g S M y 2 6 m I K V s S P v Q 1 T S m E S g v n 4 Y b W 8 d a 6 V l h I v W L 0 Z q q v z d y G i k 1 i g I 9 G V E c q H l v I v 7 n d T M M r z w d K s 0 Q Y j Y 7 F G b C w s S a N G X 1 u A S G Y q Q J Z Z L r v 1 p s Q C V l u g V V 1 i U 4 8 5 E X S e u s 5 u h m b i 8 q d b u o o 0 Q O y R G p E o d c k j q 5 I Q 3 S J I w 8 k m f y S t 6 M J + P F e D c + Z q N L R r F z Q P 7 A + P o B T o q j R w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m l C v M L t + / A X 5 H a G + Y v K b T Z Y C e z 4 = " &gt; A A A C H H i c b V D L S s N A F J 3 4 r P U V d e k m W I S 6 s C Q q 6 L L g x m U F + 4 A m h M n 0 p h 0 6 e T B z I 5 b Q D 3 H j r 7 h x o Y g b F 4 J / 4 7 T N Q l s P D B z O u Z c 7 5 w S p 4 A p t + 9 t Y W l 5 Z X V s v b Z Q 3 t 7 Z 3 d s 2 9 / Z Z K M s m g y R K R y E 5 A F Q g e Q x M 5 C u i k E m g U C G g H w + u J 3 7 4 H q X g S 3 + E o B S + i / Z i H n F H U k m + e u z g A p H 7 u I j x g z m M E O R 6 7 f X C p Z C x R V e c 0 m j N P f L N i 1 + w p r E X i F K R C C j R 8 8 9 P t J S y L I E Y m q F J d x 0 7 R y 6 l E z g S M y 2 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>e u s 5 u h m b i 8 q d b u o o 0 Q O y R G p E o d c k j q 5 I Q 3 S J I w 8 k m f y S t 6 M J + P F e D c + Z q N L R r F z Q P 7 A + P o B T o q j R w = = &lt; / l a t e x i t &gt; c i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p y y j 0 Y 0 q U / o q n F + t W m A s N / C O o v o = " &gt; A A A B + X i c b V D L S g M x F L 3 j s 9 b X q E s 3 w S K 4 K j M i 6 L L g x m U F + 4 B 2 G D K Z T B u a S Y Y k U y h D / 8 S N C 0 X c + i f u / B s z 7 S y 0 9 U D I 4 Z x 7 y c m J M s 6 0 8 b x v Z 2 N z a 3 t n t 7 Z X 3 z 8 4 P D p 2 T 0 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>m 3 h v 3 s e q q 5 q 3 L u 0 M / s j 7 / A E 4 f 4 o 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T S E W J Q T e Z P T J 0 8 g o 5 5 5 p g p k + L j 8 = " &gt; A A A B 7 n i c b Z D L S g M x G I X / q b d a q 4 5 u 3 Q S L 4 K r M u N G l 4 M Z l B X u B d h g y m U w b m s u Q Z A p l 6 J u 4 c a G I j + P O t z H T d q G t P 4 Q c z k n I y Z f k n B k b B N 9 e b W d 3 b / + g f t g 4 a h 6 f n P p n z Z 5 R h S a 0 S x R X e p B g Q z m T t G u Z 5 X S Q a 4 p F w m k / m T 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " T S E W J Q T e Z P T J 0 8 g o 5 5 5 p g p k + L j 8 = " &gt; A A A B 7 n i c b Z D L S g M x G I X / q b d a q 4 5 u 3 Q S L 4 K r M u N G l 4 M Z l B X u B d h g y m U w b m s u Q Z A p l 6 J u 4 c a G I j + P O t z H T d q G t P 4 Q c z k n I y Z f k n B k b B N 9 e b W d 3 b / + g f t g 4 a h 6 f n P p n z Z 5 R h S a 0 S x R X e p B g Q z m T t G u Z 5 X S Q a 4 p F w m k / m T 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a j m T 5 6 J x 7 5 e M T Z Z x p 4 3 n f T m 1 r e 2 d 3 r 7 7 f O D g 8 O j 5 x T 8 9 6 W u a K 0 C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F O t i N e M a P X k c Y 5 + l U d 9 T X K Z 5 h m U = " &gt; A A A B 2 X i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b n Q p u H F Z w b Z C O 5 R M 5 k 4 b m s k M y R 2 h D H 0 B F 2 5 E f C 9 3 v o 3 p z 0 J b D w Q + z k n I v S c u l L Q U B N 9 e b W d 3 b / + g f u g f N f z j k 9 N m o 2 f z 0 g j s i l z l 5 j n m F p X U 2 C V J C p 8 L g z y L F f b j 6 f 0 i 7 7 + g s T L X T z Q r M M r 4 W M t U C k 7 O 6 o y a r a A d L M W 2 I V x D C 9 Y a N b + G S S 7 K D D U J x a 0 d h E F B U c U N S a F w 7 g 9 L i w U X U z 7 G g U P N M 7 R R t R x z z i 6 d k 7 A 0 N + 5 o Y k v 3 9 4 u K Z 9 b O s t j d z D h N 7 G a 2 M P / L B i W l t 1 E l d V E S a r H 6 K C 0 V o 5 w t d m a J N C h I z R x w Y a S b l Y k J N 1 y Q a 8 Z 3 H Y S b G 2 9 D 7 7 o d u m I e A 6 j D O V z A F Y R w A 3 f w A B 3 o g o A E X u H d m 3 h v 3 s e q q 5 q 3 L u 0 M / s j 7 / A E 4 f 4 o 3 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w s 2 D b 8 o i G 3 y P Z w N w 1 B p e t 5 a w e o U = " &gt; A A A B 7 n i c b Z D L S g M x G I X / 8 V p r 1 d G t m 2 A R X J U Z N 7 o U 3 L i s Y C / Q D k M m k 2 l j c x m S T K E M f R M 3 L h T x c d z 5 N m b a L r T 1 h 5 D D O Q k 5 + Z K c M 2 O D 4 N v b 2 t 7 Z 3 d u v H d Q P G 0 f H J / 5 p o 2 t U o Q n t E M W V 7 i f Y U M 4 k 7 V h m O e 3 n m m K R c N p L J v d V 3 p t S b Z i S T 3 a W 0 0 j g k W Q Z I 9 g 6 K / b 9 Y a J 4 a m b C b S W Z x 8 + x 3 w x a w W L Q p g h X o g m r a c f + 1 z B V p B B U W s K x M Y M w y G 1 U Y m 0 Z 4 X R e H x a G 5 p h M 8 I g O n J R Y U B O V i + Z z d O m c F G V K u y U t W r i / b 5 R Y m K q c O y m w H Z v 1 r D L / y w a F z W 6 j k s m 8 s F S S 5 U N Z w Z F V q M K A U q Y p s X z m B C a a u a 6 I j L H G x D p Y d Q c h X P / y p u h e t 0 J H 5 j G A G p z D B V x B C D d w B w / Q h g 4 Q m M I L v M G 7 V 3 q v 3 s c S 1 5 a 3 4 n Y G f 8 b 7 / A H T A p K d &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w s 2 D b 8 o i G 3 y P Z w N w 1 B p e t 5 a w e o U = " &gt; A A A B 7 n i c b Z D L S g M x G I X / 8 V p r 1 d G t m 2 A R X J U Z N 7 o U 3 L i s Y C / Q D k M m k 2 l j c x m S T K E M f R M 3 L h T x c d z 5 N m b a L r T 1 h 5 D D O Q k 5 + Z K c M 2 O D 4 N v b 2 t 7 Z 3 d u v H d Q P G 0 f H J / 5 p o 2 t U o Q n t E M W V 7 i f Y U M 4 k 7 V h m O e 3 n m m K R c N p L J v d V 3 p t S b Z i S T 3 a W 0 0 j g k W Q Z I 9 g 6 K / b 9 Y a J 4 a m b C b S W Z x 8 + x 3 w x a w W L Q p g h X o g m r a c f + 1 z B V p B B U W s K x M Y M w y G 1 U Y m 0 Z 4 X R e H x a G 5 p h M 8 I g O n J R Y U B O V i + Z z d O m c F G V K u y U t W r i / b 5 R Y m K q c O y m w H Z v 1 r D L / y w a F z W 6 j k s m 8 s F S S 5 U N Z w Z F V q M K A U q Y p s X z m B C a a u a 6 I j L H G x D p Y d Q c h X P / y p u h e t 0 J H 5 j G A G p z D B V x B C D d w B w / Q h g 4 Q m M I L v M G 7 V 3 q v 3 s c S 1 5 a 3 4 n Y G f 8 b 7 / A H T A p K d &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O t v v C 3 q D m g c i T 5 8 9 v u a 4 w 2 b B 3 V c = " &gt; A A A B + X i c b V C 7 T s M w F L 3 h W c o r w M h i U S E x V Q k L j J V Y G I t E H 1 I b R Y 7 j t K a O H d l O p S r q n 7 A w g B A r f 8 L G 3 + C 0 G a D l S J a P z r l X P j 5 R x p k 2 n v f t b G x u b e / s 1 v b q + w e H R 8 f u y W l X y 1 w R 2 i G S S 9 W P s K a c C d o x z H D a z x T F a c R p L 5 r c l X 5 v S p V m U j y a W U a D F I 8 E S x j B x k q h 6 w 4 j y W M 9 S + 1 V k H n 4 F L o N r + k t g N a J X 5 E G V G i H 7 t c w l i R P q T C E Y 6 0 H v p e Z o M D K M M L p v D 7 M N c 0 w m e A R H V g q c E p 1 U C y S z 9 G l V W K U S G W P M G i h / t 4 o c K r L c H Y y x W a s V 7 1 S / M 8 b 5 C a 5 D Q o m s t x Q Q Z Y P J T l H R q K y B h Q z R Y n h M 0 s w U c x m R W S M F S b G l l W 3 J f i r X 1 4 n 3 e u m b 5 t 5 8 B o t r 6 q j B u d w A V f g w w 2 0 4 B 7 a 0 A E C U 3 i G V 3 h z C u f F e X c + l q M b T r V z B n / g f P 4 A K z a T 6 w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X R u b u H j W + + 5 L G 8 j H U m g a 7 o l N h N c = " &gt; A A A B + X i c b V D L S g M x F L 1 T X 7 W + R l 2 6 C R b B V Z k R Q Z c F N y 4 r 2 A e 0 w 5 D J p G 1 s J h m S T K E M / R M 3 L h R x 6 5 + 4 8 2 / M t L P Q 1 g M h h 3 P u J S c n S j n T x v O + n c r G 5 t b 2 T n W 3 t r d / c H j k H p 9 0 t M w U o W 0 i u V S 9 C G v K m a B t w w y n v V R R n E S c d q P J X e F 3 p 1 R p J s W j m a U 0 S P B I s C E j 2 F g p d N 1 B J H m s Z 4 m 9 c j I P n 0 K 3 7 j W 8 B d A 6 8 U t S h x K t 0 P 0 a x J J k C R W G c K x 1 3 / d S E + R Y G U Y 4 n d c G m a Y p J h M 8 o n 1 L B U 6 o D v J F 8 j m 6 s E q M h l L Z I w x a q L 8 3 c p z o I p y d T L A Z 6 1 W v E P / z + p k Z 3 g Y 5 E 2 l m q C D L h 4 Y Z R 0 a i o g Y U M 0 W J 4 T N L M F H M Z k V k j B U m x p Z V s y X 4 q 1 9 e J 5 2 r h m + b e b i u N 7 2 y j i q c w T l c g g 8 3 0 I R 7 a E E b C E z h G V 7 h z c m d F + f d + V i O V p x y 5 x T + w P n 8 A S x 2 k + 8 = &lt; / l a t e x i t &gt; O &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; (a) Intra-&amp; Inter-Category Configuration. t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; (b) Embed First-Level Local Tree. t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u d b + O i N s S 0 F U N K K E d b m j U S x G d V M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o M e C F 2 + 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T x b D F Y h G r b k A 1 C i 6 x Z b g R 2 E 0 U 0 i g Q 2 A k m t 3 O / 8 4 R K 8 1 g + m G m C f k R H k o e c U W O l 5 v 2 g X H G r 7 g J k n X g 5 q U C O x q D 8 1 R / G L I 1 Q G i a o 1 j 3 P T Y y f U W U 4 E z g r 9 V O N C W U T O s K e p Z J G q P 1 s c e i M X F h l S M J Y 2 Z K G L N T f E x m N t J 5 G g e 2 M q B n r V W 8 u / u f 1 U h P e + B m X S W p Q s u W i M B X E x G T + N R l y h c y I q S W U K W 5 v J W x M F W X G Z l O y I X i r L 6 + T d q 3 q 2 W S a V 5 V 6 L Y + j C G d w D p f g w T X U 4 Q 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z + i 8 4 z C &lt; / l a t e x i t &gt; (c) Embed Second-Level Local Trees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Spherical tree embeddings. All category center vectors reside on the unit sphere. (a) Representative terms are pushed into a spherical sector centered around the category center vector. Directional distance is enforced between categories. (b) &amp; (c) Local trees are recursively embedded onto the sphere.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Hierarchical Topic Mining results on NYT. arXiv joint embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Joint embedding space visualization. Category center vectors are denoted as stars; representative words are denoted as dots in the same color with corresponding category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Results of Hierarchical Topic Mining on arXiv: Only 5 sub-categories per super-category are shown here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>corpus = d i ∈D w j ∈d i log p(w j | d i ) + w j+k ∈d i −h ≤k ≤h,k 0 log p(w j+k | w j ) = d i ∈D w j ∈d i cos(u w j , d i ) + w j+k ∈d i −h ≤k ≤h,k 0 cos(v w j+k , u w j ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>ROOT baseball soccer music dance arts sports ROOT Text Corpus dance</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">sports</cell><cell></cell><cell>arts</cell></row><row><cell></cell><cell></cell><cell>tennis</cell><cell></cell><cell>theater</cell></row><row><cell></cell><cell cols="2">soccer</cell><cell></cell><cell>opera</cell></row><row><cell></cell><cell cols="2">basketball</cell><cell></cell><cell>artist</cell></row><row><cell></cell><cell cols="2">volleyball</cell><cell cols="2">arts festival</cell></row><row><cell></cell><cell cols="2">football</cell><cell cols="2">performing arts</cell></row><row><cell></cell><cell>baseball</cell><cell>soccer</cell><cell>music</cell><cell></cell></row><row><cell>___________ ___________ ___________ ___________ ___________ ___________ ____ ____ ___ ____ ____ ___ ____ ____ ___ ____ ____ ___ ____ ____ ___ ____ ____ ___</cell><cell>ballplayers pitching outfielder baseman</cell><cell>fifa striker midfielder goalkeeper</cell><cell>folk music jazz choral concert</cell><cell>tango dancers choreographer ballet</cell></row><row><cell></cell><cell>catcher</cell><cell>world cup</cell><cell>singers</cell><cell>troupe</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Hierarchical Topic Mining. Input: A text corpus D; a category tree T = {c i }| n i=1 ; number of terms K to retrieve per category . Output: Hierarchical Topic Mining results</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell cols="4">Corpus # super-categories # sub-categories # documents</cell></row><row><cell>NYT</cell><cell>8</cell><cell>12</cell><cell>89,768</cell></row><row><cell>arXiv</cell><cell>3</cell><cell>29</cell><cell>230,105</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation: Hierarchical Topic Mining.</figDesc><table><row><cell>Models</cell><cell>TC</cell><cell>NYT MACC</cell><cell>TC</cell><cell>arXiv MACC</cell></row><row><cell>hLDA</cell><cell cols="4">-0.0070 0.1636 -0.0124 0.1471</cell></row><row><cell>hPAM</cell><cell cols="4">0.0074 0.3091 0.0037 0.1824</cell></row><row><cell>JoSE</cell><cell cols="4">0.0140 0.6818 0.0051 0.7412</cell></row><row><cell>Poincaré GloVe</cell><cell cols="4">0.0092 0.6182 -0.0050 0.5588</cell></row><row><cell cols="5">Anchored CorEx 0.0117 0.3909 0.0060 0.4941</cell></row><row><cell>CatE</cell><cell cols="4">0.0149 0.9000 0.0066 0.8176</cell></row><row><cell>JoSH</cell><cell cols="4">0.0166 0.9091 0.0074 0.8324</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Run time (in minutes) on NYT. Models are run on a machine with 20 cores of Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80 GHz.</figDesc><table><row><cell cols="7">hLDA hPAM JoSE Poincaré GloVe Anchored CorEx CatE JoSH</cell></row><row><cell>53</cell><cell>22</cell><cell>5</cell><cell>16</cell><cell>61</cell><cell>52</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative evaluation: Weakly-supervised hierarchical classification.</figDesc><table><row><cell>Models</cell><cell cols="4">NYT Macro-F1 Micro-F1 Macro-F1 Micro-F1 arXiv</cell></row><row><cell>WeSHClass</cell><cell>0.425</cell><cell>0.581</cell><cell>0.320</cell><cell>0.542</cell></row><row><cell>JoSH</cell><cell>0.429</cell><cell>0.600</cell><cell>0.367</cell><cell>0.610</cell></row><row><cell>WeSHClass + CatE</cell><cell>0.503</cell><cell>0.679</cell><cell>0.401</cell><cell>0.622</cell></row><row><cell>WeSHClass + JoSH</cell><cell>0.582</cell><cell>0.703</cell><cell>0.412</cell><cell>0.673</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Research was sponsored in part by US DARPA KAIROS Program No. FA8750-19-2-1004 and SocialSim Program No. W911NF-17-C-0099, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS 17-41317, and DTRA HDTRA11810026. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and should not be interpreted as necessarily representing the views, either expressed or implied, of DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright annotation hereon. We thank anonymous reviewers for valuable and insightful feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A HIERARCHICAL TOPIC MINING RESULTS ON ARXIV <ref type="figure">Figure 5</ref> shows part of the Hierarchical Topic Mining results on arXiv. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pattern Learning for Relation Extraction with a Hierarchical Topic Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Delort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation with Topic-in-Set Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nonparametric spherical topic modeling with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardavan</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gershman</surname></persName>
		</author>
		<idno>ACL. 537</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical Topic Models and the Nested Chinese Restaurant Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Topic Modeling in Embedding Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<idno>abs/1907.04907</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Reing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hyperbolic Entailment Cones for Learning Hierarchical Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical topic model for political texts: Measuring expressed agendas in Senate press releases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Grimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic Latent Semantic Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Guiding Corpus-based Set Expansion by Auxiliary Sets Generation and Co-Expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incorporating Lexical Priors into Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entity disambiguation with hierarchical topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saurabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sengamedu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Linguistic Regularities in Sparse and Explicit Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pachinko allocation: DAG-structured mixture models of topic correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Tat-Seng Chua, and Maosong Sun. 2015. Topical Word Embeddings. In AAAI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSHLDA: A Semi-Supervised Hierarchical Topic Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyan</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><forename type="middle">Kan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Word Embedding Learning by Incorporating Local and Global Contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Big Data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative Topic Mining via Category-Name Guided Text Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spherical Text Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Neural Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Hierarchical Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mixtures of hierarchical topics with Pachinko allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Poincaré Embeddings for Learning Hierarchical Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchically Supervised Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">D</forename><surname>Perotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The New York Times Annotated Corpus</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automated Phrase Mining from Massive Text Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1825" to="1837" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Poincaré Glove: Hyperbolic Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
