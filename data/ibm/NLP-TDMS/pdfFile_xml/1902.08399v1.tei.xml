<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Capsule Neural Networks for Graph Classification using Explicit Tensorial Graph Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Daniel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gutierrez</forename><surname>Mallea</surname></persName>
							<email>m.gutierrez@braintree.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Bentley</surname></persName>
							<email>p.bentley@cs.ucl.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Research Lab Braintree Ltd. London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">nd Peter Meltzer Computer Science Dept</orgName>
								<orgName type="institution">University College</orgName>
								<address>
									<settlement>London London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Dept</orgName>
								<orgName type="institution">University College</orgName>
								<address>
									<settlement>London London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Capsule Neural Networks for Graph Classification using Explicit Tensorial Graph Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Graph Classification</term>
					<term>Graph Representation Learning</term>
					<term>Graph Kernels</term>
					<term>Convolutional Neural Networks</term>
					<term>Cap- sule Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph classification is a significant problem in many scientific domains. It addresses tasks such as the classification of proteins and chemical compounds into categories according to their functions, or chemical and structural properties. In a supervised setting, this problem can be framed as learning the structure, features and relationships between features within a set of labelled graphs and being able to correctly predict the labels or categories of unseen graphs.</p><p>A significant difficulty in this task arises when attempting to apply established classification algorithms due to the requirement for fixed size matrix or tensor representations of the graphs which may vary greatly in their numbers of nodes and edges. Building on prior work combining explicit tensor representations with a standard image-based classifier, we propose a model to perform graph classification by extracting fixed size tensorial information from each graph in a given set, and using a Capsule Network to perform classification.</p><p>The graphs we consider here are undirected and with categorical features on the nodes. Using standard benchmarking chemical and protein datasets, we demonstrate that our graph Capsule Network classification model using an explicit tensorial representation of the graphs is competitive with current state of the art graph kernels and graph neural network models despite only limited hyper-parameter searching.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph-structured data is prevalent in a broad set of domains including molecule representation, chemo-and bioinformatics, social network analysis, finance and many more. One reason for this being that graphical representations of data are able to model not only entities, but the connections and relationships between entities; thus offering a richer quality of information.</p><p>In order to develop successful machine learning models in these domains, we need techniques that can exploit this rich information inherent in the structure of a graph, as well as the entity feature information contained within a graph's nodes and edges <ref type="bibr" target="#b0">[1]</ref>. This work is part of an Innovate-UK funded research collaboration (GAMMA) between Braintree Ltd. (http://www.braintree.com) and University College London. Research grant no. 103971. This work has been submitted to the International Joint Conference on Neural Networks (IJCNN) 2019.</p><p>To define the scope of our work we adopt the distinction made in <ref type="bibr" target="#b1">[2]</ref> between graph focused and node focused applications for machine learning on graphs. Examples of node focused applications include link prediction and node classification, where typical applications would include recommender systems and entity disambiguation. Whereas graph focused applications, which are the concern of this work, would include defining similarity, clustering or classifying graphs as instances themselves.</p><p>Many existing successful methods for graph classification are based on kernels <ref type="bibr" target="#b2">[3]</ref> which are particularly well suited to the problem of comparing graphs of different dimensions; however, kernel methods with implicit representations suffer limited scalability <ref type="bibr" target="#b3">[4]</ref>. Explicit kernels enable greater scalability, however present the challenge of mapping instances of various dimensions to a fixed size representation; and for graphs in particular, the challenge is increased by the permutation invariance requirement sought by a graph classifier. For example, given two isomorphic graphs in which the node ids in one are a permutation of the other, the graph classifier should recognise their equivalence.</p><p>We investigate the use of Capsule Networks <ref type="bibr" target="#b4">[5]</ref> to tackle the problem of supervised graph classification on undirected graphs of differing numbers of classes and discrete node features. Our hypothesis is that a Capsule Network would be better suited to identifying the similarities between graphs where permutations of the node ordering may cause other methods to fail, thus enabling greater classification performance. To the best of our knowledge, there has been no published literature combining explicit kernel graph representations with Capsule Networks. Our experiments demonstrate results that are competitive with other current state of the art methods on a set of seven widely used chemical and protein datasets.</p><p>To present our original contribution in applying Capsule Networks to graph classification through the use of explicit kernels, we first provide a review of related work in the graph classification domain. We then provide our methodology describing our contribution, with attention to both the tensor extraction and Capsule Network phases of our algorithm. This is followed by three experiments, first investigating the impact of different labelling procedures for the tensor extraction, second, bench-marking the performance of our complete model against current state of the art methods, and third, assessing the representation power of the Capsule Networks in the given tasks. After analysing our results we close with concluding remarks and discussion of potential further work.</p><p>The full source code for our work is available at https://github.com/BraintreeLtd/PatchyCapsules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Kernels</head><p>Kernel methods have been successful in many machine learning applications <ref type="bibr" target="#b5">[6]</ref>, with many notable efforts in the graph classification setting <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. A graph kernel is a positive semi-definite function defined on the space of graphs G. This function corresponds to an inner product in some Hilbert space H to which the graphs are mapped: φ : G → H. The mapped space H may then be used with standard classification algorithms, or utilising the kernel trick (with SVMs, for example) may be exploited implicitly. In this sense, kernel methods are well suited to deal with the high and variable dimensions of graph data, where explicit computation of such a feature space may not be possible.</p><p>Despite the high number of graph kernels in published literature, as distinguished in <ref type="bibr" target="#b9">[10]</ref>, they typically fall into just three distinct classes: Graph kernels based on walks and paths where random walks between two graphs are compared <ref type="bibr" target="#b10">[11]</ref>, graph kernels based on a limited size subgraphs or graphlets, where the graph is represented by counts of subgraphs of different sizes <ref type="bibr" target="#b11">[12]</ref>, and graph kernels based on subtree patterns where a similarity matrix between two graphs is defined by the number of matching subtrees in each graph <ref type="bibr" target="#b12">[13]</ref>.</p><p>Although graph kernels are well suited to produce good graph representations with respect to the difficulties in varying dimensions, the scalability of these graph kernels is limited; they scale poorly to large graphs. In the worst case, none of them scale better than O(n 3 ) in the number of vertices <ref type="bibr" target="#b9">[10]</ref>.</p><p>One of the most notable graph kernels with respect to scalability in the size of graphs is the kernel based on the Weisfeiler-Lehman (WL) algorithm for graph isomorphism <ref type="bibr" target="#b13">[14]</ref>. This kernel consists of repeatedly applying a hashing function to a node's neighbours' attributes and using the histogram of all the labels in order to represent the graph. It has attained state of the art results in terms of graph classification accuracy and in terms of execution time <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Neural Networks</head><p>Several neural network models have been proposed for the problem of graph classification. The early works of <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b1">[2]</ref> present a Graph Neural Network (GNN) model based on information diffusion and relaxation mechanisms, and several instances of these recursive neural network models have been proposed since. Drawing upon the gains seen in the image classification domain with convolutional neural networks, <ref type="bibr" target="#b15">[16]</ref> proposed a spectral graph convolutional neural network model that was later extended by <ref type="bibr" target="#b16">[17]</ref> with fast localized convolutions. <ref type="bibr" target="#b17">[18]</ref> introduced a first order approximation of spectral convolutions on graphs with its graph convolutional network (GCN) model.</p><p>Generalising the processes involved in these graph convolutional networks, <ref type="bibr" target="#b18">[19]</ref> defined a message passing neural network framework (MPNN) able to express many of the previous GNN models as specialised instances. This framework defines two phases, a message passing phase where the hidden states of each node in the graph are updated according to the neighbours' messages, and a readout phase where a features vector is computed for the whole graph.</p><p>The GNN models described above have demonstrated state of the art results in label prediction <ref type="bibr" target="#b17">[18]</ref> and link prediction <ref type="bibr" target="#b19">[20]</ref> problems. The GCN model works very well in label prediction tasks but it has problems with graph classification. Since the GCN provides node level outputs, to answer graph level questions requires some pooling process. The main issue is that this model is equivariant with respect to the node order in a graph <ref type="bibr" target="#b20">[21]</ref>. This means that, given its non invariance to a permutation of the nodes, there are no guarantees that it would give the same results for two isomorphic graphs when the node ordering of the graphs is permuted, thus the pooling process may take different inputs for equivalent graphs.</p><p>One approach to solving this problem is proposed by <ref type="bibr" target="#b20">[21]</ref> in adding a permutation invariant layer based on computing the covariance of the data.</p><p>So far, the design of the graph neural network models has been for the most part empirical and intuitive. <ref type="bibr" target="#b21">[22]</ref> showed theoretically that GNNs are at most as powerful as the WL isomorphism test in terms of distinguishing graph structures. They also showed that the first phase of the MPNN framework described in <ref type="bibr" target="#b18">[19]</ref> can be divided into an aggregation and combination scheme followed by a readout function. The aggregation and the readout function need to be injective for a GNN model to be as powerful as the WL test.</p><p>It is interesting to note that even a powerful GNN model is bounded by the WL test in terms of discriminating graph structure; however the WL is limited to non continuous node features. A good GNN model satisfiying the injectivity conditions mentioned in <ref type="bibr" target="#b18">[19]</ref> could potentially learn better representations in a graph classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimenting with Both Worlds</head><p>Graph kernels can be divided into explicit and implicit kernels. Implicit kernels compute a similarity measure between graphs and can benefit from the kernel trick. Explicit kernels compute a feature map for each graph directly; and, for large enough graphs, can be more efficient than implicit kernels <ref type="bibr" target="#b3">[4]</ref>.</p><p>A model for learning explicit graph representations called Patchy-Sans was presented in <ref type="bibr" target="#b22">[23]</ref>. This algorithm extracts fixed size localized patches by applying a graph labeling given by the WL algorithm <ref type="bibr" target="#b13">[14]</ref> and the canonical labeling procedure from <ref type="bibr" target="#b23">[24]</ref>. It then uses these patches to form a 3dimensional tensor for each graph and uses a CNN to perform the classification.</p><p>Our experiments leverage a procedure similar to Patchy-Sans in order to convert a graph into a tensor representation.</p><p>This representation is subsequently used in combination with a Capsule Network in order to perform graph classification. To date there is limited prior work in tackling graph classification problems with Capsule Networks, with <ref type="bibr" target="#b20">[21]</ref> being one example. The rationale for replacing the CNN with a Capsule Network is that there is a potential loss of information associated with the convolution operation.</p><p>CNNs have been widely used in the machine vision community to address image classification problems <ref type="bibr" target="#b24">[25]</ref>, however CNNs are only invariant to translation, and for this reason they cannot identify an object that has gone under a different transformation, such as a rotation.</p><p>Capsule networks work better with this problem by dividing the neurons into small groups in each network layer, where these groups are known as the capsules. The capsules correspond to concepts in different levels of abstraction during the process of parsing information. While this cross-layer association and the activation status of the capsules could represent semantic features in the case of image data, we expect that it would produce similar representations in the case of graphs <ref type="bibr" target="#b25">[26]</ref>.</p><p>To our knowledge, no previous work has combined an explicit kernel representation of a graph with a Capsule Network classifier. Our work fills this gap in order capture more accurately the structural information of a set of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In our experiments, we test the hypothesis that using a Capsule Network could help to address the permutation invariance problem in graph classification, and thus offer improved classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithm</head><p>Following an introduction to the necessary notation, we present our contribution by considering the two distinct phases of our algorithm. The first phase generates a matrix representation for each graph in the dataset and the second applies a Capsule Network to these representations. We then provide the results and analysis of three experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Notation</head><p>A graph G i ∈ G is defined as a pair of vertices and edges G = (V, E) of size N = |V |, where V is the node set, E the edge set, and G is the set of graphs of size |G|. For each graph G i , it is possible to define the adjacency matrix between nodes as A = [a i,j ] where a i,j is 1 if node i is connected to node j and 0 otherwise. Let X ∈ R N ×d be the node feature matrix, where d is the dimension of the node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph to Contextual Tensor</head><p>In order to extract a tensor from a graph, the procedure described in Algorithm 1 is performed. For each graph G i ∈ G, a node sequence order is defined following a given graph labelling procedure. The number of nodes that compose this node sequence is given by the width parameter w. Note that there is no requirement that adjacent nodes in the sequence are connected in the original graph.</p><formula xml:id="formula_0">Algorithm 1 GraphToTensor for each G i in G do Input: Adjacency matrix A i , node features X i , width w, receptive field size k. NodeList ← NodeSequenceOrdering(A i , X i ,w) M i ← Array[] for n in NodeList do SubG n ← NeighbourGathering(A i , X i ,n) SubG n ← Normalization(SubG n ) M i [n] ← SubG n end for T i ← Encoding(M i ) end for</formula><p>For each node in this sequence, the closest neighbours (in terms of hop count) are gathered (SubG n ). The number of neighbours to gather is given by the height parameter k. Then, a normalization of neighbours that follows a graph labelling procedure is used to order the labels by selecting nodes the give a sensible representation of the graph's structure. This enables discrimination between candidate nodes for selection in the case that there are too many nodes of equal hop count distance, as well as providing a consistent ordering within the subset of nodes selected. Finally, the categorical attributes of the nodes are encoded using one-hot encoding.</p><p>This procedure generates a receptive field for each selected node in the graph. Two different labelling procedures for the selection and ordering of nodes are investigated: a canonical labelling procedure, and a ranking based on betweenness centrality. The purpose of using the canonical labeling procedure is to order the nodes in the graph so that they correspond to the isomorph class of a given graph, whereas the betweenness centrality procedure finds the most connected nodes in each graph. The benefit in using either of these procedures being to select nodes consistently across different graphs, thus providing the same representations for isomorphic graphs, and similar representations for similar graphs.</p><p>The resulting shape of the tensor representation T i is given by w × k × d where w is the width, k the receptive field and d is the number of dimensions of the one-hot encoded node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Graph Capsule Network</head><p>Once we have the tensorial representation of the graph X tr , we apply a Capsule Network architecture with reconstruction as regularization method.</p><p>The architecture of the Capsule Network is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The first layer is a CNN and the second (primary caps) and third (graphcaps) layers are capsule layers. The main difference between a Capsule Network layer and a standard neural network layer is the existence of a routing-by-agreement procedure. This procedure is described in detail in <ref type="bibr" target="#b4">[5]</ref> but a brief description is provided below.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, the primary caps represent the first layer of the capsules network and there is no routing between these capsules and the first convolutional layer. The graph caps represent the second layer of the Capsule Network. Between the primary caps and the graph caps a routing procedure is in place. This iterative procedure works as follows, each lower level capsule sends its input to the higher level capsule, if this capsule agrees with the lower one's input then this information will be backpropagated during training and strengthen the link between these capsules.</p><p>Finally, there is a decoder layer after the graph caps layer that acts as a regularizer. The total loss is given by</p><formula xml:id="formula_1">L = M SE + M L<label>(1)</label></formula><p>MSE is the loss that comes from reconstructing the graph using the decoder layer. It is the mean square error between the reconstructed graph and the original graph. ML is the margin loss and it is defined as the categorical cross-entropy loss in the case of two classes or, in the multiclass classification case, it is defined as follows</p><formula xml:id="formula_2">M L = T k · max(0, m + − |v k |) 2 + λ(1 − T k ) · max(0, |v k | − m − ) 2<label>(2)</label></formula><p>where T k = 1 if and only if a graph class k is present, and m + = 0.9 and m − = 0.1. The λ down-weighting of the loss for absent graph classes stops the initial learning from shrinking the lengths of the activity vectors of all the graph capsules. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Representational Power</head><p>We use the t-SNE algorithm <ref type="bibr" target="#b26">[27]</ref> to visualize the high dimensional representations learned by the Capsule Networks in both the CNN and capsule layers, as well as directly on the explicit tensor representations we use as input to the networks.</p><p>This algorithm consists of approximating the distribution of the probability distribution of the distances between points in the high dimensional space to the probability distribution in the low dimensional space. The metric used to measure the similarity between both probability distributions is the Kullback-Leibler (KL) divergence <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Implementation Details</head><p>Our implementation uses TensorFlow <ref type="bibr" target="#b28">[29]</ref>, with the Adam optimizer <ref type="bibr" target="#b29">[30]</ref> and an exponentially decaying learning rate of 1e−6 to minimize the sum of the margin losses in Equation 1.</p><p>We perform a grid search with the Mutag dataset (see <ref type="table" target="#tab_1">Table I</ref>), in order to find the optimal hyper-parameters using 10 fold cross-validation with a 90% − 10% train-test split. We search on the following space: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To investigate our hypothesis that the Capsule Networks are better suited to the permutation invariance problem of graph classification we conduct three experiments to test the two separate phases of the algorithm, and to measure the performance of our graph Capsule Network classifier for comparison against current state of the art methods.</p><p>The first stage of our algorithm generates explicit graph tensor representations which are processed downstream by models typically applied to images (CNNs + Capsule Networks) where the order of pixels in a given sample is of obvious significance. Since our model is designed to operate on graphs, where the order of nodes in a sample may be given in any order (i.e. for isomorphic graphs with permuted node ids), to ensure a fair test of our model we must guarantee that the samples we test on are in no way pre-ordered by any systematic method, whether deliberate, or by chance through manual curation of the datasets. To provide this guarantee, in all experiments we first randomly permute all node ids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The graph Capsule Networks are tested against the MUTAG, PTC, NCI1, NCI109, PROTEINS and D&amp;D. <ref type="table" target="#tab_1">Table I</ref> summarises the important features of the datasets that are analyzed and a description of each dataset can be found below.</p><p>MUTAG: This is the dataset of mutable molecules, it contains 188 chemical compounds, and it can be divided into two classes according to whether they are mutagenic or not, where 125 of them are positive and 63 are negative <ref type="bibr" target="#b30">[31]</ref>.</p><p>NCI: This collection of graph datasets is commonly used as the benchmark for graph classification. Each NCI dataset belongs to a bioassay task for anticancer activity prediction, where each chemical compound is represented as a graph, with atoms representing nodes and bonds as edges. A chemical compound is positive if it is active against the corresponding cancer, or negative otherwise <ref type="bibr" target="#b31">[32]</ref>.</p><p>PTC: This graph dataset includes a number of carcinogenicity tasks for toxicology prediction of chemical compounds. The dataset contains 417 compounds from four types of test animals: MM (male mouse), FM (female mouse), MR (male rat), and FR (female rat). Each compound is with one label selected from CE, SE, P, E, EE, IS, NE, N, which stands for Clear Evidence of Carcinogenic Activity (CE), Some Evidence of Carcinogenic Activity (SE), Positive (P),   <ref type="bibr" target="#b32">[33]</ref>. We performed the experiments in each of the datasets (MM, FM, MR and FR) and averaged the results. PROTEINS AND ENZYMES: These are sets of proteins from the BRENDA database <ref type="bibr" target="#b33">[34]</ref> and the dataset of Dobson and Doig <ref type="bibr" target="#b34">[35]</ref>, respectively. Proteins are represented by graphs where nodes represent secondary structure elements (SSEs), which are connected whenever they are neighbors either in the amino-acid sequence or in 3D space. Each node has a discrete type attribute (helix, sheet or turn) and an attribute vector containing physical and chemical measurements including length of the SSE in Angstrm (Å), distance between the C α atom of its first and last residue in A, its hydrophobicity, van der Waals volume, polarity and polarizability. ENZYMES comes with the task of classifying the enzymes to one out of 6 EC top-level classes, whereas PROTEINS comes with the task of classifying into enzymes and non-enzyme <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 1: Ablation Study and Comparison of Labelling Procedures</head><p>To provide an ablation study and make the performance difference of a Capsule Network over a CNN in the latter phase of our algorithm explicit, we compare both classifier models here on seven common graph classification benchmarking datasets. We also compare two different labelling methods: Canonical Labelling using NAUTY <ref type="bibr" target="#b23">[24]</ref> and Betweenness Centrality <ref type="bibr" target="#b36">[37]</ref> to inform the node selection process in which the contextual tensors of each sample graph are generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 2: Comparison Against Current State of the Art Methods</head><p>We compare the results of our approach with current state of the art graph kernels and the graph neural networks methods for graph classification on the same graph classification benchmarking datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment 3: Assessment of the Representational Power of the Capsule Network</head><p>We compare the representational power of the different representations that are provided by the tensor extraction phase of our algorithm. The CNN has three layers, the input layer, the inner layer and the output layer. The Capsule Network has 5 layers, an input layer, a convolutional layer, and primary capsule layer, a graph capsule layer and a decoder layer. We visualize the inner layer of the CNN and the primary capsule layer of the Capsule Network because, after the training procedure, these are the layers that contains a manifold (nonlinear) representation of the graph.</p><p>For ease of analysis, here we focus on two sets of graphs on either ends of the graph size spectrum; one with a small number of nodes per graph (Mutag), and one with a large number of nodes per graph and (Proteins).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental Setup</head><p>Experiments 1 and 2 were performed using two different hardware setups according to the sizes of the datasets. For the Mutag, PTC, Proteins and Enzymes datasets we used a computer with 16GB memory size, 3.1 GHz Intel Core i7 CPU, and 8 cores.</p><p>The NCI1, the NCI109 and the D&amp;D datasets have a larger number of graphs and a larger number of nodes in each graph, this make them computationally more expensive. For these reason we used a a p2xlarge Amazon EC2 instance with 1 GPU with 12 GB of memory, 4 vCPUs and 64 GB of memory. The latter setup was also used to measure the execution time results shown in <ref type="table" target="#tab_1">Table II</ref>. <ref type="table" target="#tab_1">Table III</ref> shows the classification accuracy for each dataset with the two versions of the algorithm. It is evident that the model using the Capsule Network (our contribution) outperforms the Patchy-Sans inspired CNN model <ref type="bibr" target="#b22">[23]</ref> on all of the datasets, thus provides evidence to support our hypothesis. We also see that in 6 out of 7 of these datasets, the Betweeness Centrality labelling procedure for the ordering and selection of nodes gives better (with respect to how well the classes are separated by the downstream classification algorithms) graph tensor representations than the canonical labelling. We also observe the effect of the large difference in the number of graphs (|G|) and number of nodes (N ) in the graphs on the computation time of the algorithms. The smaller graph dataset in both |G| and N is MUTAG, and the largest are NCI109 and D&amp;D. The time complexities of both algorithms presented here depend on both |G| and N so the computation time can be largely different. These differences are presented in table II. As expected, the Capsule Network takes more time to train given that it has a larger number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment 1: Ablation Study and Comparison of Labelling Procedures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 2: Comparison against Current State of the Art Methods</head><p>Table IV displays the results found for the two different labelling procedures against the state of the art methods in terms of classification accuracy.</p><p>Despite a very modest hyper-parameter search, our results show leading performance in 2 out of the 7 datasets. The datasets where we demonstrate the most competitive results are the ones that have between 0 and 30 different node labels (MUTAG, PTC, PROTEINS). However, when the number of node labels is higher (NCI, D&amp;D, ENZYMES) the algorithm has a lower performance in terms of classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 3: Assessment of the Representational Power of the Capsule Network</head><p>This section presents experiment results on how the explicit tensor representations, the CNNs, and the Capsule Networks encode graph representations corresponding to to the intrinsic structure and the features of graph data.</p><p>Using trained networks to process the Proteins and Mutag datasets, we collected the vectors corresponding to the intermediate layer of neurons / capsules (the capsule layer before the first routing operation and the counterpart layer in the standard CNN). We then apply the manifold embedding algorithm t-SNE <ref type="bibr" target="#b26">[27]</ref> to render the learned representations into R 2 . <ref type="figure" target="#fig_4">Figure 3</ref> illustrates the t-SNE R 2 embeddings given by the Patchy Sans algorithm (top left), the CNN (top right) and the Capsule Network (bottom). The only parameter that needed to be determined is the perplexity, which can be interpreted as a smooth measure of the effective number of neighbors used for the optimization. For this experiment we used a perplexity of 10 for the Mutag dataset and of 200 for the Proteins dataset. This values were chosen following the discussion in <ref type="bibr" target="#b26">[27]</ref>, where it is stated that the performance of t-sne is robust to changes in the perplexity, with typical values between 5 and 50.</p><p>In <ref type="figure" target="#fig_2">Figure 2</ref> and <ref type="figure" target="#fig_4">Figure 3</ref>, it is possible to see that the CNN representation appears to better separate the classes than the Capsule Network one, however the classification accuracy of the Capsule Network is significantly higher. One possible reason for this behaviour is that the primary caps layer used for assessing the Capsule Network has not passed through the routing process. This procedure, that operates between this layer and the graph caps layer, would be able to more accurately classify the graphs even if the intermediate representation does not look as clearly separable as the CNN inner layer one.</p><p>We can see in <ref type="table" target="#tab_4">Table V</ref> that the CNN produces a better representation than the capsule and the tensor extraction phase alone in terms of separating the positive and negative examples into separate clusters. We quantify this observation with the intracluster and inter-cluster distances. The intra-cluster measure is defined as the mean square distance from each point belonging to one class to the center of that class. The inter-class distance is defined as the distance between the center points of each class.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>We have tested the hypothesis that Capsule Networks are better suited to the permutation invariance problem of graph classification than CNNs when operating on explicit graph tensor representations produced by two labelling procedures. In doing so we have presented and analysed a model for tackling this problem for sets of undirected graphs with discrete node labels of varying numbers of classes.</p><p>Our results demonstrate that the Capsule Network indeed outperforms the CNN classifier at this task on all 7 of the benchmark datasets, while also indicating that the use of  Betweenness Centrality to inform node ordering and selection for the generation of explicit graph tensor representations is superior to the NAUTY canonical labelling procedure <ref type="bibr" target="#b23">[24]</ref> in 6 out of 7 of the datasets.</p><p>Although the Capsule Network performs better than the CNN, due to the vastly greater number of parameters to be learned, it also requires a larger execution time. In our experiments we found that on average, the Capsule Network is approximately 8 times slower than the CNN.</p><p>We have shown that the Capsule Network with the Betweeness Centrality labelling procedure for node ordering and selection achieves state-of-the-art classification performance on the MUTAG and the PTC datasets. However, on the rest of the datasets, which have a larger number of categorical node features, it is less competitive with these current stateof-the-art methods. We note here, however, that we performed a very limited hyper-parameter search, and do not rule out the possibility that with further search, our model's performance could be significantly improved.</p><p>For future work we wish to investigate in detail why our model behaves less well with these datasets. It would also be interesting to try different labelling procedures or perhaps a combination of procedures to investigate the potential further improvement on the model's performance, and of course improving the computational costs of training Capsule Networks is an open area for further work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Graph Capsule Network Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Mutag t-SNE representations. Top right: CNN Inner Layer. Top left: Tensor Representation. Bottom: Capsule Network inner layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Proteins t-SNE representations. Top right: CNN inner Layer. Top left: Tensor Representation. Bottom: Capsule Network inner layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Number of epochs = [100, 150, 200] • Learning rate = [0.0005, 0.001, 0.005] • Learning rate decay = [0.25, 0.4, 0.75, 1.5] Note, we do not perform a hyper-parameter search on each dataset because the computational cost is too high. Instead we use the same set of hyper-parameters found for the Mutag dataset in each of our experiments.</figDesc><table /><note>•</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Graph Statistics</figDesc><table><row><cell>Dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>NCI109</cell><cell>D &amp; D</cell><cell>ENZYMES</cell></row><row><cell>No. Graphs (|G|)</cell><cell>188</cell><cell>344</cell><cell>1113</cell><cell>4110</cell><cell>4127</cell><cell>1178</cell><cell>600</cell></row><row><cell>Max. Graph Size</cell><cell>28</cell><cell>109</cell><cell>620</cell><cell>111</cell><cell>111</cell><cell>5748</cell><cell>126</cell></row><row><cell>Avg. Graph Size</cell><cell>18</cell><cell>25.56</cell><cell>39.06</cell><cell>29.8</cell><cell>29.6</cell><cell>284.32</cell><cell>32.6</cell></row><row><cell>Number of classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>6</cell></row><row><cell>Number of node labels (n)</cell><cell>7</cell><cell>18</cell><cell>3</cell><cell>37</cell><cell>38</cell><cell>82</cell><cell></cell></row><row><cell>Class ratio (Percentage of + labels)</cell><cell>66.49%</cell><cell>39.51%</cell><cell>59.57%</cell><cell>50.05%</cell><cell>50.38%</cell><cell>58.66%</cell><cell>16.67%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Time for training the models (seconds) ± 8.95 5191.23 ± 24.56 5221.51 ± 48.21 4034.25 ± 0.51 366.24 ± 2.18</figDesc><table><row><cell>Algorithm\Dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS**</cell><cell>NCI1*</cell><cell>NCI109*</cell><cell>D &amp; D*</cell><cell>ENZYMES**</cell></row><row><cell cols="4">Nauty + Capsules 1255.6 Nauty + CNN 133.64 ± 4.59 184.9 ± 13.31 13.03 ± 0.87 76.57 ± 3.83 70.2 ± 0.24</cell><cell cols="3">640.14 ± 15.73 1632.35 ± 24.85 1033.22 ± 2.15</cell><cell>32.71 ± 0.06</cell></row><row><cell>BC + Capsules</cell><cell cols="4">133.64 ± 4.59 138.55 ± 4.23 1039.98 ± 3.25 5065.57 ± 33.32</cell><cell cols="3">5045.7 ± 39.49 3671.17 ± 0.66 366.24 ± 2.18</cell></row><row><cell>BC + CNN</cell><cell>13.03 ± 0.87</cell><cell>53.37 ± 1.63</cell><cell>72.51 ± 2.3</cell><cell cols="2">603.29 ± 12.88 1598.87 ± 45.32</cell><cell>997.28 ± 1.94</cell><cell>32.71 ± 0.06</cell></row><row><cell cols="4">Equivocal (E), Equivocal Evidence of Carcinogenic Activity</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">(EE), Inadequate Study of Carcinogenic Activity (IS), No</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Evidence of Carcinogenic Activity (NE), and Negative (N)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Ablation study -CNN vs Capsule Network classification accuracies, and comparison of labelling procedures</figDesc><table><row><cell>Algorithm\Dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>NCI109</cell><cell>D &amp; D</cell><cell>ENZYMES</cell></row><row><cell>Nauty + Capsules</cell><cell>75.7 ± 9.47</cell><cell>63.9 ± 6.36</cell><cell>72.0 ± 2.61</cell><cell>59.4 ± 2.16</cell><cell>58.0 ± 2.76</cell><cell>77.9 ± 2.49</cell><cell>26.1 ± 5.15</cell></row><row><cell>Nauty + CNN</cell><cell>85.2 ± 5.66</cell><cell>53.8 ± 6.47</cell><cell>70.4 ± 2.20</cell><cell>56.4 ± 2.09</cell><cell>58.0 ± 2.76</cell><cell>75.3 ± 4.44</cell><cell>22.3 ± 4.02</cell></row><row><cell>BC + Capsules</cell><cell>88.9 ± 5.49</cell><cell>69.0 ± 4.98</cell><cell>74.1 ± 3.24</cell><cell>65.9 ± 1.07</cell><cell>58.04 ± 2.78</cell><cell>74.86 ± 3.27</cell><cell>27.0 ± 8.45</cell></row><row><cell>BC + CNN</cell><cell>84.2 ± 5.26</cell><cell>57.6 ± 2.01</cell><cell>68.9 ± 3.38</cell><cell>57.6 ± 2.01</cell><cell>56.9 ± 2.03</cell><cell>72.3 ± 3.86</cell><cell>20.0 ± 5.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Assessing the representation power of the models</figDesc><table><row><cell></cell><cell cols="2">PROTEINS</cell><cell cols="2">MUTAG</cell></row><row><cell>Representation Layer</cell><cell>Intra-cluster distance</cell><cell>Inter-cluster distance</cell><cell>Intra-cluster distance</cell><cell>Inter-cluster distance</cell></row><row><cell>Patchy-Sans</cell><cell>1804.56</cell><cell>3.39</cell><cell>817.21</cell><cell>675.54</cell></row><row><cell>CNN</cell><cell>9.77</cell><cell>45.93</cell><cell>133.70</cell><cell>1400.57</cell></row><row><cell>Capsule</cell><cell>10.72</cell><cell>1.71</cell><cell>126.12</cell><cell>514.47</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We wish to thank Kyohei Koyama for his assistance in the implementation of our CNN baseline and data preparation with the benchmark datasets.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Glocalized weisfeiler-lehman graph kernels: Global-local feature maps of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kernels for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM workshop on active mining</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explicit versus implicit graph feature maps: A computational phase transition for walk kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="881" to="886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of statistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1171" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On Graph Kernels: Hardness Results and Efficient Alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
		<idno type="DOI">http:/citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.8681{&amp;}rep=rep1{&amp;}type=pdfhttp:/link.springer.com/10.1007/978-3-540-45167-9{_}11</idno>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.152.8681{&amp;}rep=rep1{&amp;}type=pdfhttp://link.springer.com/10.1007/978-3-540-45167-9{}11" />
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Novel kernels for error-tolerant graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Riesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<idno type="DOI">http:/booksandjournals.brillonline.com/content/journals/10.1163/156856809789476119</idno>
		<ptr target="http://booksandjournals.brillonline.com/content/journals/10.1163/156856809789476119" />
	</analytic>
	<monogr>
		<title level="j">Spatial Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="425" to="441" />
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">Sep</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast subtree kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1660" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image classification with segmentation graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2005. IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08090</idno>
		<title level="m">Graph capsule convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Practical graph isomorphism, ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piperno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Computation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="94" to="112" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On learning and learned representation with dynamic routing in capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04041</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Approximating the kullback leibler divergence between gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">317</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph classification based on sparse graph feature selection and extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cogboost: Boosting for fast cost-sensitive graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Task sensitive feature exploration and learning for multitask graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="744" to="758" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Brenda, the enzyme database: updates and major new developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schomburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gremse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schomburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="431" to="433" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable kernels for graphs with continuous attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kasenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A faster algorithm for betweenness centrality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Brandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical sociology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="177" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
